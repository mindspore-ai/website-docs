<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Integrated Ascend &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Device-side Training Sample" href="../device_train_example.html" />
    <link rel="prev" title="TensorRT Integration Information" href="tensorrt_info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_info.html">NPU Integration Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnie.html">Usage Description of the Integrated NNIE</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorrt_info.html">TensorRT Integration Information</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Integrated Ascend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#environment-preparation">Environment Preparation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#checking-system-environment-information">Checking System Environment Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-environment-variables">Configuring Environment Variables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#executing-the-converter">Executing the Converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executinge-the-benchmark">Executinge the Benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-features">Advanced Features</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-shape">Dynamic Shape</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#supported-operators">Supported Operators</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="asic.html">Application Specific Integrated Circuit Integration Instructions</a> &raquo;</li>
      <li>Integrated Ascend</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/ascend_info.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="integrated-ascend">
<h1>Integrated Ascend<a class="headerlink" href="#integrated-ascend" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/lite/docs/source_en/use/ascend_info.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<blockquote>
<div><ul class="simple">
<li><p>The Ascend backend support on device-side version will be deprecated later. For related usage of the Ascend backend, please refer to the cloud-side inference version documentation.</p></li>
<li><p><a class="reference external" href="https://mindspore.cn/lite/docs/en/r2.3/use/cloud_infer/build.html">Build Cloud-side MindSpore Lite</a></p></li>
<li><p><a class="reference external" href="https://mindspore.cn/lite/docs/en/r2.3/use/cloud_infer/converter.html">Cloud-side Model Converter</a></p></li>
<li><p><a class="reference external" href="https://mindspore.cn/lite/docs/en/r2.3/use/cloud_infer/benchmark.html">Cloud-side Benchmark Tool</a></p></li>
</ul>
</div></blockquote>
<p>This document describes how to use MindSpore Lite to perform inference and use the dynamic shape function on Linux in the Ascend environment. Currently, MindSpore Lite supports the Atlas 200/300/500 inference product and Atlas inference series (with Ascend 310P AI processor).</p>
<section id="environment-preparation">
<h2>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h2>
<section id="checking-system-environment-information">
<h3>Checking System Environment Information<a class="headerlink" href="#checking-system-environment-information" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Ensure that a 64-bit OS is installed, the <a class="reference external" href="https://www.gnu.org/software/libc/">glibc</a> version is 2.17 or later, and Ubuntu 18.04, CentOS 7.6, and EulerOS 2.8 are verified.</p></li>
<li><p>Ensure that <a class="reference external" href="https://gcc.gnu.org/releases.html">GCC 7.3.0</a> is installed.</p></li>
<li><p>Ensure that <a class="reference external" href="https://cmake.org/download/">CMake 3.18.3 or later</a> is installed.</p>
<ul class="simple">
<li><p>Ensure that the path of the installed CMake is added to a system environment variable.</p></li>
</ul>
</li>
<li><p>Ensure that Python 3.7.5 or 3.9.0 is installed. If neither of them is installed, you can download and install them via the following links:</p>
<ul class="simple">
<li><p>Links for Python 3.7.5 (64-bit): <a class="reference external" href="https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz">Official Website</a> or <a class="reference external" href="https://mirrors.huaweicloud.com/python/3.7.5/Python-3.7.5.tgz">HUAWEI CLOUD</a></p></li>
<li><p>Links for Python 3.9.0 (64-bit): <a class="reference external" href="https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tgz">Official Website</a> or <a class="reference external" href="https://mirrors.huaweicloud.com/python/3.9.0/Python-3.9.0.tgz">HUAWEI CLOUD</a></p></li>
</ul>
</li>
<li><p>If you use the ARM architecture, ensure that the pip version matching Python is 19.3 or later.</p></li>
<li><p>Ensure that Ascend AI processor software package is installed.</p>
<ul>
<li><p>Ascend software package provides two distributions, commercial edition and community edition:</p>
<ol class="arabic simple">
<li><p>Commercial edition needs approval from Ascend to download, for detailed installation guide, please refer to <a class="reference external" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100280094">Ascend Data Center Solution 22.0.RC3 Installation Guide</a>.</p></li>
<li><p>Community edition has no restrictions, choose <code class="docutils literal notranslate"><span class="pre">5.1.RC2.alpha007</span></code> in <a class="reference external" href="https://www.hiascend.com/software/cann/community-history">CANN community edition</a>, then choose relevant driver and firmware packages in <a class="reference external" href="https://www.hiascend.com/hardware/firmware-drivers?tag=community">firmware and driver</a>. Please refer to the abovementioned commercial edition installation guide to choose which packages are to be installed and how to install them.</p></li>
</ol>
</li>
<li><p>The default installation path of the installation package is <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend</span></code>. Ensure that the current user has the right to access the installation path of Ascend AI processor software package. If not, the root user needs to add the current user to the user group where <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend</span></code> is located.</p></li>
<li><p>Install the .whl packages provided in Ascend AI processor software package. If the .whl packages have been installed before, you should uninstall the .whl packages by running the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>uninstall<span class="w"> </span>te<span class="w"> </span>topi<span class="w"> </span>-y
</pre></div>
</div>
<p>Run the following command to install the .whl packages if the Ascend AI package has been installed in default path. If the installation path is not the default path, you need to replace the path in the command with the installation path.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>/usr/local/Ascend/ascend-toolkit/latest/lib64/topi-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl
pip<span class="w"> </span>install<span class="w"> </span>/usr/local/Ascend/ascend-toolkit/latest/lib64/te-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="configuring-environment-variables">
<h3>Configuring Environment Variables<a class="headerlink" href="#configuring-environment-variables" title="Permalink to this headline"></a></h3>
<p>After the Ascend software package is installed, export runtime environment variables. In the following command, <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend</span></code> in <code class="docutils literal notranslate"><span class="pre">LOCAL_ASCEND=/usr/local/Ascend</span></code> indicates the installation path of the software package. Change it to the actual installation path.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># control log level. 0-EBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>

<span class="c1"># Conda environmental options</span>
<span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend<span class="w"> </span><span class="c1"># the root directory of run package</span>

<span class="c1"># lib libraries that the run package depends on</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe/op_tiling:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># Environment variables that must be configured</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TBE_IMPL_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe<span class="w">            </span><span class="c1"># TBE operator implementation tool path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp<span class="w">                                       </span><span class="c1"># OPP path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/compiler/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span><span class="w">                  </span><span class="c1"># TBE operator compilation tool path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">TBE_IMPL_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span><span class="w">                                                       </span><span class="c1"># Python library that TBE implementation depends on</span>
</pre></div>
</div>
</section>
</section>
<section id="executing-the-converter">
<h2>Executing the Converter<a class="headerlink" href="#executing-the-converter" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides an offline model converter to convert various models (Caffe, ONNX, TensorFlow, and MindIR) into models that can be inferred on the Ascend hardware.
First, use the converter to convert a model into an <code class="docutils literal notranslate"><span class="pre">ms</span></code> model. Then, use the runtime inference framework matching the converter to perform inference. The process is as follows:</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/downloads.html">Download</a> the converter dedicated for Ascend. Currently, only Linux is supported.</p></li>
<li><p>Decompress the downloaded package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-x64.tar.gz
</pre></div>
</div>
<p>{version} indicates the version number of the release package.</p>
</li>
<li><p>Add the dynamic link library required by the converter to the environment variable LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path of the folder obtained after the decompression.</p>
</li>
<li><p>Go to the converter directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>(Optional) Configuring configFile</p>
<p>You can use this option to configure the Ascend option for model conversion. The configuration file is in the INI format. For the Ascend scenario, the configurable parameter is [acl_option_cfg_param]. For details about the parameter, see the following table,  Ascend initialization can be configured through the acl_init_options parameter, and Ascend composition can be configured through the acl_build_options parameter.</p>
</li>
<li><p>Execute the converter to generate an Ascend <code class="docutils literal notranslate"><span class="pre">ms</span></code> model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>${model_name} indicates the model file name. The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
<p>For details about parameters of the converter_lite converter, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/converter_tool.html#parameter-description">“Parameter Description” in Converting Models for Inference</a>.</p>
<p>Note: If the input shape of the original model is uncertain, specify inputShape when using the converter to convert a model. In addition, set configFile to the value of input_shape_vector parameter in acl_option_cfg_param. The command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span><span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;input:1,64,64,1&quot;</span><span class="w"> </span>--configFile<span class="o">=</span><span class="s2">&quot;./config.txt&quot;</span>
</pre></div>
</div>
<p>The content of the config.txt file is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">acl_option_cfg_param</span><span class="p">]</span>
<span class="n">input_shape_vector</span><span class="o">=</span><span class="s">&quot;[1,64,64,1]&quot;</span>
</pre></div>
</div>
</li>
</ol>
<p>Table 1 [acl_option_cfg_param] parameter configuration</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">input_format</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specifies the model input format.</p></td>
<td><p>String</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;NCHW&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;NHWC&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">input_shape_vector</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specifies the model input shapes which are arranged based on the model input sequence and are separated by semicolons (;).</p></td>
<td><p>String</p></td>
<td><p>Example: <code class="docutils literal notranslate"><span class="pre">&quot;[1,2,3,4];[4,3,2,1]&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">precision_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configures the model precision mode.</p></td>
<td><p>String</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;force_fp16&quot;</span></code> (default value), <code class="docutils literal notranslate"><span class="pre">&quot;allow_fp32_to_fp16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;must_keep_origin_dtype&quot;</span></code>, or <code class="docutils literal notranslate"><span class="pre">&quot;allow_mix_precision&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">op_select_impl_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configures the operator selection mode.</p></td>
<td><p>String</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;high_performance&quot;</span></code> (default value) or <code class="docutils literal notranslate"><span class="pre">&quot;high_precision&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dynamic_batch_size</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specifies the <a class="reference internal" href="#dynamic-batch-size"><span class="std std-doc">dynamic batch size</span></a> parameter.</p></td>
<td><p>String</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;2,4&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dynamic_image_size</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specifies the <a class="reference internal" href="#dynamic-image-size"><span class="std std-doc">dynamic image size</span></a> parameter.</p></td>
<td><p>String</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;96,96;32,32&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">fusion_switch_config_file_path</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure the path and name of the <a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/700/devtools/auxiliarydevtool/aoepar_16_034.html">fusion pattern switch</a> file.</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">insert_op_config_file_path</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Inserts the <a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/inferapplicationdev/atctool/atctool_0018.html">AIPP</a> operator into a model.</p></td>
<td><p>String</p></td>
<td><p><a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/inferapplicationdev/atctool/atctool_0018.html">AIPP</a> configuration file path</p></td>
</tr>
</tbody>
</table>
</section>
<section id="runtime">
<h2>Runtime<a class="headerlink" href="#runtime" title="Permalink to this headline"></a></h2>
<p>After obtaining the converted model, use the matching runtime inference framework to perform inference. For details about how to use runtime to perform inference, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html">Using C++ Interface to Perform Inference</a>.</p>
</section>
<section id="executinge-the-benchmark">
<h2>Executinge the Benchmark<a class="headerlink" href="#executinge-the-benchmark" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides a benchmark test tool, which can be used to perform quantitative (performance) analysis on the execution time consumed by forward inference of the MindSpore Lite model. In addition, you can perform comparative error (accuracy) analysis based on the output of a specified model.
For details about the inference tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/benchmark_tool.html">benchmark</a>.</p>
<ul>
<li><p>Performance analysis</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--device<span class="o">=</span>Ascend310<span class="w"> </span>--modelFile<span class="o">=</span>./models/test_benchmark.ms<span class="w"> </span>--timeProfiling<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
</li>
<li><p>Accuracy analysis</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--device<span class="o">=</span>Ascend310<span class="w"> </span>--modelFile<span class="o">=</span>./models/test_benchmark.ms<span class="w"> </span>--inDataFile<span class="o">=</span>./input/test_benchmark.bin<span class="w"> </span>--inputShapes<span class="o">=</span><span class="m">1</span>,32,32,1<span class="w"> </span>--accuracyThreshold<span class="o">=</span><span class="m">3</span><span class="w"> </span>--benchmarkDataFile<span class="o">=</span>./output/test_benchmark.out
</pre></div>
</div>
<p>To set environment variables, add the directory where the <code class="docutils literal notranslate"><span class="pre">so</span></code> library of <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> (in <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-{os}-{arch}/runtime/lib</span></code>) is located to <code class="docutils literal notranslate"><span class="pre">${LD_LIBRARY_PATH}</span></code>.</p>
</li>
</ul>
</section>
<section id="advanced-features">
<h2>Advanced Features<a class="headerlink" href="#advanced-features" title="Permalink to this headline"></a></h2>
<section id="dynamic-shape">
<h3>Dynamic Shape<a class="headerlink" href="#dynamic-shape" title="Permalink to this headline"></a></h3>
<p>The batch size is not fixed in certain scenarios. For example, in the target detection+facial recognition cascade scenario, the number of detected targets is subject to change, which means that the batch size of the targeted recognition input is dynamic. It would be a great waste of compute resources to perform inferences using the maximum batch size or image size. Thanks to Lite’s support for dynamic batch size and dynamic image size on the Atlas 200/300/500 inference product, you can configure the [acl_option_cfg_param] dynamic parameter through configFile to convert a model into an <code class="docutils literal notranslate"><span class="pre">ms</span></code> model, and then use the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html#resizing-the-input-dimension">resize</a> function of the model to change the input shape during inference.</p>
<section id="dynamic-batch-size">
<h4>Dynamic Batch Size<a class="headerlink" href="#dynamic-batch-size" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Parameter name</p>
<p>dynamic_batch_size</p>
</li>
<li><p>Function</p>
<p>Sets the dynamic batch size parameter. This parameter applies to the scenario where the number of images to be processed each time is not fixed during inference. This parameter must be used together with input_shape_vector and cannot be used together with dynamic_image_size.</p>
</li>
<li><p>Value</p>
<p>Up to 100 batch sizes are supported. Separate batch sizes with commas (,). The value range is [1, 2048]. For example, parameters in a configuration file are set as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">acl_option_cfg_param</span><span class="p">]</span>
<span class="n">input_shape_vector</span><span class="o">=</span><span class="s">&quot;[-1,32,32,4]&quot;</span>
<span class="n">dynamic_batch_size</span><span class="o">=</span><span class="s">&quot;2,4&quot;</span>
</pre></div>
</div>
<p>“-1” in input_shape indicates that the batch size is dynamic. The value range is “2,4”. That is, size 0: [2, 32, 32, 4] and size 1: [4, 32, 32, 4] are supported.</p>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;input:4,32,32,4&quot;</span><span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: To enable the dynamic batch size function, you need to set inputShape to the shape corresponding to the maximum size (value of size 1 in the preceding example). In addition, you need to configure the dynamic batch size of [acl_option_cfg_param] through configFile (as shown in the preceding example).</p>
</li>
<li><p>Inference</p>
<p>After the dynamic batch size is enabled, during model inference, the input shape is corresponding to the size configured in converter. To change the input shape, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html#resizing-the-input-dimension">resize</a> function.</p>
</li>
<li><p>Precautions</p>
<p>(1) This parameter allows you to run inference with dynamic batch sizes. For example, to run inference on two, four, or eight images per batch, set this parameter to 2,4,8. Memory will be allocated based on the runtime batch size.<br/>
(2) Too large batch sizes or too many batch size profiles will cause model build failures.<br/>
(3) In the scenario where you have set too large batch sizes or too many batch size profiles, you are advised to run the swapoff -a command to disable the use of swap space as memory to prevent slow running of the operating environment.<br/></p>
</li>
</ul>
</section>
<section id="dynamic-image-size">
<h4>Dynamic Image Size<a class="headerlink" href="#dynamic-image-size" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Parameter name</p>
<p>dynamic_image_size</p>
</li>
<li><p>Function</p>
<p>Sets dynamic image size profiles. This parameter applies to the scenario where the width and height of the image processed each time are not fixed during inference. This parameter must be used together with input_shape_vector and cannot be used together with dynamic_batch_size.</p>
</li>
<li><p>Value</p>
<p>A maximum of 100 image size profiles are supported. Separate image sizes with semicolons (;). The format is “imagesize1_height,imagesize1_width;imagesize2_height,imagesize2_width”. Enclose all parameters in double quotation marks (“”), and separate the parameters with semicolons (;). For example, parameters in a configuration file are set as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">acl_option_cfg_param</span><span class="p">]</span>
<span class="n">input_format</span><span class="o">=</span><span class="s">&quot;NCHW&quot;</span>
<span class="n">input_shape_vector</span><span class="o">=</span><span class="s">&quot;[2,3,-1,-1]&quot;</span>
<span class="n">dynamic_image_size</span><span class="o">=</span><span class="s">&quot;64,64;96,96&quot;</span>
</pre></div>
</div>
<p>“-1” in input_shape indicates that the image size is dynamic. That is, size 0 [2,3,64,64] and size 1 [2,3,96,96] are supported.</p>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;input:2,3,96,96&quot;</span><span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: To enable the dynamic batch size function, you need to set inputShape to the shape corresponding to the maximum size (value of size 1 in the preceding example). In addition, you need to configure the dynamic image size of [acl_option_cfg_param] through configFile (as shown in the preceding example).</p>
</li>
<li><p>Inference</p>
<p>After the dynamic image size is enabled, during model inference, the input shape is corresponding to the size configured in converter. To change the input shape, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html#resizing-the-input-dimension">resize</a> function.</p>
</li>
<li><p>Precautions</p>
<p>(1) Too large image sizes or too many image size profiles will cause model build failures.<br/>
(2) If dynamic image size is enabled, the size of the dataset images used for inference must match the runtime image size in use.<br/>
(3) In the scenario where you have set too large image sizes or too many image size profiles, you are advised to run the swapoff -a command to disable the use of swap space as memory to prevent slow operating environment.<br/></p>
</li>
</ul>
</section>
</section>
</section>
<section id="supported-operators">
<h2>Supported Operators<a class="headerlink" href="#supported-operators" title="Permalink to this headline"></a></h2>
<p>For details about the supported operators, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/operator_list_lite.html">Lite Operator List</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tensorrt_info.html" class="btn btn-neutral float-left" title="TensorRT Integration Information" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../device_train_example.html" class="btn btn-neutral float-right" title="Device-side Training Sample" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>