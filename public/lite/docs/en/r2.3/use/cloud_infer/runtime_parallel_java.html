<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Java Interface to Perform Concurrent Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using Python Interface to Perform Concurrent Inference" href="runtime_parallel_python.html" />
    <link rel="prev" title="Using C++ Interface to Perform Concurrent Inference" href="runtime_parallel_cpp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_parallel_cpp.html">Using C++ Interface to Perform Concurrent Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Java Interface to Perform Concurrent Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-configuration">Creating Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#initialization">Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-concurrent-inference">Executing Concurrent Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memory-release">Memory release</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_parallel_python.html">Using Python Interface to Perform Concurrent Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime_parallel.html">Performing Concurrent Inference</a> &raquo;</li>
      <li>Using Java Interface to Perform Concurrent Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_parallel_java.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="using-java-interface-to-perform-concurrent-inference">
<h1>Using Java Interface to Perform Concurrent Inference<a class="headerlink" href="#using-java-interface-to-perform-concurrent-inference" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/lite/docs/source_en/use/cloud_infer/runtime_parallel_java.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides multi-model concurrent inference interface <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/model_parallel_runner.html#modelparallelrunner">ModelParallelRunner</a>. Multi model concurrent inference now supports Ascend 310/310P/910, Nvidia GPU and CPU backends.</p>
<p>After exporting the <code class="docutils literal notranslate"><span class="pre">mindir</span></code> model by MindSpore or converting it by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/cloud_infer/converter_tool.html">model conversion tool</a> to obtain the <code class="docutils literal notranslate"><span class="pre">mindir</span></code> model, the concurrent inference process of the model can be executed in Runtime. This tutorial describes how to perform concurrent inference with multiple modes by using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/class_list.html">Java interface</a>.</p>
<p>To use the MindSpore Lite concurrent inference framework, perform the following steps:</p>
<ol class="arabic simple">
<li><p>Create a configuration item: Create a multi-model concurrent inference configuration item <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/runner_config.html#runnerconfig">RunnerConfig</a>, which is used to configure multiple model concurrency.</p></li>
<li><p>Initialization: initialization before multi-model concurrent inference.</p></li>
<li><p>Execute concurrent inference: Use the Predict interface of ModelParallelRunner to perform concurrent inference on multiple Models.</p></li>
<li><p>Release memory: When you do not need to use the MindSpore Lite concurrent inference framework, you need to release the ModelParallelRunner and related Tensors you created.</p></li>
</ol>
<p><img alt="" src="../../_images/server_inference.png" /></p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>The following code samples are from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3/mindspore/lite/examples/cloud_infer/quick_start_parallel_java">Sample code for performing cloud-side inference by C++ interface</a>.</p></li>
<li><p>Export the MindIR model via MindSpore, or get the MindIR model by converting it with <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/cloud_infer/converter_tool.html">model conversion tool</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_parallel_java/model</span></code> directory, and you can download the MobileNetV2 model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a>.</p></li>
<li><p>Download the Ascend, Nvidia GPU, CPU triplet MindSpore Lite cloud-side inference package <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-linux-{arch}.tar.gz</span></code> from <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/downloads.html">Official Website</a> and save it to <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_parallel_java</span></code> directory.</p></li>
</ol>
</section>
<section id="creating-configuration">
<h2>Creating Configuration<a class="headerlink" href="#creating-configuration" title="Permalink to this headline"></a></h2>
<p>The <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/runner_config.html">configuration item</a> will save some basic configuration parameters required for concurrent inference, which are used to guide the number of concurrent models, model compilation and model execution.</p>
<p>The following sample code demonstrates how to create a RunnerConfig and configure the number of workers for concurrent inference:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// use default param init context</span>
<span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="kt">boolean</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_CPU</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ret</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">System</span><span class="p">.</span><span class="na">err</span><span class="p">.</span><span class="na">println</span><span class="p">(</span><span class="s">&quot;init context failed&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">context</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// init runner config</span>
<span class="n">RunnerConfig</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RunnerConfig</span><span class="p">();</span>
<span class="n">config</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
<span class="n">config</span><span class="p">.</span><span class="na">setWorkersNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>For details on the configuration method of Context, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_java.html#creating-a-configuration-context">Context</a>.</p>
<p>Multi-model concurrent inference currently only supports <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/mscontext.html#devicetype">CPUDeviceInfo</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/mscontext.html#devicetype">GPUDeviceInfo</a>, and <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/api_java/mscontext.html#devicetype">AscendDeviceInfo</a> several different hardware backends. When setting the GPU backend, you need to set the GPU backend first and then the CPU backend, otherwise it will report an error and exit.</p>
<p>Multi-model concurrent inference does not support FP32 type data reasoning. Binding cores only supports no core binding or binding large cores. It does not support the parameter settings of the bound cores, and does not support configuring the binding core list.</p>
</div></blockquote>
</section>
<section id="initialization">
<h2>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline"></a></h2>
<p>When using MindSpore Lite to execute concurrent inference, ModelParallelRunner is the main entry of concurrent inference. Through ModelParallelRunner, you can initialize and execute concurrent inference. Use the RunnerConfig created in the previous step and call the init interface of ModelParallelRunner to initialize ModelParallelRunner.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runner</span><span class="p">.</span><span class="na">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ret</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">System</span><span class="p">.</span><span class="na">err</span><span class="p">.</span><span class="na">println</span><span class="p">(</span><span class="s">&quot;MindSpore Lite predict failed.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">freeTensor</span><span class="p">();</span>
<span class="w">    </span><span class="n">runner</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>For Initialization of ModelParallelRunner, you do not need to set the RunnerConfig configuration parameters, and the default parameters will be used for concurrent inference of multiple models.</p>
</div></blockquote>
</section>
<section id="executing-concurrent-inference">
<h2>Executing Concurrent Inference<a class="headerlink" href="#executing-concurrent-inference" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite calls the Predict interface of ModelParallelRunner for model concurrent inference.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runner</span><span class="p">.</span><span class="na">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ret</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">System</span><span class="p">.</span><span class="na">err</span><span class="p">.</span><span class="na">println</span><span class="p">(</span><span class="s">&quot;MindSpore Lite predict failed.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">freeTensor</span><span class="p">();</span>
<span class="w">    </span><span class="n">runner</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="memory-release">
<h2>Memory release<a class="headerlink" href="#memory-release" title="Permalink to this headline"></a></h2>
<p>When you do not need to use the MindSpore Lite reasoning framework, you need to release the created ModelParallelRunner.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">freeTensor</span><span class="p">();</span>
<span class="n">runner</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_parallel_cpp.html" class="btn btn-neutral float-left" title="Using C++ Interface to Perform Concurrent Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_parallel_python.html" class="btn btn-neutral float-right" title="Using Python Interface to Perform Concurrent Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>