<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TensorRT Integration Information &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Integrated Ascend" href="ascend_info.html" />
    <link rel="prev" title="Usage Description of the Integrated NNIE" href="nnie.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_info.html">NPU Integration Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnie.html">Usage Description of the Integrated NNIE</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">TensorRT Integration Information</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#steps">Steps</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#build">Build</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration">Integration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#supported-operators">Supported Operators</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ascend_info.html">Integrated Ascend</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="asic.html">Application Specific Integrated Circuit Integration Instructions</a> &raquo;</li>
      <li>TensorRT Integration Information</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/tensorrt_info.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="tensorrt-integration-information">
<h1>TensorRT Integration Information<a class="headerlink" href="#tensorrt-integration-information" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/lite/docs/source_en/use/tensorrt_info.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<section id="steps">
<h2>Steps<a class="headerlink" href="#steps" title="Permalink to this headline"></a></h2>
<section id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h3>
<p>Besides basic <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/build.html">Environment Preparation</a>, CUDA and TensorRT is required as well. Current version supports <a class="reference external" href="https://developer.nvidia.com/cuda-10.1-download-archive-base">CUDA 10.1</a> and <a class="reference external" href="https://developer.nvidia.com/nvidia-tensorrt-6x-download">TensorRT 6.0.1.5</a>, and <a class="reference external" href="https://developer.nvidia.com/cuda-11.1.1-download-archive">CUDA 11.1</a> and <a class="reference external" href="https://developer.nvidia.com/nvidia-tensorrt-8x-download">TensorRT 8.5.1</a>.</p>
<p>Install the appropriate version of CUDA and set the installed directory as environment variable <code class="docutils literal notranslate"><span class="pre">${CUDA_HOME}</span></code>. Our build script uses this environment variable to seek CUDA.</p>
<p>Install TensorRT of the corresponding CUDA version, and set the installed directory as environment viriable <code class="docutils literal notranslate"><span class="pre">${TENSORRT_PATH}</span></code>. Our build script uses this environment viriable to seek TensorRT.</p>
</section>
<section id="build">
<h3>Build<a class="headerlink" href="#build" title="Permalink to this headline"></a></h3>
<p>In the Linux environment, use the build.sh script in the root directory of MindSpore <a class="reference external" href="https://gitee.com/mindspore/mindspore">Source Code</a> to build the MindSpore Lite package integrated with TensorRT. First configure the environment variable <code class="docutils literal notranslate"><span class="pre">MSLITE_GPU_BACKEND=tensorrt</span></code>, and then execute the compilation command as follows.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh<span class="w"> </span>-I<span class="w"> </span>x86_64
</pre></div>
</div>
<p>For more information about compilation, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/build.html#linux-environment-compilation">Linux Environment Compilation</a>.</p>
</section>
<section id="integration">
<h3>Integration<a class="headerlink" href="#integration" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Integration instructions</p>
<p>When developers need to integrate the use of TensorRT features, it is important to note:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html#configuring-the-gpu-backend">Configure the TensorRT backend</a>,
For more information about using Runtime to perform inference, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html">Using Runtime to Perform Inference (C++)</a>.</p></li>
<li><p>Compile and execute the binary. If you use dynamic linking, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/build.html#directory-structure">Compilation Output</a> with compilation option <code class="docutils literal notranslate"><span class="pre">-I</span> <span class="pre">x86_64</span></code>.
Please set environment variables to dynamically link related libs.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-<span class="o">{</span>os<span class="o">}</span>-<span class="o">{</span>arch<span class="o">}</span>/runtime/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>user-installed-tensorrt-path/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>user-installed-cuda-path/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</li>
<li><p>Using Benchmark testing TensorRT inference</p>
<p>Pass the build package to a device with a TensorRT environment(TensorRT 6.0.1.5) and use the Benchmark tool to test TensorRT inference. Examples are as follows:</p>
<ul class="simple">
<li><p>Test performance</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--device<span class="o">=</span>GPU<span class="w"> </span>--modelFile<span class="o">=</span>./models/test_benchmark.ms<span class="w"> </span>--timeProfiling<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Test precision</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--device<span class="o">=</span>GPU<span class="w"> </span>--modelFile<span class="o">=</span>./models/test_benchmark.ms<span class="w"> </span>--inDataFile<span class="o">=</span>./input/test_benchmark.bin<span class="w"> </span>--inputShapes<span class="o">=</span><span class="m">1</span>,32,32,1<span class="w"> </span>--accuracyThreshold<span class="o">=</span><span class="m">3</span><span class="w"> </span>--benchmarkDataFile<span class="o">=</span>./output/test_benchmark.out
</pre></div>
</div>
<p>For more information about the use of Benchmark, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/benchmark.html">Benchmark Use</a>.</p>
<p>For environment variable settings, you need to set the directory where the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>
(under the directory <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-{os}-{arch}/runtime/lib</span></code>), TensorRT and CUDA <code class="docutils literal notranslate"><span class="pre">so</span></code> libraries are located, to <code class="docutils literal notranslate"><span class="pre">${LD_LIBRARY_PATH}</span></code>.</p>
</li>
<li><p>Using TensorRT engine serialization</p>
<p>TensorRT backend inference supports serializing the built TensorRT model (Engine) into a binary file and saves it locally. When it is used the next time, the model can be deserialized and loaded from the local, avoiding rebuilding and reducing overhead. To support this function, users need to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_Model.html">LoadConfig</a> interface to load the configuration file in the code, you need to specify the saving path of serialization file in the configuration file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ms_cache</span><span class="p">]</span>
<span class="n">serialize_path</span><span class="o">=/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">config</span>
</pre></div>
</div>
</li>
<li><p>Using TensorRT dynamic shapes</p>
<p>By default, TensorRT optimizes the model based on the input shapes (batch size, image size, and so on) at which it was defined. However, the input dimension can be adjusted at runtime by configuring the profile. In the profile, the minimum, dynamic and optimal shape of each input can be set.</p>
<p>TensorRT creates an optimized engine for each profile, choosing CUDA kernels that work for all shapes within the [minimum ~ maximum] range. And in the profile, multiple input dimensions can be configured for a single input. To support this function, users need to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_Model.html">LoadConfig</a> interface to load the configuration file in the code.</p>
<p>If min, opt, and Max are the minimum, optimal, and maximum dimensions, and real_shape is the shape of the input tensor, the following conditions must hold:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">len(min)</span></code> == <code class="docutils literal notranslate"><span class="pre">len(opt)</span></code> == <code class="docutils literal notranslate"><span class="pre">len(max)</span></code> == <code class="docutils literal notranslate"><span class="pre">len(real_shape)</span></code></p></li>
<li><p>0 &lt; <code class="docutils literal notranslate"><span class="pre">min[i]</span></code> &lt;= <code class="docutils literal notranslate"><span class="pre">opt[i]</span></code> &lt;= <code class="docutils literal notranslate"><span class="pre">max[i]</span></code> for all <code class="docutils literal notranslate"><span class="pre">i</span></code></p></li>
<li><p>if <code class="docutils literal notranslate"><span class="pre">real_shape[i]</span></code> != -1, then <code class="docutils literal notranslate"><span class="pre">min[i]</span></code> == <code class="docutils literal notranslate"><span class="pre">opt[i]</span></code> == <code class="docutils literal notranslate"><span class="pre">max[i]</span></code> == <code class="docutils literal notranslate"><span class="pre">real_shape[i]</span></code></p></li>
<li><p>When using tensor input without dynamic dimensions, all shapes must be equal to real_shape.</p></li>
</ol>
<p>For example, if the model input1’s name is “input_name1”, its input shape is [3,-1,-1] (-1 means that this dimension supports dynamic shape), the minimum dimension is [3,100,200], the maximum dimension is [3,200,300], and the optimized dimension is [3,150,250]. The name of model input2 is “input_name2”, the input dimension is [-1,-1,1], the minimum size is [700,800,1], the maximum size is [800,900,1], and the optimized size is [750,850,1]. The following configuration file needs to be configured:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">gpu_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="n">input_name1</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span><span class="n">input_name2</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="o">~</span><span class="mi">200</span><span class="p">,</span><span class="mi">200</span><span class="o">~</span><span class="mi">300</span><span class="p">];[</span><span class="mi">700</span><span class="o">~</span><span class="mi">800</span><span class="p">,</span><span class="mi">800</span><span class="o">~</span><span class="mi">900</span><span class="p">]</span>
<span class="n">opt_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">150</span><span class="p">,</span><span class="mi">250</span><span class="p">];[</span><span class="mi">750</span><span class="p">,</span><span class="mi">850</span><span class="p">]</span>
</pre></div>
</div>
<p>It also support configuring multiple profiles at the same time. According to the above example, if we add a profile configuration for each model input, for the input1, the minimum size of the added profile is [3,201,200], the maximum size is [3,150,300], and the optimized size is [3,220,250]. Add a profile for input2, whose minimum size is [801,800,1], maximum size is [850,900,1], and optimized size is [810,850,1]. The following is an example of the profile:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">gpu_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="n">input_name1</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span><span class="n">input_name2</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="o">~</span><span class="mi">200</span><span class="p">,</span><span class="mi">200</span><span class="o">~</span><span class="mi">300</span><span class="p">],[</span><span class="mi">201</span><span class="o">~</span><span class="mi">250</span><span class="p">,</span><span class="mi">200</span><span class="o">~</span><span class="mi">300</span><span class="p">];[</span><span class="mi">700</span><span class="o">~</span><span class="mi">800</span><span class="p">,</span><span class="mi">800</span><span class="o">~</span><span class="mi">900</span><span class="p">],[</span><span class="mi">801</span><span class="o">~</span><span class="mi">850</span><span class="p">,</span><span class="mi">800</span><span class="o">~</span><span class="mi">900</span><span class="p">]</span>
<span class="n">opt_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">150</span><span class="p">,</span><span class="mi">250</span><span class="p">],[</span><span class="mi">220</span><span class="p">,</span><span class="mi">250</span><span class="p">];[</span><span class="mi">750</span><span class="p">,</span><span class="mi">850</span><span class="p">],[</span><span class="mi">810</span><span class="p">,</span><span class="mi">850</span><span class="p">]</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="supported-operators">
<h2>Supported Operators<a class="headerlink" href="#supported-operators" title="Permalink to this headline"></a></h2>
<p>For supported TensorRT operators, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/operator_list_lite.html">Lite Operator List</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nnie.html" class="btn btn-neutral float-left" title="Usage Description of the Integrated NNIE" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ascend_info.html" class="btn btn-neutral float-right" title="Integrated Ascend" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>