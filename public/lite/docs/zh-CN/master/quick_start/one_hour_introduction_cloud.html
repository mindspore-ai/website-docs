<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>云侧推理快速入门 &mdash; MindSpore Lite master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script><script src="../_static/translations.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="端侧推理样例" href="../device_infer_example.html" />
    <link rel="prev" title="端侧推理快速入门" href="one_hour_introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">编译端侧MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/build.html">编译云侧MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction.html">端侧推理快速入门</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">云侧推理快速入门</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#准备工作">准备工作</a></li>
<li class="toctree-l2"><a class="reference internal" href="#环境变量">环境变量</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-lite环境变量">MindSpore Lite环境变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ascend硬件后端环境变量">Ascend硬件后端环境变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nvidia-gpu硬件后端环境变量">Nvidia GPU硬件后端环境变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#设置host侧日志级别">设置Host侧日志级别</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#集成推理">集成推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#配置cmake">配置CMake</a></li>
<li class="toctree-l3"><a class="reference internal" href="#编写代码">编写代码</a></li>
<li class="toctree-l3"><a class="reference internal" href="#编译">编译</a></li>
<li class="toctree-l3"><a class="reference internal" href="#运行推理程序">运行推理程序</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">端侧推理样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">在MCU或小型系统上执行推理或训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">端侧训练样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime.html">基础推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_parallel.html">并发推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_distributed.html">分布式推理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/benchmark.html">基准测试工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>云侧推理快速入门</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/one_hour_introduction_cloud.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="云侧推理快速入门">
<h1>云侧推理快速入门<a class="headerlink" href="#云侧推理快速入门" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_zh_cn/quick_start/one_hour_introduction_cloud.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>本文通过使用MindSpore Lite执行云侧推理为例，向大家介绍MindSpore Lite的基础功能和用法。</p>
<p>MindSpore Lite云侧推理仅支持在Linux环境部署运行。支持Ascend 310/310P/910、Nvidia GPU和CPU硬件后端。
在开始本章的MindSpore Lite使用之旅之前，用户需拥有一个Linux（如Ubuntu/CentOS/EulerOS）的环境，以便随时操作验证。</p>
<p>如需体验MindSpore Lite端侧推理流程，请参考文档<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/quick_start/one_hour_introduction.html">端侧推理快速入门</a>。</p>
<p>我们将以使用MindSpore Lite的C++接口进行集成为例，演示如何使用MindSpore Lite的发布件，进行集成开发，编写自己的推理程序。MindSpore Lite的C++接口的详细用法用户可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/cloud_infer/runtime_cpp.html">使用C++接口进行云侧推理</a>。</p>
<p>另外，用户可以使用MindSpore Lite的Python接口Java接口进行集成。详情可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/cloud_infer/runtime_python.html">使用Python接口进行云侧推理</a>和<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/cloud_infer/runtime_java.html">使用Java接口进行云侧推理</a>。</p>
</section>
<section id="准备工作">
<h2>准备工作<a class="headerlink" href="#准备工作" title="永久链接至标题"></a></h2>
<ol class="arabic">
<li><p>环境要求</p>
<ul class="simple">
<li><p>系统环境：Linux x86_64，推荐使用Ubuntu 18.04.02LTS</p></li>
</ul>
</li>
<li><p>下载发布件</p>
<p>用户可在MindSpore官网<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/downloads.html">下载页面</a>下载MindSpore Lite云侧推理包<code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-linux-{arch}.tar.gz</span></code>，<code class="docutils literal notranslate"><span class="pre">{arch}</span></code>为<code class="docutils literal notranslate"><span class="pre">x64</span></code>或者<code class="docutils literal notranslate"><span class="pre">aarch64</span></code>，<code class="docutils literal notranslate"><span class="pre">x64</span></code>版本支持Ascend、Nvidia GPU、CPU三个硬件后端，<code class="docutils literal notranslate"><span class="pre">aarch64</span></code>仅支持Ascend、CPU硬件后端。</p>
<p>以下为<code class="docutils literal notranslate"><span class="pre">x64</span></code>云侧推理包内容。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
├── runtime
│   ├── include                          # MindSpore Lite集成开发的API头文件
│   ├── lib
│   │   ├── libascend_ge_plugin.so       # Ascend硬件后端拉远模式插件
│   │   ├── libascend_kernel_plugin.so   # Ascend硬件后端插件
│   │   ├── libdvpp_utils.so             # Ascend硬件后端DVPP插件
│   │   ├── libminddata-lite.a           # 图像处理静态库
│   │   ├── libminddata-lite.so          # 图像处理动态库
│   │   ├── libmindspore_core.so         # MindSpore Lite推理框架的动态库
│   │   ├── libmindspore_glog.so.0       # MindSpore Lite日志动态库
│   │   ├── libmindspore-lite-jni.so     # MindSpore Lite推理框架的JNI动态库
│   │   ├── libmindspore-lite.so         # MindSpore Lite推理框架的动态库
│   │   ├── libmsplugin-ge-litert.so     # CPU硬件后端插件
│   │   ├── libruntime_convert_plugin.so # 在线转换插件
│   │   ├── libtensorrt_plugin.so        # Nvidia GPU硬件后端插件
│   │   ├── libtransformer-shared.so     # Transformer动态库
│   │   └── mindspore-lite-java.jar      # MindSpore Lite推理框架jar包
│   └── third_party
└── tools
    ├── benchmark       # 基准测试工具目录
    └── converter       # 模型转换工具目录
</pre></div>
</div>
</li>
<li><p>获取模型</p>
<p>MindSpore Lite云侧推理当前仅支持MindSpore的MindIR模型格式，可以通过MindSpore导出MindIR模型，或者由<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/cloud_infer/converter_tool.html">模型转换工具</a>转换Tensorflow、Onnx、Caffe等格式的模型获得MindIR模型。</p>
<p>可下载模型文件<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a>作为样例模型。</p>
</li>
<li><p>获取样例</p>
<p>本节样例代码放置在<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/master/mindspore/lite/examples/cloud_infer/quick_start_cpp">mindspore/lite/examples/cloud_infer/quick_start_cpp</a>目录。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>quick_start_cpp
├── CMakeLists.txt
├── main.cc
├── build                           # 临时的构建目录
└── model
    └── mobilenetv2.mindir          # 模型文件
</pre></div>
</div>
</li>
</ol>
</section>
<section id="环境变量">
<h2>环境变量<a class="headerlink" href="#环境变量" title="永久链接至标题"></a></h2>
<p><strong>为了确保脚本能够正常地运行，需要在构建和执行推理前设置环境变量。</strong></p>
<section id="mindspore-lite环境变量">
<h3>MindSpore Lite环境变量<a class="headerlink" href="#mindspore-lite环境变量" title="永久链接至标题"></a></h3>
<p>MindSpore Lite云侧推理包解压缩后，设置<code class="docutils literal notranslate"><span class="pre">LITE_HOME</span></code>环境变量为解压缩的路径，比如：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LITE_HOME</span><span class="o">=</span><span class="nv">$some_path</span>/mindpsore-lite-2.0.0-linux-x64
</pre></div>
</div>
<p>设置环境变量<code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LITE_HOME</span>/runtime/lib:<span class="nv">$LITE_HOME</span>/runtime/third_party/dnnl:<span class="nv">$LITE_HOME</span>/tools/converter/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>如果需要使用<code class="docutils literal notranslate"><span class="pre">convert_lite</span></code>或者<code class="docutils literal notranslate"><span class="pre">benchmark</span></code>工具，则需要设置环境变量<code class="docutils literal notranslate"><span class="pre">PATH</span></code>。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$LITE_HOME</span>/tools/converter/converter:<span class="nv">$LITE_HOME</span>/tools/benchmark:<span class="nv">$PATH</span>
</pre></div>
</div>
</section>
<section id="ascend硬件后端环境变量">
<h3>Ascend硬件后端环境变量<a class="headerlink" href="#ascend硬件后端环境变量" title="永久链接至标题"></a></h3>
<ol class="arabic">
<li><p>确认run包安装路径</p>
<p>若使用root用户完成run包安装，默认路径为’/usr/local/Ascend’，非root用户的默认安装路径为’/home/HwHiAiUser/Ascend’。</p>
<p>以root用户的路径为例，设置环境变量如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_HOME</span><span class="o">=</span>/usr/local/Ascend<span class="w">  </span><span class="c1"># the root directory of run package</span>
</pre></div>
</div>
</li>
<li><p>区分run包版本</p>
<p>run包分为2个版本，用安装目录下是否存在’ascend-toolkit’文件夹进行区分。</p>
<p>如果存在’ascend-toolkit’文件夹，设置环境变量如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_HOME</span><span class="o">=</span>/usr/local/Ascend
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/compiler/bin:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/compiler/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/lib64:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/opp
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_AICPU_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/compiler/python/site-packages:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TOOLCHAIN_HOME</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/toolkit
</pre></div>
</div>
<p>若不存在，设置环境变量为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_HOME</span><span class="o">=</span>/usr/local/Ascend
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/compiler/bin:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/compiler/ccec_compiler/bin:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/lib64:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/opp
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_AICPU_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/compiler/python/site-packages:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TOOLCHAIN_HOME</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/toolkit
</pre></div>
</div>
</li>
</ol>
</section>
<section id="nvidia-gpu硬件后端环境变量">
<h3>Nvidia GPU硬件后端环境变量<a class="headerlink" href="#nvidia-gpu硬件后端环境变量" title="永久链接至标题"></a></h3>
<p>硬件后端为Nvidia GPU时，推理依赖cuda和TensorRT，用户需要先安装cuda和TensorRT。</p>
<p>以下以cuda11.1和TensorRT8.5.1.7为例，用户需要根据实际安装路径设置环境变量。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda-11.1
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$CUDA_HOME</span>/bin:<span class="nv">$PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$CUDA_HOME</span>/lib64:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">TENSORRT_PATH</span><span class="o">=</span>/usr/local/TensorRT-8.5.1.7
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$TENSORRT_PATH</span>/bin:<span class="nv">$PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$TENSORRT_PATH</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</section>
<section id="设置host侧日志级别">
<h3>设置Host侧日志级别<a class="headerlink" href="#设置host侧日志级别" title="永久链接至标题"></a></h3>
<p>Host日志级别默认为<code class="docutils literal notranslate"><span class="pre">WARNING</span></code>。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="c1"># 0-DEBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.</span>
</pre></div>
</div>
</section>
</section>
<section id="集成推理">
<h2>集成推理<a class="headerlink" href="#集成推理" title="永久链接至标题"></a></h2>
<p>我们将以使用MindSpore Lite的C++接口进行集成为例，演示如何使用MindSpore Lite的发布件，进行集成开发，编写自己的推理程序。</p>
<p>在进行集成前，用户也可以直接使用随发布件发布的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/cloud_infer/benchmark_tool.html">基准测试工具（benchmark）</a>来进行推理测试。</p>
<section id="配置cmake">
<h3>配置CMake<a class="headerlink" href="#配置cmake" title="永久链接至标题"></a></h3>
<p>用户需要集成发布件内的<code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code>库文件，并通过MindSpore Lite头文件中声明的API接口，来进行模型推理。</p>
<p>以下是通过CMake集成<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>动态库时的示例代码。通过读取环境变量<code class="docutils literal notranslate"><span class="pre">LITE_HOME</span></code>以获取MindSpore Lite tar包解压后的头文件和库文件目录。</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.14</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">QuickStartCpp</span><span class="p">)</span>

<span class="nb">if</span><span class="p">(</span><span class="s">CMAKE_CXX_COMPILER_ID</span><span class="w"> </span><span class="s">STREQUAL</span><span class="w"> </span><span class="s2">&quot;GNU&quot;</span><span class="w"> </span><span class="s">AND</span><span class="w"> </span><span class="s">CMAKE_CXX_COMPILER_VERSION</span><span class="w"> </span><span class="s">VERSION_LESS</span><span class="w"> </span><span class="s">7.3.0</span><span class="p">)</span>
<span class="w">    </span><span class="nb">message</span><span class="p">(</span><span class="s">FATAL_ERROR</span><span class="w"> </span><span class="s2">&quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="nb">if</span><span class="p">(</span><span class="s">DEFINED</span><span class="w"> </span><span class="s">ENV{LITE_HOME}</span><span class="p">)</span>
<span class="w">    </span><span class="nb">set</span><span class="p">(</span><span class="s">LITE_HOME</span><span class="w"> </span><span class="o">$ENV{</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="c"># Add directory to include search path</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="s">/runtime</span><span class="p">)</span>
<span class="c"># Add directory to linker search path</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="s">/runtime/lib</span><span class="p">)</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="s">/tools/converter/lib</span><span class="p">)</span>

<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span><span class="w"> </span><span class="s">QUICK_START_CXX</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/*.cc</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">mindspore_quick_start_cpp</span><span class="w"> </span><span class="o">${</span><span class="nv">QUICK_START_CXX</span><span class="o">}</span><span class="p">)</span>

<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">mindspore_quick_start_cpp</span><span class="w"> </span><span class="s">mindspore-lite</span><span class="w"> </span><span class="s">pthread</span><span class="w"> </span><span class="s">dl</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="编写代码">
<h3>编写代码<a class="headerlink" href="#编写代码" title="永久链接至标题"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">main.cc</span></code>中代码如下所示：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;algorithm&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;random&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;fstream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstring&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;memory&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/model.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/context.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/status.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">QuickStart</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">argc</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model file must be provided.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Read model file.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model path &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">model_path</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; is invalid.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Generate random data as input data.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Generate Random Input Data failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Model Predict</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Print Output Tensor Data.</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">Name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">QuickStart</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="n">argv</span><span class="p">);</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>代码功能解析如下：</p>
<ol class="arabic">
<li><p>初始化Context配置</p>
<p>Context保存了模型推理时所需的相关配置，包括算子偏好、线程数、自动并发以及推理处理器相关的其他配置。
关于Context的详细说明，请参考Context的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_cpp/mindspore.html#context">API接口说明</a>。
在MindSpore Lite加载模型时，必须提供一个<code class="docutils literal notranslate"><span class="pre">Context</span></code>类的对象，所以在本例中，首先申请了一个<code class="docutils literal notranslate"><span class="pre">Context</span></code>类的对象<code class="docutils literal notranslate"><span class="pre">context</span></code>。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>接着，通过<code class="docutils literal notranslate"><span class="pre">Context::MutableDeviceInfo</span></code>接口，得到<code class="docutils literal notranslate"><span class="pre">context</span></code>对象的设备管理列表。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>在本例中，由于使用CPU进行推理，故需申请一个<code class="docutils literal notranslate"><span class="pre">CPUDeviceInfo</span></code>类的对象<code class="docutils literal notranslate"><span class="pre">device_info</span></code>。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>因为采用了CPU的默认设置，所以不需对<code class="docutils literal notranslate"><span class="pre">device_info</span></code>对象做任何设置，直接添加到<code class="docutils literal notranslate"><span class="pre">context</span></code>的设备管理列表。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>加载模型</p>
<p>首先创建一个<code class="docutils literal notranslate"><span class="pre">Model</span></code>类对象<code class="docutils literal notranslate"><span class="pre">model</span></code>，<code class="docutils literal notranslate"><span class="pre">Model</span></code>类定义了MindSpore中的模型，用于计算图管理。
关于<code class="docutils literal notranslate"><span class="pre">Model</span></code>类的详细说明，可参考<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_cpp/mindspore.html#model">API文档</a>。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
<p>接着调用<code class="docutils literal notranslate"><span class="pre">Build</span></code>接口传入模型，将模型编译至可在设备上运行的状态。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>传入数据</p>
<p>在执行模型推理前，需要设置推理的输入数据。
此例，通过<code class="docutils literal notranslate"><span class="pre">Model.GetInputs</span></code>接口，获取模型的所有输入张量。单个张量的格式为<code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>。
关于<code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>张量的详细说明，请参考<code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_cpp/mindspore.html#mstensor">API说明</a>。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>通过张量的<code class="docutils literal notranslate"><span class="pre">MutableData</span></code>接口可以获取张量的数据内存指针，通过张量的<code class="docutils literal notranslate"><span class="pre">DataSize</span></code>接口可以获取张量的数据字节长度。可通过张量的<code class="docutils literal notranslate"><span class="pre">DataType</span></code>接口得到该张量的数据类型，用户可根据自己模型的数据格式做不同处理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
</pre></div>
</div>
<p>接着，通过数据指针，将我们要推理的数据传入张量内部。
在本例中我们传入的是随机生成的0.1至1的浮点数据，且数据呈平均分布。
在实际的推理中，用户在读取图片或音频等实际数据后，需进行算法特定的预处理操作，并将处理后的数据传入模型。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Generate random data as input data.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Generate Random Input Data failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>执行推理</p>
<p>首先申请一个放置模型推理输出张量的数组<code class="docutils literal notranslate"><span class="pre">outputs</span></code>，然后调用模型推理接口<code class="docutils literal notranslate"><span class="pre">Predict</span></code>，将输入张量和输出张量作它的参数。
在推理成功后，输出张量被保存在<code class="docutils literal notranslate"><span class="pre">outputs</span></code>内。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>获取推理结果</p>
<p>通过<code class="docutils literal notranslate"><span class="pre">Data</span></code>得到输出张量的数据指针。
本例中，将它强转为浮点指针，用户可以根据自己模型的数据类型进行对应类型的转换，也可通过张量的<code class="docutils literal notranslate"><span class="pre">DataType</span></code>接口得到数据类型。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
</pre></div>
</div>
<p>在本例中，直接打印推理输出结果。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>释放model对象</p>
<p>Model析构时将释放模型相关资源。</p>
</li>
</ol>
</section>
<section id="编译">
<h3>编译<a class="headerlink" href="#编译" title="永久链接至标题"></a></h3>
<p>按照环境变量一节所述，设置环境变量。接着按如下方式编译程序。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>../
make
</pre></div>
</div>
<p>在编译成功后，可以在<code class="docutils literal notranslate"><span class="pre">build</span></code>目录下得到<code class="docutils literal notranslate"><span class="pre">quick_start_cpp</span></code>可执行程序。</p>
</section>
<section id="运行推理程序">
<h3>运行推理程序<a class="headerlink" href="#运行推理程序" title="永久链接至标题"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mindspore_quick_start_cpp<span class="w"> </span>../model/mobilenetv2.mindir
</pre></div>
</div>
<p>执行完成后将能得到如下结果，打印输出Tensor的名称、输出Tensor的大小，输出Tensor的数量以及前50个数据：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:Default/head-MobileNetV2Head/Softmax-op204 tensor size is:4000 tensor elements num is:1000
output data is:5.07155e-05 0.00048712 0.000312549 0.00035624 0.0002022 8.58958e-05 0.000187147 0.000365937 0.000281044 0.000255672 0.00108948 0.00390996 0.00230398 0.00128984 0.00307477 0.00147607 0.00106759 0.000589853 0.000848115 0.00143693 0.000685777 0.00219331 0.00160639 0.00215123 0.000444315 0.000151986 0.000317552 0.00053971 0.00018703 0.000643944 0.000218269 0.000931556 0.000127084 0.000544278 0.000887942 0.000303909 0.000273875 0.00035335 0.00229062 0.000453207 0.0011987 0.000621194 0.000628335 0.000838564 0.000611029 0.000372603 0.00147742 0.000270685 8.29869e-05 0.000116974 0.000876237
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="one_hour_introduction.html" class="btn btn-neutral float-left" title="端侧推理快速入门" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../device_infer_example.html" class="btn btn-neutral float-right" title="端侧推理样例" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>