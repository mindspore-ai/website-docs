<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>体验C语言接口极简推理Demo &mdash; MindSpore Lite master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script><script src="../_static/translations.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="基于JNI接口的Android应用开发" href="quick_start.html" />
    <link rel="prev" title="体验Java极简推理Demo" href="quick_start_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">编译端侧MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/build.html">编译云侧MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction.html">端侧推理快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction_cloud.html">云侧推理快速入门</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../device_infer_example.html">端侧推理样例</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">体验C语言接口极简推理Demo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#构建与运行">构建与运行</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#linux-x86">Linux X86</a></li>
<li class="toctree-l4"><a class="reference internal" href="#windows">Windows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#配置cmake">配置CMake</a></li>
<li class="toctree-l3"><a class="reference internal" href="#创建配置上下文">创建配置上下文</a></li>
<li class="toctree-l3"><a class="reference internal" href="#模型创建加载与编译">模型创建加载与编译</a></li>
<li class="toctree-l3"><a class="reference internal" href="#模型推理">模型推理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#内存释放">内存释放</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_segmentation.html">基于Java接口的Android应用开发</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">在MCU或小型系统上执行推理或训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">端侧训练样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime.html">基础推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_parallel.html">并发推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_distributed.html">分布式推理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/benchmark.html">基准测试工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../device_infer_example.html">端侧推理样例</a> &raquo;</li>
      <li>体验C语言接口极简推理Demo</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/quick_start_c.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="体验c语言接口极简推理demo">
<h1>体验C语言接口极简推理Demo<a class="headerlink" href="#体验c语言接口极简推理demo" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_zh_cn/quick_start/quick_start_c.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>本教程提供了MindSpore Lite执行推理的示例程序，通过随机输入、执行推理、打印推理结果的方式，演示了C语言进行端侧推理的基本流程，用户能够快速了解MindSpore Lite执行推理相关API的使用。本教程通过随机生成的数据作为输入数据，执行MobileNetV2模型的推理，打印获得输出数据。相关代码放置在<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/master/mindspore/lite/examples/quick_start_c">mindspore/lite/examples/quick_start_c</a>目录。</p>
<p>使用MindSpore Lite执行推理主要包括以下步骤：</p>
<ol class="arabic simple">
<li><p>模型读取：从文件系统中读取由<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/converter_tool.html">模型转换工具</a>转换得到的<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型。</p></li>
<li><p>创建配置上下文：创建配置上下文<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/context_c.html">Context</a>，保存需要的一些基本配置参数，用于指导模型编译和模型执行。</p></li>
<li><p>模型创建、加载与编译：执行推理之前，需要调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html#msmodelbuildfromfile">MSModelBuildFromFile</a>接口进行模型加载和模型编译，并将上一步得到的Context配置到Model中。模型加载阶段将文件缓存解析成运行时的模型。模型编译阶段主要进行算子选型调度、子图切分等过程，该阶段会耗费较多时间，所以建议Model创建一次，编译一次，多次推理。</p></li>
<li><p>输入数据：模型执行之前需要向<code class="docutils literal notranslate"><span class="pre">输入Tensor</span></code>中填充数据。</p></li>
<li><p>执行推理：使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html#msmodelpredict">MSModelPredict</a>接口进行模型推理。</p></li>
<li><p>获得输出：模型执行结束之后，可以通过<code class="docutils literal notranslate"><span class="pre">输出Tensor</span></code>得到推理结果。</p></li>
<li><p>释放内存：无需使用MindSpore Lite推理框架时，需要释放已创建的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html">Model</a>。</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
</section>
<section id="构建与运行">
<h2>构建与运行<a class="headerlink" href="#构建与运行" title="永久链接至标题"></a></h2>
<section id="linux-x86">
<h3>Linux X86<a class="headerlink" href="#linux-x86" title="永久链接至标题"></a></h3>
<ul>
<li><p>环境要求</p>
<ul class="simple">
<li><p>系统环境：Linux x86_64，推荐使用Ubuntu 18.04.02LTS</p></li>
<li><p>编译依赖：</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>编译构建</p>
<p>在<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c</span></code>目录下执行<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/quick_start_c/build.sh">build脚本</a>，将自动下载MindSpore Lite推理框架库以及模型文件并编译Demo。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<blockquote>
<div><p>若使用该build脚本下载MindSpore Lite推理框架失败，请手动下载硬件平台为CPU、操作系统为Ubuntu-x64的MindSpore Lite 模型推理框架<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a>，将解压后<code class="docutils literal notranslate"><span class="pre">runtime/lib</span></code>目录下的<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/lib</span></code>目录、<code class="docutils literal notranslate"><span class="pre">runtime/include</span></code>目录里的文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/include</span></code>目录下、<code class="docutils literal notranslate"><span class="pre">runtime/third_party/glog/</span></code>目录下的<code class="docutils literal notranslate"><span class="pre">libmindspore_glog.so.0</span></code>文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/lib</span></code>目录下的<code class="docutils literal notranslate"><span class="pre">libmindspore_glog.so</span></code>。</p>
<p>若MobileNetV2模型下载失败，请手动下载相关模型文件<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a>，并将其拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/model</span></code>目录。</p>
<p>通过手动下载并且将文件放到指定位置后，需要再次执行build.sh脚本才能完成编译构建。</p>
</div></blockquote>
</li>
<li><p>执行推理</p>
<p>编译构建后，进入<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/build</span></code>目录，并执行以下命令，体验MindSpore Lite推理MobileNetV2模型。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mindspore_quick_start_c<span class="w"> </span>../model/mobilenetv2.ms
</pre></div>
</div>
<p>执行完成后将能得到如下结果，打印输出Tensor的名称、输出Tensor的大小，输出Tensor的数量以及前50个数据：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Tensor name: Softmax-65, tensor size is 4004 ,elements num: 1001.
output data is:
0.000011 0.000015 0.000015 0.000079 0.000070 0.000702 0.000120 0.000590 0.000009 0.000004 0.000004 0.000002 0.000002 0.000002 0.000010 0.000055 0.000006 0.000010 0.000003 0.000010 0.000002 0.000005 0.000001 0.000002 0.000004 0.000006 0.000008 0.000003 0.000015 0.000005 0.000011 0.000020 0.000006 0.000002 0.000011 0.000170 0.000005 0.000009 0.000006 0.000002 0.000003 0.000009 0.000005 0.000006 0.000003 0.000011 0.000005 0.000027 0.000003 0.000050 0.000016
</pre></div>
</div>
</li>
</ul>
</section>
<section id="windows">
<h3>Windows<a class="headerlink" href="#windows" title="永久链接至标题"></a></h3>
<ul>
<li><p>环境要求</p>
<ul class="simple">
<li><p>系统环境：Windows 7，Windows 10；64位。</p></li>
<li><p>编译依赖：</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://sourceforge.net/projects/mingw-w64/files/ToolchainstargettingWin64/PersonalBuilds/mingw-builds/7.3.0/threads-posix/seh/x86_64-7.3.0-release-posix-seh-rt_v5-rev0.7z/download">MinGW GCC</a> = 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>编译构建</p>
<ul class="simple">
<li><p>库下载：请手动下载硬件平台为CPU、操作系统为Windows-x64的MindSpore Lite模型推理框架<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/downloads.html">mindspore-lite-{version}-win-x64.zip</a>，将解压后<code class="docutils literal notranslate"><span class="pre">runtime\lib</span></code>目录下的所有文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c\lib</span></code>工程目录、<code class="docutils literal notranslate"><span class="pre">runtime\include</span></code>目录里的文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c\include</span></code>工程目录下。（注意：工程项目下的<code class="docutils literal notranslate"><span class="pre">lib</span></code>、<code class="docutils literal notranslate"><span class="pre">include</span></code>目录需手工创建）</p></li>
<li><p>模型下载：请手动下载相关模型文件<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a>，并将其拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c\model</span></code>目录。</p></li>
<li><p>编译：在<code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c</span></code>目录下执行<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/quick_start_c/build.bat">build脚本</a>，将能够自动下载相关文件并编译Demo。</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>build.bat
</pre></div>
</div>
</li>
<li><p>执行推理</p>
<p>编译构建后，进入<code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c\build</span></code>目录，并执行以下命令，体验MindSpore Lite推理MobileNetV2模型。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>..<span class="se">\l</span>ib<span class="p">;</span>%PATH%
call<span class="w"> </span>mindspore_quick_start_c.exe<span class="w"> </span>..<span class="se">\m</span>odel<span class="se">\m</span>obilenetv2.ms
</pre></div>
</div>
<p>执行完成后将能得到如下结果，打印输出Tensor的名称、输出Tensor的大小，输出Tensor的数量以及前50个数据：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Tensor name: Softmax-65, tensor size is 4004 ,elements num: 1001.
output data is:
0.000011 0.000015 0.000015 0.000079 0.000070 0.000702 0.000120 0.000590 0.000009 0.000004 0.000004 0.000002 0.000002 0.000002 0.000010 0.000055 0.000006 0.000010 0.000003 0.000010 0.000002 0.000005 0.000001 0.000002 0.000004 0.000006 0.000008 0.000003 0.000015 0.000005 0.000011 0.000020 0.000006 0.000002 0.000011 0.000170 0.000005 0.000009 0.000006 0.000002 0.000003 0.000009 0.000005 0.000006 0.000003 0.000011 0.000005 0.000027 0.000003 0.000050 0.000016
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="配置cmake">
<h2>配置CMake<a class="headerlink" href="#配置cmake" title="永久链接至标题"></a></h2>
<p>以下是通过CMake集成<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>静态库时的示例代码。</p>
<blockquote>
<div><p>集成<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>静态库时需要将<code class="docutils literal notranslate"><span class="pre">-Wl,--whole-archive</span></code>的选项传递给链接器。</p>
<p>由于在编译MindSpore Lite的时候增加了<code class="docutils literal notranslate"><span class="pre">-fstack-protector-strong</span></code>栈保护的编译选项，所以在Windows平台上还需要链接MinGW中的<code class="docutils literal notranslate"><span class="pre">ssp</span></code>库。</p>
<p>由于在编译MindSpore Lite的时候增加了对so库文件处理的支持，所以在Linux平台上还需要链接<code class="docutils literal notranslate"><span class="pre">dl</span></code>库。</p>
</div></blockquote>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.18.3</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">QuickStartC</span><span class="p">)</span>

<span class="nb">if</span><span class="p">(</span><span class="s">CMAKE_CXX_COMPILER_ID</span><span class="w"> </span><span class="s">STREQUAL</span><span class="w"> </span><span class="s2">&quot;GNU&quot;</span><span class="w"> </span><span class="s">AND</span><span class="w"> </span><span class="s">CMAKE_CXX_COMPILER_VERSION</span><span class="w"> </span><span class="s">VERSION_LESS</span><span class="w"> </span><span class="s">7.3.0</span><span class="p">)</span>
<span class="w">    </span><span class="nb">message</span><span class="p">(</span><span class="s">FATAL_ERROR</span><span class="w"> </span><span class="s2">&quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="c"># Add directory to include search path</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="p">)</span>

<span class="c"># Add directory to linker search path</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>

<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span><span class="w"> </span><span class="s">QUICK_START_CXX</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/*.cc</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">mindspore_quick_start_c</span><span class="w"> </span><span class="o">${</span><span class="nv">QUICK_START_CXX</span><span class="o">}</span><span class="p">)</span>

<span class="nb">target_link_libraries</span><span class="p">(</span>
<span class="w">        </span><span class="s">mindspore_quick_start_c</span>
<span class="w">        </span><span class="s">-Wl,--whole-archive</span><span class="w"> </span><span class="s">libmindspore-lite</span><span class="w"> </span><span class="s">-Wl,--no-whole-archive</span>
<span class="w">        </span><span class="s">pthread</span>
<span class="p">)</span>

<span class="c"># Due to the increased compilation options for stack protection,</span>
<span class="c"># it is necessary to target link ssp library when Use the static library in Windows.</span>
<span class="nb">if</span><span class="p">(</span><span class="s">WIN32</span><span class="p">)</span>
<span class="w">    </span><span class="nb">target_link_libraries</span><span class="p">(</span>
<span class="w">            </span><span class="s">mindspore_quick_start_c</span>
<span class="w">            </span><span class="s">ssp</span>
<span class="w">    </span><span class="p">)</span>
<span class="nb">else</span><span class="p">()</span>
<span class="w">    </span><span class="nb">target_link_libraries</span><span class="p">(</span>
<span class="w">            </span><span class="s">mindspore_quick_start_c</span>
<span class="w">            </span><span class="s">dl</span>
<span class="w">    </span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="创建配置上下文">
<h2>创建配置上下文<a class="headerlink" href="#创建配置上下文" title="永久链接至标题"></a></h2>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="n">MSContextHandle</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSContextCreate</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSContextCreate failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kMSStatusLiteError</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">thread_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">  </span><span class="n">MSContextSetThreadNum</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">thread_num</span><span class="p">);</span>
<span class="w">  </span><span class="n">MSContextSetThreadAffinityMode</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">MSDeviceInfoHandle</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSDeviceInfoCreate</span><span class="p">(</span><span class="n">kMSDeviceTypeCPU</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSDeviceInfoCreate failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kMSStatusLiteError</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">MSDeviceInfoSetEnableFP16</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="n">MSContextAddDeviceInfo</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="模型创建加载与编译">
<h2>模型创建加载与编译<a class="headerlink" href="#模型创建加载与编译" title="永久链接至标题"></a></h2>
<p>模型加载与编译可以调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_c/model_c.html#msmodelbuildfromfile">MSModelBuildFromFile</a>接口，从文件路径加载、编译得到运行时的模型。本例中<code class="docutils literal notranslate"><span class="pre">argv[1]</span></code>对应的是从控制台中输入的模型文件路径。</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="n">MSModelHandle</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelCreate</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelCreate failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kMSStatusLiteError</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelBuildFromFile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">kMSModelTypeMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelBuildFromFile failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="模型推理">
<h2>模型推理<a class="headerlink" href="#模型推理" title="永久链接至标题"></a></h2>
<p>模型推理主要包括输入数据、执行推理、获得输出等步骤，其中本示例中的输入数据是通过随机数据构造生成，最后将执行推理后的输出结果打印出来。</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Get Inputs</span>
<span class="w">  </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelGetInputs</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">handle_list</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelGetInputs failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Generate random data as input data.</span>
<span class="w">  </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;GenerateInputDataWithRandom failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Model Predict</span>
<span class="w">  </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelPredict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelPredict failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Print Output Tensor Data.</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">handle_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MSTensorHandle</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">element_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSTensorGetElementNum</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Tensor name: %s, tensor size is %ld ,elements num: %ld.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">MSTensorGetName</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span>
<span class="w">           </span><span class="n">MSTensorGetDataSize</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span><span class="w"> </span><span class="n">element_num</span><span class="p">);</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="p">)</span><span class="n">MSTensorGetData</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;output data is:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">max_print_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">element_num</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">max_print_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%f &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="内存释放">
<h2>内存释放<a class="headerlink" href="#内存释放" title="永久链接至标题"></a></h2>
<p>无需使用MindSpore Lite推理框架时，需要释放已经创建的<code class="docutils literal notranslate"><span class="pre">Model</span></code>。</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model.</span>
<span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quick_start_java.html" class="btn btn-neutral float-left" title="体验Java极简推理Demo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="quick_start.html" class="btn btn-neutral float-right" title="基于JNI接口的Android应用开发" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>