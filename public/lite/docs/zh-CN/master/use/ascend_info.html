

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>集成Ascend使用说明 &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="端侧训练样例" href="../device_train_example.html" />
    <link rel="prev" title="集成TensorRT使用说明" href="tensorrt_info.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译端侧MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">编译云侧MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">端侧推理快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">云侧推理快速入门</a></li>
</ul>
<p class="caption"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">端侧推理样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在MCU或小型系统上执行推理或训练</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="asic.html">专用芯片集成说明</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_info.html">集成NPU使用说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnie.html">集成NNIE使用说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorrt_info.html">集成TensorRT使用说明</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">集成Ascend使用说明</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#环境准备">环境准备</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#确认系统环境信息">确认系统环境信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置环境变量">配置环境变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#执行converter工具">执行converter工具</a></li>
<li class="toctree-l3"><a class="reference internal" href="#推理工具runtime">推理工具runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="#执行benchmark">执行benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#高级特性">高级特性</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#动态shape特性">动态shape特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#算子支持">算子支持</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">端侧训练样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption"><span class="caption-text">端侧第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption"><span class="caption-text">端侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption"><span class="caption-text">云侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">执行并发推理</a></li>
</ul>
<p class="caption"><span class="caption-text">云侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">基准测试工具</a></li>
</ul>
<p class="caption"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="asic.html">专用芯片集成说明</a> &raquo;</li>
        
      <li>集成Ascend使用说明</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/ascend_info.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="集成ascend使用说明">
<h1>集成Ascend使用说明<a class="headerlink" href="#集成ascend使用说明" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_zh_cn/use/ascend_info.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png"></a></p>
<p>本文档介绍如何在Ascend环境的Linux系统上，使用MindSpore Lite 进行推理，以及动态shape功能的使用。目前，MindSpore Lite支持Ascend310和310P芯片。</p>
<div class="section" id="环境准备">
<h2>环境准备<a class="headerlink" href="#环境准备" title="Permalink to this headline">¶</a></h2>
<div class="section" id="确认系统环境信息">
<h3>确认系统环境信息<a class="headerlink" href="#确认系统环境信息" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>确认安装64位操作系统，<a class="reference external" href="https://www.gnu.org/software/libc/">glibc</a>&gt;=2.17，其中Ubuntu 18.04/CentOS 7.6/EulerOS 2.8是经过验证的。</p></li>
<li><p>确认安装<a class="reference external" href="https://gcc.gnu.org/releases.html">GCC 7.3.0版本</a>。</p></li>
<li><p>确认安装<a class="reference external" href="https://cmake.org/download/">CMake 3.18.3及以上版本</a>。</p>
<ul class="simple">
<li><p>安装完成后将CMake所在路径添加到系统环境变量。</p></li>
</ul>
</li>
<li><p>确认安装Python 3.7.5或3.9.0版本。如果未安装或者已安装其他版本的Python，可以选择下载并安装：</p>
<ul class="simple">
<li><p>Python 3.7.5版本 64位，下载地址：<a class="reference external" href="https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz">官网</a>或<a class="reference external" href="https://mirrors.huaweicloud.com/python/3.7.5/Python-3.7.5.tgz">华为云</a>。</p></li>
<li><p>Python 3.9.0版本 64位，下载地址：<a class="reference external" href="https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tgz">官网</a>或<a class="reference external" href="https://mirrors.huaweicloud.com/python/3.9.0/Python-3.9.0.tgz">华为云</a>。</p></li>
</ul>
</li>
<li><p>如果您的环境为ARM架构，请确认当前使用的Python配套的pip版本&gt;=19.3。</p></li>
<li><p>确认安装昇腾AI处理器配套软件包。</p>
<ul>
<li><p>昇腾软件包提供商用版和社区版两种下载途径：</p>
<ol class="simple">
<li><p>商用版下载需要申请权限，下载链接与安装方式请参考<a class="reference external" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100280094">Ascend Data Center Solution 22.0.RC3安装指引文档</a>。</p></li>
<li><p>社区版下载不受限制，下载链接请前往<a class="reference external" href="https://www.hiascend.com/software/cann/community-history">CANN社区版</a>，选择<code class="docutils literal notranslate"><span class="pre">5.1.RC2.alpha007</span></code>版本，以及在<a class="reference external" href="https://www.hiascend.com/hardware/firmware-drivers?tag=community">固件与驱动</a>链接中获取对应的固件和驱动安装包，安装包的选择与安装方式请参照上述的商用版安装指引文档。</p></li>
</ol>
</li>
<li><p>安装包默认安装路径为<code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend</span></code>。安装后确认当前用户有权限访问昇腾AI处理器配套软件包的安装路径，若无权限，需要root用户将当前用户添加到<code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend</span></code>所在的用户组。</p></li>
<li><p>安装昇腾AI处理器配套软件所包含的whl包。如果之前已经安装过昇腾AI处理器配套软件包，需要先使用如下命令卸载对应的whl包。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>uninstall<span class="w"> </span>te<span class="w"> </span>topi<span class="w"> </span>-y
</pre></div>
</div>
<p>默认安装路径使用以下指令安装。如果安装路径不是默认路径，需要将命令中的路径替换为安装路径。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>/usr/local/Ascend/ascend-toolkit/latest/lib64/topi-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl
pip<span class="w"> </span>install<span class="w"> </span>/usr/local/Ascend/ascend-toolkit/latest/lib64/te-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="配置环境变量">
<h3>配置环境变量<a class="headerlink" href="#配置环境变量" title="Permalink to this headline">¶</a></h3>
<p>安装好Ascend软件包之后，需要导出Runtime相关环境变量，下述命令中<code class="docutils literal notranslate"><span class="pre">LOCAL_ASCEND=/usr/local/Ascend</span></code>的<code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend</span></code>表示配套软件包的安装路径，需注意将其改为配套软件包的实际安装路径。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># control log level. 0-EBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>

<span class="c1"># Conda environmental options</span>
<span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend<span class="w"> </span><span class="c1"># the root directory of run package</span>

<span class="c1"># lib libraries that the run package depends on</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe/op_tiling:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># Environment variables that must be configured</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TBE_IMPL_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe<span class="w">            </span><span class="c1"># TBE operator implementation tool path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp<span class="w">                                       </span><span class="c1"># OPP path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/compiler/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span><span class="w">                  </span><span class="c1"># TBE operator compilation tool path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">TBE_IMPL_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span><span class="w">                                                       </span><span class="c1"># Python library that TBE implementation depends on</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="执行converter工具">
<h2>执行converter工具<a class="headerlink" href="#执行converter工具" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite提供离线转换模型功能的工具，将多种类型的模型（Caffe、ONNX、TensorFlow、MindIR）转换为可在Ascend硬件上推理的模型。
首先，通过转换工具转换成的<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型；然后，使用转换工具配套的Runtime推理框架执行推理，具体流程如下：</p>
<ol>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/downloads.html">下载</a>Ascend专用converter工具，当前仅支持Linux</p></li>
<li><p>解压下载的包</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-x64.tar.gz
</pre></div>
</div>
<p>{version}是发布包的版本号。</p>
</li>
<li><p>将转换工具需要的动态链接库加入环境变量LD_LIBRARY_PATH</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH}是解压得到的文件夹路径。</p>
</li>
<li><p>进入转换目录</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>配置configFile(可选)</p>
<p>用户可以通过此选项配置用于转模型时的Ascend Option选项， 配置文件采用INI的风格，针对Ascend场景，可配置的参数为[acl_option_cfg_param]，参数的详细介绍如下表1所示。</p>
</li>
<li><p>执行converter，生成Ascend<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>${model_name}为模型文件名称，运行后的结果显示为：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>用户若想了解converter_lite转换工具的相关参数，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/converter_tool.html#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E">参数说明</a>。</p>
<p>说明：当原始模型输入shape不确定时，converter工具转换模型时要指定inputShape，同时configFile配置acl_option_cfg_param中input_shape_vector参数，取值相同，命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span><span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;input:1,64,64,1&quot;</span><span class="w"> </span>--configFile<span class="o">=</span><span class="s2">&quot;./config.txt&quot;</span>
</pre></div>
</div>
<p>其中，config.txt内容如下:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">acl_option_cfg_param</span><span class="p">]</span>
<span class="n">input_shape_vector</span><span class="o">=</span><span class="s">&quot;[1,64,64,1]&quot;</span>
</pre></div>
</div>
</li>
</ol>
<p>表1：配置[acl_option_cfg_param]参数</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>参数</th>
<th>属性</th>
<th>功能描述</th>
<th>参数类型</th>
<th>取值说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>input_format</code></td>
<td>可选</td>
<td>指定模型输入format。</td>
<td>String</td>
<td>可选有<code>"NCHW"</code>，<code>"NHWC"</code></td>
</tr>
<tr>
<td><code>input_shape_vector</code></td>
<td>可选</td>
<td>指定模型输入Shape， 按模型输入次序排列，用<code>；</code>隔开。</td>
<td>String</td>
<td>例如: <code>"[1,2,3,4];[4,3,2,1]"</code></td>
</tr>
<tr>
<td><code>precision_mode</code></td>
<td>可选</td>
<td>配置模型精度模式。</td>
<td>String</td>
<td>可选有<code>"force_fp16"</code>，<code>"allow_fp32_to_fp16"</code>，<code>"must_keep_origin_dtype"</code>或者<code>"allow_mix_precision"</code>，默认为<code>"force_fp16"</code></td>
</tr>
<tr>
<td><code>op_select_impl_mode</code></td>
<td>可选</td>
<td>配置算子选择模式。</td>
<td>String</td>
<td>可选有<code>"high_performance"</code>和<code>"high_precision"</code>，默认为<code>"high_performance"</code></td>
</tr>
<tr>
<td><code>dynamic_batch_size</code></td>
<td>可选</td>
<td>指定<a href="#动态batch-size">动态BatchSize</a>参数。</td>
<td>String</td>
<td><code>"2,4"</code></td>
</tr>
<tr>
<td><code>dynamic_image_size</code></td>
<td>可选</td>
<td>指定<a href="#动态分辨率">动态分辨率</a>参数。</td>
<td>String</td>
<td><code>"96,96;32,32"</code></td>
</tr>
<tr>
<td><code>fusion_switch_config_file_path</code></td>
<td>可选</td>
<td>配置<a href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/51RC2alpha007/infacldevg/atctool/atlasatc_16_0078.html">融合规则开关配置</a>文件路径及文件名。</td>
<td>String</td>
<td>-</td>
</tr>
<tr>
<td><code>insert_op_config_file_path</code></td>
<td>可选</td>
<td>模型插入<a href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/inferapplicationdev/atctool/atctool_0018.html">AIPP</a>算子</td>
<td>String</td>
<td><a href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/inferapplicationdev/atctool/atctool_0018.html">AIPP</a>配置文件路径</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="推理工具runtime">
<h2>推理工具runtime<a class="headerlink" href="#推理工具runtime" title="Permalink to this headline">¶</a></h2>
<p>converter得到转换模型后，使用配套的Runtime推理框架执行推理， 有关使用Runtime执行推理详情见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/runtime_cpp.html">使用Runtime执行推理（C++）</a>。</p>
</div>
<div class="section" id="执行benchmark">
<h2>执行benchmark<a class="headerlink" href="#执行benchmark" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite提供benchmark基准测试工具，它可以对MindSpore Lite模型前向推理的执行耗时进行定量分析（性能），还可以通过指定模型输出进行可对比的误差分析（精度）。
关于推理工具的一般说明，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/benchmark_tool.html">benchmark</a>。</p>
<ul>
<li><p>测性能</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--device<span class="o">=</span>Ascend310<span class="w"> </span>--modelFile<span class="o">=</span>./models/test_benchmark.ms<span class="w"> </span>--timeProfiling<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
</li>
<li><p>测精度</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--device<span class="o">=</span>Ascend310<span class="w"> </span>--modelFile<span class="o">=</span>./models/test_benchmark.ms<span class="w"> </span>--inDataFile<span class="o">=</span>./input/test_benchmark.bin<span class="w"> </span>--inputShapes<span class="o">=</span><span class="m">1</span>,32,32,1<span class="w"> </span>--accuracyThreshold<span class="o">=</span><span class="m">3</span><span class="w"> </span>--benchmarkDataFile<span class="o">=</span>./output/test_benchmark.out
</pre></div>
</div>
<p>有关环境变量设置，将<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>（目录为<code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-{os}-{arch}/runtime/lib</span></code>）的<code class="docutils literal notranslate"><span class="pre">so</span></code>库所在的目录加入<code class="docutils literal notranslate"><span class="pre">${LD_LIBRARY_PATH}</span></code>。</p>
</li>
</ul>
</div>
<div class="section" id="高级特性">
<h2>高级特性<a class="headerlink" href="#高级特性" title="Permalink to this headline">¶</a></h2>
<div class="section" id="动态shape特性">
<h3>动态shape特性<a class="headerlink" href="#动态shape特性" title="Permalink to this headline">¶</a></h3>
<p>在某些推理场景，如检测出目标后再执行目标识别网络，由于目标个数不固定导致目标识别网络输入BatchSize不固定。如果每次推理都按照最大的BatchSize或最大分辨率进行计算，会造成计算资源浪费。因此，推理需要支持动态BatchSize和动态分辨率的场景，Lite在Ascend310上推理支持动态BatchSize和动态分辨率场景，在convert阶段通过congFile配置[acl_option_cfg_param]动态参数，转成<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型，推理时使用model的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/runtime_cpp.html#%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6resize">resize</a>功能，改变输入shape。</p>
<div class="section" id="动态batch-size">
<h4>动态Batch size<a class="headerlink" href="#动态batch-size" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p>参数名</p>
<p>dynamic_batch_size</p>
</li>
<li><p>功能</p>
<p>设置动态batch档位参数，适用于执行推理时，每次处理图片数量不固定的场景，该参数需要与input_shape_vector配合使用，不能与dynamic_image_size同时使用。</p>
</li>
<li><p>取值</p>
<p>最多支持100档配置，每一档通过英文逗号分隔，每个档位数值限制为：[1~2048]。 例如配置文件中参数配置如下：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>[acl_option_cfg_param]
input_shape_vector=&quot;[-1,32,32,4]&quot;
dynamic_batch_size=&quot;2,4&quot;
其中，input_shape中的&quot;-1&quot;表示设置动态batch，档位可取值为&quot;2,4&quot;，即支持档位0: [2,32,32,4]，档位1: [4,32,32,4].
</pre></div>
</div>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;input:4,32,32,4&quot;</span><span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>说明：使能动态BatchSize时，需要指定inputShape，值为最大档位对应的shape，即上节中档位1的值；同时通过configFile配置[acl_option_cfg_param]动态batch size，即上节示例中配置内容。</p>
</li>
<li><p>推理</p>
<p>使能动态BatchSize，进行模型推理时，输入shape只能选择converter时设置的档位值，想切换到其他档位对应的输入shape，使用model <a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/runtime_cpp.html#%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6resize">resize</a>功能。</p>
</li>
<li><p>注意事项</p>
<p>1）若用户执行推理业务时，每次处理的图片数量不固定，则可以通过配置该参数来动态分配每次处理的图片数量。例如用户执行推理业务时需要每次处理2张，4张，8张图片，则可以配置为2,4,8，申请了档位后，模型推理时会根据实际档位申请内存。<br/>
2）如果用户设置的档位数值过大或档位过多，可能会导致模型编译失败，此时建议用户减少档位或调低档位数值。<br/>
3）如果用户设置的档位数值过大或档位过多，在运行环境执行推理时，建议执行swapoff -a命令关闭swap交换区间作为内存的功能，防止出现由于内存不足，将swap交换空间作为内存继续调用，导致运行环境异常缓慢的情况。<br/></p>
</li>
</ul>
</div>
<div class="section" id="动态分辨率">
<h4>动态分辨率<a class="headerlink" href="#动态分辨率" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p>参数名</p>
<p>dynamic_image_size</p>
</li>
<li><p>功能</p>
<p>设置输入图片的动态分辨率参数。适用于执行推理时，每次处理图片宽和高不固定的场景，该参数需要与input_shape_vector配合使用，不能与dynamic_batch_size同时使用。</p>
</li>
<li><p>取值</p>
<p>最多支持100档配置，每一档通过英文分号分隔。 例如： “imagesize1_height,imagesize1_width;imagesize2_height,imagesize2_width”，指定的参数必须放在双引号中，每一组参数中间使用英文分号分隔。 例如配置文件中参数配置如下：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>[acl_option_cfg_param]
input_format=&quot;NCHW&quot;
input_shape_vector=&quot;[2,3,-1,-1]&quot;
dynamic_image_size=&quot;64,64;96,96&quot;
其中，input_shape中的&quot;-1&quot;表示设置动态分辨率，即支持档位0: [2,3,64,64]，档位1: [2,3,96,96].
</pre></div>
</div>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;input:2,3,96,96&quot;</span><span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>说明： 使能动态BatchSize时， 需要指定inputShape，值为最大档位对应的shape，即上节中档位1的值；同时通过configFile配置[acl_option_cfg_param]动态分辨率，即上节示例中配置内容。</p>
</li>
<li><p>推理</p>
<p>使能动态分辨率， 进行模型推理时，输入shape只能选择converter时设置的档位值，想切换到其他档位对应的输入shape，使用model的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/use/runtime_cpp.html#%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6resize">resize</a>功能。</p>
</li>
<li><p>注意事项</p>
<p>1）如果用户设置的分辨率数值过大或档位过多，可能会导致模型编译失败，此时建议用户减少档位或调低档位数值。<br/>
2）如果用户设置了动态分辨率，实际推理时，使用的数据集图片大小需要与具体使用的分辨率相匹配。<br/>
3）如果用户设置的分辨率数值过大或档位过多，在运行环境执行推理时，建议执行swapoff -a命令关闭swap交换区间作为内存的功能，防止出现由于内存不足，将swap交换空间作为内存继续调用，导致运行环境异常缓慢的情况。<br/></p>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="算子支持">
<h2>算子支持<a class="headerlink" href="#算子支持" title="Permalink to this headline">¶</a></h2>
<p>算子支持见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/master/operator_list_lite.html">Lite 算子支持</a>。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../device_train_example.html" class="btn btn-neutral float-right" title="端侧训练样例" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="tensorrt_info.html" class="btn btn-neutral float-left" title="集成TensorRT使用说明" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>