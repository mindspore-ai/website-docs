<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>集成NNIE使用说明 &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="集成TensorRT使用说明" href="tensorrt_info.html" />
    <link rel="prev" title="集成NPU使用说明" href="npu_info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">优化模型(训练后量化)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在轻量和小型系统上执行推理</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="asic.html">专用芯片集成说明</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_info.html">集成NPU使用说明</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">集成NNIE使用说明</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">目录结构</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#converter">模型转换工具converter目录结构说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#runtime">模型推理工具runtime目录结构说明</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id2">工具使用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">转换工具converter</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">推理工具runtime</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id10">集成使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#svp">SVP工具链相关功能支持及注意事项（高级选项）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#formatnhwc">板端运行输入Format须是NHWC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-list">image_list说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-type">image_type限制</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-listroi-coordinate-file">image_list和roi_coordinate_file个数说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prototxt-cpu">prototxt中节点名_cpu后缀支持</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prototxtcustom">prototxt中Custom算子支持</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prototxttop-report">prototxt中top域的_report后缀支持</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inplace">inplace机制</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batchstep">多图片batch运行及多step运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">节点名称的变动</a></li>
<li class="toctree-l4"><a class="reference internal" href="#proposal">proposal算子使用说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">分段机制说明及8段限制</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tensorrt_info.html">集成TensorRT使用说明</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库剪裁工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="asic.html">专用芯片集成说明</a> &raquo;</li>
      <li>集成NNIE使用说明</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/nnie.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nnie">
<h1>集成NNIE使用说明<a class="headerlink" href="#nnie" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">NNIE</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">环境准备</span></code> <code class="docutils literal notranslate"><span class="pre">中级</span></code> <code class="docutils literal notranslate"><span class="pre">高级</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/lite/docs/source_zh_cn/use/nnie.md"><img alt="查看源文件" src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png" /></a></p>
<section id="id1">
<h2>目录结构<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<section id="converter">
<h3>模型转换工具converter目录结构说明<a class="headerlink" href="#converter" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-runtime-linux-x64
└── tools
    └── converter
        └── providers
            └── Hi3516D                # 嵌入式板型号
                ├── libmslite_nnie_converter.so        # 集成NNIE转换的动态库
                ├── libmslite_nnie_data_process.so     # 处理NNIE输入数据的动态库
                ├── libnnie_mapper.so        # 构建NNIE二进制文件的动态库
                └── third_party       # NNIE依赖的三方动态库
                    ├── opencv-4.2.0
                    │   └── libopencv_xxx.so
                    └── protobuf-3.9.0
                        ├── libprotobuf.so
                        └── libprotoc.so
</pre></div>
</div>
<p>上述是NNIE的集成目录结构，转换工具converter的其余目录结构详情，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/converter_tool.html">模型转换工具</a>。</p>
</section>
<section id="runtime">
<h3>模型推理工具runtime目录结构说明<a class="headerlink" href="#runtime" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-aarch32
└── providers
    └── Hi3516D        # 嵌入式板型号
        └── libmslite_nnie.so  # 集成NNIE的动态库
        └── libmslite_proposal.so  # 集成proposal的样例动态库
</pre></div>
</div>
<p>上述是NNIE的集成目录结构，推理工具runtime的其余目录结构详情，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/build.html#id4">目录结构</a>。</p>
</section>
</section>
<section id="id2">
<h2>工具使用<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<section id="id3">
<h3>转换工具converter<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<section id="id4">
<h4>概述<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h4>
<p>MindSpore Lite提供离线转换模型功能的工具，将多种类型的模型（当前只支持Caffe）转换为可使用NNIE硬件加速推理的板端专属模型，可运行在Hi3516板上。
通过转换工具转换成的NNIE<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型，仅支持在关联的嵌入式板上，使用转换工具配套的Runtime推理框架执行推理。关于转换工具的更一般说明，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/converter_tool.html">推理模型转换</a>。</p>
</section>
<section id="id5">
<h4>环境准备<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h4>
<p>使用MindSpore Lite模型转换工具，需要进行如下环境准备工作。</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/downloads.html">下载</a>NNIE专用converter工具，当前仅支持Linux</p></li>
<li><p>解压下载的包</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-x64.tar.gz
</pre></div>
</div>
<p>{version}是发布包的版本号。</p>
</li>
<li><p>将转换工具需要的动态链接库加入环境变量LD_LIBRARY_PATH</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/runtime/lib:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/third_party/opencv-4.2.0:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/third_party/protobuf-3.9.0
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH}是解压得到的文件夹路径。</p>
</li>
<li><p>使能NNIE模型转换</p>
<p>NNIE模型可以使用NNIE硬件以提高模型运行速度，用户需配置以下两点，以使能NNIE模型转换。</p>
<ul>
<li><p>NNIE转换配置文件</p>
<p>MindSpore Lite所需的NNIE转换配置文件，需参照海思提供的《HiSVP 开发指南》中表格<code class="docutils literal notranslate"><span class="pre">nnie_mapper</span> <span class="pre">配置选项说明</span></code>来进行配置，以nnie.cfg指代此配置文件：</p>
<p>nnie.cfg文件的示例参考如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[net_type] 0
[image_list] ./input_nchw.txt
[image_type] 0
[norm_type] 0
[mean_file] null
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">input_nchw.txt</span></code>为被转换CAFFE模型的浮点文本格式的输入数据，详情请参照《HiSVP 开发指南》中的<code class="docutils literal notranslate"><span class="pre">image_list</span></code>说明。在配置文件中，配置选项caffemodel_file、prototxt_file、is_simulation、instructions_name不可配置，其他选项功能可正常配置。</p>
</li>
<li><p>NNIE动态库路径配置（可选）</p>
<p>在NNIE转换时，通过参数configFile传入配置文件(<code class="docutils literal notranslate"><span class="pre">--configFile=./converter.cfg</span></code>)以使能NNIE转换，配置方式请参见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/converter_tool.html#id5">推理模型转换的参数说明</a>。在配置文件中，保存着NNIE动态库的相对路径，用户可手动修改该路径，默认不需修改即可。</p>
</li>
</ul>
</li>
</ol>
</section>
<section id="id6">
<h4>执行converter<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>进入转换目录</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>配置环境变量（可选）</p>
<p>若已执行第1步，进入到转换目录，则此步无需配置，默认值将使能。若用户未进入转换目录，则需在环境变量中声明转换工具所依赖的so和benchmark二进制执行程序的路径，如下所示：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_MAPPER_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/libnnie_mapper.so
<span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_DATA_PROCESS_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/libmslite_nnie_data_process.so
<span class="nb">export</span><span class="w"> </span><span class="nv">BENCHMARK_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/benchmark
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH}是下载得到的包解压后的路径。</p>
</li>
<li><p>将nnie.cfg拷贝到转换目录并设置如下环境变量</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_CONFIG_PATH</span><span class="o">=</span>./nnie.cfg
</pre></div>
</div>
<p>如果用户实际的配置文件就叫nnie.cfg，且与converter_lite在同级路径上，则可不用配置。</p>
</li>
<li><p>执行converter，生成NNIE<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.prototxt<span class="w"> </span>--weightFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.caffemodel<span class="w"> </span>--configFile<span class="o">=</span>./converter.cfg<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>${model_name}为模型文件名称，运行后的结果显示为：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>用户若想了解converter_lite转换工具的相关参数，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/converter_tool.html#id4">参数说明</a>。</p>
</li>
</ol>
</section>
</section>
<section id="id7">
<h3>推理工具runtime<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<section id="id8">
<h4>概述<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h4>
<p>得到转换模型后，可在关联的嵌入式板上，使用板子配套的Runtime推理框架执行推理。MindSpore Lite提供benchmark基准测试工具，它可以对MindSpore Lite模型前向推理的执行耗时进行定量分析（性能），还可以通过指定模型输出进行可对比的误差分析（精度）。
关于推理工具的一般说明，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/benchmark_tool.html">benchmark</a>。</p>
</section>
<section id="id9">
<h4>环境准备<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h4>
<p>以下为示例用法，用户可根据实际情况进行等价操作。</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/downloads.html">下载</a>NNIE专用模型推理工具，当前仅支持Hi3516D</p></li>
<li><p>解压下载的包</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-aarch32.tar.gz
</pre></div>
</div>
<p>{version}是发布包的版本号。</p>
</li>
<li><p>在Hi3516D板上创建存放目录</p>
<p>登陆板端，创建工作目录</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>/user/mindspore<span class="w">          </span><span class="c1"># 存放benchmark执行文件及模型</span>
mkdir<span class="w"> </span>/user/mindspore/lib<span class="w">      </span><span class="c1"># 存放依赖库文件</span>
</pre></div>
</div>
</li>
<li><p>传输文件</p>
<p>向Hi3516D板端传输benchmark工具、模型、so库。其中libmslite_proposal.so为MindSpore Lite提供的proposal算子实现样例so，若用户模型里含有自定义的proposal算子，用户需参考<a class="reference internal" href="#proposal"><span class="std std-doc">proposal算子使用说明</span></a>生成libnnie_proposal.so替换该so文件，以进行正确推理。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>libmindspore-lite.so<span class="w"> </span>libmslite_nnie.so<span class="w"> </span>libmslite_proposal.so<span class="w"> </span>root@<span class="si">${</span><span class="nv">device_ip</span><span class="si">}</span>:/user/mindspore/lib
scp<span class="w"> </span>benchmark<span class="w"> </span><span class="si">${</span><span class="nv">model_path</span><span class="si">}</span><span class="w"> </span>root@<span class="si">${</span><span class="nv">device_ip</span><span class="si">}</span>:/user/mindspore
</pre></div>
</div>
<p>${model_path}为转换后ms模型文件路径</p>
</li>
<li><p>设置动态库路径</p>
<p>NNIE模型的推理，还依赖海思提供NNIE相关板端动态库，包括：libnnie.so、libmpi.so、libVoiceEngine.so、libupvqe.so、libdnvqe.so。</p>
<p>用户需在板端保存这些so，并将路径传递给LD_LIBRARY_PATH环境变量。
在示例中，这些so位于/usr/lib下，用户需按实际情况进行配置：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/user/mindspore/lib:/usr/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
</li>
<li><p>设置配置项（可选）</p>
<p>若用户模型含有proposal算子，需根据proposal算子实现情况，配置MAX_ROI_NUM环境变量：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MAX_ROI_NUM</span><span class="o">=</span><span class="m">300</span><span class="w">    </span><span class="c1"># 单张图片支持roi区域的最大数量，范围：正整数，默认值：300。</span>
</pre></div>
</div>
<p>若用户模型为循环或lstm网络，需根据实际网络运行情况，配置TIME_STEP环境变量，其他要求<a class="reference internal" href="#batchstep"><span class="std std-doc">见多图片batch运行及多step运行</span></a>：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">TIME_STEP</span><span class="o">=</span><span class="m">1</span><span class="w">        </span><span class="c1"># 循环或lstm网络运行的step数，范围：正整数，默认值：1。</span>
</pre></div>
</div>
<p>若板端含有多个NNIE硬件，用户可通过CORE_IDS环境变量指定模型运行在哪个NNIE设备上，
若模型被分段（用户可用netron打开模型，观察模型被分段情况），可依序分别配置每个分段运行在哪个设备上，未被配置分段运行在最后被配置的NNIE设备上：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">CORE_IDS</span><span class="o">=</span><span class="m">0</span><span class="w">         </span><span class="c1"># NNIE运行内核id，支持模型分段独立配置，使用逗号分隔(如export CORE_IDS=1,1)，默认值：0</span>
</pre></div>
</div>
</li>
<li><p>构建图片输入（可选）</p>
<p>若converter导出模型时喂给mapper的校正集用的是图片，则传递给benchmark的输入需是int8的输入数据，即需要把图片转成int8传递给benchmark。
这里采用python给出转换示范样例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="k">def</span> <span class="nf">usage</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;usage:</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;example: python generate_input_bin.py xxx.img BGR 224 224</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[1]: origin image path</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[2]: RGB_order[BGR, RGB], should be same as nnie mapper config file&#39;s [RGB_order], default is BGR</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[3]: input_h</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[4]: input_w&quot;</span>
          <span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">argvs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;-h&quot;</span><span class="p">:</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="n">img_path</span> <span class="o">=</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">rgb_order</span> <span class="o">=</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">input_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">input_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rgb_order</span> <span class="o">==</span> <span class="s2">&quot;RGB&quot;</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">img_hwc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">input_w</span><span class="p">,</span> <span class="n">input_h</span><span class="p">))</span>
    <span class="n">outfile_name</span> <span class="o">=</span> <span class="s2">&quot;1_</span><span class="si">%s</span><span class="s2">_</span><span class="si">%s</span><span class="s2">_3_nhwc.bin&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">img_hwc</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">outfile_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated &quot;</span> <span class="o">+</span> <span class="n">outfile_name</span> <span class="o">+</span> <span class="s2">&quot; file success in current dir.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input argument is invalid.&quot;</span><span class="p">)</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">main</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="benchmark">
<h4>执行benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline"></a></h4>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd /user/mindspore
./benchmark --modelFile=${model_path}
</pre></div>
</div>
<p>${model_path}为转换后ms模型文件路径</p>
<p>执行该命令，会生成模型的随机输入，并执行前向推理。有关benchmark的其他使用详情，如耗时分析与推理误差分析等，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/benchmark_tool.html">Benchmark使用</a>。</p>
<p>有关模型的输入数据格式要求，见<span class="xref myst">SVP工具链相关功能支持及注意事项（可选）</span>。</p>
</section>
</section>
</section>
<section id="id10">
<h2>集成使用<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h2>
<p>有关集成使用详情，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/runtime_cpp.html">集成c++接口</a>。</p>
</section>
<section id="svp">
<h2>SVP工具链相关功能支持及注意事项（高级选项）<a class="headerlink" href="#svp" title="Permalink to this headline"></a></h2>
<p>在模型转换时，由NNIE_CONFIG_PATH环境变量声明的nnie.cfg文件，提供原先SVP工具链相关功能，支持除caffemodel_file、prototxt_file、is_simulation、instructions_name外其他字段的配置，相关注意实现如下：</p>
<section id="formatnhwc">
<h3>板端运行输入Format须是NHWC<a class="headerlink" href="#formatnhwc" title="Permalink to this headline"></a></h3>
<p>转换后的<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型只接受NHWC格式的数据输入，若image_type被声明为0，则接收NHWC格式的float32数据，若image_type被声明为1，则接收NHWC的uint8数据输入。</p>
</section>
<section id="image-list">
<h3>image_list说明<a class="headerlink" href="#image-list" title="Permalink to this headline"></a></h3>
<p>nnie.cfg中image_list字段含义与原先不变，当image_type声明为0时，按行提供chw格式数据，无论原先模型是否是nchw输入。</p>
</section>
<section id="image-type">
<h3>image_type限制<a class="headerlink" href="#image-type" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite不支持image_type为3和5时的网络输入，用户设为0或1。</p>
</section>
<section id="image-listroi-coordinate-file">
<h3>image_list和roi_coordinate_file个数说明<a class="headerlink" href="#image-listroi-coordinate-file" title="Permalink to this headline"></a></h3>
<p>用户只需提供与模型输入个数相同数量的image_list，若模型中含有ROI Pooling或PSROI Pooling层，用户需提供roi_coordinate_file，数量与顺序和prototxt内的ROI Pooling或PSROI Pooling层的个数与顺序对应。</p>
</section>
<section id="prototxt-cpu">
<h3>prototxt中节点名_cpu后缀支持<a class="headerlink" href="#prototxt-cpu" title="Permalink to this headline"></a></h3>
<p>SVP工具链中，可通过在prototxt文件的节点名后使用_cpu后缀来，声明cpu自定义算子。mindspore Lite中忽略_cpu后缀，不做支持。用户若想重定义MindSpore Lite已有的算子实现或新增新的算子，可通过自定义算子的方式进行注册。</p>
</section>
<section id="prototxtcustom">
<h3>prototxt中Custom算子支持<a class="headerlink" href="#prototxtcustom" title="Permalink to this headline"></a></h3>
<p>SVP工具链中，通过在prototxt中声明custom层，实现推理时分段，并由用户实现cpu代码。在mindspore Lite中，用户需在Custom层中增加op_type属性，并通过自定义算子的方式进行在线推理代码的注册。</p>
<p>Custom层的修改样例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>layer {
  name: &quot;custom1&quot;
  type: &quot;Custom&quot;
  bottom: &quot;conv1&quot;
  top: &quot;custom1_1&quot;
  custom_param {
    type: &quot;MY_CUSTOM&quot;
    shape {
        dim: 1
        dim: 256
        dim: 64
        dim: 64
    }
}
}
</pre></div>
</div>
<p>在该示例中定义了一个MY_CUSTOM类型的自定义算子，推理时用户需注册一个类型为MY_CUSTOM的自定义算子。</p>
</section>
<section id="prototxttop-report">
<h3>prototxt中top域的_report后缀支持<a class="headerlink" href="#prototxttop-report" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite在转换NNIE模型时，会将大部分的算子融合为NNIE运行的二进制文件，用户无法观察到中间算子的输出，通过在top域上添加”_report“后缀，转换构图时会将中间算子的输出添加到融合后的层输出中，若原先该算子便有输出（未被融合），则维持不变。</p>
<p>在推理运行时，用户可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/runtime_cpp.html#id15">回调运行</a>得到中间算子输出。</p>
<p>MindSpore Lite解析_report的相应规则，及与<a class="reference internal" href="#inplace"><span class="std std-doc">inplace机制</span></a>的冲突解决，参照《HiSVP 开发指南》中的定义说明。</p>
</section>
<section id="inplace">
<h3>inplace机制<a class="headerlink" href="#inplace" title="Permalink to this headline"></a></h3>
<p>使用Inplace层写法，可运行芯片高效模式。转换工具默认将Prototxt中符合芯片支持Inplace层的所有层进行改写，用户如需关闭该功能，可通过如下环境声明：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_DISABLE_INPLACE_FUSION</span><span class="o">=</span>off<span class="w">         </span><span class="c1"># 设置为on或未设置时，使能Inplace自动改写</span>
</pre></div>
</div>
<p>当自动改写被关闭时，若需对个别层使能芯片高效模式，可手动改写Prototxt里面的相应层。</p>
</section>
<section id="batchstep">
<h3>多图片batch运行及多step运行<a class="headerlink" href="#batchstep" title="Permalink to this headline"></a></h3>
<p>用户若需同时前向推理多个输入数据（多个图片），可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/runtime_cpp.html#resize">输入维度Resize</a>将模型输入的第一维resize为输入数据个数。NNIE模型只支持对第一个维度（’n’维）进行resize，其他维度（’hwc’）不可变。</p>
<p>对于循环或lstm网络，用户需根据step值，配置TIME_STEP环境变量，同时resize模型输入。
设一次同时前向推理的数据的个数为input_num，对于序列数据输入的节点resize为input_num * step，非序列数据输入的节点resize为input_num。</p>
<p>含有proposal算子的模型，不支持batch运行，不支持resize操作。</p>
</section>
<section id="id11">
<h3>节点名称的变动<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h3>
<p>模型转换为NNIE模型后，各节点名称可能发生变化，用户可通过netron打开模型，得到变化后的节点名。</p>
</section>
<section id="proposal">
<h3>proposal算子使用说明<a class="headerlink" href="#proposal" title="Permalink to this headline"></a></h3>
<p>mindspore Lite提供Proposal算子的样例代码，在该样例中，以自定义算子注册的方式实现了proposal算子及该算子infer shape的注册。用户可将其修改为自身模型匹配的实现后，进行[集成使用]（#集成使用）。</p>
<blockquote>
<div><p>你可以在这里下载完整的样例代码：</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/nnie_proposal">https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/nnie_proposal</a></p>
</div></blockquote>
</section>
<section id="id12">
<h3>分段机制说明及8段限制<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h3>
<p>由于NNIE芯片支持的算子限制，在含有NNIE芯片不支持的算子时，需将模型分段为可支持层与不可支持层。
板端芯片支持最多8段的可支持层，当分段后的可支持层数量大于8段时，模型将无法运行，用户可通过netron观察Custom算子（其属性中含有type:NNIE），得到转换后的NNIE支持层数量。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="npu_info.html" class="btn btn-neutral float-left" title="集成NPU使用说明" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tensorrt_info.html" class="btn btn-neutral float-right" title="集成TensorRT使用说明" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>