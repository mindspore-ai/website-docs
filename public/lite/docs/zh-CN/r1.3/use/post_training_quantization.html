<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>优化模型(训练后量化) &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="预处理数据" href="data_preprocessing.html" />
    <link rel="prev" title="推理模型转换" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">优化模型(训练后量化)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">权重量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">参数说明</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">使用步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">部分模型精度结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id7">全量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">参数说明</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">使用步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">部分模型精度结果</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在轻量和小型系统上执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库剪裁工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>优化模型(训练后量化)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>优化模型(训练后量化)<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">模型转换</span></code> <code class="docutils literal notranslate"><span class="pre">模型调优</span></code> <code class="docutils literal notranslate"><span class="pre">中级</span></code> <code class="docutils literal notranslate"><span class="pre">高级</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/lite/docs/source_zh_cn/use/post_training_quantization.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>对于已经训练好的<code class="docutils literal notranslate"><span class="pre">float32</span></code>模型，通过训练后量化将其转为<code class="docutils literal notranslate"><span class="pre">int8</span></code>，不仅能减小模型大小，而且能显著提高推理性能。在MindSpore Lite中，这部分功能集成在模型转换工具<code class="docutils literal notranslate"><span class="pre">conveter_lite</span></code>内，通过增加命令行参数，便能够转换得到量化后模型。</p>
<p>MindSpore Lite训练后量化分为两类：</p>
<ol class="arabic simple">
<li><p>权重量化：对模型的权值进行量化，仅压缩模型大小，推理时仍然执行<code class="docutils literal notranslate"><span class="pre">float32</span></code>推理；</p></li>
<li><p>全量化：对模型的权值、激活值等统一进行量化，推理时执行<code class="docutils literal notranslate"><span class="pre">int</span></code>运算，能提升模型推理速度、降低功耗。</p></li>
</ol>
<p>训练后量化在两种情况下所需的数据类型和参数设定不同，但均可通过转换工具设定。有关转换工具<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>的使用方法可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/converter_tool.html">转换为MindSpore Lite模型</a>。在此基础之上进行配置，启用训练后量化。</p>
</section>
<section id="id3">
<h2>权重量化<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>支持1~16之间的任意比特量化，量化比特数越低，模型压缩率越大，但是精度损失通常也比较大。可以结合使用<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/benchmark_tool.html">Benchmark工具</a>进行精度评估，确定合适的量化比特数；通常平均相对误差(accuracyThreshold)满足4%以内，精度误差是比较小的。下面对权重量化的使用方式和效果进行阐述。</p>
<section id="id4">
<h3>参数说明<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>权重量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--bitNum<span class="o">=</span>BitNumValue<span class="w"> </span>--quantWeightSize<span class="o">=</span>ConvWeightQuantSizeThresholdValue<span class="w"> </span>--quantWeightChannel<span class="o">=</span>ConvWeightQuantChannelThresholdValue
</pre></div>
</div>
<p>下面对此命令的量化相关参数进行说明：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantType=&lt;QUANTTYPE&gt;</span></code></p></td>
<td><p>必选</p></td>
<td><p>设置为WeightQuant，启用权重量化</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>必须设置为WeightQuant</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--bitNum=&lt;BITNUM&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定权重量化的比特数，目前支持1bit～16bit量化</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>[1，16]</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantWeightSize=&lt;QUANTWEIGHTSIZE&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定参与权重量化的卷积核尺寸阈值，若卷积核尺寸大于该值，则对此权重进行量化；建议设置为500</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
<td><p>[0，+∞）</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--quantWeightChannel=&lt;QUANTWEIGHTCHANNEL&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定参与权重量化的卷积通道数阈值，若卷积通道数大于该值，则对此权重进行量化；建议设置为16</p></td>
<td><p>Integer</p></td>
<td><p>16</p></td>
<td><p>[0，+∞）</p></td>
</tr>
</tbody>
</table>
<p>用户可根据模型及自身需要对权重量化的参数作出调整。</p>
<blockquote>
<div><p>为保证权重量化的精度，建议<code class="docutils literal notranslate"><span class="pre">--bitNum</span></code>参数设定范围为8bit～16bit。</p>
</div></blockquote>
</section>
<section id="id5">
<h3>使用步骤<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>正确编译出<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>可执行文件。该部分可参考构建文档<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/build.html">编译MindSpore Lite</a>，获得<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>工具，并配置环境变量。</p></li>
<li><p>以TensorFlow Lite模型为例，执行权重量化模型转换命令:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>Inception_v3.tflite<span class="w"> </span>--outputFile<span class="o">=</span>Inception_v3.tflite<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--bitNum<span class="o">=</span><span class="m">8</span><span class="w"> </span>--quantWeightSize<span class="o">=</span><span class="m">0</span><span class="w"> </span>--quantWeightChannel<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
</li>
<li><p>上述命令执行成功后，便可得到量化后的模型<code class="docutils literal notranslate"><span class="pre">Inception_v3.tflite.ms</span></code>，量化后的模型大小通常会下降到FP32模型的1/4。</p></li>
</ol>
</section>
<section id="id6">
<h3>部分模型精度结果<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>测试数据集</p></th>
<th class="head"><p>FP32模型精度</p></th>
<th class="head"><p>权重量化精度（8bit）</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>77.60%</p></td>
<td><p>77.53%</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>70.96%</p></td>
<td><p>70.56%</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>71.56%</p></td>
<td><p>71.53%</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得。</p>
</div></blockquote>
</section>
</section>
<section id="id7">
<h2>全量化<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<p>针对需要提升模型运行速度、降低模型运行功耗的场景，可以使用训练后全量化功能。下面对全量化的使用方式和效果进行阐述。</p>
<section id="id8">
<h3>参数说明<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<p>全量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>PostTraining<span class="w"> </span>--bitNum<span class="o">=</span><span class="m">8</span><span class="w"> </span>--configFile<span class="o">=</span>config.cfg
</pre></div>
</div>
<p>下面对此命令的量化相关参数进行说明：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantType=&lt;QUANTTYPE&gt;</span></code></p></td>
<td><p>必选</p></td>
<td><p>设置为PostTraining，启用全量化</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>必须设置为PostTraining</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--configFile=&lt;CONFIGFILE&gt;</span></code></p></td>
<td><p>必选</p></td>
<td><p>校准数据集配置文件路径</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--bitNum=&lt;BITNUM&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定全量化的比特数，目前支持1bit～8bit量化</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>[1，8]</p></td>
</tr>
</tbody>
</table>
<p>为了计算激活值的量化参数，用户需要提供校准数据集。校准数据集最好来自真实推理场景，能表征模型的实际输入情况，数量在100个左右。<code class="docutils literal notranslate"><span class="pre">configFile</span></code>的配置方式请参见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/converter_tool.html#id5">推理模型转换的参数说明</a>。</p>
<blockquote>
<div><p>对于多输入模型，要求不同输入数据分别存放在各自不同的目录，同时各自目录中的所有文件的文件名按照字典序递增排序后，能够一一对应。例如：模型有两个输入input0、input1，校准数据集共2组（batch_count=2）；input0的对应数据存放在/dir/input0/目录，输入数据文件名为：data_1.bin、data_2.bin；input1的对应数据存放在/dir/input1/目录，输入数据文件名为：data_a.bin、data_b.bin，则认为(data_1.bin, data_a.bin)构成一组输入，（data_2.bin, data_b.bin）构成另一组输入。</p>
</div></blockquote>
</section>
<section id="id9">
<h3>使用步骤<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>正确编译出<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>可执行文件。</p></li>
<li><p>准备校准数据集，假设存放在<code class="docutils literal notranslate"><span class="pre">/dir/images</span></code>目录，编写配置文件<code class="docutils literal notranslate"><span class="pre">config.cfg</span></code>，内容如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>image_path=/dir/images
batch_count=100
method_x=MAX_MIN
thread_num=1
bias_correction=true
</pre></div>
</div>
<p>校准数据集可以选择测试数据集的子集，要求<code class="docutils literal notranslate"><span class="pre">/dir/images</span></code>目录下存放的每个文件均是预处理好的输入数据，每个文件都可以直接用于推理的输入。</p>
</li>
<li><p>以MindSpore模型为例，执行全量化的模型转换命令:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>lenet.mindir<span class="w"> </span>--outputFile<span class="o">=</span>lenet_quant<span class="w"> </span>--quantType<span class="o">=</span>PostTraining<span class="w"> </span>--configFile<span class="o">=</span>config.cfg
</pre></div>
</div>
</li>
<li><p>上述命令执行成功后，便可得到量化后的模型<code class="docutils literal notranslate"><span class="pre">lenet_quant.ms</span></code>，通常量化后的模型大小会下降到FP32模型的1/4。</p></li>
</ol>
</section>
<section id="id10">
<h3>部分模型精度结果<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>测试数据集</p></th>
<th class="head"><p>method_x</p></th>
<th class="head"><p>FP32模型精度</p></th>
<th class="head"><p>全量化精度（8bit）</p></th>
<th class="head"><p>说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>77.60%</p></td>
<td><p>77.40%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>70.96%</p></td>
<td><p>70.31%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>MAX_MIN</p></td>
<td><p>71.56%</p></td>
<td><p>71.16%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得，均设置<code class="docutils literal notranslate"><span class="pre">bias_correction=true</span></code>。</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="推理模型转换" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="预处理数据" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>