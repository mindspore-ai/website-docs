<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>在轻量和小型系统上执行推理 &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="专用芯片集成说明" href="asic.html" />
    <link rel="prev" title="使用Java接口执行推理" href="runtime_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">优化模型(训练后量化)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">在轻量和小型系统上执行推理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#codegen">获取codegen工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">codegen目录结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">codegen运行参数说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">如何使用codegen</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stm">在STM开发板上执行推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746">STM32F746编译依赖</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">STM32F746工程构建</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id7">代码工程编译</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">编译模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id9">STM32F746工程部署</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">在轻鸿蒙设备上执行推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id11">安装轻鸿蒙编译环境</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">开发板环境配置</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id13">编译模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">编写构建脚本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#benchmark">编译benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id15">执行benchmark</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id16">自定义算子</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id17">准备模型文件</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id18">执行codegen生成源码</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id19">用户实现自定义算子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id20">其它平台使用说明</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linux-x86-64">Linux_x86_64平台编译部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="#android">Android平台编译部署</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="register_kernel.html">自定义南向算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库剪裁工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>在轻量和小型系统上执行推理</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/micro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>在轻量和小型系统上执行推理<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">IoT</span></code> <code class="docutils literal notranslate"><span class="pre">C++</span></code> <code class="docutils literal notranslate"><span class="pre">模型代码生成</span></code> <code class="docutils literal notranslate"><span class="pre">推理应用</span></code> <code class="docutils literal notranslate"><span class="pre">初级</span></code> <code class="docutils literal notranslate"><span class="pre">中级</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/lite/docs/source_zh_cn/use/micro.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source.png"></a></p>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>相较于移动终端，IoT设备上系统资源有限，对ROM空间占用、运行时内存和功耗要求较高。MindSpore Lite提供代码生成工具codegen，将运行时编译、解释计算图，移至离线编译阶段。仅保留推理所必须的信息，生成极简的推理代码。codegen可对接NNACL和CMSIS算子库，支持生成可在x86/ARM64/ARM32A/ARM32M平台部署的推理代码。</p>
<p>代码生成工具codegen的使用流程如下：</p>
<ol class="arabic simple">
<li><p>通过MindSpore Lite转换工具<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/converter_tool.html">Converter</a>，将训练好的模型文件转换为<code class="docutils literal notranslate"><span class="pre">*.ms</span></code>格式；</p></li>
<li><p>通过自动代码生成工具codegen，输入<code class="docutils literal notranslate"><span class="pre">*.ms</span></code>模型自动生成源代码。</p></li>
</ol>
<p><img alt="img" src="../_images/lite_codegen.png" /></p>
</section>
<section id="codegen">
<h2>获取codegen工具<a class="headerlink" href="#codegen" title="Permalink to this headline"></a></h2>
<p>codegen是一个自动代码生成的工具，可以通过两种方式获取：</p>
<ol class="arabic simple">
<li><p>MindSpore官网下载<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/downloads.html">Release版本</a>。</p></li>
<li><p>从源码开始<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/build.html">编译构建</a>。</p></li>
</ol>
<blockquote>
<div><p>目前codegen工具仅支持在Linux x86_64下运行。</p>
</div></blockquote>
</section>
<section id="id3">
<h2>codegen目录结构<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── tools
    └── codegen # 代码生成工具
        ├── codegen          # 可执行程序
        ├── include          # 推理框架头文件
        │   ├── nnacl        # nnacl 算子头文件
        │   └── wrapper
        ├── lib
        │   └── libwrapper.a # MindSpore Lite codegen生成代码依赖的部分算子静态库
        └── third_party
            ├── include
            │   └── CMSIS    # ARM CMSIS NN 算子头文件
            └── lib
                └── libcmsis_nn.a # ARM CMSIS NN 算子静态库
</pre></div>
</div>
</section>
<section id="id4">
<h2>codegen运行参数说明<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h2>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>是否必选</p></th>
<th class="head"><p>参数说明</p></th>
<th class="head"><p>取值范围</p></th>
<th class="head"><p>默认值</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>help</p></td>
<td><p>否</p></td>
<td><p>打印使用说明信息</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>codePath</p></td>
<td><p>是</p></td>
<td><p>生成代码的路径</p></td>
<td><p>-</p></td>
<td><p>./(当前目录)</p></td>
</tr>
<tr class="row-even"><td><p>target</p></td>
<td><p>是</p></td>
<td><p>生成代码针对的平台</p></td>
<td><p>x86, ARM32M, ARM32A, ARM64</p></td>
<td><p>x86</p></td>
</tr>
<tr class="row-odd"><td><p>modelPath</p></td>
<td><p>是</p></td>
<td><p>输入模型文件路径</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>supportParallel</p></td>
<td><p>否</p></td>
<td><p>是否生成支持多线程的代码</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-odd"><td><p>debugMode</p></td>
<td><p>否</p></td>
<td><p>是否以生成调试模式的代码</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>输入模型文件，需要经过MindSpore Lite Converter工具转换成.ms格式。</p>
<p>os不支持文件系统时，debugMode不可用。</p>
<p>生成的推理接口详细使用说明，请参考<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.5/index.html">API文档</a>。</p>
<p>以下三个接口暂不支持：</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">std::unordered_map&lt;String,</span> <span class="pre">mindspore::tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">GetOutputs()</span> <span class="pre">const</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">Vector&lt;tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">GetOutputsByNodeName(const</span> <span class="pre">String</span> <span class="pre">&amp;node_name)</span> <span class="pre">const</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">int</span> <span class="pre">Resize(const</span> <span class="pre">Vector&lt;tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">&amp;inputs,</span> <span class="pre">const</span> <span class="pre">Vector&lt;Vector&lt;int&gt;&gt;</span> <span class="pre">&amp;dims)</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
</ol>
</div></blockquote>
</section>
<section id="id5">
<h2>如何使用codegen<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h2>
<p>以MNIST分类网络为例：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./codegen<span class="w"> </span>--modelPath<span class="o">=</span>./mnist.ms<span class="w"> </span>--codePath<span class="o">=</span>./
</pre></div>
</div>
<p>如果没有指定target参数，默认目标平台为x86。执行成功后，会在codePath指定的目录下生成名为mnist的文件夹，内容如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mnist
├── benchmark                  # 集成调试相关的例程
│   ├── benchmark.cc
│   ├── calib_output.cc
│   ├── calib_output.h
│   ├── load_input.c
│   └── load_input.h
├── CMakeLists.txt
└── src                        # 源文件
    ├── CMakeLists.txt
    ├── mmodel.h
    ├── net.bin                # 二进制形式的模型权重
    ├── net.c
    ├── net.cmake
    ├── net.h
    ├── session.cc
    ├── session.h
    ├── tensor.cc
    ├── tensor.h
    ├── weight.c
    └── weight.h
</pre></div>
</div>
</section>
<section id="stm">
<h2>在STM开发板上执行推理<a class="headerlink" href="#stm" title="Permalink to this headline"></a></h2>
<p>本教程以在STM32F746单板上编译部署生成模型代码为例，演示了codegen编译模型在Cortex-M平台的使用。更多关于Arm Cortex-M的详情可参见其<a class="reference external" href="https://developer.arm.com/ip-products/processors/cortex-m">官网</a>。</p>
<section id="stm32f746">
<h3>STM32F746编译依赖<a class="headerlink" href="#stm32f746" title="Permalink to this headline"></a></h3>
<p>模型推理代码的编译部署需要在Windows上安装<a class="reference external" href="https://www.segger.com/">J-Link</a>、<a class="reference external" href="https://www.st.com/content/st_com/en.html">STM32CubeMX</a>、<a class="reference external" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm">GNU Arm Embedded Toolchain</a>等工具来进行交叉编译。</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.st.com/content/ccc/resource/technical/software/sw_development_suite/group0/0b/05/f0/25/c7/2b/42/9d/stm32cubemx_v6-1-1/files/stm32cubemx_v6-1-1.zip/jcr:content/translations/en.stm32cubemx_v6-1-1.zip">STM32CubeMX Windows版本</a> &gt;= 6.0.1</p></li>
<li><p><a class="reference external" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm/downloads">GNU Arm Embedded Toolchain</a>  &gt;= 9-2019-q4-major-win32</p></li>
<li><p><a class="reference external" href="https://www.segger.com/downloads/jlink/">J-Link Windows版本</a> &gt;= 6.56</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
</ul>
</section>
<section id="id6">
<h3>STM32F746工程构建<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<ul>
<li><p>需要组织的工程目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── mnist              # codegen生成的模型推理代码
├── include            # 模型推理对外API头文件目录(需要自建)
└── operator_library   # 模型推理算子相关文件(需要自建)
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>模型推理对外API头文件可由MindSpore团队发布的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/downloads.html">Release包</a>中获取。</p>
<p>在编译此工程之前需要预先获取对应平台所需要的算子文件，由于Cortex-M平台工程编译一般涉及到较复杂的交叉编译，此处不提供直接预编译的算子库静态库，而是用户根据模型自行组织文件，自主编译Cortex-M7 、Coretex-M4、Cortex-M3等工程(对应工程目录结构已在示例代码中给出，用户可自主将对应ARM官方的CMSIS源码放置其中即可)。</p>
</div></blockquote>
<ul>
<li><p>使用codegen编译<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mnist_lite/mnist.ms">MNIST手写数字识别模型</a>，生成对应的STM32F46推理代码。具体命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./codegen<span class="w"> </span>--codePath<span class="o">=</span>.<span class="w"> </span>--modelPath<span class="o">=</span>mnist.ms<span class="w"> </span>--target<span class="o">=</span>ARM32M
</pre></div>
</div>
</li>
<li><p>生成代码工程目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── mnist               # 生成代码的根目录
    ├── benchmark       # 生成代码的benchmark目录
    └── src             # 模型推理代码目录
</pre></div>
</div>
</li>
<li><p>预置算子静态库的目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── operator_library    # 平台算子库目录
    ├── include         # 平台算子库头文件目录
    └── nnacl           # MindSpore团队提供的平台算子库源文件
    └── wrapper         # MindSpore团队提供的平台算子库源文件
    └── CMSIS           # Arm官方提供的CMSIS平台算子库源文件
</pre></div>
</div>
<blockquote>
<div><p>在使用过程中，引入CMSIS v5.7.0 Softmax相关的CMSIS算子文件时，头文件中需要加入<code class="docutils literal notranslate"><span class="pre">arm_nnfunctions.h</span></code>。</p>
</div></blockquote>
</li>
</ul>
<section id="id7">
<h4>代码工程编译<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>环境测试</p>
<p>安装好交叉编译所需环境后，需要在Windows环境中依次将其加入到环境变量中。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>gcc -v               # 查看GCC版本
arm-none-eabi-gdb -v # 查看交叉编译环境
jlink -v             # 查看J-Link版本
make -v              # 查看Make版本
</pre></div>
</div>
<p>以上命令均成功返回值时，表明环境准备已完成，可以继续进入下一步，否则请务必先安装上述环境。</p>
</li>
<li><p>生成STM32F746单板初始化代码（<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/mindspore/lite/micro/example/mnist_stm32f746">详情示例代码</a>）</p>
<ul class="simple">
<li><p>启动 STM32CubeMX，新建project，选择单板STM32F746IG。</p></li>
<li><p>成功以后，选择<code class="docutils literal notranslate"><span class="pre">Makefile</span></code> ，<code class="docutils literal notranslate"><span class="pre">generator</span> <span class="pre">code</span></code>。</p></li>
<li><p>在生成的工程目录下打开<code class="docutils literal notranslate"><span class="pre">cmd</span></code>，执行<code class="docutils literal notranslate"><span class="pre">make</span></code>，测试初始代码是否成功编译。</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># make成功结果
arm-none-eabi-size build/test_stm32f746.elf
  text    data     bss     dec     hex filename
  3660      20    1572    5252    1484 build/test_stm32f746.elf
arm-none-eabi-objcopy -O ihex build/test_stm32f746.elf build/test_stm32f746.hex
arm-none-eabi-objcopy -O binary -S build/test_stm32f746.elf build/test_stm32f746.bin
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id8">
<h4>编译模型<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>拷贝MindSpore团队提供算子文件以及对应头文件到STM32CubeMX生成的工程目录中。</p></li>
<li><p>拷贝codegen生成模型推理代码到 STM32CubeMX生成的代码工程目录中。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── .mxproject
├── build             # 工程编译输出目录
├── Core
├── Drivers
├── mnist             # codegen生成的cortex-m7 模型推理代码
├── Makefile          # 编写工程makefile文件组织mnist &amp;&amp; operator_library源文件到工程目录中
├── startup_stm32f746xx.s
├── STM32F746IGKx_FLASH.ld
└── test_stm32f746.ioc
</pre></div>
</div>
</li>
<li><p>修改makefile文件，组织算子静态库以及模型推理代码，具体makefile文件内容参见<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/mindspore/lite/micro/example/mnist_stm32f746">示例</a>。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># C includes
C_INCLUDES =  \
-ICore/Inc \
-IDrivers/STM32F7xx_HAL_Driver/Inc \
-IDrivers/STM32F7xx_HAL_Driver/Inc/Legacy \
-IDrivers/CMSIS/Device/ST/STM32F7xx/Include \
-Imnist/operator_library/include \                # 新增，指定算子库头文件目录
-Imnist/include \                                 # 新增，指定模型推理代码头文件
-Imnist/src                                       # 新增，指定模型推理代码源文件
......
</pre></div>
</div>
</li>
<li><p>在工程目录的Core/Src的main.c编写模型调用代码，具体代码新增如下：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cm">/* USER CODE END WHILE */</span>
<span class="w">    </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;***********mnist test start***********</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">model_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="w"> </span><span class="o">*</span><span class="n">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">model_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">model_size</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">    </span><span class="n">Vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">inputs_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">inputs_num</span><span class="p">];</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">inputs_num</span><span class="p">];</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// here mnist only have one input data,just hard code to it&#39;s array;</span>
<span class="w">    </span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mnist_inputs_data</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">      </span><span class="n">memcpy</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Vector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs_name</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="n">output_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">outputs_name</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">output_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">casted_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">casted_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">output_tensor</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">();</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;output: [%d] is : [%d]/100</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">casted_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
<span class="w">    </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;***********mnist test end***********</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>在工程跟目中目录使用管理员权限打开<code class="docutils literal notranslate"><span class="pre">cmd</span></code> 执行 <code class="docutils literal notranslate"><span class="pre">make</span></code>进行编译。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="id9">
<h3>STM32F746工程部署<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h3>
<p>使用J-Link将可执行文件拷贝到单板上并做推理。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>jlinkgdbserver           # 启动jlinkgdbserver 选定target device为STM32F746IG
jlinkRTTViewer           # 启动jlinkRTTViewer 选定target devices为STM32F746IG
arm-none-eabi-gdb        # 启动arm-gcc gdb服务
file build/target.elf    # 打开调测文件
target remote 127.0.0.1  # 连接jlink服务器
monitor reset            # 重置单板
monitor halt             # 挂起单板
load                     # 加载可执行文件到单板
c                        # 执行模型推理
</pre></div>
</div>
</section>
</section>
<section id="id10">
<h2>在轻鸿蒙设备上执行推理<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h2>
<section id="id11">
<h3>安装轻鸿蒙编译环境<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h3>
<p>详细请参考<a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-env-setup-lin-0000001105407498">Ubuntu编译环境准备</a>。</p>
</section>
<section id="id12">
<h3>开发板环境配置<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h3>
<p>以Hi3516开发板为例，请参考<a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-steps-board3516-setting-0000001105829366">安装开发板环境</a>。</p>
</section>
<section id="id13">
<h3>编译模型<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h3>
<p>使用codegen编译<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mnist_lite/mnist.ms">lenet模型</a>，生成对应轻鸿蒙平台的推理代码，命令如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./codegen<span class="w"> </span>--modelPath<span class="o">=</span>./mnist.ms<span class="w"> </span>--codePath<span class="o">=</span>./<span class="w"> </span>--target<span class="o">=</span>ARM32A
</pre></div>
</div>
</section>
<section id="id14">
<h3>编写构建脚本<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h3>
<p>轻鸿蒙应用程序开发请先参考<a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-steps-board3516-running-0000001151888681">运行Hello OHOS</a>。将上一步生成的mnist目录拷贝到任意鸿蒙源码路径下，假设为applications/sample/，然后新建BUILD.gn文件：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;harmony-source-path&gt;/applications/sample/mnist
├── benchmark
├── CMakeLists.txt
├── BUILD.gn
└── src  
</pre></div>
</div>
<p>下载适用于OpenHarmony的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/downloads.html">预编译推理runtime包</a>，然后将其解压至任意鸿蒙源码路径下。编写BUILD.gn文件：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import(&quot;//build/lite/config/component/lite_component.gni&quot;)
import(&quot;//build/lite/ndk/ndk.gni&quot;)

lite_component(&quot;mnist_benchmark&quot;) {
    target_type = &quot;executable&quot;
    sources = [
         &quot;benchmark/benchmark.cc&quot;,
         &quot;benchmark/calib_output.cc&quot;,
         &quot;benchmark/load_input.c&quot;,
         &quot;src/net.c&quot;,
         &quot;src/weight.c&quot;,
         &quot;src/session.cc&quot;,
         &quot;src/tensor.cc&quot;,
    ]
    features = []
    include_dirs = [
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime&quot;,
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/include&quot;,
         &quot;//applications/sample/mnist/benchmark&quot;,
         &quot;//applications/sample/mnist/src&quot;,
    ]
    ldflags = [
         &quot;-fno-strict-aliasing&quot;,
         &quot;-Wall&quot;,
         &quot;-pedantic&quot;,
         &quot;-std=gnu99&quot;,
    ]
    libs = [
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime/lib/libmindspore-lite.a&quot;,
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/lib/libwrapper.a&quot;,
    ]
    defines = [
        &quot;NOT_USE_STL&quot;,
        &quot;ENABLE_NEON&quot;,
        &quot;ENABLE_ARM&quot;,
        &quot;ENABLE_ARM32&quot;
    ]
    cflags = [
         &quot;-fno-strict-aliasing&quot;,
         &quot;-Wall&quot;,
         &quot;-pedantic&quot;,
         &quot;-std=gnu99&quot;,
    ]
    cflags_cc = [
        &quot;-fno-strict-aliasing&quot;,
        &quot;-Wall&quot;,
        &quot;-pedantic&quot;,
        &quot;-std=c++17&quot;,
    ]
}
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&lt;YOUR</span> <span class="pre">MINDSPORE</span> <span class="pre">LITE</span> <span class="pre">RUNTIME</span> <span class="pre">PATH&gt;</span></code>是解压出来的推理runtime包路径，比如//applications/sample/mnist/mindspore-lite-1.3.0-ohos-aarch32。
修改文件build/lite/components/applications.json，添加组件mnist_benchmark的配置：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{
   &quot;component&quot;: &quot;mnist_benchmark&quot;,
   &quot;description&quot;: &quot;Communication related samples.&quot;,
   &quot;optional&quot;: &quot;true&quot;,
   &quot;dirs&quot;: [
     &quot;applications/sample/mnist&quot;
   ],
   &quot;targets&quot;: [
     &quot;//applications/sample/mnist:mnist_benchmark&quot;
   ],
   &quot;rom&quot;: &quot;&quot;,
   &quot;ram&quot;: &quot;&quot;,
   &quot;output&quot;: [],
   &quot;adapted_kernel&quot;: [ &quot;liteos_a&quot; ],
   &quot;features&quot;: [],
   &quot;deps&quot;: {
     &quot;components&quot;: [],
     &quot;third_party&quot;: []
   }
 },
</pre></div>
</div>
<p>修改文件vendor/hisilicon/hispark_taurus/config.json，新增mnist_benchmark组件的条目:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> { &quot;component&quot;: &quot;mnist_benchmark&quot;, &quot;features&quot;:[] }
</pre></div>
</div>
</section>
<section id="benchmark">
<h3>编译benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd &lt;openharmony-source-path&gt;
hb set(设置编译路径)
.（选择当前路径）
选择ipcamera_hispark_taurus@hisilicon并回车
hb build mnist_benchmark（执行编译）
</pre></div>
</div>
<p>生成结果文件out/hispark_taurus/ipcamera_hispark_taurus/bin/mnist_benchmark。</p>
</section>
<section id="id15">
<h3>执行benchmark<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h3>
<p>将mnist_benchmark、权重文件（mnist/src/net.bin）以及<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/lite/micro/example/mnist_x86/mnist_input.bin">输入文件</a>拷贝到开发板上，然后执行：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> OHOS # ./mnist_benchmark mnist_input.bin net.bin 1
 OHOS # =======run benchmark======
 input 0: mnist_input.bin

 loop count: 1
 total time: 10.11800ms, per time: 10.11800ms

 outputs:
 name: int8toft32_Softmax-7_post0/output-0, DataType: 43, Elements: 10, Shape: [1 10 ], Data:
 0.000000, 0.000000, 0.003906, 0.000000, 0.000000, 0.992188, 0.000000, 0.000000, 0.000000, 0.000000,
 ========run success=======
</pre></div>
</div>
</section>
</section>
<section id="id16">
<h2>自定义算子<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h2>
<p>使用前请先参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/register_kernel.html">自定义南向算子</a>了解基本概念。Codegen目前仅支持custom类型的自定义算子注册和实现，暂不支持内建算子（比如conv2d、fc等）的注册和自定义实现。下面以海思Hi3516D开发板为例，说明如何在codegen中使用自定义算子。</p>
<section id="id17">
<h3>准备模型文件<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<p>使用最新的转换工具生成带NNIE类型custom算子的ms格式模型，具体步骤请参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/nnie.html">集成NNIE使用说明</a>。</p>
</section>
<section id="id18">
<h3>执行codegen生成源码<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h3>
<p>对于含有custom类型算子的ms模型，codegen能够自动生成该算子的函数声明和调用。假设模型文件名为nnie.ms，执行如下命令生成源代码：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./codegen<span class="w"> </span>--modelPath<span class="o">=</span>./nnie.ms<span class="w"> </span>--target<span class="o">=</span>ARM32A
</pre></div>
</div>
</section>
<section id="id19">
<h3>用户实现自定义算子<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h3>
<p>上一步会在当前路径下生成nnie源码目录，其有一个叫registered_kernel.h的头文件指定了custom算子的函数声明：</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CustomKernel</span><span class="p">(</span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">input_num</span><span class="p">,</span><span class="w"> </span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">output_num</span><span class="p">,</span><span class="w"> </span><span class="n">CustomParameter</span><span class="w"> </span><span class="o">*</span><span class="n">param</span><span class="p">);</span>
</pre></div>
</div>
<p>用户需要提供该函数的实现，并将相关源码或者库集成到生成代码的cmake工程中。例如，我们提供了支持海思NNIE的custom kernel示例动态库libmicro_nnie.so，该文件包含在<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.5/use/downloads.html">官网下载页</a>《NNIE 推理runtime及benchmark工具》组件中。用户需要修改生成代码的CMakeLists.txt，填加链接的库名称和路径。例如：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>link_directories<span class="o">(</span>&lt;YOUR_PATH&gt;/mindspore-lite-1.5.0-linux-aarch32/providers/Hi3516D<span class="o">)</span>
link_directories<span class="o">(</span>&lt;HI3516D_SDK_PATH&gt;<span class="o">)</span>
target_link_libraries<span class="o">(</span>benchmark<span class="w"> </span>net<span class="w"> </span>micro_nnie<span class="w"> </span>nnie<span class="w"> </span>mpi<span class="w"> </span>VoiceEngine<span class="w"> </span>upvqe<span class="w"> </span>securec<span class="w"> </span>-lm<span class="w"> </span>-pthread<span class="o">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>nnie<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mkdir<span class="w"> </span>buid<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>-DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>&lt;MS_SRC_PATH&gt;/mindspore/lite/cmake/himix200.toolchain.cmake<span class="w"> </span>-DPLATFORM_ARM32<span class="o">=</span>ON<span class="w"> </span>-DPKG_PATH<span class="o">=</span>&lt;RUNTIME_PKG_PATH&gt;<span class="w"> </span>..
make
</pre></div>
</div>
</section>
</section>
<section id="id20">
<h2>其它平台使用说明<a class="headerlink" href="#id20" title="Permalink to this headline"></a></h2>
<section id="linux-x86-64">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/mindspore/lite/micro/example/mnist_x86">Linux_x86_64平台编译部署</a><a class="headerlink" href="#linux-x86-64" title="Permalink to this headline"></a></h3>
</section>
<section id="android">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/mindspore/lite/micro/example/mobilenetv2">Android平台编译部署</a><a class="headerlink" href="#android" title="Permalink to this headline"></a></h3>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_java.html" class="btn btn-neutral float-left" title="使用Java接口执行推理" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="asic.html" class="btn btn-neutral float-right" title="专用芯片集成说明" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>