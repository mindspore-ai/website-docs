<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>使用C++接口执行推理 &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="使用Java接口执行推理" href="runtime_java.html" />
    <link rel="prev" title="执行推理" href="runtime.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">一小时入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">执行推理</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">使用C++接口执行推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">模型读取</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">创建配置上下文</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">配置线程数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">配置线程亲和性</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">配置并行策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gpu">配置使用GPU后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#npu">配置使用NPU后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nnie">配置使用NNIE后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ascend">配置使用Ascend后端</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id7">模型创建加载与编译</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">输入数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">执行推理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">获取输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">内存释放</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">高级用法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resize">输入维度Resize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model">Model并行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">混合精度运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">多硬件异构运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opengl">OpenGL纹理输入</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">共享内存池</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">回调运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id17">模型加载与编译独立调用流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">查看日志</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">获取版本号</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">扩展使用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">使用Java接口执行推理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在轻量和小型系统上执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库剪裁工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">执行推理</a> &raquo;</li>
      <li>使用C++接口执行推理</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_cpp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="c">
<h1>使用C++接口执行推理<a class="headerlink" href="#c" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">macOS</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">iOS</span></code> <code class="docutils literal notranslate"><span class="pre">Android</span></code> <code class="docutils literal notranslate"><span class="pre">C++</span></code> <code class="docutils literal notranslate"><span class="pre">推理应用</span></code> <code class="docutils literal notranslate"><span class="pre">模型加载</span></code> <code class="docutils literal notranslate"><span class="pre">数据准备</span></code> <code class="docutils literal notranslate"><span class="pre">中级</span></code> <code class="docutils literal notranslate"><span class="pre">高级</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/lite/docs/source_zh_cn/use/runtime_cpp.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source.png"></a></p>
<blockquote>
<div><p>MindSpore已经统一了端边云推理API，如您想继续使用MindSpore Lite独立API进行端侧推理，可以参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/runtime_cpp.html">此文档</a>。</p>
</div></blockquote>
<section id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.6/use/converter_tool.html">MindSpore Lite模型转换工具</a>转换成<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型后，即可在Runtime中执行模型的推理流程。本教程介绍如何使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/index.html">C++接口</a>执行推理。</p>
<p>使用MindSpore Lite推理框架主要包括以下步骤：</p>
<ol class="arabic simple">
<li><p>模型读取：从文件系统中读取由<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.6/use/converter_tool.html">模型转换工具</a>转换得到的<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型。</p></li>
<li><p>创建配置上下文：创建配置上下文<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>，保存需要的一些基本配置参数，用于指导模型编译和模型执行。</p></li>
<li><p>模型创建、加载与编译：执行推理之前，需要调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#build">Build</a>接口进行模型加载和模型编译。模型加载阶段将文件缓存解析成运行时的模型。模型编译阶段主要进行算子选型调度、子图切分等过程，该阶段会耗费较多时间所以建议Model创建一次，编译一次，多次推理。</p></li>
<li><p>输入数据：模型执行之前需要向<code class="docutils literal notranslate"><span class="pre">输入Tensor</span></code>中填充数据。</p></li>
<li><p>执行推理：使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#predict">Predict</a>进行模型推理。</p></li>
<li><p>获得输出：模型执行结束之后，可以通过<code class="docutils literal notranslate"><span class="pre">输出Tensor</span></code>得到推理结果。</p></li>
<li><p>释放内存：无需使用MindSpore Lite推理框架时，需要释放已创建的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>。</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
<blockquote>
<div><p>快速了解MindSpore Lite执行推理的完整调用流程，请参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.6/quick_start/quick_start_cpp.html">体验MindSpore Lite C++极简Demo</a>。</p>
</div></blockquote>
</section>
<section id="id2">
<h2>模型读取<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>通过MindSpore Lite进行模型推理时，需要从文件系统读取<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.6/use/converter_tool.html">模型转换工具</a>转换得到的<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型文件。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L332">示例代码</a>演示了从文件系统读取MindSpore Lite模型。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Read model file.</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id3">
<h2>创建配置上下文<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>上下文会保存一些所需的基本配置参数，用于指导模型编译和模型执行，如果用户通过<code class="docutils literal notranslate"><span class="pre">new</span></code>创建<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>，不再需要时，需要用户通过<code class="docutils literal notranslate"><span class="pre">delete</span></code>释放。一般在创建编译完<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>后，<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>即可释放。</p>
<p>MindSpore Lite默认执行的后端是CPU，Context创建后调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#mutabledeviceinfo">MutableDeviceInfo</a>返回后端信息列表的引用，向列表中添加默认的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L250">示例代码</a>演示了如何创建Context，配置默认的CPU后端，并设定CPU使能Float16推理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code>中支持用户设置设备信息，包括<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#gpudeviceinfo">GPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#kirinnpudeviceinfo">KirinNPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#ascend310deviceinfo">Ascend310DeviceInfo</a>。设置的设备个数不能超过3个，推理过程按照用户设置的先后顺序选择后端设备进行部署推理。</p>
<p>Float16需要CPU为ARM v8.2架构的机型才能生效，其他不支持的机型和x86平台会自动回退到Float32执行。</p>
<p>对于iOS设备,暂时只支持向<code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code>添加CPU后端，且暂时不支持CPU后端Float16的执行。</p>
</div></blockquote>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>中包含的配置API如下：</p>
<section id="id4">
<h3>配置线程数<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#setthreadnum">SetThreadNum</a>配置线程数：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="id5">
<h3>配置线程亲和性<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#setthreadaffinity">SetThreadAffinity</a>配置线程和CPU绑定。如果参数是<code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">mode</span></code>，配置绑核策略，有效值为0-2，0为默认不绑核，1为优先绑大核，2为优先绑小核。如果参数是<code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">std::vector&lt;int&gt;</span> <span class="pre">&amp;core_list</span></code>，配置绑核列表。同时配置时，core_list生效，mode不生效。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the thread to be bound to the big core first.</span>
<span class="c1">// Valid value: 0: no affinities, 1: big cores first, 2: little cores first</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadAffinity</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="id6">
<h3>配置并行策略<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#setenableparallel">SetEnableParallel</a>配置执行推理时是否支持并行。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the inference supports parallel.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetEnableParallel</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="gpu">
<h3>配置使用GPU后端<a class="headerlink" href="#gpu" title="Permalink to this headline"></a></h3>
<p>当需要执行的后端为GPU时，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#gpudeviceinfo">GPUDeviceInfo</a>为首选推理后端。建议设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为次选后端，排在GPU后，以保证泛化模型的推理。其中GPUDeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>使能Float16推理。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L114">示例代码</a>演示如何创建CPU与GPU异构推理后端，同时GPU也设定使能Float16推理：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set GPU device first, make GPU preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New GPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// GPU use float16 operator as priority.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="c1">// Set VNIDIA device id, only valid when GPU backend is TensorRT.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// The GPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after GPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>目前GPU的后端，区分<code class="docutils literal notranslate"><span class="pre">arm64</span></code>和<code class="docutils literal notranslate"><span class="pre">x86_64</span></code>平台。</p>
<ul class="simple">
<li><p>在<code class="docutils literal notranslate"><span class="pre">arm64</span></code>上是基于OpenCL，支持Mali、Adreno的GPU，OpenCL版本为2.0。</p></li>
</ul>
<p>具体配置为：</p>
<p>CL_TARGET_OPENCL_VERSION=200</p>
<p>CL_HPP_TARGET_OPENCL_VERSION=120</p>
<p>CL_HPP_MINIMUM_OPENCL_VERSION=120</p>
<ul class="simple">
<li><p>在<code class="docutils literal notranslate"><span class="pre">x86_64</span></code>上是基于TensorRT的GPU，TensorRT版本为6.0.1.5。</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>属性是否设置成功取决于当前设备的<a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix">CUDA计算能力</a>。
<code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>属性仅在TensorRT的GPU上有效，用于指定NVIDIA显卡。</p>
</div></blockquote>
</section>
<section id="npu">
<h3>配置使用NPU后端<a class="headerlink" href="#npu" title="Permalink to this headline"></a></h3>
<p>当需要执行的后端为NPU时，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#kirinnpudeviceinfo">KirinNPUDeviceInfo</a>为首选推理后端。建议设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为次选后端，排在NPU后，以保证泛化模型的推理。其中KirinNPUDeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetFrequency</span></code>来设置NPU频率。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L127">示例代码</a>如何创建CPU与NPU异构推理后端，同时NPU频率设置为3。频率值默认为3，可设置为1（低功耗）、2（均衡）、3（高性能）、4（极致性能）：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set NPU device first, make NPU preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">npu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">KirinNPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">npu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New KirinNPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// NPU set frequency to be 3.</span>
<span class="n">npu_device_info</span><span class="o">-&gt;</span><span class="n">SetFrequency</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="c1">// The NPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">npu_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after NPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="nnie">
<h3>配置使用NNIE后端<a class="headerlink" href="#nnie" title="Permalink to this headline"></a></h3>
<p>当需要执行的后端为CPU和NNIE的异构推理时，只需要按照<span class="xref myst">配置使用CPU后端</span>的方法创建好Context即可，无需指定provider。</p>
</section>
<section id="ascend">
<h3>配置使用Ascend后端<a class="headerlink" href="#ascend" title="Permalink to this headline"></a></h3>
<p>当需要执行的后端为Ascend时(目前支持Ascend310)，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#ascend310deviceinfo">Ascend310DeviceInfo</a>为首选推理后端。建议设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为次选后端，排在Ascend后，以保证泛化模型的推理。其中Ascend310DeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>来设置设备ID。</p>
<p>下面[示例代码]如何创建CPU与Ascend异构推理后端，同时设备ID设置为0：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set Ascend310 device first, make Ascend310 preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Ascend310DeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ascend_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Ascend310DeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Ascend310 set device id to be 0.</span>
<span class="n">ascend_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceId</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// The ascend310 device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after Ascend310 as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="id7">
<h2>模型创建加载与编译<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<p>使用MindSpore Lite执行推理时，<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>是推理的主入口，通过Model可以实现模型加载、模型编译和模型执行。采用上一步创建得到的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>，调用Model的复合<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#build">Build</a>接口来实现模型加载与模型编译。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L265">示例代码</a>演示了Model创建、加载与编译的过程：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="c1">// After the model is built, the Context can be released.</span>
<span class="p">...</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>创建并编译完<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>后，上一步创建得到的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>即可释放。</p>
</div></blockquote>
</section>
<section id="id8">
<h2>输入数据<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h2>
<p>在模型执行前，需要获取到模型的输入<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#mstensor">MSTensor</a>，将输入数据通过<code class="docutils literal notranslate"><span class="pre">memcpy</span></code>拷贝到模型的输入Tensor。可以通过MSTensor的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#datasize">DataSize</a>方法来获取Tensor应该填入的数据大小，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#datatype">DataType</a>方法来获取Tensor的数据类型，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#mutabledata">MutableData</a>方法来获取可写的指针。</p>
<p>MindSpore Lite提供两种方法来获取模型的输入Tensor。</p>
<ol class="arabic">
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getinputbytensorname">GetInputByTensorName</a>方法，根据Tensor的名称来获取模型输入Tensor，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L154">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetInputByTensorName</span></code>获得输入Tensor并填充数据。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume that the model has only one input tensor named graph_input-173.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputByTensorName</span><span class="p">(</span><span class="s">&quot;graph_input-173&quot;</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getinputs">GetInputs</a>方法，直接获取所有的模型输入Tensor的vector，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L137">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetInputs</span></code>获得输入Tensor并填充数据。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one input tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of in_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>MindSpore Lite的模型输入Tensor中的数据排布必须是<code class="docutils literal notranslate"><span class="pre">NHWC</span></code>。如果需要了解更多数据前处理过程，可参考基于JNI接口的Android应用开发中<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.6/quick_start/quick_start.html#id10">编写端侧推理代码</a>的第2步，将输入图片转换为传入MindSpore模型的Tensor格式。</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getinputs">GetInputs</a>和<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getinputbytensorname">GetInputByTensorName</a>方法返回的数据不需要用户释放。</p>
</div></blockquote>
</section>
<section id="id9">
<h2>执行推理<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#predict">Predict</a>进行模型推理。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L355">示例代码</a>演示调用<code class="docutils literal notranslate"><span class="pre">Predict</span></code>执行推理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id10">
<h2>获取输出<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite在执行完推理后，就可以获取模型的推理结果。MindSpore Lite提供三种方法来获取模型的输出<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#mstensor">MSTensor</a>。</p>
<ol class="arabic">
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getoutputsbynodename">GetOutputsByNodeName</a>方法，根据模型输出节点的名称来获取模型输出Tensor中连接到该节点的Tensor的vector，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L170">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code>获得输出Tensor。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model before.</span>
<span class="c1">// Assume that model has a output node named Softmax-65.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_vec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;Softmax-65&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named Default/Sigmoid-op204 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Post-processing your result data.</span>
</pre></div>
</div>
</li>
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getoutputbytensorname">GetOutputByTensorName</a>方法，根据模型输出Tensor的名称来获取对应的模型输出Tensor，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L200">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code>获得输出Tensor。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="c1">// We can use GetOutputTensorNames method to get all name of output tensor of model which is in order.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Assume we have created a Model instance named model before.</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor_name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tensor_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getoutputs">GetOutputs</a>方法，直接获取所有的模型输出Tensor的vector，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L226">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code>获得输出Tensor。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">out_tensors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getoutputsbynodename">GetOutputsByNodeName</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getoutputbytensorname">GetOutputByTensorName</a>和<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#getoutputs">GetOutputs</a>方法返回的数据不需要用户释放。</p>
</div></blockquote>
</section>
<section id="id11">
<h2>内存释放<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h2>
<p>无需使用MindSpore Lite推理框架时，需要释放已经创建的Model，下列<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L370">示例代码</a>演示如何在程序结束前进行内存释放。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model.</span>
<span class="c1">// Assume that the variable of Model * is named model.</span>
<span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="id12">
<h2>高级用法<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h2>
<section id="resize">
<h3>输入维度Resize<a class="headerlink" href="#resize" title="Permalink to this headline"></a></h3>
<p>使用MindSpore Lite进行推理时，如果需要对输入的shape进行Resize，则可以在已完成创建<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>与模型编译<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#build">Build</a>之后调用Model的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#resize">Resize</a>接口，对输入的Tensor重新设置shape。</p>
<blockquote>
<div><p>某些网络是不支持可变维度，会提示错误信息后异常退出，比如，模型中有MatMul算子，并且MatMul的一个输入Tensor是权重，另一个输入Tensor是输入时，调用可变维度接口会导致输入Tensor和权重Tensor的Shape不匹配，最终导致推理失败。</p>
<p>TensorRT的GPU后端只支持在NHWC输入格式下的NHW维度的resize，且resize维度的shape值，不能大于创建的Model的输入shape值。</p>
</div></blockquote>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L321">示例代码</a>演示如何对MindSpore Lite的输入Tensor进行Resize：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">};</span>
<span class="c1">// Assume the model has only one input,resize input shape to [1, 128, 128, 3]</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">;</span>
<span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">resize_shape</span><span class="p">);</span>
<span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="model">
<h3>Model并行<a class="headerlink" href="#model" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite支持多个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>并行推理，每个Model的线程池和内存池都是独立的。但不支持多个线程同时调用单个Model的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#predict">Predict</a>接口。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L470">示例代码</a>演示如何并行执行推理多个Model的过程：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">RunModelParallel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_path</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Create and Build MindSpore model.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateAndBuildModel</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateAndBuildModel</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">model2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create and build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="w"> </span><span class="n">thread1</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">generate_input_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GetInputsByTensorNameAndSetData</span><span class="p">(</span><span class="n">model1</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">generate_input_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model1 set input data error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">generate_input_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model1</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model1</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model1</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model1 predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model1 predict success&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="p">});</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="w"> </span><span class="n">thread2</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">generate_input_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GetInputsByTensorNameAndSetData</span><span class="p">(</span><span class="n">model2</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">generate_input_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model2 set input data error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">generate_input_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model2</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model2</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model2</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model2 predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model2 predict success&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="p">});</span>

<span class="w">  </span><span class="n">thread1</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
<span class="w">  </span><span class="n">thread2</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>

<span class="w">  </span><span class="c1">// Get outputs data.</span>
<span class="w">  </span><span class="c1">// You can also get output through other methods,</span>
<span class="w">  </span><span class="c1">// and you can refer to GetOutputByTensorName() or GetOutputs().</span>
<span class="w">  </span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="n">model1</span><span class="p">);</span>
<span class="w">  </span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="n">model2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Delete model.</span>
<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">model1</span><span class="p">;</span>
<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">model2</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id13">
<h3>混合精度运行<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite 支持混合精度推理。
用户可以在完成创建<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>之后，在模型编译<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#build">Build</a>之前，调用Model的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#loadconfig">LoadConfig</a>接口，配置混合精度信息。
配置文件举例，内容如下:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[execution_plan]
op_name1=data_type:float16
op_name2=data_type:float32
</pre></div>
</div>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L321">示例代码</a>演示如何进行混合精度推理：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_path</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model load config error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">Status</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model build error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="n">Status</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id14">
<h3>多硬件异构运行<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite 支持多硬件异构推理。
用户可以在<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#context">Context</a>中配置多个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#deviceinfocontext">DeviceInfoContext</a>，并且根据设备的先后顺序，设置异构硬件的运行优先级。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L546">示例代码</a>演示如何进行多硬件异构推理：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="c1">// enable NPU CPU GPU in inference. NPU is preferentially used, then the CPU, and GPU get the lowest priority.</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">KirinNPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>

<span class="n">Status</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model build error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="n">Status</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="opengl">
<h3>OpenGL纹理输入<a class="headerlink" href="#opengl" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite 支持 OpenGL纹理输入，进行端到端的GPU同构推理，推理结果以OpenGL纹理数据返回。该功能在使用过程中需要配置到Context中，和在运行推理时绑定OpenGL纹理数据，这两个过程。</p>
<ol class="arabic">
<li><p>配置 Context</p>
<p>用户需要将 Context 中的 devgpu_device_info_中的 SetEnableGLTexture 属性设置为 true，并且将用户当前的OpenGL EGLContext 、EGLDisplay分别通过SetGLContext接口和SetGLDisplay接口进行配置。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// 1. Set EnableGLTexture true</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableGLTexture</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>

<span class="c1">// 2. Set GLContext</span>
<span class="n">EGLContext</span><span class="w"> </span><span class="o">*</span><span class="n">gl_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">EGLContext</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gl_context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;new EGLContext failed&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="o">*</span><span class="n">gl_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eglGetCurrentContext</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetGLContext</span><span class="p">(</span><span class="n">gl_context</span><span class="p">);</span>

<span class="c1">// 3. Set GLDisplay</span>
<span class="n">EGLDisplay</span><span class="w"> </span><span class="o">*</span><span class="n">gl_display</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">EGLDisplay</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gl_display</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;new EGLDisplay failed&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="o">*</span><span class="n">gl_display</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eglGetCurrentDisplay</span><span class="p">();</span>
<span class="p">}</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetGLDisplay</span><span class="p">(</span><span class="n">gl_display</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>绑定OpenGL纹理数据</p>
<p>在模型编译阶段后，模型运行前，用户需要调用 BindGLTexture2DMemory(const std::map&lt;std::string, GLuint&gt; &amp;inputGlTexture, std::map&lt;std::string, GLuint&gt; *outputGLTexture;) 函数绑定输入输出纹理，代替原有输入数据的步骤，因为 MindSpore Lite 本身并没有分配 OpenGL 内存的功能，所以要求用户根据模型输入输出的 tensor size 事先创建好输入输出纹理的内存，并将纹理内存对应的纹理 ID 绑定到模型的输入输出，示例代码如下</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gl_texture</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gl_texture</span><span class="p">;</span>

<span class="p">...</span><span class="w"> </span><span class="c1">// Write OpenGL Texture data(GLuint) into input_gl_texture and output_gl_texture</span>

<span class="c1">// Bind texture data with input and output tensors</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">BindGLTexture2DMemory</span><span class="p">(</span><span class="n">input_gl_texture</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output_gl_texture</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;BindGLTexture2DMemory failed&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">return</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">;</span>
</pre></div>
</div>
<p>std::map&lt;std::string, GLuint&gt;  input_gl_texture 变量中key为模型输入tensor name，value为对应的GLuint 纹理；std::map&lt;std::string, GLuint&gt;  output_gl_texture 变量中key为模型输出tensor name，value为对应的GLuint 纹理。模型输入输出tensor name可以通过tensor.Name()接口获取，示例代码如下：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inTextureIDs</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">inputGlTexture</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">inTextureIDs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outTextureIDs</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">outputGlTexture</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">outTextureIDs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Predict结果</p>
<p>绑定完成后直接调用ms_model_的 Predict 接口进行推理即可，模型输出会被拷贝到绑定的输出纹理 ID 对应的内存上，用户可从outputs上面获取推理结果</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">ms_inputs_for_api_</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">ms_before_call_back_</span><span class="p">,</span><span class="w"> </span><span class="n">ms_after_call_back_</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id15">
<h3>共享内存池<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h3>
<p>如果存在多个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>的情况，可以通过在<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#deviceinfocontext">DeviceInfoContext</a>中配置同一个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#allocator">Allocator</a>，实现共享内存池来减少运行时内存大小。其中，内存池的内存总大小限制为<code class="docutils literal notranslate"><span class="pre">3G</span></code>，单次分配的内存限制为<code class="docutils literal notranslate"><span class="pre">2G</span></code>。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L546">示例代码</a>演示如何在两个Model间共享内存池的功能：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context1</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info1</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model1</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context1</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">context2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context2</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Use the same allocator to share the memory pool.</span>
<span class="n">device_info2</span><span class="o">-&gt;</span><span class="n">SetAllocator</span><span class="p">(</span><span class="n">device_info1</span><span class="o">-&gt;</span><span class="n">GetAllocator</span><span class="p">());</span>
<span class="n">device_list2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info2</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model2</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context2</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id16">
<h3>回调运行<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite可以在调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#predict">Predict</a>时，传入两个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#mskernelcallback">MSKernelCallBack</a>函数指针来回调推理模型，相比于一般的模型执行，回调运行可以在运行过程中获取额外的信息，帮助开发者进行性能分析、Bug调试等。额外的信息包括：</p>
<ul class="simple">
<li><p>当前运行的节点名称</p></li>
<li><p>推理当前节点前的输入输出Tensor</p></li>
<li><p>推理当前节点后的输入输出Tensor</p></li>
</ul>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L672">示例代码</a>演示如何定义了两个回调函数作为前置回调指针和后置回调指针，传入到Predict接口进行回调推理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Definition of callback function before forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">before_call_back</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_inputs</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_outputs</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Before forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_type_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Definition of callback function after forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">after_call_back</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_inputs</span><span class="p">,</span>
<span class="w">                          </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_outputs</span><span class="p">,</span>
<span class="w">                          </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;After forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_type_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">before_call_back</span><span class="p">,</span><span class="w"> </span><span class="n">after_call_back</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id17">
<h3>模型加载与编译独立调用流程<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<p>模型加载与编译也可以分别调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#serialization">Serialization</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#load">Load</a>接口和<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#build">Build</a>实现。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L282">示例代码</a>演示模型加载与编译独立调用的流程：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>

<span class="c1">// Load graph</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">load_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">load_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Load graph failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Build model</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">GraphCell</span><span class="w"> </span><span class="n">graph_cell</span><span class="p">(</span><span class="n">graph</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id18">
<h3>查看日志<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h3>
<p>当推理出现异常的时候，可以通过查看日志信息来定位问题。针对Android平台，采用<code class="docutils literal notranslate"><span class="pre">Logcat</span></code>命令行工具查看MindSpore Lite推理的日志信息，并利用<code class="docutils literal notranslate"><span class="pre">MS_LITE</span></code> 进行筛选。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>logcat<span class="w"> </span>-s<span class="w"> </span><span class="s2">&quot;MS_LITE&quot;</span>
</pre></div>
</div>
<blockquote>
<div><p>对iOS设备暂不支持日志查看。</p>
</div></blockquote>
</section>
<section id="id19">
<h3>获取版本号<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite提供了<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore.html#version">Version</a>方法可以获取版本号，包含在<code class="docutils literal notranslate"><span class="pre">include/api/types.h</span></code>头文件中，调用该方法可以得到当前MindSpore Lite的版本号。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_cpp/main.cc#L717">示例代码</a>演示如何获取MindSpore Lite的版本号：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Version</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="id20">
<h3>扩展使用<a class="headerlink" href="#id20" title="Permalink to this headline"></a></h3>
<p>本章节提供了扩展MindSpore Lite推理框架的示例程序，通过演示自定义算子的构建、注册的全流程，用户能够快速了解推理框架的扩展API的使用，能够在推理框架中集成自定义算子。本章节以一个具有简易Add计算能力的Custom单算子为模型。相关代码放置在<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.6/mindspore/lite/examples/runtime_extend">mindspore/lite/examples/runtime_extend</a>目录。</p>
<p>本章节仅提供了在Linux环境下的使用说明。</p>
<section id="infershape">
<h4>算子InferShape扩展<a class="headerlink" href="#infershape" title="Permalink to this headline"></a></h4>
<p>用户需继承<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore_kernel.html#kernelinterface">KernelInterface</a>类，重载<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore_kernel.html#infer">Infer</a>接口函数。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CheckInputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">         </span><span class="c1">// 输入校验函数，校验输入张量的shape是否合规</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">input_shape</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">input_shape</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">input_shape</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_INFER_INVALID</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CustomAddInfer</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">CustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">CustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="n">Status</span><span class="w"> </span><span class="nf">Infer</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">               </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w">        </span><span class="c1">// 重载Infer公有函数</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetFormat</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">format</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetDataType</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CheckInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_INFER_INVALID</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">({</span><span class="mi">-1</span><span class="p">});</span><span class="w">        </span><span class="c1">// 输出张量的shape设为{-1}，表示在运行时需要再次推断</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddInfer</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>
<span class="n">REGISTER_CUSTOM_KERNEL_INTERFACE</span><span class="p">(</span><span class="n">CustomOpTutorial</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">)</span><span class="w">       </span><span class="c1">// 调用注册接口</span>
</pre></div>
</div>
<blockquote>
<div><p>shape推断分为两个时期，一是图编译时的静态推断，二是图运行时的动态推断。</p>
<p>静态推断：</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CheckInputs</span></code>失败或者当前节点需要动态推断的情形下，需将输出张量的shape设为{-1}，以便在图运行时的识别标识,且返回码需设置为<code class="docutils literal notranslate"><span class="pre">RET_INFER_INVALID</span></code>。</p></li>
<li><p>其他情形下，返回其他错误码，程序将会停止，请进行必要的检查。</p></li>
</ol>
<p>动态推断：</p>
<p>在算子运行时，动态推断是否需要，依据对输出张量的shape校验。请参考下面的算子扩展说明。</p>
</div></blockquote>
</section>
<section id="id21">
<h4>算子扩展<a class="headerlink" href="#id21" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>用户需继承<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore_kernel.html#kernel">Kernel</a>类，重载必要的接口。</p>
<ul>
<li><p>Prepare：此接口将在图编译期间调用，用户可对算子做运行前的准备或者必要的校验。</p></li>
<li><p>Execute：此接口是算子的运行接口，用户可将<strong>动态推断</strong>逻辑<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.6/mindspore/lite/examples/runtime_extend/src/custom_add_kernel.cc">PreProcess</a>放置于此接口内调用。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CheckOutputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">           </span><span class="c1">// 算子运行时校验，以确定是否调用InferShape过程</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">output_shape</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">output_shape</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">output_shape</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_INFER_INVALID</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>ReSize：此接口用于在图输入shape变化的情形下，当前算子所需的相应变动。</p></li>
<li><p>属性解析： 用户需自行提供对算子属性的解析，可参考<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.6/mindspore/lite/examples/runtime_extend/src/custom_add_kernel.cc">ParseAttrData</a>。</p></li>
</ul>
</li>
<li><p>算子注册，API接口可参考<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.6/api_cpp/mindspore_registry.html#register-custom-kernel">REGISTER_CUSTOM_KERNEL</a>。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kFloat32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                         </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                         </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddKernel</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">REGISTER_CUSTOM_KERNEL</span><span class="p">(</span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">CustomOpTutorial</span><span class="p">,</span><span class="w"> </span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id22">
<h4>示例演示<a class="headerlink" href="#id22" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>编译</p>
<ul>
<li><p>环境要求</p>
<ul class="simple">
<li><p>系统环境：Linux x86_64，推荐使用Ubuntu 18.04.02LTS</p></li>
<li><p>编译依赖：</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>编译构建</p>
<p>在<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend</span></code>目录下执行<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/lite/examples/runtime_extend/build.sh">build.sh</a>，将自动下载MindSpore Lite发布件并编译Demo。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<blockquote>
<div><p>若使用该build脚本下载MindSpore Lite发布件失败，请手动下载硬件平台为CPU、操作系统为Ubuntu-x64的MindSpore Lite发布件<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.6/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a>，将解压后<code class="docutils literal notranslate"><span class="pre">runtime/lib</span></code>目录下的<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/lib</span></code>目录、<code class="docutils literal notranslate"><span class="pre">runtime/include</span></code>目录拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend</span></code>目录下。</p>
<p>若<code class="docutils literal notranslate"><span class="pre">add_extend.ms</span></code>模型下载失败，请手动下载相关模型文件<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/add_extend.ms">add_extend.ms</a>，并将其拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/model</span></code>目录。</p>
<p>通过手动下载并且将该文件放到指定位置后，需要再次执行build.sh脚本才能完成编译构建。</p>
</div></blockquote>
</li>
<li><p>编译输出</p>
<p>在<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/build</span></code>目录下生成了runtime_extend_tutorial的可执行程序。</p>
</li>
</ul>
</li>
<li><p>执行程序</p>
<p>编译构建后，进入<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/build</span></code>目录，并执行以下命令，体验扩展的MindSpore Lite推理add_extend.ms模型。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./runtime_extend_tutorial<span class="w"> </span>../model/add_extend.ms
</pre></div>
</div>
<p>执行完成后将能得到如下结果，打印输出Tensor的名称、输出Tensor的大小，输出Tensor的数量以及前20个数据：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:add-0 tensor size is:400 tensor elements num is:100
output data is:2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime.html" class="btn btn-neutral float-left" title="执行推理" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_java.html" class="btn btn-neutral float-right" title="使用Java接口执行推理" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>