<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>在轻量和小型系统上执行推理 &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="专用芯片集成说明" href="asic.html" />
    <link rel="prev" title="使用Java接口执行推理" href="runtime_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">一小时入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">体验C++极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">体验Java极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">在轻量和小型系统上执行推理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#模型生成代码">模型生成代码</a></li>
<li class="toctree-l2"><a class="reference internal" href="#自动生成的代码部署时依赖的头文件和lib的目录结构">自动生成的代码部署时依赖的头文件和lib的目录结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="#在stm开发板上执行推理">在STM开发板上执行推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746编译依赖">STM32F746编译依赖</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746工程构建">STM32F746工程构建</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#代码工程编译">代码工程编译</a></li>
<li class="toctree-l4"><a class="reference internal" href="#编译模型">编译模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746工程部署">STM32F746工程部署</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#在轻鸿蒙设备上执行推理">在轻鸿蒙设备上执行推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#安装轻鸿蒙编译环境">安装轻鸿蒙编译环境</a></li>
<li class="toctree-l3"><a class="reference internal" href="#开发板环境配置">开发板环境配置</a></li>
<li class="toctree-l3"><a class="reference internal" href="#编译模型-1">编译模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#编写构建脚本">编写构建脚本</a></li>
<li class="toctree-l3"><a class="reference internal" href="#编译benchmark">编译benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#执行benchmark">执行benchmark</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#自定义算子">自定义算子</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#用户实现自定义算子">用户实现自定义算子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#其它平台使用说明">其它平台使用说明</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linux-x86-64平台编译部署">Linux_x86_64平台编译部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="#android平台编译部署">Android平台编译部署</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>在轻量和小型系统上执行推理</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/micro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="在轻量和小型系统上执行推理">
<h1>在轻量和小型系统上执行推理<a class="headerlink" href="#在轻量和小型系统上执行推理" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.7/docs/lite/docs/source_zh_cn/use/micro.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png"></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>相较于移动设备，IoT设备上通常使用MicroControllerUnits(MCUs)，不仅设备系统ROM资源非常有限，而且硬件资源内存和算力都非常弱小。
因此IOT设备上的AI应用对AI模型推理的运行时内存和功耗都有严格限制。
MindSpore Lite针对MCUs部署硬件后端，提供了一种超轻量Micro AI部署解决方案：离线阶段直接将模型生成轻量化代码，不再需要在线解析模型和图编译，生成的Micro推理代码非常直观易懂，运行时内存小，代码体积也更小。
用户使用MindSpore Lite转换工具非常容易生成可在x86/ARM64/ARM32A/ARM32M平台部署的推理代码，其中在x86/ARM64/ARM32A平台上推理会调用NNACL算子库，
在ARM32M平台上调用CMSIS-NN算子库。</p>
<p>通过MindSpore Lite转换工具<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/converter_tool.html">Converter</a>，
输入Micro配置文件，就能把输入模型生成代码。</p>
<p><img alt="img" src="../_images/lite_codegen.png" /></p>
</section>
<section id="模型生成代码">
<h2>模型生成代码<a class="headerlink" href="#模型生成代码" title="Permalink to this headline"></a></h2>
<p>可以通过两种方式获取：</p>
<ol class="arabic simple">
<li><p>MindSpore官网下载<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/downloads.html">Release版本</a>。</p></li>
<li><p>从源码开始<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/build.html">编译构建</a>。</p></li>
</ol>
<p>以MNIST分类模型为例，如下命令将模型生成代码：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_dir</span><span class="si">}</span>/mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>其中 config 文件配置字段如下</p>
<div class="highlight-txt notranslate"><div class="highlight"><pre><span></span>[micro_param]

# enable code-generation for MCU HW

enable_micro=true

# specify HW target, support x86,ARM32M, AMR32A, ARM64 only.

target=x86

# code generation for Inference or Train

codegen_mode=Inference

# enable parallel inference or not

support_parallel=false

# enable debug

debug_mode=false
</pre></div>
</div>
<p>其中</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>是否必选</p></th>
<th class="head"><p>参数说明</p></th>
<th class="head"><p>取值范围</p></th>
<th class="head"><p>默认值</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enable_micro</p></td>
<td><p>是</p></td>
<td><p>模型会生成代码，否则生成.ms</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-odd"><td><p>target</p></td>
<td><p>是</p></td>
<td><p>生成代码针对的平台</p></td>
<td><p>x86, ARM32M, ARM32A, ARM64</p></td>
<td><p>x86</p></td>
</tr>
<tr class="row-even"><td><p>codegen_mode</p></td>
<td><p>是</p></td>
<td><p>生成推理还是训练代码</p></td>
<td><p>Inference, Train</p></td>
<td><p>Inference</p></td>
</tr>
<tr class="row-odd"><td><p>supportParallel</p></td>
<td><p>否</p></td>
<td><p>是否生成支持多线程的代码</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-even"><td><p>debugMode</p></td>
<td><p>否</p></td>
<td><p>是否以生成调试模式的代码</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>os不支持文件系统时，debugMode不可用。</p>
<p>生成的推理接口详细使用说明，请参考<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.7/index.html">API文档</a>。</p>
<p>以下三个接口暂不支持：</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">std::unordered_map&lt;String,</span> <span class="pre">mindspore::tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">GetOutputs()</span> <span class="pre">const</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">Vector&lt;tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">GetOutputsByNodeName(const</span> <span class="pre">String</span> <span class="pre">&amp;node_name)</span> <span class="pre">const</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">int</span> <span class="pre">Resize(const</span> <span class="pre">Vector&lt;tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">&amp;inputs,</span> <span class="pre">const</span> <span class="pre">Vector&lt;Vector&lt;int&gt;&gt;</span> <span class="pre">&amp;dims)</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
</ol>
</div></blockquote>
<p>转换工具执行成功后，生成的代码在指定的outputFile路径下，内容如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mnist
├── benchmark                  # 集成调试相关的例程
│   ├── benchmark.c
│   ├── calib_output.c
│   ├── calib_output.h
│   ├── load_input.c
│   └── load_input.h
├── CMakeLists.txt
└── src                        # 源文件
    ├── CMakeLists.txt
    ├── net.bin                # 二进制形式的模型权重
    ├── net.c
    ├── net.cmake
    ├── net.h
    ├── model.c
    ├── context.c
    ├── context.h
    ├── tensor.c
    ├── tensor.h
    ├── weight.c
    └── weight.h
</pre></div>
</div>
</section>
<section id="自动生成的代码部署时依赖的头文件和lib的目录结构">
<h2>自动生成的代码部署时依赖的头文件和lib的目录结构<a class="headerlink" href="#自动生成的代码部署时依赖的头文件和lib的目录结构" title="Permalink to this headline"></a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── tools
    └── codegen # 代码生成的source code 依赖include和lib
        ├── include          # 推理框架头文件
        │   ├── nnacl        # nnacl 算子头文件
        │   └── wrapper
        ├── lib
        │   └── libwrapper.a # MindSpore Lite codegen生成代码依赖的部分算子静态库
        └── third_party
            ├── include
            │   └── CMSIS    # ARM CMSIS NN 算子头文件
            └── lib
                └── libcmsis_nn.a # ARM CMSIS NN 算子静态库
</pre></div>
</div>
</section>
<section id="在stm开发板上执行推理">
<h2>在STM开发板上执行推理<a class="headerlink" href="#在stm开发板上执行推理" title="Permalink to this headline"></a></h2>
<p>本教程以在STM32F746单板上编译部署生成模型代码为例，演示了codegen编译模型在Cortex-M平台的使用。更多关于Arm Cortex-M的详情可参见其<a class="reference external" href="https://developer.arm.com/ip-products/processors/cortex-m">官网</a>。</p>
<section id="stm32f746编译依赖">
<h3>STM32F746编译依赖<a class="headerlink" href="#stm32f746编译依赖" title="Permalink to this headline"></a></h3>
<p>模型推理代码的编译部署需要在Windows上安装<a class="reference external" href="https://www.segger.com/">J-Link</a>、<a class="reference external" href="https://www.st.com/content/st_com/en.html">STM32CubeMX</a>、<a class="reference external" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm">GNU Arm Embedded Toolchain</a>等工具来进行交叉编译。</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.st.com/content/ccc/resource/technical/software/sw_development_suite/group0/0b/05/f0/25/c7/2b/42/9d/stm32cubemx_v6-1-1/files/stm32cubemx_v6-1-1.zip/jcr:content/translations/en.stm32cubemx_v6-1-1.zip">STM32CubeMX Windows版本</a> &gt;= 6.0.1</p></li>
<li><p><a class="reference external" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm/downloads">GNU Arm Embedded Toolchain</a>  &gt;= 9-2019-q4-major-win32</p></li>
<li><p><a class="reference external" href="https://www.segger.com/downloads/jlink/">J-Link Windows版本</a> &gt;= 6.56</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
</ul>
</section>
<section id="stm32f746工程构建">
<h3>STM32F746工程构建<a class="headerlink" href="#stm32f746工程构建" title="Permalink to this headline"></a></h3>
<ul>
<li><p>需要组织的工程目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── mnist              # codegen生成的模型推理代码
├── include            # 模型推理对外API头文件目录(需要自建)
└── operator_library   # 模型推理算子相关文件(需要自建)
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>模型推理对外API头文件可由MindSpore团队发布的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/downloads.html">Release包</a>中获取。</p>
<p>在编译此工程之前需要预先获取对应平台所需要的算子文件，由于Cortex-M平台工程编译一般涉及到较复杂的交叉编译，此处不提供直接预编译的算子库静态库，而是用户根据模型自行组织文件，自主编译Cortex-M7 、Coretex-M4、Cortex-M3等工程(对应工程目录结构已在示例代码中给出，用户可自主将对应ARM官方的CMSIS源码放置其中即可)。</p>
</div></blockquote>
<ul class="simple">
<li><p>使用codegen编译<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">MNIST手写数字识别模型</a>，生成对应的STM32F46推理代码。具体命令如下：</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>其中config 文件中配置target = ARM32M。</p>
<ul>
<li><p>生成代码工程目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── mnist               # 生成代码的根目录
    ├── benchmark       # 生成代码的benchmark目录
    └── src             # 模型推理代码目录
</pre></div>
</div>
</li>
<li><p>预置算子静态库的目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── operator_library    # 平台算子库目录
    ├── include         # 平台算子库头文件目录
    └── nnacl           # MindSpore团队提供的平台算子库源文件
    └── wrapper         # MindSpore团队提供的平台算子库源文件
    └── CMSIS           # Arm官方提供的CMSIS平台算子库源文件
</pre></div>
</div>
<blockquote>
<div><p>在使用过程中，引入CMSIS v5.7.0 Softmax相关的CMSIS算子文件时，头文件中需要加入<code class="docutils literal notranslate"><span class="pre">arm_nnfunctions.h</span></code>。</p>
</div></blockquote>
</li>
</ul>
<section id="代码工程编译">
<h4>代码工程编译<a class="headerlink" href="#代码工程编译" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>环境测试</p>
<p>安装好交叉编译所需环境后，需要在Windows环境中依次将其加入到环境变量中。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>gcc -v               # 查看GCC版本
arm-none-eabi-gdb -v # 查看交叉编译环境
jlink -v             # 查看J-Link版本
make -v              # 查看Make版本
</pre></div>
</div>
<p>以上命令均成功返回值时，表明环境准备已完成，可以继续进入下一步，否则请务必先安装上述环境。</p>
</li>
<li><p>生成STM32F746单板初始化代码（[详情示例代码](https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mnist_stm32f746）</p>
<ul class="simple">
<li><p>启动 STM32CubeMX，新建project，选择单板STM32F746IG。</p></li>
<li><p>成功以后，选择<code class="docutils literal notranslate"><span class="pre">Makefile</span></code> ，<code class="docutils literal notranslate"><span class="pre">generator</span> <span class="pre">code</span></code>。</p></li>
<li><p>在生成的工程目录下打开<code class="docutils literal notranslate"><span class="pre">cmd</span></code>，执行<code class="docutils literal notranslate"><span class="pre">make</span></code>，测试初始代码是否成功编译。</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># make成功结果
arm-none-eabi-size build/test_stm32f746.elf
  text    data     bss     dec     hex filename
  3660      20    1572    5252    1484 build/test_stm32f746.elf
arm-none-eabi-objcopy -O ihex build/test_stm32f746.elf build/test_stm32f746.hex
arm-none-eabi-objcopy -O binary -S build/test_stm32f746.elf build/test_stm32f746.bin
</pre></div>
</div>
</li>
</ol>
</section>
<section id="编译模型">
<h4>编译模型<a class="headerlink" href="#编译模型" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>拷贝MindSpore团队提供算子文件以及对应头文件到STM32CubeMX生成的工程目录中。</p></li>
<li><p>拷贝codegen生成模型推理代码到 STM32CubeMX生成的代码工程目录中。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── .mxproject
├── build             # 工程编译输出目录
├── Core
├── Drivers
├── mnist             # codegen生成的cortex-m7 模型推理代码
├── Makefile          # 编写工程makefile文件组织mnist &amp;&amp; operator_library源文件到工程目录中
├── startup_stm32f746xx.s
├── STM32F746IGKx_FLASH.ld
└── test_stm32f746.ioc
</pre></div>
</div>
</li>
<li><p>修改makefile文件，组织算子静态库以及模型推理代码，具体makefile文件内容参见<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mnist_stm32f746">示例</a>。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># C includes
C_INCLUDES =  \
-ICore/Inc \
-IDrivers/STM32F7xx_HAL_Driver/Inc \
-IDrivers/STM32F7xx_HAL_Driver/Inc/Legacy \
-IDrivers/CMSIS/Device/ST/STM32F7xx/Include \
-Imnist/operator_library/include \                # 新增，指定算子库头文件目录
-Imnist/include \                                 # 新增，指定模型推理代码头文件
-Imnist/src                                       # 新增，指定模型推理代码源文件
......
</pre></div>
</div>
</li>
<li><p>在工程目录的Core/Src的main.c编写模型调用代码，参考代码如下：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cm">/* USER CODE END WHILE */</span>
<span class="w">    </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;***********mnist test start***********</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextHandle</span><span class="w"> </span><span class="n">ms_context_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>
<span class="w">    </span><span class="n">ms_context_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSContextCreate</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ms_context_handle</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSContextSetThreadNum</span><span class="p">(</span><span class="n">ms_context_handle</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">      </span><span class="n">MSContextSetThreadAffinityMode</span><span class="p">(</span><span class="n">ms_context_handle</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">model_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="c1">// read net.bin</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">model_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadInputData</span><span class="p">(</span><span class="s">&quot;net.bin&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_size</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelHandle</span><span class="w"> </span><span class="n">model_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelCreate</span><span class="p">();</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelBuild</span><span class="p">(</span><span class="n">model_handle</span><span class="p">,</span><span class="w"> </span><span class="n">model_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">model_size</span><span class="p">,</span><span class="w"> </span><span class="n">kMSModelTypeMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">ms_context_handle</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ms_context_handle</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buffer</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">free</span><span class="p">(</span><span class="n">model_buffer</span><span class="p">);</span>
<span class="w">      </span><span class="n">model_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// read input_data.bin</span>
<span class="w">    </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">inputs_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelGetInputs</span><span class="p">(</span><span class="n">model_handle</span><span class="p">);</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">inputs_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_handle</span><span class="p">.</span><span class="n">handle_num</span><span class="p">;</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">inputs_num</span><span class="p">];</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">inputs_num</span><span class="p">];</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSTensorHandle</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_handle</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">      </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">MSTensorGetDataSize</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadInputsFile</span><span class="p">(</span><span class="s">&quot;input.bin&quot;</span><span class="w"> </span><span class="n">inputs_binbuf</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">inputs_num</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSTensorGetMutableData</span><span class="p">(</span><span class="n">inputs_handle</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="n">memcpy</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="n">free</span><span class="p">(</span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">outputs_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelGetOutputs</span><span class="p">(</span><span class="n">model_handle</span><span class="p">);</span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelPredict</span><span class="p">(</span><span class="n">model_handle</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_handle</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs_handle</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model_handle</span><span class="p">);</span>
<span class="w">      </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="s">&quot;MSModelPredict failed, ret: %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">);</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs_handle</span><span class="p">.</span><span class="n">handle_num</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSTensorHandle</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs_handle</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">      </span><span class="n">PrintTensorHandle</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;***********mnist test end***********</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>在工程跟目中目录使用管理员权限打开<code class="docutils literal notranslate"><span class="pre">cmd</span></code> 执行 <code class="docutils literal notranslate"><span class="pre">make</span></code>进行编译。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="stm32f746工程部署">
<h3>STM32F746工程部署<a class="headerlink" href="#stm32f746工程部署" title="Permalink to this headline"></a></h3>
<p>使用J-Link将可执行文件拷贝到单板上并做推理。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>jlinkgdbserver           # 启动jlinkgdbserver 选定target device为STM32F746IG
jlinkRTTViewer           # 启动jlinkRTTViewer 选定target devices为STM32F746IG
arm-none-eabi-gdb        # 启动arm-gcc gdb服务
file build/target.elf    # 打开调测文件
target remote 127.0.0.1  # 连接jlink服务器
monitor reset            # 重置单板
monitor halt             # 挂起单板
load                     # 加载可执行文件到单板
c                        # 执行模型推理
</pre></div>
</div>
</section>
</section>
<section id="在轻鸿蒙设备上执行推理">
<h2>在轻鸿蒙设备上执行推理<a class="headerlink" href="#在轻鸿蒙设备上执行推理" title="Permalink to this headline"></a></h2>
<section id="安装轻鸿蒙编译环境">
<h3>安装轻鸿蒙编译环境<a class="headerlink" href="#安装轻鸿蒙编译环境" title="Permalink to this headline"></a></h3>
<p>详细请参考<a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-env-setup-lin-0000001105407498">Ubuntu编译环境准备</a>。</p>
</section>
<section id="开发板环境配置">
<h3>开发板环境配置<a class="headerlink" href="#开发板环境配置" title="Permalink to this headline"></a></h3>
<p>以Hi3516开发板为例，请参考<a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-steps-board3516-setting-0000001105829366">安装开发板环境</a>。</p>
</section>
<section id="编译模型-1">
<h3>编译模型<a class="headerlink" href="#编译模型-1" title="Permalink to this headline"></a></h3>
<p>使用codegen编译<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">lenet模型</a>，生成对应轻鸿蒙平台的推理代码，命令如下:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>其中config配置文件设置target = ARM32A。</p>
</section>
<section id="编写构建脚本">
<h3>编写构建脚本<a class="headerlink" href="#编写构建脚本" title="Permalink to this headline"></a></h3>
<p>轻鸿蒙应用程序开发请先参考<a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-steps-board3516-running-0000001151888681">运行Hello OHOS</a>。将上一步生成的mnist目录拷贝到任意鸿蒙源码路径下，假设为applications/sample/，然后新建BUILD.gn文件：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;harmony-source-path&gt;/applications/sample/mnist
├── benchmark
├── CMakeLists.txt
├── BUILD.gn
└── src  
</pre></div>
</div>
<p>下载适用于OpenHarmony的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/downloads.html">预编译推理runtime包</a>，然后将其解压至任意鸿蒙源码路径下。编写BUILD.gn文件：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import(&quot;//build/lite/config/component/lite_component.gni&quot;)
import(&quot;//build/lite/ndk/ndk.gni&quot;)

lite_component(&quot;mnist_benchmark&quot;) {
    target_type = &quot;executable&quot;
    sources = [
         &quot;benchmark/benchmark.cc&quot;,
         &quot;benchmark/calib_output.cc&quot;,
         &quot;benchmark/load_input.c&quot;,
         &quot;src/net.c&quot;,
         &quot;src/weight.c&quot;,
         &quot;src/session.cc&quot;,
         &quot;src/tensor.cc&quot;,
    ]
    features = []
    include_dirs = [
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime&quot;,
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/include&quot;,
         &quot;//applications/sample/mnist/benchmark&quot;,
         &quot;//applications/sample/mnist/src&quot;,
    ]
    ldflags = [
         &quot;-fno-strict-aliasing&quot;,
         &quot;-Wall&quot;,
         &quot;-pedantic&quot;,
         &quot;-std=gnu99&quot;,
    ]
    libs = [
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime/lib/libmindspore-lite.a&quot;,
         &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/lib/libwrapper.a&quot;,
    ]
    defines = [
        &quot;NOT_USE_STL&quot;,
        &quot;ENABLE_NEON&quot;,
        &quot;ENABLE_ARM&quot;,
        &quot;ENABLE_ARM32&quot;
    ]
    cflags = [
         &quot;-fno-strict-aliasing&quot;,
         &quot;-Wall&quot;,
         &quot;-pedantic&quot;,
         &quot;-std=gnu99&quot;,
    ]
    cflags_cc = [
        &quot;-fno-strict-aliasing&quot;,
        &quot;-Wall&quot;,
        &quot;-pedantic&quot;,
        &quot;-std=c++17&quot;,
    ]
}
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&lt;YOUR</span> <span class="pre">MINDSPORE</span> <span class="pre">LITE</span> <span class="pre">RUNTIME</span> <span class="pre">PATH&gt;</span></code>是解压出来的推理runtime包路径，比如//applications/sample/mnist/mindspore-lite-1.3.0-ohos-aarch32。
修改文件build/lite/components/applications.json，添加组件mnist_benchmark的配置：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{
   &quot;component&quot;: &quot;mnist_benchmark&quot;,
   &quot;description&quot;: &quot;Communication related samples.&quot;,
   &quot;optional&quot;: &quot;true&quot;,
   &quot;dirs&quot;: [
     &quot;applications/sample/mnist&quot;
   ],
   &quot;targets&quot;: [
     &quot;//applications/sample/mnist:mnist_benchmark&quot;
   ],
   &quot;rom&quot;: &quot;&quot;,
   &quot;ram&quot;: &quot;&quot;,
   &quot;output&quot;: [],
   &quot;adapted_kernel&quot;: [ &quot;liteos_a&quot; ],
   &quot;features&quot;: [],
   &quot;deps&quot;: {
     &quot;components&quot;: [],
     &quot;third_party&quot;: []
   }
 },
</pre></div>
</div>
<p>修改文件vendor/hisilicon/hispark_taurus/config.json，新增mnist_benchmark组件的条目:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> { &quot;component&quot;: &quot;mnist_benchmark&quot;, &quot;features&quot;:[] }
</pre></div>
</div>
</section>
<section id="编译benchmark">
<h3>编译benchmark<a class="headerlink" href="#编译benchmark" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd &lt;openharmony-source-path&gt;
hb set(设置编译路径)
.（选择当前路径）
选择ipcamera_hispark_taurus@hisilicon并回车
hb build mnist_benchmark（执行编译）
</pre></div>
</div>
<p>生成结果文件out/hispark_taurus/ipcamera_hispark_taurus/bin/mnist_benchmark。</p>
</section>
<section id="执行benchmark">
<h3>执行benchmark<a class="headerlink" href="#执行benchmark" title="Permalink to this headline"></a></h3>
<p>将mnist_benchmark、权重文件（mnist/src/net.bin）以及<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">输入文件</a>解压后拷贝到开发板上，然后执行：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> OHOS # ./mnist_benchmark mnist_input.bin net.bin 1
 OHOS # =======run benchmark======
 input 0: mnist_input.bin

 loop count: 1
 total time: 10.11800ms, per time: 10.11800ms

 outputs:
 name: int8toft32_Softmax-7_post0/output-0, DataType: 43, Elements: 10, Shape: [1 10 ], Data:
 0.000000, 0.000000, 0.003906, 0.000000, 0.000000, 0.992188, 0.000000, 0.000000, 0.000000, 0.000000,
 ========run success=======
</pre></div>
</div>
</section>
</section>
<section id="自定义算子">
<h2>自定义算子<a class="headerlink" href="#自定义算子" title="Permalink to this headline"></a></h2>
<p>使用前请先参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/register_kernel.html">自定义南向算子</a>了解基本概念。Codegen目前仅支持custom类型的自定义算子注册和实现，暂不支持内建算子（比如conv2d、fc等）的注册和自定义实现。下面以海思Hi3516D开发板为例，说明如何在codegen中使用自定义算子。</p>
<p>使用最新的转换工具生成带NNIE类型custom算子具体步骤请参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/nnie.html">集成NNIE使用说明</a>。</p>
<p>模型生成代码方式与非定义算子模型保持一致：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>其中config配置文件设置target = ARM32A。</p>
<section id="用户实现自定义算子">
<h3>用户实现自定义算子<a class="headerlink" href="#用户实现自定义算子" title="Permalink to this headline"></a></h3>
<p>上一步会在当前路径下生成nnie源码目录，其有一个叫registered_kernel.h的头文件指定了custom算子的函数声明：</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CustomKernel</span><span class="p">(</span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">input_num</span><span class="p">,</span><span class="w"> </span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">output_num</span><span class="p">,</span><span class="w"> </span><span class="n">CustomParameter</span><span class="w"> </span><span class="o">*</span><span class="n">param</span><span class="p">);</span>
</pre></div>
</div>
<p>用户需要提供该函数的实现，并将相关源码或者库集成到生成代码的cmake工程中。例如，我们提供了支持海思NNIE的custom kernel示例动态库libmicro_nnie.so，该文件包含在<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/downloads.html">官网下载页</a>《NNIE 推理runtime及benchmark工具》组件中。用户需要修改生成代码的CMakeLists.txt，填加链接的库名称和路径。例如：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>link_directories<span class="o">(</span>&lt;YOUR_PATH&gt;/mindspore-lite-1.5.0-linux-aarch32/providers/Hi3516D<span class="o">)</span>

link_directories<span class="o">(</span>&lt;HI3516D_SDK_PATH&gt;<span class="o">)</span>

target_link_libraries<span class="o">(</span>benchmark<span class="w"> </span>net<span class="w"> </span>micro_nnie<span class="w"> </span>nnie<span class="w"> </span>mpi<span class="w"> </span>VoiceEngine<span class="w"> </span>upvqe<span class="w"> </span>securec<span class="w"> </span>-lm<span class="w"> </span>-pthread<span class="o">)</span>
</pre></div>
</div>
<p>最后进行源码编译：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>nnie<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mkdir<span class="w"> </span>buid<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build

cmake<span class="w"> </span>-DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>&lt;MS_SRC_PATH&gt;/mindspore/lite/cmake/himix200.toolchain.cmake<span class="w"> </span>-DPLATFORM_ARM32<span class="o">=</span>ON<span class="w"> </span>-DPKG_PATH<span class="o">=</span>&lt;RUNTIME_PKG_PATH&gt;<span class="w"> </span>..

make
</pre></div>
</div>
</section>
</section>
<section id="其它平台使用说明">
<h2>其它平台使用说明<a class="headerlink" href="#其它平台使用说明" title="Permalink to this headline"></a></h2>
<section id="linux-x86-64平台编译部署">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mnist_x86">Linux_x86_64平台编译部署</a><a class="headerlink" href="#linux-x86-64平台编译部署" title="Permalink to this headline"></a></h3>
</section>
<section id="android平台编译部署">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mobilenetv2_arm64">Android平台编译部署</a><a class="headerlink" href="#android平台编译部署" title="Permalink to this headline"></a></h3>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_java.html" class="btn btn-neutral float-left" title="使用Java接口执行推理" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="asic.html" class="btn btn-neutral float-right" title="专用芯片集成说明" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>