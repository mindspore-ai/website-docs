

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>训练后量化 &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="预处理数据" href="data_preprocessing.html" />
    <link rel="prev" title="推理模型转换" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">一小时入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">体验C++极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">体验Java极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">训练后量化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#配置参数">配置参数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#通用量化参数">通用量化参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#混合比特权重量化参数">混合比特权重量化参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#全量化参数">全量化参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#数据预处理">数据预处理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#权重量化">权重量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#混合比特量化">混合比特量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#固定比特量化">固定比特量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#部分模型精度结果">部分模型精度结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#全量化">全量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#部分模型精度结果-1">部分模型精度结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#动态量化">动态量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#部分模型性能结果">部分模型性能结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#量化debug">量化Debug</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#设置无需量化node">设置无需量化Node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#使用建议">使用建议</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在轻量和小型系统上执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption"><span class="caption-text">第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>训练后量化</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="训练后量化">
<h1>训练后量化<a class="headerlink" href="#训练后量化" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.7/docs/lite/docs/source_zh_cn/use/post_training_quantization.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png"></a></p>
<div class="section" id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline">¶</a></h2>
<p>对于已经训练好的<code class="docutils literal notranslate"><span class="pre">float32</span></code>模型，通过训练后量化将其转为<code class="docutils literal notranslate"><span class="pre">int8</span></code>，不仅能减小模型大小，而且能显著提高推理性能。在MindSpore Lite中，这部分功能集成在模型转换工具<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>内，通过配置<code class="docutils literal notranslate"><span class="pre">量化配置文件</span></code>的方式，便能够转换得到量化后模型。</p>
<p>MindSpore Lite训练后量化分为两类：</p>
<ol class="simple">
<li><p>权重量化：对模型的权值进行量化，仅压缩模型大小，推理时仍然执行<code class="docutils literal notranslate"><span class="pre">float32</span></code>推理；</p></li>
<li><p>全量化：对模型的权值、激活值等统一进行量化，推理时执行<code class="docutils literal notranslate"><span class="pre">int</span></code>运算，能提升模型推理速度、降低功耗。</p></li>
</ol>
</div>
<div class="section" id="配置参数">
<h2>配置参数<a class="headerlink" href="#配置参数" title="Permalink to this headline">¶</a></h2>
<p>训练后量化可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/converter_tool.html">转换工具</a>配置<code class="docutils literal notranslate"><span class="pre">configFile</span></code>的方式启用训练后量化。配置文件采用<code class="docutils literal notranslate"><span class="pre">INI</span></code>的风格，针对量化场景，目前可配置的参数包括<code class="docutils literal notranslate"><span class="pre">通用量化参数[common_quant_param]</span></code>、<code class="docutils literal notranslate"><span class="pre">混合比特权重量化参数[mixed_bit_weight_quant_param]</span></code>、<code class="docutils literal notranslate"><span class="pre">全量化参数[full_quant_param]</span></code>和<code class="docutils literal notranslate"><span class="pre">数据预处理参数[data_preprocess_param]</span></code>。</p>
<div class="section" id="通用量化参数">
<h3>通用量化参数<a class="headerlink" href="#通用量化参数" title="Permalink to this headline">¶</a></h3>
<p>通用量化参数是训练后量化的基本设置，主要包括<code class="docutils literal notranslate"><span class="pre">quant_type</span></code>、<code class="docutils literal notranslate"><span class="pre">bit_num</span></code>、<code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>和<code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>。参数的详细介绍如下所示：</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>参数</th>
<th>属性</th>
<th>功能描述</th>
<th>参数类型</th>
<th>默认值</th>
<th>取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>quant_type</code></td>
<td>必选</td>
<td>设置量化类型，设置为WEIGHT_QUANT时，启用权重量化；设置为FULL_QUANT时，启用全量化；设置为DYNAMIC_QUANT时，启用动态量化。</td>
<td>String</td>
<td>-</td>
<td>WEIGHT_QUANT、FULL_QUANT、DYNAMIC_QUANT</td>
</tr>
<tr>
<td><code>bit_num</code></td>
<td>可选</td>
<td>设置量化的比特数，目前权重量化支持0-16bit量化，设置为1-16bit时为固定比特量化，设置为0bit时，启用混合比特量化。全量化、动态量化支持8bit量化。</td>
<td>Integer</td>
<td>8</td>
<td>权重量化：[0，16]<br/>全量化：8<br/>动态量化：8</td>
</tr>
<tr>
<td><code>min_quant_weight_size</code></td>
<td>可选</td>
<td>设置参与量化的权重尺寸阈值，若权重数大于该值，则对此权重进行量化。</td>
<td>Integer</td>
<td>0</td>
<td>[0, 65535]</td>
</tr>
<tr>
<td><code>min_quant_weight_channel</code></td>
<td>可选</td>
<td>设置参与量化的权重通道数阈值，若权重通道数大于该值，则对此权重进行量化。</td>
<td>Integer</td>
<td>16</td>
<td>[0, 65535]</td>
</tr>
<tr>
<td><code>skip_quant_node</code></td>
<td>可选</td>
<td>设置无需量化的算子名称，多个算子之间用<code>,</code>分割。</td>
<td>String</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>debug_info_save_path</code></td>
<td>可选</td>
<td>设置量化Debug信息文件保存的文件夹路径。</td>
<td>String</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>目前<code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>、<code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>仅对权重量化有效。</p>
<p>建议：全量化在精度不满足的情况下，可设置<code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code>开启Debug模式得到相关统计报告，针对不适合量化的算子设置<code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code>对其不进行量化。</p>
</div></blockquote>
<p>通用量化参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
<span class="c1"># Set the name of the operator that skips the quantization, and use `,` to split between multiple operators.</span>
<span class="na">skip_quant_node</span><span class="o">=</span><span class="s">node_name1,node_name2,node_name3</span>
<span class="c1"># Set the folder path where the quantization debug information file is saved.</span>
<span class="na">debug_info_save_path</span><span class="o">=</span><span class="s">/home/workspace/mindspore/debug_info_save_path</span>
</pre></div>
</div>
</div>
<div class="section" id="混合比特权重量化参数">
<h3>混合比特权重量化参数<a class="headerlink" href="#混合比特权重量化参数" title="Permalink to this headline">¶</a></h3>
<p>混合比特权重量化参数包括<code class="docutils literal notranslate"><span class="pre">init_scale</span></code>，启用混合比特权重量化后，将会针对不同层自动搜索最优的比特数。参数的详细介绍如下所示：</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>参数</th>
<th>属性</th>
<th>功能描述</th>
<th>参数类型</th>
<th>默认值</th>
<th>取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td>init_scale</td>
<td>可选</td>
<td>初始化scale，数值越大可以带来更大的压缩率，但是也会造成不同程度的精度损失</td>
<td>float</td>
<td>0.02</td>
<td>(0 , 1)</td>
</tr>
<tr>
<td>auto_tune</td>
<td>可选</td>
<td>自动搜索init_scale参数，设置后将自动会搜索一组模型输出Tensor在余弦相似度在0.995左右的<code>init_scale</code>值</td>
<td>Boolean</td>
<td>False</td>
<td>True，False</td>
</tr>
</tbody>
</table>
<p>混合比特量化参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[mixed_bit_weight_quant_param]</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
<span class="na">auto_tune</span><span class="o">=</span><span class="s">false</span>
</pre></div>
</div>
</div>
<div class="section" id="全量化参数">
<h3>全量化参数<a class="headerlink" href="#全量化参数" title="Permalink to this headline">¶</a></h3>
<p>全量化参数主要包括<code class="docutils literal notranslate"><span class="pre">activation_quant_method</span></code>，<code class="docutils literal notranslate"><span class="pre">bias_correction</span></code> 和<code class="docutils literal notranslate"><span class="pre">target_device</span></code>。参数的详细介绍如下所示：</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>参数</th>
<th>属性</th>
<th>功能描述</th>
<th>参数类型</th>
<th>默认值</th>
<th>取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td>activation_quant_method</td>
<td>可选</td>
<td>激活值量化算法</td>
<td>String</td>
<td>MAX_MIN</td>
<td>KL，MAX_MIN，RemovalOutlier。 <br>KL：基于<a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">KL散度</a>对数据范围作量化校准。 <br>MAX_MIN：基于最大值、最小值计算数据的量化参数。 <br>RemovalOutlier：按照一定比例剔除数据的极大极小值，再计算量化参数。 <br>在校准数据集与实际推理时的输入数据相吻合的情况下，推荐使用MAX_MIN；而在校准数据集噪声比较大的情况下，推荐使用KL或者REMOVAL_OUTLIER</td>
</tr>
<tr>
<td>bias_correction</td>
<td>可选</td>
<td>是否对量化误差进行校正</td>
<td>Boolean</td>
<td>True</td>
<td>True，False。使能后，将能提升量化模型的精度。</td>
</tr>
<tr>
<td>target_device</td>
<td>可选</td>
<td>全量化支持多硬件后端。设置特定硬件后，量化模型会调用专有硬件量化算子库进行推理；如果未设置，转换模型调用通用量化算子库。</td>
<td>String</td>
<td>-</td>
<td>NVGPU: 转换后的量化模型可以在NVIDIA GPU上执行量化推理。</td>
</tr>
</tbody>
</table>
<p>通用全量化参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<p>NVIDIA GPU全量化参数配置如下：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Supports specific hardware backends</span>
<span class="na">target_device</span><span class="o">=</span><span class="s">NVGPU</span>
</pre></div>
</div>
</div>
<div class="section" id="数据预处理">
<h3>数据预处理<a class="headerlink" href="#数据预处理" title="Permalink to this headline">¶</a></h3>
<p>全量化需要提供100-500张的校准数据集进行预推理，用于计算全量化激活值的量化参数。如果存在多个输入Tensor，每个输入Tensor的校准数据集需要各自保存一个文件夹。</p>
<p>针对BIN格式的校准数据集，<code class="docutils literal notranslate"><span class="pre">.bin</span></code>文件存储的是输入的数据Buffer，同时输入数据的Format需要和推理时输入数据的Format保持一致。针对四维数据，默认是<code class="docutils literal notranslate"><span class="pre">NHWC</span></code>。如果配置了转换工具的命令参数<code class="docutils literal notranslate"><span class="pre">inputDataFormat</span></code>，输入的Buffer的Format需要保持一致。</p>
<p>针对图片格式的校准数据集，后量化提供通道转换、归一化、缩放和裁剪等数据预处理功能。</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>参数</th>
<th>属性</th>
<th>功能描述</th>
<th>参数类型</th>
<th>默认值</th>
<th>取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td>calibrate_path</td>
<td>必选</td>
<td>存放校准数据集的目录；如果模型有多个输入，请依次填写对应的数据所在目录，目录路径间请用<code>,</code>隔开</td>
<td>String</td>
<td>-</td>
<td>input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</td>
</tr>
<tr>
<td>calibrate_size</td>
<td>必选</td>
<td>矫正集数量</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>input_type</td>
<td>必选</td>
<td>矫正数据文件格式类型</td>
<td>String</td>
<td>-</td>
<td>IMAGE、BIN <br>IMAGE：图片文件数据 <br>BIN：满足推理要求的输入二进制<code>.bin</code>文件数据</td>
</tr>
<tr>
<td>image_to_format</td>
<td>可选</td>
<td>图像格式转换</td>
<td>String</td>
<td>-</td>
<td>RGB、GRAY、BGR</td>
</tr>
<tr>
<td>normalize_mean</td>
<td>可选</td>
<td>图像归一化的均值<br/>dst = (src - mean) / std</td>
<td>Vector</td>
<td>-</td>
<td>3通道：[mean_1, mean_2, mean_3] <br/>1通道：[mean_1]</td>
</tr>
<tr>
<td>normalize_std</td>
<td>可选</td>
<td>图像归一化的标准差<br/>dst = (src - mean) / std</td>
<td>Vector</td>
<td>-</td>
<td>3通道：[std_1, std_2, std_3] <br/>1通道：[std_1]</td>
</tr>
<tr>
<td>resize_width</td>
<td>可选</td>
<td>图像缩放宽度</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>resize_height</td>
<td>可选</td>
<td>图像缩放高度</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>resize_method</td>
<td>可选</td>
<td>图像缩放算法</td>
<td>String</td>
<td>-</td>
<td>LINEAR、NEAREST、CUBIC<br/>LINEAR：线性插值<br/>NEARST：最邻近插值<br/>CUBIC：三次样条插值</td>
</tr>
<tr>
<td>center_crop_width</td>
<td>可选</td>
<td>中心裁剪宽度</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>center_crop_height</td>
<td>可选</td>
<td>中心裁剪高度</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
</tbody>
</table>
<p>数据预处理参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="权重量化">
<h2>权重量化<a class="headerlink" href="#权重量化" title="Permalink to this headline">¶</a></h2>
<p>权重量化支持混合比特量化，同时也支持1~16之间的固定比特量化，比特数越低，模型压缩率越大，但是精度损失通常也比较大。下面对权重量化的使用方式和效果进行阐述。</p>
<div class="section" id="混合比特量化">
<h3>混合比特量化<a class="headerlink" href="#混合比特量化" title="Permalink to this headline">¶</a></h3>
<p>目前权重量化支持混合比特量化，会根据模型参数的分布情况，根据用户设置的<code class="docutils literal notranslate"><span class="pre">init_scale</span></code>作为初始值，自动搜索出最适合当前层的比特数。配置参数的<code class="docutils literal notranslate"><span class="pre">bit_num</span></code>设置为0时，将启用混合比特量化。</p>
<p>混合比特权重量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/mixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>混合比特权重量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">5000</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">5</span>

<span class="k">[mixed_bit_weight_quant_param]</span>
<span class="c1"># Initialization scale in (0,1).</span>
<span class="c1"># A larger value can get a larger compression ratio, but it may also cause a larger error.</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
<p>用户可根据模型及自身需要对权重量化的参数作出调整。</p>
<blockquote>
<div><p>init_scale默认的初始值为0.02，搜索的压缩率相当与6-7固定比特的压缩效果。</p>
<p>混合比特需要搜索最佳比特位，等待时间可能较长，如果需要查看日志，可以在执行前设置export GLOG_v=1，用于打印相关Info级别日志。</p>
</div></blockquote>
</div>
<div class="section" id="固定比特量化">
<h3>固定比特量化<a class="headerlink" href="#固定比特量化" title="Permalink to this headline">¶</a></h3>
<p>固定比特的权重量化支持1~16之间的固定比特量化，用户可根据模型及自身需要对权重量化的参数作出调整。</p>
<p>固定比特权重量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/fixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>固定比特权重量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</div>
<div class="section" id="部分模型精度结果">
<h3>部分模型精度结果<a class="headerlink" href="#部分模型精度结果" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>模型</th>
<th>测试数据集</th>
<th>FP32模型精度</th>
<th>权重量化精度（8bit）</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>77.60%</td>
<td>77.53%</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>70.96%</td>
<td>70.56%</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>71.56%</td>
<td>71.53%</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得。</p>
</div></blockquote>
</div>
</div>
<div class="section" id="全量化">
<h2>全量化<a class="headerlink" href="#全量化" title="Permalink to this headline">¶</a></h2>
<p>针对CV模型需要提升模型运行速度、降低模型运行功耗的场景，可以使用训练后全量化功能。下面对全量化的使用方式和效果进行阐述。</p>
<p>全量化计算激活值的量化参数，用户需要提供校准数据集。校准数据集最好来自真实推理场景，能表征模型的实际输入情况，数量在100 - 500个左右。</p>
<p>针对图片数据，目前支持通道调整、归一化、缩放、裁剪等预处理的功能。用户可以根据推理时所需的预处理操作，设置相应的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/post_training_quantization.html#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">参数</a>。</p>
<p>注意：</p>
<ul class="simple">
<li><p>模型校准数据必须与训练数据同分布，并且校准数据与模型输入的format（例如：NCHW、NHWC）需要保持一致。</p></li>
</ul>
<p>全量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg
</pre></div>
</div>
<p>全量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>

<span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>

<span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<blockquote>
<div><p>全量化需要执行推理，等待时间可能较长，如果需要查看日志，可以在执行前设置export GLOG_v=1，用于打印相关Info级别日志。</p>
</div></blockquote>
<div class="section" id="部分模型精度结果-1">
<h3>部分模型精度结果<a class="headerlink" href="#部分模型精度结果-1" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>模型</th>
<th>测试数据集</th>
<th>量化方法</th>
<th>FP32模型精度</th>
<th>全量化精度（8bit）</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>KL</td>
<td>77.60%</td>
<td>77.40%</td>
<td>校准数据集随机选择ImageNet Validation数据集中的100张</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>KL</td>
<td>70.96%</td>
<td>70.31%</td>
<td>校准数据集随机选择ImageNet Validation数据集中的100张</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>MAX_MIN</td>
<td>71.56%</td>
<td>71.16%</td>
<td>校准数据集随机选择ImageNet Validation数据集中的100张</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得。</p>
</div></blockquote>
</div>
</div>
<div class="section" id="动态量化">
<h2>动态量化<a class="headerlink" href="#动态量化" title="Permalink to this headline">¶</a></h2>
<p>针对NLP模型需要提升模型运行速度、降低模型运行功耗的场景，可以使用动态量化功能。下面对动态量化的使用方式和效果进行阐述。</p>
<p>动态量化的权重是离线转换阶段量化，而激活是在运行阶段才进行量化。</p>
<p>动态量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/dynamic_quant.cfg
</pre></div>
</div>
<p>动态量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">DYNAMIC_QUANT</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
</pre></div>
</div>
<blockquote>
<div><p>为了保证量化精度，目前动态量化不支持设置FP16的运行模式。</p>
<p>目前动态量化在支持SDOT指令的ARM架构会有进一步的加速效果。</p>
</div></blockquote>
<div class="section" id="部分模型性能结果">
<h3>部分模型性能结果<a class="headerlink" href="#部分模型性能结果" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>tinybert_encoder</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>模型类型</th>
<th>运行模式</th>
<th>Model Size(M)</th>
<th>RAM(K)</th>
<th>Latency(ms)</th>
<th>Cos-Similarity</th>
<th>压缩率</th>
<th>内存相比FP32</th>
<th>时延相比FP32</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>FP32</td>
<td>20</td>
<td>29,029</td>
<td>9.916</td>
<td>1</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>FP32</td>
<td>FP16</td>
<td>20</td>
<td>18,208</td>
<td>5.75</td>
<td>0.99999</td>
<td>1</td>
<td>-37.28%</td>
<td>-42.01%</td>
</tr>
<tr>
<td>FP16</td>
<td>FP16</td>
<td>12</td>
<td>18,105</td>
<td>5.924</td>
<td>0.99999</td>
<td>1.66667</td>
<td>-37.63%</td>
<td>-40.26%</td>
</tr>
<tr>
<td>Weight Quant(8 Bit)</td>
<td>FP16</td>
<td>5.3</td>
<td>19,324</td>
<td>5.764</td>
<td>0.99994</td>
<td>3.77358</td>
<td>-33.43%</td>
<td>-41.87%</td>
</tr>
<tr>
<td><strong>Dynamic Quant</strong></td>
<td><strong>INT8+FP32</strong></td>
<td><strong>5.2</strong></td>
<td><strong>15,709</strong></td>
<td><strong>4.517</strong></td>
<td><strong>0.99668</strong></td>
<td><strong>3.84615</strong></td>
<td><strong>-45.89%</strong></td>
<td><strong>-54.45%</strong></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>tinybert_decoder</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>模型类型</th>
<th>运行模式</th>
<th>Model Size(M)</th>
<th>RAM(K)</th>
<th>Latency(ms)</th>
<th>Cos-Similarity</th>
<th>压缩率</th>
<th>内存相比FP32</th>
<th>时延相比FP32</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>FP32</td>
<td>43</td>
<td>51,355</td>
<td>4.161</td>
<td>1</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>FP32</td>
<td>FP16</td>
<td>43</td>
<td>29,462</td>
<td>2.184</td>
<td>0.99999</td>
<td>1</td>
<td>-42.63%</td>
<td>-47.51%</td>
</tr>
<tr>
<td>FP16</td>
<td>FP16</td>
<td>22</td>
<td>29,440</td>
<td>2.264</td>
<td>0.99999</td>
<td>1.95455</td>
<td>-42.67%</td>
<td>-45.59%</td>
</tr>
<tr>
<td>Weight Quant(8 Bit)</td>
<td>FP16</td>
<td>12</td>
<td>32,285</td>
<td>2.307</td>
<td>0.99998</td>
<td>3.58333</td>
<td>-37.13%</td>
<td>-44.56%</td>
</tr>
<tr>
<td><strong>Dynamic Quant</strong></td>
<td><strong>INT8+FP32</strong></td>
<td><strong>12</strong></td>
<td><strong>22,181</strong></td>
<td><strong>2.074</strong></td>
<td><strong>0.9993</strong></td>
<td><strong>3.58333</strong></td>
<td><strong>-56.81%</strong></td>
<td><strong>-50.16%</strong></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="量化debug">
<h2>量化Debug<a class="headerlink" href="#量化debug" title="Permalink to this headline">¶</a></h2>
<p>开启量化Debug功能，能够得到数据分布统计报告，用于评估量化误差，辅助决策模型（算子）是否适合量化。针对全量化，会根据所提供矫正数据集的数量，生成N份数据分布统计报告，即每一轮都会生成一份报告；针对权重量化，只会生成1份数据分布统计报告。</p>
<p>设置<code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code>参数后，将会在<code class="docutils literal notranslate"><span class="pre">/home/workspace/mindspore/debug_info_save_path</span></code>文件夹中生成相关Debug报告：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">debug_info_save_path</span><span class="o">=</span><span class="s">/home/workspace/mindspore/debug_info_save_path</span>
</pre></div>
</div>
<p>数据分布统计报告会统计每个Tensor原始数据分布以及量化Tensor反量化后的数据分布情况。数据分布统计报告相关字段如下所示：</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Type</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>NodeName</td>
<td>节点名</td>
</tr>
<tr>
<td>NodeType</td>
<td>节点类型</td>
</tr>
<tr>
<td>TensorName</td>
<td>Tensor名</td>
</tr>
<tr>
<td>InOutFlag</td>
<td>Tensor输出、输出类型</td>
</tr>
<tr>
<td>DataTypeFlag</td>
<td>数据类型，原始数据用Origin，反量化后的数据用Dequant</td>
</tr>
<tr>
<td>TensorTypeFlag</td>
<td>针对输入输出等数据类用Activation表示，常量等用Weight表示</td>
</tr>
<tr>
<td>Min</td>
<td>最小值，0%分位点</td>
</tr>
<tr>
<td>Q1</td>
<td>25%分位点</td>
</tr>
<tr>
<td>Median</td>
<td>中位数，50%分位点</td>
</tr>
<tr>
<td>Q3</td>
<td>75%分位点</td>
</tr>
<tr>
<td>MAX</td>
<td>最大值，100%分位点</td>
</tr>
<tr>
<td>Mean</td>
<td>均值</td>
</tr>
<tr>
<td>Var</td>
<td>方差</td>
</tr>
<tr>
<td>Sparsity</td>
<td>稀疏度</td>
</tr>
<tr>
<td>Clip</td>
<td>截断率</td>
</tr>
<tr>
<td>CosineSimilarity</td>
<td>和原始数据对比的余弦相似度</td>
</tr>
</tbody>
</table>
<p>量化参数文件<code class="docutils literal notranslate"><span class="pre">quant_param.csv</span></code>包含所有量化Tensor的量化参数信息，量化参数相关字段如下所示：</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Type</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>NodeName</td>
<td>节点名</td>
</tr>
<tr>
<td>NodeType</td>
<td>节点类型</td>
</tr>
<tr>
<td>TensorName</td>
<td>Tensor名</td>
</tr>
<tr>
<td>ElementsNum</td>
<td>Tensor数据量</td>
</tr>
<tr>
<td>Dims</td>
<td>Tensor维度</td>
</tr>
<tr>
<td>Scale</td>
<td>量化参数scale</td>
</tr>
<tr>
<td>ZeroPoint</td>
<td>量化参数ZeroPoint</td>
</tr>
<tr>
<td>Bits</td>
<td>量化比特数</td>
</tr>
<tr>
<td>CorrectionVar</td>
<td>误差矫正系数-方差</td>
</tr>
<tr>
<td>CorrectionMean</td>
<td>误差矫正系数-均值</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>由于混合比特量化是非标准量化，该量化参数文件可能不存在。</p>
</div></blockquote>
<div class="section" id="设置无需量化node">
<h3>设置无需量化Node<a class="headerlink" href="#设置无需量化node" title="Permalink to this headline">¶</a></h3>
<p>量化是将Float32算子转换Int8算子，目前的量化策略是针对可支持的某一类算子所包含的Node都会进行量化，但是存在部分Node敏感度较高，量化后会引发较大的误差，同时某些层量化后推理速度远低于Float16的推理速度。支持指定层不量化，可以有效提高精度和推理速度。</p>
<p>下面将<code class="docutils literal notranslate"><span class="pre">conv2d_1</span></code> <code class="docutils literal notranslate"><span class="pre">add_8</span></code> <code class="docutils literal notranslate"><span class="pre">concat_1</span></code>三个Node不进行量化的示例：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Set the name of the operator that skips the quantization, and use `,` to split between multiple operators.</span>
<span class="na">skip_quant_node</span><span class="o">=</span><span class="s">conv2d_1,add_8,concat_1</span>
</pre></div>
</div>
</div>
<div class="section" id="使用建议">
<h3>使用建议<a class="headerlink" href="#使用建议" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>通过过滤<code class="docutils literal notranslate"><span class="pre">InOutFlag</span> <span class="pre">==</span> <span class="pre">Output</span> <span class="pre">&amp;&amp;</span> <span class="pre">DataTypeFlag</span> <span class="pre">==</span> <span class="pre">Dequant</span></code>，可以筛选出所有量化算子的输出层，通过查看量化输出的<code class="docutils literal notranslate"><span class="pre">CosineSimilarity</span></code>来判断算子的精度损失，越接近1损失越小。</p></li>
<li><p>针对Add、Concat等合并类算子，如果不同输入Tensor之间<code class="docutils literal notranslate"><span class="pre">min</span></code>、<code class="docutils literal notranslate"><span class="pre">max</span></code>分布差异较大，容易引发较大误差，可以设置<code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code>，将其不量化。</p></li>
<li><p>针对截断率<code class="docutils literal notranslate"><span class="pre">Clip</span></code>较高的算子，可以设置<code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code>，将其不量化。</p></li>
</ol>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="预处理数据" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="推理模型转换" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>