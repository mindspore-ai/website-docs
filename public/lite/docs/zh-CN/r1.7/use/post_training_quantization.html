<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>训练后量化 &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="预处理数据" href="data_preprocessing.html" />
    <link rel="prev" title="推理模型转换" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">一小时入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">体验C++极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">体验Java极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">训练后量化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#配置参数">配置参数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#通用量化参数">通用量化参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#混合比特权重量化参数">混合比特权重量化参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#全量化参数">全量化参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#数据预处理">数据预处理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#权重量化">权重量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#混合比特量化">混合比特量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#固定比特量化">固定比特量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#部分模型精度结果">部分模型精度结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#全量化">全量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#部分模型精度结果-1">部分模型精度结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#动态量化">动态量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#部分模型性能结果">部分模型性能结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#量化debug">量化Debug</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#设置无需量化node">设置无需量化Node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#使用建议">使用建议</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在轻量和小型系统上执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>训练后量化</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="训练后量化">
<h1>训练后量化<a class="headerlink" href="#训练后量化" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.7/docs/lite/docs/source_zh_cn/use/post_training_quantization.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png"></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>对于已经训练好的<code class="docutils literal notranslate"><span class="pre">float32</span></code>模型，通过训练后量化将其转为<code class="docutils literal notranslate"><span class="pre">int8</span></code>，不仅能减小模型大小，而且能显著提高推理性能。在MindSpore Lite中，这部分功能集成在模型转换工具<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>内，通过配置<code class="docutils literal notranslate"><span class="pre">量化配置文件</span></code>的方式，便能够转换得到量化后模型。</p>
<p>MindSpore Lite训练后量化分为两类：</p>
<ol class="arabic simple">
<li><p>权重量化：对模型的权值进行量化，仅压缩模型大小，推理时仍然执行<code class="docutils literal notranslate"><span class="pre">float32</span></code>推理；</p></li>
<li><p>全量化：对模型的权值、激活值等统一进行量化，推理时执行<code class="docutils literal notranslate"><span class="pre">int</span></code>运算，能提升模型推理速度、降低功耗。</p></li>
</ol>
</section>
<section id="配置参数">
<h2>配置参数<a class="headerlink" href="#配置参数" title="Permalink to this headline"></a></h2>
<p>训练后量化可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/converter_tool.html">转换工具</a>配置<code class="docutils literal notranslate"><span class="pre">configFile</span></code>的方式启用训练后量化。配置文件采用<code class="docutils literal notranslate"><span class="pre">INI</span></code>的风格，针对量化场景，目前可配置的参数包括<code class="docutils literal notranslate"><span class="pre">通用量化参数[common_quant_param]</span></code>、<code class="docutils literal notranslate"><span class="pre">混合比特权重量化参数[mixed_bit_weight_quant_param]</span></code>、<code class="docutils literal notranslate"><span class="pre">全量化参数[full_quant_param]</span></code>和<code class="docutils literal notranslate"><span class="pre">数据预处理参数[data_preprocess_param]</span></code>。</p>
<section id="通用量化参数">
<h3>通用量化参数<a class="headerlink" href="#通用量化参数" title="Permalink to this headline"></a></h3>
<p>通用量化参数是训练后量化的基本设置，主要包括<code class="docutils literal notranslate"><span class="pre">quant_type</span></code>、<code class="docutils literal notranslate"><span class="pre">bit_num</span></code>、<code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>和<code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>。参数的详细介绍如下所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">quant_type</span></code></p></td>
<td><p>必选</p></td>
<td><p>设置量化类型，设置为WEIGHT_QUANT时，启用权重量化；设置为FULL_QUANT时，启用全量化；设置为DYNAMIC_QUANT时，启用动态量化。</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>WEIGHT_QUANT、FULL_QUANT、DYNAMIC_QUANT</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bit_num</span></code></p></td>
<td><p>可选</p></td>
<td><p>设置量化的比特数，目前权重量化支持0-16bit量化，设置为1-16bit时为固定比特量化，设置为0bit时，启用混合比特量化。全量化、动态量化支持8bit量化。</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>权重量化：[0，16]<br/>全量化：8<br/>动态量化：8</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code></p></td>
<td><p>可选</p></td>
<td><p>设置参与量化的权重尺寸阈值，若权重数大于该值，则对此权重进行量化。</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
<td><p>[0, 65535]</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code></p></td>
<td><p>可选</p></td>
<td><p>设置参与量化的权重通道数阈值，若权重通道数大于该值，则对此权重进行量化。</p></td>
<td><p>Integer</p></td>
<td><p>16</p></td>
<td><p>[0, 65535]</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code></p></td>
<td><p>可选</p></td>
<td><p>设置无需量化的算子名称，多个算子之间用<code class="docutils literal notranslate"><span class="pre">,</span></code>分割。</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code></p></td>
<td><p>可选</p></td>
<td><p>设置量化Debug信息文件保存的文件夹路径。</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>目前<code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>、<code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>仅对权重量化有效。</p>
<p>建议：全量化在精度不满足的情况下，可设置<code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code>开启Debug模式得到相关统计报告，针对不适合量化的算子设置<code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code>对其不进行量化。</p>
</div></blockquote>
<p>通用量化参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
<span class="c1"># Set the name of the operator that skips the quantization, and use `,` to split between multiple operators.</span>
<span class="na">skip_quant_node</span><span class="o">=</span><span class="s">node_name1,node_name2,node_name3</span>
<span class="c1"># Set the folder path where the quantization debug information file is saved.</span>
<span class="na">debug_info_save_path</span><span class="o">=</span><span class="s">/home/workspace/mindspore/debug_info_save_path</span>
</pre></div>
</div>
</section>
<section id="混合比特权重量化参数">
<h3>混合比特权重量化参数<a class="headerlink" href="#混合比特权重量化参数" title="Permalink to this headline"></a></h3>
<p>混合比特权重量化参数包括<code class="docutils literal notranslate"><span class="pre">init_scale</span></code>，启用混合比特权重量化后，将会针对不同层自动搜索最优的比特数。参数的详细介绍如下所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>init_scale</p></td>
<td><p>可选</p></td>
<td><p>初始化scale，数值越大可以带来更大的压缩率，但是也会造成不同程度的精度损失</p></td>
<td><p>float</p></td>
<td><p>0.02</p></td>
<td><p>(0 , 1)</p></td>
</tr>
<tr class="row-odd"><td><p>auto_tune</p></td>
<td><p>可选</p></td>
<td><p>自动搜索init_scale参数，设置后将自动会搜索一组模型输出Tensor在余弦相似度在0.995左右的<code class="docutils literal notranslate"><span class="pre">init_scale</span></code>值</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
<td><p>True，False</p></td>
</tr>
</tbody>
</table>
<p>混合比特量化参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[mixed_bit_weight_quant_param]</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
<span class="na">auto_tune</span><span class="o">=</span><span class="s">false</span>
</pre></div>
</div>
</section>
<section id="全量化参数">
<h3>全量化参数<a class="headerlink" href="#全量化参数" title="Permalink to this headline"></a></h3>
<p>全量化参数主要包括<code class="docutils literal notranslate"><span class="pre">activation_quant_method</span></code>，<code class="docutils literal notranslate"><span class="pre">bias_correction</span></code> 和<code class="docutils literal notranslate"><span class="pre">target_device</span></code>。参数的详细介绍如下所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>activation_quant_method</p></td>
<td><p>可选</p></td>
<td><p>激活值量化算法</p></td>
<td><p>String</p></td>
<td><p>MAX_MIN</p></td>
<td><p>KL，MAX_MIN，RemovalOutlier。 <br>KL：基于<a class="reference external" href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">KL散度</a>对数据范围作量化校准。 <br>MAX_MIN：基于最大值、最小值计算数据的量化参数。 <br>RemovalOutlier：按照一定比例剔除数据的极大极小值，再计算量化参数。 <br>在校准数据集与实际推理时的输入数据相吻合的情况下，推荐使用MAX_MIN；而在校准数据集噪声比较大的情况下，推荐使用KL或者REMOVAL_OUTLIER</p></td>
</tr>
<tr class="row-odd"><td><p>bias_correction</p></td>
<td><p>可选</p></td>
<td><p>是否对量化误差进行校正</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
<td><p>True，False。使能后，将能提升量化模型的精度。</p></td>
</tr>
<tr class="row-even"><td><p>target_device</p></td>
<td><p>可选</p></td>
<td><p>全量化支持多硬件后端。设置特定硬件后，量化模型会调用专有硬件量化算子库进行推理；如果未设置，转换模型调用通用量化算子库。</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>NVGPU: 转换后的量化模型可以在NVIDIA GPU上执行量化推理。</p></td>
</tr>
</tbody>
</table>
<p>通用全量化参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<p>NVIDIA GPU全量化参数配置如下：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Supports specific hardware backends</span>
<span class="na">target_device</span><span class="o">=</span><span class="s">NVGPU</span>
</pre></div>
</div>
</section>
<section id="数据预处理">
<h3>数据预处理<a class="headerlink" href="#数据预处理" title="Permalink to this headline"></a></h3>
<p>全量化需要提供100-500张的校准数据集进行预推理，用于计算全量化激活值的量化参数。如果存在多个输入Tensor，每个输入Tensor的校准数据集需要各自保存一个文件夹。</p>
<p>针对BIN格式的校准数据集，<code class="docutils literal notranslate"><span class="pre">.bin</span></code>文件存储的是输入的数据Buffer，同时输入数据的Format需要和推理时输入数据的Format保持一致。针对四维数据，默认是<code class="docutils literal notranslate"><span class="pre">NHWC</span></code>。如果配置了转换工具的命令参数<code class="docutils literal notranslate"><span class="pre">inputDataFormat</span></code>，输入的Buffer的Format需要保持一致。</p>
<p>针对图片格式的校准数据集，后量化提供通道转换、归一化、缩放和裁剪等数据预处理功能。</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>calibrate_path</p></td>
<td><p>必选</p></td>
<td><p>存放校准数据集的目录；如果模型有多个输入，请依次填写对应的数据所在目录，目录路径间请用<code class="docutils literal notranslate"><span class="pre">,</span></code>隔开</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</p></td>
</tr>
<tr class="row-odd"><td><p>calibrate_size</p></td>
<td><p>必选</p></td>
<td><p>矫正集数量</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-even"><td><p>input_type</p></td>
<td><p>必选</p></td>
<td><p>矫正数据文件格式类型</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>IMAGE、BIN <br>IMAGE：图片文件数据 <br>BIN：满足推理要求的输入二进制<code class="docutils literal notranslate"><span class="pre">.bin</span></code>文件数据</p></td>
</tr>
<tr class="row-odd"><td><p>image_to_format</p></td>
<td><p>可选</p></td>
<td><p>图像格式转换</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>RGB、GRAY、BGR</p></td>
</tr>
<tr class="row-even"><td><p>normalize_mean</p></td>
<td><p>可选</p></td>
<td><p>图像归一化的均值<br/>dst = (src - mean) / std</p></td>
<td><p>Vector</p></td>
<td><p>-</p></td>
<td><p>3通道：[mean_1, mean_2, mean_3] <br/>1通道：[mean_1]</p></td>
</tr>
<tr class="row-odd"><td><p>normalize_std</p></td>
<td><p>可选</p></td>
<td><p>图像归一化的标准差<br/>dst = (src - mean) / std</p></td>
<td><p>Vector</p></td>
<td><p>-</p></td>
<td><p>3通道：[std_1, std_2, std_3] <br/>1通道：[std_1]</p></td>
</tr>
<tr class="row-even"><td><p>resize_width</p></td>
<td><p>可选</p></td>
<td><p>图像缩放宽度</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-odd"><td><p>resize_height</p></td>
<td><p>可选</p></td>
<td><p>图像缩放高度</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-even"><td><p>resize_method</p></td>
<td><p>可选</p></td>
<td><p>图像缩放算法</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>LINEAR、NEAREST、CUBIC<br/>LINEAR：线性插值<br/>NEARST：最邻近插值<br/>CUBIC：三次样条插值</p></td>
</tr>
<tr class="row-odd"><td><p>center_crop_width</p></td>
<td><p>可选</p></td>
<td><p>中心裁剪宽度</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-even"><td><p>center_crop_height</p></td>
<td><p>可选</p></td>
<td><p>中心裁剪高度</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
</tbody>
</table>
<p>数据预处理参数配置如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>
</pre></div>
</div>
</section>
</section>
<section id="权重量化">
<h2>权重量化<a class="headerlink" href="#权重量化" title="Permalink to this headline"></a></h2>
<p>权重量化支持混合比特量化，同时也支持1~16之间的固定比特量化，比特数越低，模型压缩率越大，但是精度损失通常也比较大。下面对权重量化的使用方式和效果进行阐述。</p>
<section id="混合比特量化">
<h3>混合比特量化<a class="headerlink" href="#混合比特量化" title="Permalink to this headline"></a></h3>
<p>目前权重量化支持混合比特量化，会根据模型参数的分布情况，根据用户设置的<code class="docutils literal notranslate"><span class="pre">init_scale</span></code>作为初始值，自动搜索出最适合当前层的比特数。配置参数的<code class="docutils literal notranslate"><span class="pre">bit_num</span></code>设置为0时，将启用混合比特量化。</p>
<p>混合比特权重量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/mixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>混合比特权重量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">5000</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">5</span>

<span class="k">[mixed_bit_weight_quant_param]</span>
<span class="c1"># Initialization scale in (0,1).</span>
<span class="c1"># A larger value can get a larger compression ratio, but it may also cause a larger error.</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
<p>用户可根据模型及自身需要对权重量化的参数作出调整。</p>
<blockquote>
<div><p>init_scale默认的初始值为0.02，搜索的压缩率相当与6-7固定比特的压缩效果。</p>
<p>混合比特需要搜索最佳比特位，等待时间可能较长，如果需要查看日志，可以在执行前设置export GLOG_v=1，用于打印相关Info级别日志。</p>
</div></blockquote>
</section>
<section id="固定比特量化">
<h3>固定比特量化<a class="headerlink" href="#固定比特量化" title="Permalink to this headline"></a></h3>
<p>固定比特的权重量化支持1~16之间的固定比特量化，用户可根据模型及自身需要对权重量化的参数作出调整。</p>
<p>固定比特权重量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/fixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>固定比特权重量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</section>
<section id="部分模型精度结果">
<h3>部分模型精度结果<a class="headerlink" href="#部分模型精度结果" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>测试数据集</p></th>
<th class="head"><p>FP32模型精度</p></th>
<th class="head"><p>权重量化精度（8bit）</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>77.60%</p></td>
<td><p>77.53%</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>70.96%</p></td>
<td><p>70.56%</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>71.56%</p></td>
<td><p>71.53%</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得。</p>
</div></blockquote>
</section>
</section>
<section id="全量化">
<h2>全量化<a class="headerlink" href="#全量化" title="Permalink to this headline"></a></h2>
<p>针对CV模型需要提升模型运行速度、降低模型运行功耗的场景，可以使用训练后全量化功能。下面对全量化的使用方式和效果进行阐述。</p>
<p>全量化计算激活值的量化参数，用户需要提供校准数据集。校准数据集最好来自真实推理场景，能表征模型的实际输入情况，数量在100 - 500个左右。</p>
<p>针对图片数据，目前支持通道调整、归一化、缩放、裁剪等预处理的功能。用户可以根据推理时所需的预处理操作，设置相应的<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.7/use/post_training_quantization.html#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">参数</a>。</p>
<p>注意：</p>
<ul class="simple">
<li><p>模型校准数据必须与训练数据同分布，并且校准数据与模型输入的format（例如：NCHW、NHWC）需要保持一致。</p></li>
</ul>
<p>全量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg
</pre></div>
</div>
<p>全量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>

<span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>

<span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<blockquote>
<div><p>全量化需要执行推理，等待时间可能较长，如果需要查看日志，可以在执行前设置export GLOG_v=1，用于打印相关Info级别日志。</p>
</div></blockquote>
<section id="部分模型精度结果-1">
<h3>部分模型精度结果<a class="headerlink" href="#部分模型精度结果-1" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>测试数据集</p></th>
<th class="head"><p>量化方法</p></th>
<th class="head"><p>FP32模型精度</p></th>
<th class="head"><p>全量化精度（8bit）</p></th>
<th class="head"><p>说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>77.60%</p></td>
<td><p>77.40%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>70.96%</p></td>
<td><p>70.31%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>MAX_MIN</p></td>
<td><p>71.56%</p></td>
<td><p>71.16%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得。</p>
</div></blockquote>
</section>
</section>
<section id="动态量化">
<h2>动态量化<a class="headerlink" href="#动态量化" title="Permalink to this headline"></a></h2>
<p>针对NLP模型需要提升模型运行速度、降低模型运行功耗的场景，可以使用动态量化功能。下面对动态量化的使用方式和效果进行阐述。</p>
<p>动态量化的权重是离线转换阶段量化，而激活是在运行阶段才进行量化。</p>
<p>动态量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/dynamic_quant.cfg
</pre></div>
</div>
<p>动态量化配置文件如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">DYNAMIC_QUANT</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
</pre></div>
</div>
<blockquote>
<div><p>为了保证量化精度，目前动态量化不支持设置FP16的运行模式。</p>
<p>目前动态量化在支持SDOT指令的ARM架构会有进一步的加速效果。</p>
</div></blockquote>
<section id="部分模型性能结果">
<h3>部分模型性能结果<a class="headerlink" href="#部分模型性能结果" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>tinybert_encoder</p></li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型类型</p></th>
<th class="head"><p>运行模式</p></th>
<th class="head"><p>Model Size(M)</p></th>
<th class="head"><p>RAM(K)</p></th>
<th class="head"><p>Latency(ms)</p></th>
<th class="head"><p>Cos-Similarity</p></th>
<th class="head"><p>压缩率</p></th>
<th class="head"><p>内存相比FP32</p></th>
<th class="head"><p>时延相比FP32</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>FP32</p></td>
<td><p>20</p></td>
<td><p>29,029</p></td>
<td><p>9.916</p></td>
<td><p>1</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>FP32</p></td>
<td><p>FP16</p></td>
<td><p>20</p></td>
<td><p>18,208</p></td>
<td><p>5.75</p></td>
<td><p>0.99999</p></td>
<td><p>1</p></td>
<td><p>-37.28%</p></td>
<td><p>-42.01%</p></td>
</tr>
<tr class="row-even"><td><p>FP16</p></td>
<td><p>FP16</p></td>
<td><p>12</p></td>
<td><p>18,105</p></td>
<td><p>5.924</p></td>
<td><p>0.99999</p></td>
<td><p>1.66667</p></td>
<td><p>-37.63%</p></td>
<td><p>-40.26%</p></td>
</tr>
<tr class="row-odd"><td><p>Weight Quant(8 Bit)</p></td>
<td><p>FP16</p></td>
<td><p>5.3</p></td>
<td><p>19,324</p></td>
<td><p>5.764</p></td>
<td><p>0.99994</p></td>
<td><p>3.77358</p></td>
<td><p>-33.43%</p></td>
<td><p>-41.87%</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dynamic Quant</strong></p></td>
<td><p><strong>INT8+FP32</strong></p></td>
<td><p><strong>5.2</strong></p></td>
<td><p><strong>15,709</strong></p></td>
<td><p><strong>4.517</strong></p></td>
<td><p><strong>0.99668</strong></p></td>
<td><p><strong>3.84615</strong></p></td>
<td><p><strong>-45.89%</strong></p></td>
<td><p><strong>-54.45%</strong></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>tinybert_decoder</p></li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型类型</p></th>
<th class="head"><p>运行模式</p></th>
<th class="head"><p>Model Size(M)</p></th>
<th class="head"><p>RAM(K)</p></th>
<th class="head"><p>Latency(ms)</p></th>
<th class="head"><p>Cos-Similarity</p></th>
<th class="head"><p>压缩率</p></th>
<th class="head"><p>内存相比FP32</p></th>
<th class="head"><p>时延相比FP32</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>FP32</p></td>
<td><p>43</p></td>
<td><p>51,355</p></td>
<td><p>4.161</p></td>
<td><p>1</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>FP32</p></td>
<td><p>FP16</p></td>
<td><p>43</p></td>
<td><p>29,462</p></td>
<td><p>2.184</p></td>
<td><p>0.99999</p></td>
<td><p>1</p></td>
<td><p>-42.63%</p></td>
<td><p>-47.51%</p></td>
</tr>
<tr class="row-even"><td><p>FP16</p></td>
<td><p>FP16</p></td>
<td><p>22</p></td>
<td><p>29,440</p></td>
<td><p>2.264</p></td>
<td><p>0.99999</p></td>
<td><p>1.95455</p></td>
<td><p>-42.67%</p></td>
<td><p>-45.59%</p></td>
</tr>
<tr class="row-odd"><td><p>Weight Quant(8 Bit)</p></td>
<td><p>FP16</p></td>
<td><p>12</p></td>
<td><p>32,285</p></td>
<td><p>2.307</p></td>
<td><p>0.99998</p></td>
<td><p>3.58333</p></td>
<td><p>-37.13%</p></td>
<td><p>-44.56%</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dynamic Quant</strong></p></td>
<td><p><strong>INT8+FP32</strong></p></td>
<td><p><strong>12</strong></p></td>
<td><p><strong>22,181</strong></p></td>
<td><p><strong>2.074</strong></p></td>
<td><p><strong>0.9993</strong></p></td>
<td><p><strong>3.58333</strong></p></td>
<td><p><strong>-56.81%</strong></p></td>
<td><p><strong>-50.16%</strong></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="量化debug">
<h2>量化Debug<a class="headerlink" href="#量化debug" title="Permalink to this headline"></a></h2>
<p>开启量化Debug功能，能够得到数据分布统计报告，用于评估量化误差，辅助决策模型（算子）是否适合量化。针对全量化，会根据所提供矫正数据集的数量，生成N份数据分布统计报告，即每一轮都会生成一份报告；针对权重量化，只会生成1份数据分布统计报告。</p>
<p>设置<code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code>参数后，将会在<code class="docutils literal notranslate"><span class="pre">/home/workspace/mindspore/debug_info_save_path</span></code>文件夹中生成相关Debug报告：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">debug_info_save_path</span><span class="o">=</span><span class="s">/home/workspace/mindspore/debug_info_save_path</span>
</pre></div>
</div>
<p>数据分布统计报告会统计每个Tensor原始数据分布以及量化Tensor反量化后的数据分布情况。数据分布统计报告相关字段如下所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NodeName</p></td>
<td><p>节点名</p></td>
</tr>
<tr class="row-odd"><td><p>NodeType</p></td>
<td><p>节点类型</p></td>
</tr>
<tr class="row-even"><td><p>TensorName</p></td>
<td><p>Tensor名</p></td>
</tr>
<tr class="row-odd"><td><p>InOutFlag</p></td>
<td><p>Tensor输出、输出类型</p></td>
</tr>
<tr class="row-even"><td><p>DataTypeFlag</p></td>
<td><p>数据类型，原始数据用Origin，反量化后的数据用Dequant</p></td>
</tr>
<tr class="row-odd"><td><p>TensorTypeFlag</p></td>
<td><p>针对输入输出等数据类用Activation表示，常量等用Weight表示</p></td>
</tr>
<tr class="row-even"><td><p>Min</p></td>
<td><p>最小值，0%分位点</p></td>
</tr>
<tr class="row-odd"><td><p>Q1</p></td>
<td><p>25%分位点</p></td>
</tr>
<tr class="row-even"><td><p>Median</p></td>
<td><p>中位数，50%分位点</p></td>
</tr>
<tr class="row-odd"><td><p>Q3</p></td>
<td><p>75%分位点</p></td>
</tr>
<tr class="row-even"><td><p>MAX</p></td>
<td><p>最大值，100%分位点</p></td>
</tr>
<tr class="row-odd"><td><p>Mean</p></td>
<td><p>均值</p></td>
</tr>
<tr class="row-even"><td><p>Var</p></td>
<td><p>方差</p></td>
</tr>
<tr class="row-odd"><td><p>Sparsity</p></td>
<td><p>稀疏度</p></td>
</tr>
<tr class="row-even"><td><p>Clip</p></td>
<td><p>截断率</p></td>
</tr>
<tr class="row-odd"><td><p>CosineSimilarity</p></td>
<td><p>和原始数据对比的余弦相似度</p></td>
</tr>
</tbody>
</table>
<p>量化参数文件<code class="docutils literal notranslate"><span class="pre">quant_param.csv</span></code>包含所有量化Tensor的量化参数信息，量化参数相关字段如下所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NodeName</p></td>
<td><p>节点名</p></td>
</tr>
<tr class="row-odd"><td><p>NodeType</p></td>
<td><p>节点类型</p></td>
</tr>
<tr class="row-even"><td><p>TensorName</p></td>
<td><p>Tensor名</p></td>
</tr>
<tr class="row-odd"><td><p>ElementsNum</p></td>
<td><p>Tensor数据量</p></td>
</tr>
<tr class="row-even"><td><p>Dims</p></td>
<td><p>Tensor维度</p></td>
</tr>
<tr class="row-odd"><td><p>Scale</p></td>
<td><p>量化参数scale</p></td>
</tr>
<tr class="row-even"><td><p>ZeroPoint</p></td>
<td><p>量化参数ZeroPoint</p></td>
</tr>
<tr class="row-odd"><td><p>Bits</p></td>
<td><p>量化比特数</p></td>
</tr>
<tr class="row-even"><td><p>CorrectionVar</p></td>
<td><p>误差矫正系数-方差</p></td>
</tr>
<tr class="row-odd"><td><p>CorrectionMean</p></td>
<td><p>误差矫正系数-均值</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>由于混合比特量化是非标准量化，该量化参数文件可能不存在。</p>
</div></blockquote>
<section id="设置无需量化node">
<h3>设置无需量化Node<a class="headerlink" href="#设置无需量化node" title="Permalink to this headline"></a></h3>
<p>量化是将Float32算子转换Int8算子，目前的量化策略是针对可支持的某一类算子所包含的Node都会进行量化，但是存在部分Node敏感度较高，量化后会引发较大的误差，同时某些层量化后推理速度远低于Float16的推理速度。支持指定层不量化，可以有效提高精度和推理速度。</p>
<p>下面将<code class="docutils literal notranslate"><span class="pre">conv2d_1</span></code> <code class="docutils literal notranslate"><span class="pre">add_8</span></code> <code class="docutils literal notranslate"><span class="pre">concat_1</span></code>三个Node不进行量化的示例：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Set the name of the operator that skips the quantization, and use `,` to split between multiple operators.</span>
<span class="na">skip_quant_node</span><span class="o">=</span><span class="s">conv2d_1,add_8,concat_1</span>
</pre></div>
</div>
</section>
<section id="使用建议">
<h3>使用建议<a class="headerlink" href="#使用建议" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>通过过滤<code class="docutils literal notranslate"><span class="pre">InOutFlag</span> <span class="pre">==</span> <span class="pre">Output</span> <span class="pre">&amp;&amp;</span> <span class="pre">DataTypeFlag</span> <span class="pre">==</span> <span class="pre">Dequant</span></code>，可以筛选出所有量化算子的输出层，通过查看量化输出的<code class="docutils literal notranslate"><span class="pre">CosineSimilarity</span></code>来判断算子的精度损失，越接近1损失越小。</p></li>
<li><p>针对Add、Concat等合并类算子，如果不同输入Tensor之间<code class="docutils literal notranslate"><span class="pre">min</span></code>、<code class="docutils literal notranslate"><span class="pre">max</span></code>分布差异较大，容易引发较大误差，可以设置<code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code>，将其不量化。</p></li>
<li><p>针对截断率<code class="docutils literal notranslate"><span class="pre">Clip</span></code>较高的算子，可以设置<code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code>，将其不量化。</p></li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="推理模型转换" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="预处理数据" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>