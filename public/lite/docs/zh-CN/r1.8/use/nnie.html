<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>集成NNIE使用说明 &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="集成TensorRT使用说明" href="tensorrt_info.html" />
    <link rel="prev" title="集成NPU使用说明" href="npu_info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">一小时入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">体验C++极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">体验C++极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">体验Java极简推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">体验Java极简并发推理Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">基于JNI接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">基于Java接口的Android应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">基于C++接口实现端侧训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">基于Java接口实现端侧训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在MCU或小型系统上执行推理</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="asic.html">专用芯片集成说明</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_info.html">集成NPU使用说明</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">集成NNIE使用说明</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#目录结构">目录结构</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#模型转换工具converter目录结构说明">模型转换工具converter目录结构说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#模型推理工具runtime目录结构说明">模型推理工具runtime目录结构说明</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#工具使用">工具使用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#转换工具converter">转换工具converter</a></li>
<li class="toctree-l4"><a class="reference internal" href="#推理工具runtime">推理工具runtime</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#集成使用">集成使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#svp工具链相关功能支持及注意事项高级选项">SVP工具链相关功能支持及注意事项（高级选项）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#板端运行输入format须是nhwc">板端运行输入Format须是NHWC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-list说明">image_list说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-type限制">image_type限制</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-list和roi-coordinate-file个数说明">image_list和roi_coordinate_file个数说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prototxt中节点名-cpu后缀支持">prototxt中节点名_cpu后缀支持</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prototxt中custom算子支持">prototxt中Custom算子支持</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prototxt中top域的-report后缀支持">prototxt中top域的_report后缀支持</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inplace机制">inplace机制</a></li>
<li class="toctree-l4"><a class="reference internal" href="#多图片batch运行及多step运行">多图片batch运行及多step运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#节点名称的变动">节点名称的变动</a></li>
<li class="toctree-l4"><a class="reference internal" href="#proposal算子使用说明">proposal算子使用说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="#分段机制说明及8段限制">分段机制说明及8段限制</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tensorrt_info.html">集成TensorRT使用说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="ascend_info.html">集成Ascend使用说明</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">服务端推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime_server_inference.html">执行并发推理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">其他工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="asic.html">专用芯片集成说明</a> &raquo;</li>
      <li>集成NNIE使用说明</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/nnie.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="集成nnie使用说明">
<h1>集成NNIE使用说明<a class="headerlink" href="#集成nnie使用说明" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/lite/docs/source_zh_cn/use/nnie.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source.png" /></a></p>
<section id="目录结构">
<h2>目录结构<a class="headerlink" href="#目录结构" title="Permalink to this headline"></a></h2>
<section id="模型转换工具converter目录结构说明">
<h3>模型转换工具converter目录结构说明<a class="headerlink" href="#模型转换工具converter目录结构说明" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-runtime-linux-x64
└── tools
    └── converter
        └── providers
            └── Hi3516D                # 嵌入式板型号
                ├── libmslite_nnie_converter.so        # 集成NNIE转换的动态库
                ├── libmslite_nnie_data_process.so     # 处理NNIE输入数据的动态库
                ├── libnnie_mapper.so        # 构建NNIE二进制文件的动态库
                └── third_party       # NNIE依赖的三方动态库
                    ├── opencv-4.2.0
                    │   └── libopencv_xxx.so
                    └── protobuf-3.9.0
                        ├── libprotobuf.so
                        └── libprotoc.so
</pre></div>
</div>
<p>上述是NNIE的集成目录结构，转换工具converter的其余目录结构详情，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/converter_tool.html">模型转换工具</a>。</p>
</section>
<section id="模型推理工具runtime目录结构说明">
<h3>模型推理工具runtime目录结构说明<a class="headerlink" href="#模型推理工具runtime目录结构说明" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-aarch32
└── providers
    └── Hi3516D        # 嵌入式板型号
        └── libmslite_nnie.so  # 集成NNIE的动态库
        └── libmslite_proposal.so  # 集成proposal的样例动态库
</pre></div>
</div>
<p>上述是NNIE的集成目录结构，推理工具runtime的其余目录结构详情，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/build.html#%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84">目录结构</a>。</p>
</section>
</section>
<section id="工具使用">
<h2>工具使用<a class="headerlink" href="#工具使用" title="Permalink to this headline"></a></h2>
<section id="转换工具converter">
<h3>转换工具converter<a class="headerlink" href="#转换工具converter" title="Permalink to this headline"></a></h3>
<section id="概述">
<h4>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h4>
<p>MindSpore Lite提供离线转换模型功能的工具，将多种类型的模型转换为可使用NNIE硬件加速推理的板端专属模型，可运行在Hi3516板上。
通过转换工具转换成的NNIE<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型，仅支持在关联的嵌入式板上，使用与该嵌入式板配套的Runtime推理框架执行推理。关于转换工具的更一般说明，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/converter_tool.html">推理模型转换</a>。</p>
</section>
<section id="环境准备">
<h4>环境准备<a class="headerlink" href="#环境准备" title="Permalink to this headline"></a></h4>
<p>使用MindSpore Lite模型转换工具，需要进行如下环境准备工作。</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/downloads.html">下载</a>NNIE专用converter工具，当前仅支持Linux</p></li>
<li><p>解压下载的包</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-x64.tar.gz
</pre></div>
</div>
<p>{version}是发布包的版本号。</p>
</li>
<li><p>将转换工具需要的动态链接库加入环境变量LD_LIBRARY_PATH</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/runtime/lib:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/third_party/opencv-4.2.0:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/third_party/protobuf-3.9.0
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH}是解压得到的文件夹路径。</p>
</li>
</ol>
</section>
<section id="扩展配置">
<h4>扩展配置<a class="headerlink" href="#扩展配置" title="Permalink to this headline"></a></h4>
<p>在转换阶段，为了能够加载扩展模块，用户需要配置扩展动态库路径。扩展相关的参数有<code class="docutils literal notranslate"><span class="pre">plugin_path</span></code>，<code class="docutils literal notranslate"><span class="pre">disable_fusion</span></code>。参数的详细介绍如下所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>plugin_path</p></td>
<td><p>可选</p></td>
<td><p>第三方库加载路径</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>如有多个请用<code class="docutils literal notranslate"><span class="pre">;</span></code>分隔</p></td>
</tr>
<tr class="row-odd"><td><p>disable_fusion</p></td>
<td><p>可选</p></td>
<td><p>是否关闭融合优化</p></td>
<td><p>String</p></td>
<td><p>off</p></td>
<td><p>off、on</p></td>
</tr>
</tbody>
</table>
<p>发布件中已为用户生成好默认的配置文件（converter.cfg）。文件内保存着NNIE动态库的相对路径，用户需要依据实际情况，决定是否需要手动修改该配置文件。该配置文件内容如下：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[registry]</span>
<span class="na">plugin_path</span><span class="o">=</span><span class="s">../providers/Hi3516D/libmslite_nnie_converter.so</span>
</pre></div>
</div>
</section>
<section id="nnie配置">
<h4>NNIE配置<a class="headerlink" href="#nnie配置" title="Permalink to this headline"></a></h4>
<p>NNIE模型可以使用NNIE硬件以提高模型运行速度，用户还需要配置NNIE自身的配置文件。用户需参照海思提供的《HiSVP 开发指南》中表格<code class="docutils literal notranslate"><span class="pre">nnie_mapper</span> <span class="pre">配置选项说明</span></code>来进行配置，以nnie.cfg指代此配置文件：</p>
<p>nnie.cfg文件的示例参考如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[net_type] 0
[image_list] ./input_nchw.txt
[image_type] 0
[norm_type] 0
[mean_file] null
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">input_nchw.txt</span></code>为被转换CAFFE模型的浮点文本格式的输入数据，详情请参照《HiSVP 开发指南》中的<code class="docutils literal notranslate"><span class="pre">image_list</span></code>说明。在配置文件中，配置选项caffemodel_file、prototxt_file、is_simulation、instructions_name不可配置，其他选项功能可正常配置。</p>
</div></blockquote>
</section>
<section id="执行converter">
<h4>执行converter<a class="headerlink" href="#执行converter" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>进入转换目录</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>配置环境变量（可选）</p>
<p>若已执行第1步，进入到转换目录，则此步无需配置，默认值将使能。若用户未进入转换目录，则需在环境变量中声明转换工具所依赖的so和benchmark二进制执行程序的路径，如下所示：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_MAPPER_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/libnnie_mapper.so
<span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_DATA_PROCESS_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/libmslite_nnie_data_process.so
<span class="nb">export</span><span class="w"> </span><span class="nv">BENCHMARK_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/benchmark
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH}是下载得到的包解压后的路径。</p>
</li>
<li><p>将nnie.cfg拷贝到转换目录并设置如下环境变量</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_CONFIG_PATH</span><span class="o">=</span>./nnie.cfg
</pre></div>
</div>
<p>如果用户实际的配置文件就叫nnie.cfg，且与converter_lite在同级路径上，则可不用配置。</p>
</li>
<li><p>执行converter，生成NNIE<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.prototxt<span class="w"> </span>--weightFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.caffemodel<span class="w"> </span>--configFile<span class="o">=</span>./converter.cfg<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>${model_name}为模型文件名称，运行后的结果显示为：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>用户若想了解converter_lite转换工具的相关参数，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/converter_tool.html#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E">参数说明</a>。</p>
</li>
</ol>
</section>
</section>
<section id="推理工具runtime">
<h3>推理工具runtime<a class="headerlink" href="#推理工具runtime" title="Permalink to this headline"></a></h3>
<section id="概述-1">
<h4>概述<a class="headerlink" href="#概述-1" title="Permalink to this headline"></a></h4>
<p>得到转换模型后，可在关联的嵌入式板上，使用板子配套的Runtime推理框架执行推理。MindSpore Lite提供benchmark基准测试工具，它可以对MindSpore Lite模型前向推理的执行耗时进行定量分析（性能），还可以通过指定模型输出进行可对比的误差分析（精度）。
关于推理工具的一般说明，可参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/benchmark_tool.html">benchmark</a>。</p>
</section>
<section id="环境准备-1">
<h4>环境准备<a class="headerlink" href="#环境准备-1" title="Permalink to this headline"></a></h4>
<p>以下为示例用法，用户可根据实际情况进行等价操作。</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/downloads.html">下载</a>NNIE专用模型推理工具，当前仅支持Hi3516D</p></li>
<li><p>解压下载的包</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-aarch32.tar.gz
</pre></div>
</div>
<p>{version}是发布包的版本号。</p>
</li>
<li><p>在Hi3516D板上创建存放目录</p>
<p>登陆板端，创建工作目录</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>/user/mindspore<span class="w">          </span><span class="c1"># 存放benchmark执行文件及模型</span>
mkdir<span class="w"> </span>/user/mindspore/lib<span class="w">      </span><span class="c1"># 存放依赖库文件</span>
</pre></div>
</div>
</li>
<li><p>传输文件</p>
<p>向Hi3516D板端传输benchmark工具、模型、so库。其中libmslite_proposal.so为MindSpore Lite提供的proposal算子实现样例so，若用户模型里含有自定义的proposal算子，用户需参考<a class="reference internal" href="#proposal算子使用说明"><span class="std std-doc">proposal算子使用说明</span></a>生成libnnie_proposal.so替换该so文件，以进行正确推理。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>libmindspore-lite.so<span class="w"> </span>libmslite_nnie.so<span class="w"> </span>libmslite_proposal.so<span class="w"> </span>root@<span class="si">${</span><span class="nv">device_ip</span><span class="si">}</span>:/user/mindspore/lib
scp<span class="w"> </span>benchmark<span class="w"> </span><span class="si">${</span><span class="nv">model_path</span><span class="si">}</span><span class="w"> </span>root@<span class="si">${</span><span class="nv">device_ip</span><span class="si">}</span>:/user/mindspore
</pre></div>
</div>
<p>${model_path}为转换后ms模型文件路径</p>
</li>
<li><p>设置动态库路径</p>
<p>NNIE模型的推理，还依赖海思提供NNIE相关板端动态库，包括：libnnie.so、libmpi.so、libVoiceEngine.so、libupvqe.so、libdnvqe.so。</p>
<p>用户需在板端保存这些so，并将路径传递给LD_LIBRARY_PATH环境变量。
在示例中，这些so位于/usr/lib下，用户需按实际情况进行配置：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/user/mindspore/lib:/usr/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
</li>
<li><p>构建图片输入（可选）</p>
<p>若converter导出模型时喂给mapper的校正集用的是图片，则传递给benchmark的输入需是int8的输入数据，即需要把图片转成int8传递给benchmark。
这里采用python给出转换示范样例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="k">def</span> <span class="nf">usage</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;usage:</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;example: python generate_input_bin.py xxx.img BGR 224 224</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[1]: origin image path</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[2]: RGB_order[BGR, RGB], should be same as nnie mapper config file&#39;s [RGB_order], default is BGR</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[3]: input_h</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[4]: input_w&quot;</span>
          <span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">argvs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;-h&quot;</span><span class="p">:</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="n">img_path</span> <span class="o">=</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">rgb_order</span> <span class="o">=</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">input_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">input_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rgb_order</span> <span class="o">==</span> <span class="s2">&quot;RGB&quot;</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">img_hwc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">input_w</span><span class="p">,</span> <span class="n">input_h</span><span class="p">))</span>
    <span class="n">outfile_name</span> <span class="o">=</span> <span class="s2">&quot;1_</span><span class="si">%s</span><span class="s2">_</span><span class="si">%s</span><span class="s2">_3_nhwc.bin&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">img_hwc</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">outfile_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated &quot;</span> <span class="o">+</span> <span class="n">outfile_name</span> <span class="o">+</span> <span class="s2">&quot; file success in current dir.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input argument is invalid.&quot;</span><span class="p">)</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">main</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="执行benchmark">
<h4>执行benchmark<a class="headerlink" href="#执行benchmark" title="Permalink to this headline"></a></h4>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd /user/mindspore
./benchmark --modelFile=${model_path}
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model_path</span></code>为转换后ms模型文件路径</p>
<p>执行该命令，会生成模型的随机输入，并执行前向推理。有关benchmark的其他使用详情，如耗时分析与推理误差分析等，见<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/benchmark_tool.html">Benchmark使用</a>。</p>
<p>有关模型的输入数据格式要求，见<a class="reference internal" href="#svp工具链相关功能支持及注意事项高级选项"><span class="std std-doc">SVP工具链相关功能支持及注意事项（高级选项）</span></a>。</p>
</section>
</section>
</section>
<section id="集成使用">
<h2>集成使用<a class="headerlink" href="#集成使用" title="Permalink to this headline"></a></h2>
<p>得到转换模型后，可在关联的嵌入式板上，使用板子配套的MindSpore Lite推理框架进行集成推理。
在阅读本节前，用户需对MindSpore Lite的C++接口集成开发有一定了解，用户可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/runtime_cpp.html">使用C++接口执行推理</a>来了解MindSpore Lite的集成使用基本用法。
针对NNIE的集成使用，有如下几条注意事项：</p>
<ol class="arabic">
<li><p>编译时链接libmslite_nnie.so动态库</p>
<p>MindSpore Lite对NNIE的集成，是通过注册MindSpore Lite自定义算子的方式进行开发，开发完成的自定义算子(<code class="docutils literal notranslate"><span class="pre">NNIE自定义算子</span></code>)被编译为libmslite_nnie.so。
故用户想要通过MindSpore Lite集成使用NNIE推理，必须在编译时链接该so，以完成<code class="docutils literal notranslate"><span class="pre">NNIE自定义算子</span></code>的注册。</p>
</li>
<li><p>按需使用配置项（可选）</p>
<p>MindSpore Lite提供<code class="docutils literal notranslate"><span class="pre">mindspore::Model::LoadConfig</span></code>和<code class="docutils literal notranslate"><span class="pre">mindspore::Model::UpdateConfig</span></code>接口，接收配置参数或者配置文件，从而让用户通过他们向所有算子（包括自定义算子）传递配置项。
NNIE(<code class="docutils literal notranslate"><span class="pre">NNIE自定义算子</span></code>)开放了四个配置项，如下所示：</p>
<ul>
<li><p>KeepOriginalOutput：保持原始NNIE硬件推理输出结果。</p>
<p>NNIE硬件芯片推理的输出为量化的Int32数据（实际数值乘以4096），在给出模型输出时会将芯片推理结果反量化为真实浮点值输出，当该选项被配置为<code class="docutils literal notranslate"><span class="pre">on</span></code>时，模型的输出会保持为量化的Int32输出。在默认情况下，该选项为<code class="docutils literal notranslate"><span class="pre">off</span></code>。</p>
</li>
<li><p>MaxROINum：单张图片ROI区域的最大数量，正整数。</p>
<p>若用户模型含有proposal算子，需根据proposal算子实现情况，配置MaxROINum，若未配置，则采用默认值300。
若用户模型未含有proposal算子，无需配置该项。</p>
</li>
<li><p>TimeStep：循环或lstm网络运行的step数，正整数。</p>
<p>若用户模型为循环或lstm网络，需根据实际网络运行情况，配置TimeStep，若未配置，采用默认值1。
若用户模型不是循环或lstm网络，则无需配置该项，对于多图片batch运行及多step运行的情况，参考<a class="reference internal" href="#多图片batch运行及多step运行"><span class="std std-doc">多图片batch运行及多step运行</span></a>。</p>
</li>
<li><p>CoreIds：NNIE运行内核id，支持模型分段独立配置，使用逗号分隔。</p>
<p>若板端含有多个NNIE硬件，用户可通过CoreIds指定模型运行在哪个NNIE设备上。
若模型被分段（用户可用netron打开模型，观察模型被分段情况，其中算子类型为Custom，属性type值为NNIE的算子，就是运行在NNIE上的分段），可依序分别配置每个Custom分段运行在哪个NNIE设备上，未被配置分段运行在默认NNIE设备0上。</p>
</li>
</ul>
<p>接口配置流程如下所示：</p>
<ul>
<li><p>使用<code class="docutils literal notranslate"><span class="pre">mindspore::Model::LoadConfig</span></code>接口进行配置</p>
<p>在加载模型之后，在用户调用<code class="docutils literal notranslate"><span class="pre">mindspore::Model::Build</span></code>接口进行模型编译前，通过调用<code class="docutils literal notranslate"><span class="pre">mindspore::Model::LoadConfig</span></code>接口，将配置文件路径传入实现配置，一个nnie配置文件内容示例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[nnie]
TimeStep=1
MaxROINum=0
CoreIds=0
KeepOriginalOutput=off
</pre></div>
</div>
</li>
<li><p>使用<code class="docutils literal notranslate"><span class="pre">mindspore::Model::UpdateConfig</span></code>接口进行配置</p>
<p>在加载模型之后，在用户调用<code class="docutils literal notranslate"><span class="pre">mindspore::Model::Build</span></code>接口进行模型编译前，通过调用<code class="docutils literal notranslate"><span class="pre">mindspore::Model::UpdateConfig</span></code>接口，也可以配置以上配置项，如下示例所示：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>// ms_model是`mindspore::Model`类的一个实例
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;TimeStep&quot;, &quot;1&quot;));
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;MaxROINum&quot;, &quot;0&quot;));
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;CoreIds&quot;, &quot;0&quot;));
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;KeepOriginalOutput&quot;, &quot;off&quot;));
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
<section id="svp工具链相关功能支持及注意事项高级选项">
<h2>SVP工具链相关功能支持及注意事项（高级选项）<a class="headerlink" href="#svp工具链相关功能支持及注意事项高级选项" title="Permalink to this headline"></a></h2>
<p>在模型转换时，由NNIE_CONFIG_PATH环境变量声明的nnie.cfg文件，提供原先SVP工具链相关功能，支持除caffemodel_file、prototxt_file、is_simulation、instructions_name外其他字段的配置，相关注意实现如下：</p>
<section id="板端运行输入format须是nhwc">
<h3>板端运行输入Format须是NHWC<a class="headerlink" href="#板端运行输入format须是nhwc" title="Permalink to this headline"></a></h3>
<p>转换后的<code class="docutils literal notranslate"><span class="pre">ms</span></code>模型只接受NHWC格式的数据输入，若image_type被声明为0，则接收NHWC格式的float32数据，若image_type被声明为1，则接收NHWC的uint8数据输入。</p>
</section>
<section id="image-list说明">
<h3>image_list说明<a class="headerlink" href="#image-list说明" title="Permalink to this headline"></a></h3>
<p>nnie.cfg中image_list字段含义与原先不变，当image_type声明为0时，按行提供chw格式数据，无论原先模型是否是nchw输入。</p>
</section>
<section id="image-type限制">
<h3>image_type限制<a class="headerlink" href="#image-type限制" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite不支持image_type为3和5时的网络输入，用户设为0或1。</p>
</section>
<section id="image-list和roi-coordinate-file个数说明">
<h3>image_list和roi_coordinate_file个数说明<a class="headerlink" href="#image-list和roi-coordinate-file个数说明" title="Permalink to this headline"></a></h3>
<p>用户只需提供与模型输入个数相同数量的image_list，若模型中含有ROI Pooling或PSROI Pooling层，用户需提供roi_coordinate_file，数量与顺序和prototxt内的ROI Pooling或PSROI Pooling层的个数与顺序对应。</p>
</section>
<section id="prototxt中节点名-cpu后缀支持">
<h3>prototxt中节点名_cpu后缀支持<a class="headerlink" href="#prototxt中节点名-cpu后缀支持" title="Permalink to this headline"></a></h3>
<p>SVP工具链中，可通过在prototxt文件的节点名后使用_cpu后缀来，声明cpu自定义算子。MindSpore Lite中忽略_cpu后缀，不做支持。用户若想重定义MindSpore Lite已有的算子实现或新增新的算子，可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/register_kernel.html">自定义算子注册</a>的方式进行注册。</p>
</section>
<section id="prototxt中custom算子支持">
<h3>prototxt中Custom算子支持<a class="headerlink" href="#prototxt中custom算子支持" title="Permalink to this headline"></a></h3>
<p>SVP工具链中，通过在prototxt中声明custom层，实现推理时分段，并由用户实现cpu代码。在MindSpore Lite中，用户需在Custom层中增加op_type属性，并通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/register_kernel.html">自定义算子注册</a>的方式进行在线推理代码的注册。</p>
<p>Custom层的修改样例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>layer {
  name: &quot;custom1&quot;
  type: &quot;Custom&quot;
  bottom: &quot;conv1&quot;
  top: &quot;custom1_1&quot;
  custom_param {
    type: &quot;MY_CUSTOM&quot;
    shape {
        dim: 1
        dim: 256
        dim: 64
        dim: 64
    }
}
}
</pre></div>
</div>
<p>在该示例中定义了一个MY_CUSTOM类型的自定义算子，推理时用户需注册一个类型为MY_CUSTOM的自定义算子。</p>
</section>
<section id="prototxt中top域的-report后缀支持">
<h3>prototxt中top域的_report后缀支持<a class="headerlink" href="#prototxt中top域的-report后缀支持" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite在转换NNIE模型时，会将大部分的算子融合为NNIE运行的二进制文件，用户无法观察到中间算子的输出，通过在top域上添加”_report“后缀，转换构图时会将中间算子的输出添加到融合后的层输出中，若原先该算子便有输出（未被融合），则维持不变。</p>
<p>在推理运行时，用户可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/runtime_cpp.html#%E5%9B%9E%E8%B0%83%E8%BF%90%E8%A1%8C">回调运行</a>得到中间算子输出。</p>
<p>MindSpore Lite解析_report的相应规则，及与<a class="reference internal" href="#inplace机制"><span class="std std-doc">inplace机制</span></a>的冲突解决，参照《HiSVP 开发指南》中的定义说明。</p>
</section>
<section id="inplace机制">
<h3>inplace机制<a class="headerlink" href="#inplace机制" title="Permalink to this headline"></a></h3>
<p>使用Inplace层写法，可运行芯片高效模式。转换工具默认将Prototxt中符合芯片支持Inplace层的所有层进行改写，用户如需关闭该功能，可通过如下环境声明：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_DISABLE_INPLACE_FUSION</span><span class="o">=</span>off<span class="w">         </span><span class="c1"># 设置为on或未设置时，使能Inplace自动改写</span>
</pre></div>
</div>
<p>当自动改写被关闭时，若需对个别层使能芯片高效模式，可手动改写Prototxt里面的相应层。</p>
</section>
<section id="多图片batch运行及多step运行">
<h3>多图片batch运行及多step运行<a class="headerlink" href="#多图片batch运行及多step运行" title="Permalink to this headline"></a></h3>
<p>用户若需同时前向推理多个输入数据（多个图片），可通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/runtime_cpp.html#%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6resize">输入维度Resize</a>将模型输入的第一维resize为输入数据个数。NNIE模型只支持对第一个维度（’n’维）进行resize，其他维度（’hwc’）不可变。</p>
<p>对于循环或lstm网络，用户需根据step值，配置TIME_STEP环境变量，同时resize模型输入。
设一次同时前向推理的数据的个数为input_num，对于序列数据输入的节点resize为input_num * step，非序列数据输入的节点resize为input_num。</p>
<p>含有proposal算子的模型，不支持batch运行，不支持resize操作。</p>
</section>
<section id="节点名称的变动">
<h3>节点名称的变动<a class="headerlink" href="#节点名称的变动" title="Permalink to this headline"></a></h3>
<p>模型转换为NNIE模型后，各节点名称可能发生变化，用户可通过netron打开模型，得到变化后的节点名。</p>
</section>
<section id="proposal算子使用说明">
<h3>proposal算子使用说明<a class="headerlink" href="#proposal算子使用说明" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite提供Proposal算子的样例代码，在该样例中，以<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/register_kernel.html">自定义算子注册</a>的方式实现proposal算子及该算子infer shape的注册。用户可将其修改为自身模型匹配的实现后，进行集成使用。</p>
<blockquote>
<div><p>你可以在这里下载完整的样例代码：</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/nnie_proposal">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/nnie_proposal</a></p>
</div></blockquote>
</section>
<section id="分段机制说明及8段限制">
<h3>分段机制说明及8段限制<a class="headerlink" href="#分段机制说明及8段限制" title="Permalink to this headline"></a></h3>
<p>由于NNIE芯片支持的算子限制，在含有NNIE芯片不支持的算子时，需将模型分段为可支持层与不可支持层。
板端芯片支持最多8段的可支持层，当分段后的可支持层数量大于8段时，模型将无法运行，用户可通过netron观察Custom算子（其属性中含有type:NNIE），得到转换后的NNIE支持层数量。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="npu_info.html" class="btn btn-neutral float-left" title="集成NPU使用说明" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tensorrt_info.html" class="btn btn-neutral float-right" title="集成TensorRT使用说明" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>