<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>使用C++接口执行推理 &mdash; MindSpore Lite master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script><script src="../_static/translations.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="使用Java接口执行推理" href="runtime_java.html" />
    <link rel="prev" title="执行推理" href="runtime.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译端侧MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">编译云侧MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">端侧推理快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">云侧推理快速入门</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">端侧推理样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">执行推理</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">使用C++接口执行推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#模型读取">模型读取</a></li>
<li class="toctree-l3"><a class="reference internal" href="#创建配置上下文">创建配置上下文</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#配置线程数">配置线程数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置线程亲和性">配置线程亲和性</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置并行策略">配置并行策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用gpu后端">配置使用GPU后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用npu后端">配置使用NPU后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用nnie后端">配置使用NNIE后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用ascend后端">配置使用Ascend后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用coreml后端">配置使用CoreML后端</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#模型创建加载与编译">模型创建加载与编译</a></li>
<li class="toctree-l3"><a class="reference internal" href="#输入数据">输入数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="#执行推理">执行推理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#获取输出">获取输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="#内存释放">内存释放</a></li>
<li class="toctree-l3"><a class="reference internal" href="#高级用法">高级用法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#输入维度resize">输入维度Resize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#混合精度运行">混合精度运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#多硬件异构运行">多硬件异构运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opengl纹理输入">OpenGL纹理输入</a></li>
<li class="toctree-l4"><a class="reference internal" href="#共享内存池">共享内存池</a></li>
<li class="toctree-l4"><a class="reference internal" href="#回调运行">回调运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#模型加载与编译独立调用流程">模型加载与编译独立调用流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="#模型解密推理">模型解密推理</a></li>
<li class="toctree-l4"><a class="reference internal" href="#查看日志">查看日志</a></li>
<li class="toctree-l4"><a class="reference internal" href="#获取版本号">获取版本号</a></li>
<li class="toctree-l4"><a class="reference internal" href="#扩展使用">扩展使用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">使用Java接口执行推理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">在MCU或小型系统上执行推理或训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">端侧训练样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">基础推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">并发推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_distributed.html">分布式推理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">基准测试工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">执行推理</a> &raquo;</li>
      <li>使用C++接口执行推理</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_cpp.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="使用c接口执行推理">
<h1>使用C++接口执行推理<a class="headerlink" href="#使用c接口执行推理" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/lite/docs/source_zh_cn/use/runtime_cpp.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source.svg" /></a></p>
<blockquote>
<div><p>MindSpore已经统一了端边云推理API，如您想继续使用MindSpore Lite独立API进行端侧推理，可以参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.3/use/runtime_cpp.html">此文档</a>。</p>
</div></blockquote>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>通过<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.2/use/converter_tool.html">MindSpore Lite模型转换工具</a>转换成<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型后，即可在Runtime中执行模型的推理流程。本教程介绍如何使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/index.html">C++接口</a>执行推理。</p>
<p>使用MindSpore Lite推理框架主要包括以下步骤：</p>
<ol class="arabic simple">
<li><p>模型读取：从文件系统中读取由<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.2/use/converter_tool.html">模型转换工具</a>转换得到的<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型。</p></li>
<li><p>创建配置上下文：创建配置上下文<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>，保存需要的一些基本配置参数，用于指导模型编译和模型执行。</p></li>
<li><p>模型创建、加载与编译：执行推理之前，需要调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#build">Build</a>接口进行模型加载和模型编译。模型加载阶段将文件缓存解析成运行时的模型。模型编译阶段主要进行算子选型调度、子图切分等过程，该阶段会耗费较多时间所以建议Model创建一次，编译一次，多次推理。</p></li>
<li><p>输入数据：模型执行之前需要向<code class="docutils literal notranslate"><span class="pre">输入Tensor</span></code>中填充数据。</p></li>
<li><p>执行推理：使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#predict">Predict</a>进行模型推理。</p></li>
<li><p>获得输出：模型执行结束之后，可以通过<code class="docutils literal notranslate"><span class="pre">输出Tensor</span></code>得到推理结果。</p></li>
<li><p>释放内存：无需使用MindSpore Lite推理框架时，需要释放已创建的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>。</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
<blockquote>
<div><p>快速了解MindSpore Lite执行推理的完整调用流程，请参考<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.2/quick_start/quick_start_cpp.html">体验MindSpore Lite C++极简Demo</a>。</p>
</div></blockquote>
</section>
<section id="模型读取">
<h2>模型读取<a class="headerlink" href="#模型读取" title="永久链接至标题"></a></h2>
<p>通过MindSpore Lite进行模型推理时，需要从文件系统读取<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.2/use/converter_tool.html">模型转换工具</a>转换得到的<code class="docutils literal notranslate"><span class="pre">.ms</span></code>模型文件。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L332">示例代码</a>演示了从文件系统读取MindSpore Lite模型。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Read model file.</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="创建配置上下文">
<h2>创建配置上下文<a class="headerlink" href="#创建配置上下文" title="永久链接至标题"></a></h2>
<p>上下文会保存一些所需的基本配置参数，用于指导模型编译和模型执行，如果用户通过<code class="docutils literal notranslate"><span class="pre">new</span></code>创建<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>，不再需要时，需要用户通过<code class="docutils literal notranslate"><span class="pre">delete</span></code>释放。一般在创建编译完<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>后，<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>即可释放。</p>
<p>MindSpore Lite默认执行的后端是CPU，Context创建后调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#mutabledeviceinfo">MutableDeviceInfo</a>返回后端信息列表的引用，向列表中添加默认的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L250">示例代码</a>演示了如何创建Context，配置默认的CPU后端，并设定CPU使能Float16推理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code>中支持用户设置设备信息，包括<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#gpudeviceinfo">GPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#kirinnpudeviceinfo">KirinNPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#ascenddeviceinfo">AscendDeviceInfo</a>。设置的设备个数不能超过3个，推理过程按照用户设置的先后顺序选择后端设备进行部署推理。</p>
<p>Float16需要CPU为ARM v8.2架构的机型才能生效，其他不支持的机型和x86平台会自动回退到Float32执行。</p>
<p>对于iOS设备,暂时只支持向<code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code>添加CPU后端，且暂时不支持CPU后端Float16的执行。</p>
</div></blockquote>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>中包含的配置API如下：</p>
<section id="配置线程数">
<h3>配置线程数<a class="headerlink" href="#配置线程数" title="永久链接至标题"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#setthreadnum">SetThreadNum</a>配置线程数：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="配置线程亲和性">
<h3>配置线程亲和性<a class="headerlink" href="#配置线程亲和性" title="永久链接至标题"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#setthreadaffinity">SetThreadAffinity</a>配置线程和CPU绑定。如果参数是<code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">mode</span></code>，配置绑核策略，有效值为0-2，0为默认不绑核，1为优先绑大核，2为优先绑小核。如果参数是<code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">std::vector&lt;int&gt;</span> <span class="pre">&amp;core_list</span></code>，配置绑核列表。同时配置时，core_list生效，mode不生效。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the thread to be bound to the big core first.</span>
<span class="c1">// Valid value: 0: no affinities, 1: big cores first, 2: little cores first</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadAffinity</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="配置并行策略">
<h3>配置并行策略<a class="headerlink" href="#配置并行策略" title="永久链接至标题"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#setenableparallel">SetEnableParallel</a>配置执行推理时是否支持并行。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the inference supports parallel.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetEnableParallel</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="配置使用gpu后端">
<h3>配置使用GPU后端<a class="headerlink" href="#配置使用gpu后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为GPU时，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#gpudeviceinfo">GPUDeviceInfo</a>为首选推理后端。建议设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为次选后端，排在GPU后，以保证泛化模型的推理。其中GPUDeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>使能Float16推理。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L114">示例代码</a>演示如何创建CPU与GPU异构推理后端，同时GPU也设定使能Float16推理：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set GPU device first, make GPU preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New GPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// GPU use float16 operator as priority.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="c1">// Set VNIDIA device id, only valid when GPU backend is TensorRT.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// The GPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after GPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>目前GPU的后端，区分<code class="docutils literal notranslate"><span class="pre">arm64</span></code>和<code class="docutils literal notranslate"><span class="pre">x86_64</span></code>平台。</p>
<ul class="simple">
<li><p>在<code class="docutils literal notranslate"><span class="pre">arm64</span></code>上是基于OpenCL，支持Mali、Adreno的GPU，OpenCL版本为2.0。</p></li>
</ul>
<p>具体配置为：</p>
<p>CL_TARGET_OPENCL_VERSION=200</p>
<p>CL_HPP_TARGET_OPENCL_VERSION=120</p>
<p>CL_HPP_MINIMUM_OPENCL_VERSION=120</p>
<ul class="simple">
<li><p>在<code class="docutils literal notranslate"><span class="pre">x86_64</span></code>上是基于TensorRT的GPU，TensorRT版本为6.0.1.5。</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>属性是否设置成功取决于当前设备的<a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix">CUDA计算能力</a>。
<code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>属性仅在TensorRT的GPU上有效，用于指定NVIDIA显卡。</p>
</div></blockquote>
</section>
<section id="配置使用npu后端">
<h3>配置使用NPU后端<a class="headerlink" href="#配置使用npu后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为NPU时，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#kirinnpudeviceinfo">KirinNPUDeviceInfo</a>为首选推理后端。建议设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为次选后端，排在NPU后，以保证泛化模型的推理。其中KirinNPUDeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetFrequency</span></code>来设置NPU频率。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L127">示例代码</a>如何创建CPU与NPU异构推理后端，同时NPU频率设置为3。频率值默认为3，可设置为1（低功耗）、2（均衡）、3（高性能）、4（极致性能）：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set NPU device first, make NPU preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">npu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">KirinNPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">npu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New KirinNPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// NPU set frequency to be 3.</span>
<span class="n">npu_device_info</span><span class="o">-&gt;</span><span class="n">SetFrequency</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="c1">// The NPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">npu_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after NPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="配置使用nnie后端">
<h3>配置使用NNIE后端<a class="headerlink" href="#配置使用nnie后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为CPU和NNIE的异构推理时，只需要按照<a class="reference internal" href="#创建配置上下文"><span class="std std-doc">配置使用CPU后端</span></a>的方法创建好Context即可，无需指定provider。</p>
</section>
<section id="配置使用ascend后端">
<h3>配置使用Ascend后端<a class="headerlink" href="#配置使用ascend后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为Ascend时(目前支持Ascend310)，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#ascenddeviceinfo">AscendDeviceInfo</a>为首选推理后端。建议设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为次选后端，排在Ascend后，以保证泛化模型的推理。其中Ascend310DeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>来设置设备ID。</p>
<p>下面[示例代码]如何创建CPU与Ascend异构推理后端，同时设备ID设置为0：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set Ascend310 device first, make Ascend310 preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ascend_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Ascend310 set device id to be 0.</span>
<span class="n">ascend_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// The ascend310 device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after Ascend310 as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="配置使用coreml后端">
<h3>配置使用CoreML后端<a class="headerlink" href="#配置使用coreml后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为CoreML时，只需实例化<a class="reference external" href="https://mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#coremldelegate">CoreMLDelegate</a>类，并将实例对象通过<a class="reference external" href="https://mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#setdelegate">SetDelegate</a>接口传入上下文对象(context)即可。这与NPU和GPU等以硬件为区分的后端配置步骤有些许不同。</p>
<p>下面示例代码演示了如何创建CPU与CoreML异构推理后端：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set CPU device after NPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">coreml_delegate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CoreMLDelegate</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">coreml_delegate</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CoreMLDelegate failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetDelegate</span><span class="p">(</span><span class="n">coreml_delegate</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>当前CoreML后端暂时只支持操作系统版本不低于iOS 11的设备。</p>
</div></blockquote>
</section>
</section>
<section id="模型创建加载与编译">
<h2>模型创建加载与编译<a class="headerlink" href="#模型创建加载与编译" title="永久链接至标题"></a></h2>
<p>使用MindSpore Lite执行推理时，<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>是推理的主入口，通过Model可以实现模型加载、模型编译和模型执行。采用上一步创建得到的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>，调用Model的复合<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#build">Build</a>接口来实现模型加载与模型编译。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L265">示例代码</a>演示了Model创建、加载与编译的过程：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="c1">// After the model is built, the Context can be released.</span>
<span class="p">...</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>创建并编译完<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>后，上一步创建得到的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>即可释放。</p>
<p>针对大模型，使用model buffer进行加载编译的时候需要单独设置权重文件的路径，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#loadconfig">LoadConfig</a>或<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#updateconfig">UpdateConfig</a>接口设置模型路径，其中<code class="docutils literal notranslate"><span class="pre">section</span></code>为<code class="docutils literal notranslate"><span class="pre">model_file</span></code>，<code class="docutils literal notranslate"><span class="pre">key</span></code>为<code class="docutils literal notranslate"><span class="pre">mindir_path</span></code>；使用model path进行加载编译的时候不需要设置其他参数，会自动读取权重参数。</p>
<p>用户在源码编译时如果启用了<code class="docutils literal notranslate"><span class="pre">MSLITE_ENABLE_MODEL_PRE_INFERENCE</span></code>功能，运行时会在Build阶段（非加密场景）默认进行预推理，以检测程序是否能正常执行。该功能可通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#loadconfig">LoadConfig</a>或<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#updateconfig">UpdateConfig</a>接口设置关闭，其中<code class="docutils literal notranslate"><span class="pre">section</span></code>为<code class="docutils literal notranslate"><span class="pre">common</span></code>，<code class="docutils literal notranslate"><span class="pre">key</span></code>为<code class="docutils literal notranslate"><span class="pre">enable_pre_inference</span></code>，<code class="docutils literal notranslate"><span class="pre">value</span></code>为<code class="docutils literal notranslate"><span class="pre">true</span></code>或<code class="docutils literal notranslate"><span class="pre">false</span></code>。</p>
</div></blockquote>
</section>
<section id="输入数据">
<h2>输入数据<a class="headerlink" href="#输入数据" title="永久链接至标题"></a></h2>
<p>在模型执行前，需要获取到模型的输入<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#mstensor">MSTensor</a>，将输入数据通过<code class="docutils literal notranslate"><span class="pre">memcpy</span></code>拷贝到模型的输入Tensor。可以通过MSTensor的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#datasize">DataSize</a>方法来获取Tensor应该填入的数据大小，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#datatype">DataType</a>方法来获取Tensor的数据类型，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#mutabledata">MutableData</a>方法来获取可写的指针。</p>
<p>MindSpore Lite提供两种方法来获取模型的输入Tensor。</p>
<ol class="arabic">
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getinputbytensorname">GetInputByTensorName</a>方法，根据Tensor的名称来获取模型输入Tensor，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L154">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetInputByTensorName</span></code>获得输入Tensor并填充数据。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume that the model has only one input tensor named graph_input-173.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputByTensorName</span><span class="p">(</span><span class="s">&quot;graph_input-173&quot;</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getinputs">GetInputs</a>方法，直接获取所有的模型输入Tensor的vector，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L137">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetInputs</span></code>获得输入Tensor并填充数据。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one input tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of in_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>MindSpore Lite的模型输入Tensor中的数据排布必须是<code class="docutils literal notranslate"><span class="pre">NHWC</span></code>。如果需要了解更多数据前处理过程，可参考基于JNI接口的Android应用开发中<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.2/quick_start/quick_start.html#%E7%BC%96%E5%86%99%E7%AB%AF%E4%BE%A7%E6%8E%A8%E7%90%86%E4%BB%A3%E7%A0%81">编写端侧推理代码</a>的第2步，将输入图片转换为传入MindSpore模型的Tensor格式。</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getinputs">GetInputs</a>和<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getinputbytensorname">GetInputByTensorName</a>方法返回的数据不需要用户释放。</p>
</div></blockquote>
</section>
<section id="执行推理">
<h2>执行推理<a class="headerlink" href="#执行推理" title="永久链接至标题"></a></h2>
<p>MindSpore Lite调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#predict">Predict</a>进行模型推理。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L355">示例代码</a>演示调用<code class="docutils literal notranslate"><span class="pre">Predict</span></code>执行推理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="获取输出">
<h2>获取输出<a class="headerlink" href="#获取输出" title="永久链接至标题"></a></h2>
<p>MindSpore Lite在执行完推理后，就可以获取模型的推理结果。MindSpore Lite提供三种方法来获取模型的输出<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#mstensor">MSTensor</a>。</p>
<ol class="arabic">
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getoutputsbynodename">GetOutputsByNodeName</a>方法，根据模型输出节点的名称来获取模型输出Tensor中连接到该节点的Tensor的vector，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L170">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code>获得输出Tensor。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model before.</span>
<span class="c1">// Assume that model has a output node named Softmax-65.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_vec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;Softmax-65&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named Default/Sigmoid-op204 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Post-processing your result data.</span>
</pre></div>
</div>
</li>
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getoutputbytensorname">GetOutputByTensorName</a>方法，根据模型输出Tensor的名称来获取对应的模型输出Tensor，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L200">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code>获得输出Tensor。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="c1">// We can use GetOutputTensorNames method to get all name of output tensor of model which is in order.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Assume we have created a Model instance named model before.</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor_name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tensor_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getoutputs">GetOutputs</a>方法，直接获取所有的模型输出Tensor的vector，下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L226">示例代码</a>演示如何调用<code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code>获得输出Tensor。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">out_tensors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getoutputsbynodename">GetOutputsByNodeName</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getoutputbytensorname">GetOutputByTensorName</a>和<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#getoutputs">GetOutputs</a>方法返回的数据不需要用户释放。</p>
</div></blockquote>
</section>
<section id="内存释放">
<h2>内存释放<a class="headerlink" href="#内存释放" title="永久链接至标题"></a></h2>
<p>无需使用MindSpore Lite推理框架时，需要释放已经创建的Model，下列<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L370">示例代码</a>演示如何在程序结束前进行内存释放。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model.</span>
<span class="c1">// Assume that the variable of Model * is named model.</span>
<span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="高级用法">
<h2>高级用法<a class="headerlink" href="#高级用法" title="永久链接至标题"></a></h2>
<section id="输入维度resize">
<h3>输入维度Resize<a class="headerlink" href="#输入维度resize" title="永久链接至标题"></a></h3>
<p>使用MindSpore Lite进行推理时，如果需要对输入的shape进行Resize，则可以在已完成创建<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>与模型编译<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#build">Build</a>之后调用Model的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#resize">Resize</a>接口，对输入的Tensor重新设置shape。</p>
<blockquote>
<div><p>某些网络是不支持可变维度，会提示错误信息后异常退出，比如，模型中有MatMul算子，并且MatMul的一个输入Tensor是权重，另一个输入Tensor是输入时，调用可变维度接口会导致输入Tensor和权重Tensor的Shape不匹配，最终导致推理失败。</p>
<p>TensorRT的GPU后端只支持在NHWC输入格式下的NHW维度的resize，且resize维度的shape值，不能大于创建的Model的输入shape值。</p>
</div></blockquote>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L321">示例代码</a>演示如何对MindSpore Lite的输入Tensor进行Resize：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">};</span>
<span class="c1">// Assume the model has only one input,resize input shape to [1, 128, 128, 3]</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">;</span>
<span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">resize_shape</span><span class="p">);</span>
<span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="混合精度运行">
<h3>混合精度运行<a class="headerlink" href="#混合精度运行" title="永久链接至标题"></a></h3>
<p>MindSpore Lite 支持混合精度推理。
用户可以在完成创建<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>之后，在模型编译<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#build">Build</a>之前，调用Model的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#loadconfig">LoadConfig</a>接口，配置混合精度信息。
配置文件举例，内容如下:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[execution_plan]
op_name1=data_type:float16
op_name2=data_type:float32
</pre></div>
</div>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L321">示例代码</a>演示如何进行混合精度推理：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_path</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model load config error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">Status</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model build error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="n">Status</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="多硬件异构运行">
<h3>多硬件异构运行<a class="headerlink" href="#多硬件异构运行" title="永久链接至标题"></a></h3>
<p>MindSpore Lite 支持多硬件异构推理。
用户可以在<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#context">Context</a>中配置多个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#deviceinfocontext">DeviceInfoContext</a>，并且根据设备的先后顺序，设置异构硬件的运行优先级。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L546">示例代码</a>演示如何进行多硬件异构推理：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="c1">// enable NPU CPU GPU in inference. NPU is preferentially used, then the CPU, and GPU get the lowest priority.</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">KirinNPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>

<span class="n">Status</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model build error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="n">Status</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="opengl纹理输入">
<h3>OpenGL纹理输入<a class="headerlink" href="#opengl纹理输入" title="永久链接至标题"></a></h3>
<p>MindSpore Lite 支持 OpenGL纹理输入，进行端到端的GPU同构推理，推理结果以OpenGL纹理数据返回。该功能在使用过程中需要配置到Context中，和在运行推理时绑定OpenGL纹理数据，这两个过程。</p>
<ol class="arabic">
<li><p>配置 Context</p>
<p>用户需要将 Context 中的 devgpu_device_info_中的 SetEnableGLTexture 属性设置为 true，并且将用户当前的OpenGL EGLContext 、EGLDisplay分别通过SetGLContext接口和SetGLDisplay接口进行配置。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// 1. Set EnableGLTexture true</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableGLTexture</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>

<span class="c1">// 2. Set GLContext</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gl_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eglGetCurrentContext</span><span class="p">();</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetGLContext</span><span class="p">(</span><span class="n">gl_context</span><span class="p">);</span>

<span class="c1">// 3. Set GLDisplay</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gl_display</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eglGetCurrentDisplay</span><span class="p">();</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetGLDisplay</span><span class="p">(</span><span class="n">gl_display</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>绑定OpenGL纹理数据</p>
<p>在模型编译阶段后，模型运行前，用户需要调用 BindGLTexture2DMemory(const std::map&lt;std::string, GLuint&gt; &amp;inputGlTexture, std::map&lt;std::string, GLuint&gt; *outputGLTexture;) 函数绑定输入输出纹理，代替原有输入数据的步骤，因为 MindSpore Lite 本身并没有分配 OpenGL 内存的功能，所以要求用户根据模型输入输出的 tensor size 事先创建好输入输出纹理的内存，并将纹理内存对应的纹理 ID 绑定到模型的输入输出，示例代码如下</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gl_texture</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gl_texture</span><span class="p">;</span>

<span class="p">...</span><span class="w"> </span><span class="c1">// Write OpenGL Texture data(GLuint) into input_gl_texture and output_gl_texture</span>

<span class="c1">// Bind texture data with input and output tensors</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">BindGLTexture2DMemory</span><span class="p">(</span><span class="n">input_gl_texture</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output_gl_texture</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;BindGLTexture2DMemory failed&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
</pre></div>
</div>
<p>std::map&lt;std::string, GLuint&gt;  input_gl_texture 变量中key为模型输入tensor name，value为对应的GLuint 纹理；std::map&lt;std::string, GLuint&gt;  output_gl_texture 变量中key为模型输出tensor name，value为对应的GLuint 纹理。模型输入输出tensor name可以通过tensor.Name()接口获取，示例代码如下：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inTextureIDs</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">inputGlTexture</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">inTextureIDs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outTextureIDs</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">outputGlTexture</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">outTextureIDs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Predict结果</p>
<p>绑定完成后直接调用ms_model_的 Predict 接口进行推理即可，模型输出会被拷贝到绑定的输出纹理 ID 对应的内存上，用户可从outputs上面获取推理结果</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">ms_inputs_for_api_</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">ms_before_call_back_</span><span class="p">,</span><span class="w"> </span><span class="n">ms_after_call_back_</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="共享内存池">
<h3>共享内存池<a class="headerlink" href="#共享内存池" title="永久链接至标题"></a></h3>
<p>如果存在多个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>的情况，可以通过在<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#deviceinfocontext">DeviceInfoContext</a>中配置同一个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#allocator">Allocator</a>，实现共享内存池来减少运行时内存大小。其中，内存池的内存总大小限制为<code class="docutils literal notranslate"><span class="pre">3G</span></code>，单次分配的内存限制为<code class="docutils literal notranslate"><span class="pre">2G</span></code>。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L546">示例代码</a>演示如何在两个Model间共享内存池的功能：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context1</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info1</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model1</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context1</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">context2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context2</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Use the same allocator to share the memory pool.</span>
<span class="n">device_info2</span><span class="o">-&gt;</span><span class="n">SetAllocator</span><span class="p">(</span><span class="n">device_info1</span><span class="o">-&gt;</span><span class="n">GetAllocator</span><span class="p">());</span>
<span class="n">device_list2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info2</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model2</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context2</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="回调运行">
<h3>回调运行<a class="headerlink" href="#回调运行" title="永久链接至标题"></a></h3>
<p>MindSpore Lite可以在调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#predict">Predict</a>时，传入两个<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#mskernelcallback">MSKernelCallBack</a>函数指针来回调推理模型，相比于一般的模型执行，回调运行可以在运行过程中获取额外的信息，帮助开发者进行性能分析、Bug调试等。额外的信息包括：</p>
<ul class="simple">
<li><p>当前运行的节点名称</p></li>
<li><p>推理当前节点前的输入输出Tensor</p></li>
<li><p>推理当前节点后的输入输出Tensor</p></li>
</ul>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L672">示例代码</a>演示如何定义了两个回调函数作为前置回调指针和后置回调指针，传入到Predict接口进行回调推理。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Definition of callback function before forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">before_call_back</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_inputs</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_outputs</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Before forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_type_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Definition of callback function after forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">after_call_back</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_inputs</span><span class="p">,</span>
<span class="w">                          </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_outputs</span><span class="p">,</span>
<span class="w">                          </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;After forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_type_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">before_call_back</span><span class="p">,</span><span class="w"> </span><span class="n">after_call_back</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="模型加载与编译独立调用流程">
<h3>模型加载与编译独立调用流程<a class="headerlink" href="#模型加载与编译独立调用流程" title="永久链接至标题"></a></h3>
<p>模型加载与编译也可以分别调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#serialization">Serialization</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#load">Load</a>接口和<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#build">Build</a>实现。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L282">示例代码</a>演示模型加载与编译独立调用的流程：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>

<span class="c1">// Load graph</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">load_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">load_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Load graph failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Build model</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">GraphCell</span><span class="w"> </span><span class="n">graph_cell</span><span class="p">(</span><span class="n">graph</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="模型解密推理">
<h3>模型解密推理<a class="headerlink" href="#模型解密推理" title="永久链接至标题"></a></h3>
<p>当模型被<a class="reference external" href="https://mindspore.cn/mindarmour/docs/zh-CN/master/model_encrypt_protection.html#%E7%AB%AF%E4%BE%A7%E6%A8%A1%E5%9E%8B%E4%BF%9D%E6%8A%A4">converter_lite工具</a>转换时加密，在lite加载模型时需通过传入密钥和解密工具相关参数。其中，dec_key应与使用converter_lite工具加密时的密钥一致，均十六进制表示的字符串。如b’0123456789ABCDEF’对应的十六进制表示为30313233343536373839414243444546，Linux平台用户可以使用xxd工具对字节表示的密钥进行十六进制表达转换。crypto_lib_path为该环境中openssl的安装路径，如”/home/root/openssl”。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc">示例代码</a>演示模型解密加载及推理的流程：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">RunEncryptedInfer</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">dec_key_str</span><span class="p">,</span>
<span class="w">                      </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">crypto_lib_path</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Set Context</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>

<span class="w">  </span><span class="c1">// Set Decrypt Parameters</span>
<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Key</span><span class="w"> </span><span class="n">dec_key</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">dec_mode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;AES-GCM&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">dec_key</span><span class="p">.</span><span class="n">len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Hex2ByteArray</span><span class="p">(</span><span class="n">dec_key_str</span><span class="p">,</span><span class="w"> </span><span class="n">dec_key</span><span class="p">.</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">kEncMaxLen</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">dec_key</span><span class="p">,</span><span class="w"> </span><span class="n">dec_mode</span><span class="p">,</span><span class="w"> </span><span class="n">crypto_lib_path</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Predict</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Delete model.</span>
<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
<p>如使用converter_lite工具的命令为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>./lenet.mindir<span class="w"> </span>--outputFile<span class="o">=</span>lenet_enc<span class="w"> </span>--encryptKey<span class="o">=</span><span class="m">30313233343536373839414243444546</span><span class="w"> </span>--encryption<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>在mindspore/lite/examples/runtime_cpp目录下编译源码生成build/runtime_cpp文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>mindspore/lite/examples/runtime_cpp
bash<span class="w"> </span>build.sh
<span class="nb">cd</span><span class="w"> </span>build
</pre></div>
</div>
<p>运行Lite端侧使用加密后的模型进行推理：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./runtime_cpp<span class="w">  </span>--modelFile<span class="o">=</span>./lenet_enc.ms<span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">30313233343536373839414243444546</span><span class="w"> </span><span class="si">${</span><span class="nv">your_openssl_path</span><span class="si">}</span>
</pre></div>
</div>
</section>
<section id="查看日志">
<h3>查看日志<a class="headerlink" href="#查看日志" title="永久链接至标题"></a></h3>
<p>当推理出现异常的时候，可以通过查看日志信息来定位问题。针对Android平台，采用<code class="docutils literal notranslate"><span class="pre">Logcat</span></code>命令行工具查看MindSpore Lite推理的日志信息，并利用<code class="docutils literal notranslate"><span class="pre">MS_LITE</span></code> 进行筛选。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>logcat<span class="w"> </span>-s<span class="w"> </span><span class="s2">&quot;MS_LITE&quot;</span>
</pre></div>
</div>
<blockquote>
<div><p>对iOS设备暂不支持日志查看。</p>
</div></blockquote>
</section>
<section id="获取版本号">
<h3>获取版本号<a class="headerlink" href="#获取版本号" title="永久链接至标题"></a></h3>
<p>MindSpore Lite提供了<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore.html#version">Version</a>方法可以获取版本号，包含在<code class="docutils literal notranslate"><span class="pre">include/api/types.h</span></code>头文件中，调用该方法可以得到当前MindSpore Lite的版本号。</p>
<p>下面<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_cpp/main.cc#L717">示例代码</a>演示如何获取MindSpore Lite的版本号：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Version</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="扩展使用">
<h3>扩展使用<a class="headerlink" href="#扩展使用" title="永久链接至标题"></a></h3>
<p>本章节提供了扩展MindSpore Lite推理框架的示例程序，通过演示自定义算子的构建、注册的全流程，用户能够快速了解推理框架的扩展API的使用，能够在推理框架中集成自定义算子。本章节以一个具有简易Add计算能力的Custom单算子为模型。相关代码放置在<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/runtime_extend">mindspore/lite/examples/runtime_extend</a>目录。</p>
<p>本章节仅提供了在Linux环境下的使用说明。</p>
<section id="算子infershape扩展">
<h4>算子InferShape扩展<a class="headerlink" href="#算子infershape扩展" title="永久链接至标题"></a></h4>
<p>用户需继承<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore_kernel.html#kernelinterface">KernelInterface</a>类，重载<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore_kernel.html#infer">Infer</a>接口函数。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="nf">CheckInputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">         </span><span class="c1">// 输入校验函数，校验输入张量的shape是否合规</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">input_shape</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">input_shape</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">input_shape</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CustomAddInfer</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">CustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">CustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="n">Status</span><span class="w"> </span><span class="nf">Infer</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">               </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w">        </span><span class="c1">// 重载Infer公有函数</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetFormat</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">format</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetDataType</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CheckInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">({</span><span class="mi">-1</span><span class="p">});</span><span class="w">        </span><span class="c1">// 输出张量的shape设为{-1}，表示在运行时需要再次推断</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddInfer</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>
<span class="n">REGISTER_CUSTOM_KERNEL_INTERFACE</span><span class="p">(</span><span class="n">CustomOpTutorial</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">)</span><span class="w">       </span><span class="c1">// 调用注册接口</span>
</pre></div>
</div>
<blockquote>
<div><p>shape推断分为两个时期，一是图编译时的静态推断，二是图运行时的动态推断。</p>
<p>静态推断：</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CheckInputs</span></code>失败或者当前节点需要动态推断的情形下，需将输出张量的shape设为{-1}，以便在图运行时的识别标识,且返回码需设置为<code class="docutils literal notranslate"><span class="pre">kLiteInferInvalid</span></code>。</p></li>
<li><p>其他情形下，返回其他错误码，程序将会停止，请进行必要的检查。</p></li>
</ol>
<p>动态推断：</p>
<p>在算子运行时，动态推断是否需要，依据对输出张量的shape校验。请参考下面的算子扩展说明。</p>
</div></blockquote>
</section>
<section id="算子扩展">
<h4>算子扩展<a class="headerlink" href="#算子扩展" title="永久链接至标题"></a></h4>
<ol class="arabic">
<li><p>用户需继承<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore_kernel.html#kernel">Kernel</a>类，重载必要的接口。</p>
<ul>
<li><p>Prepare：此接口将在图编译期间调用，用户可对算子做运行前的准备或者必要的校验。</p></li>
<li><p>Execute：此接口是算子的运行接口，用户可将<strong>动态推断</strong>逻辑<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_extend/src/custom_add_kernel.cc">PreProcess</a>放置于此接口内调用。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="nf">CheckOutputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">           </span><span class="c1">// 算子运行时校验，以确定是否调用InferShape过程</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">output_shape</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">output_shape</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">output_shape</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>ReSize：此接口用于在图输入shape变化的情形下，当前算子所需的相应变动。</p></li>
<li><p>属性解析： 用户需自行提供对算子属性的解析，可参考<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_extend/src/custom_add_kernel.cc">ParseAttrData</a>。</p></li>
</ul>
</li>
<li><p>算子注册，API接口可参考<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.2/api_cpp/mindspore_registry.html#register-custom-kernel">REGISTER_CUSTOM_KERNEL</a>。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kFloat32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                         </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                         </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddKernel</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">REGISTER_CUSTOM_KERNEL</span><span class="p">(</span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">CustomOpTutorial</span><span class="p">,</span><span class="w"> </span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="示例演示">
<h4>示例演示<a class="headerlink" href="#示例演示" title="永久链接至标题"></a></h4>
<ol class="arabic">
<li><p>编译</p>
<ul>
<li><p>环境要求</p>
<ul class="simple">
<li><p>系统环境：Linux x86_64，推荐使用Ubuntu 18.04.02LTS</p></li>
<li><p>编译依赖：</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>编译构建</p>
<p>在<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend</span></code>目录下执行<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/runtime_extend/build.sh">build.sh</a>，将自动下载MindSpore Lite发布件并编译Demo。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<blockquote>
<div><p>若使用该build脚本下载MindSpore Lite发布件失败，请手动下载硬件平台为CPU、操作系统为Ubuntu-x64的MindSpore Lite发布件<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.2/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a>，将解压后<code class="docutils literal notranslate"><span class="pre">runtime/lib</span></code>目录下的<code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code>文件拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/lib</span></code>目录、<code class="docutils literal notranslate"><span class="pre">runtime/include</span></code>目录拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend</span></code>目录下。</p>
<p>若<code class="docutils literal notranslate"><span class="pre">add_extend.ms</span></code>模型下载失败，请手动下载相关模型文件<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/add_extend.ms">add_extend.ms</a>，并将其拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/model</span></code>目录。</p>
<p>通过手动下载并且将该文件放到指定位置后，需要再次执行build.sh脚本才能完成编译构建。</p>
</div></blockquote>
</li>
<li><p>编译输出</p>
<p>在<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/build</span></code>目录下生成了runtime_extend_tutorial的可执行程序。</p>
</li>
</ul>
</li>
<li><p>执行程序</p>
<p>编译构建后，进入<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/build</span></code>目录，并执行以下命令，体验扩展的MindSpore Lite推理add_extend.ms模型。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./runtime_extend_tutorial<span class="w"> </span>../model/add_extend.ms
</pre></div>
</div>
<p>执行完成后将能得到如下结果，打印输出Tensor的名称、输出Tensor的大小，输出Tensor的数量以及前20个数据：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:add-0 tensor size is:400 tensor elements num is:100
output data is:2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime.html" class="btn btn-neutral float-left" title="执行推理" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="runtime_java.html" class="btn btn-neutral float-right" title="使用Java接口执行推理" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>