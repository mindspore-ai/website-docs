<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>使用C++接口执行云侧推理 &mdash; MindSpore Lite master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script><script src="../../_static/translations.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="使用Java接口执行云侧推理" href="runtime_java.html" />
    <link rel="prev" title="基础推理" href="runtime.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">编译端侧MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译云侧MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">端侧推理快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">云侧推理快速入门</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">端侧推理样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">训练后量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">在MCU或小型系统上执行推理或训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">专用芯片集成说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">端侧训练样例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">执行训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧第三方接入</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">使用Delegate支持第三方AI框架接入</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">基准测试工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">静态库裁剪工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">模型混淆工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧推理</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">基础推理</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">使用C++接口执行云侧推理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#准备工作">准备工作</a></li>
<li class="toctree-l3"><a class="reference internal" href="#创建配置上下文">创建配置上下文</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#配置使用cpu后端">配置使用CPU后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用gpu后端">配置使用GPU后端</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置使用ascend后端">配置使用Ascend后端</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#模型创建加载与编译">模型创建加载与编译</a></li>
<li class="toctree-l3"><a class="reference internal" href="#输入数据">输入数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="#执行推理">执行推理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#编译和执行">编译和执行</a></li>
<li class="toctree-l3"><a class="reference internal" href="#高级用法">高级用法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#动态shape输入">动态shape输入</a></li>
<li class="toctree-l4"><a class="reference internal" href="#指定输入输出host内存">指定输入输出host内存</a></li>
<li class="toctree-l4"><a class="reference internal" href="#指定输入输出设备device内存">指定输入输出设备（device）内存</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ascend后端ge推理">Ascend后端GE推理</a></li>
<li class="toctree-l4"><a class="reference internal" href="#多线程加载模型">多线程加载模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#多模型共享权重">多模型共享权重</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#实验特性">实验特性</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#多后端异构能力">多后端异构能力</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">使用Java接口执行云侧推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="runtime_python.html">使用Python接口执行云侧推理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">并发推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">分布式推理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">云侧工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">模型转换工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">基准测试工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">模型支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">问题定位指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">日志</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">基础推理</a> &raquo;</li>
      <li>使用C++接口执行云侧推理</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_cpp.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="使用c接口执行云侧推理">
<h1>使用C++接口执行云侧推理<a class="headerlink" href="#使用c接口执行云侧推理" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/lite/docs/source_zh_cn/use/cloud_infer/runtime_cpp.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>本教程介绍如何使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/index.html">C++接口</a>执行MindSpore Lite云侧推理。</p>
<p>MindSpore Lite云侧推理仅支持在Linux环境部署运行。支持Atlas 200/300/500推理产品、Atlas推理系列产品（配置Ascend310P AI 处理器）、Atlas训练系列产品、Nvidia GPU和CPU硬件后端。</p>
<p>如需体验MindSpore Lite端侧推理流程，请参考文档<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.3/use/runtime_cpp.html">使用C++接口执行端侧推理</a>。</p>
<p>使用MindSpore Lite推理框架主要包括以下步骤：</p>
<ol class="arabic simple">
<li><p>模型读取：通过MindSpore导出MindIR模型，或者由<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.3/use/cloud_infer/converter_tool.html">模型转换工具</a>转换获得MindIR模型。</p></li>
<li><p>创建配置上下文：创建配置上下文<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#context">Context</a>，保存需要的一些基本配置参数，用于指导模型编译和模型执行。</p></li>
<li><p>模型加载与编译：执行推理之前，需要调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#build-3">Build</a>接口进行模型加载和模型编译。模型加载阶段将文件缓存解析成运行时的模型。模型编译阶段会耗费较多时间所以建议Model创建一次，编译一次，多次推理。</p></li>
<li><p>输入数据：模型执行之前需要填充输入数据。</p></li>
<li><p>执行推理：使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#model">Model</a>的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#predict">Predict</a>进行模型推理。</p></li>
</ol>
<p><img alt="img" src="../../_images/lite_runtime.png" /></p>
</section>
<section id="准备工作">
<h2>准备工作<a class="headerlink" href="#准备工作" title="永久链接至标题"></a></h2>
<ol class="arabic simple">
<li><p>以下代码样例来自于<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3/mindspore/lite/examples/cloud_infer/runtime_cpp">使用C++接口执行云侧推理示例代码</a>。</p></li>
<li><p>通过MindSpore导出MindIR模型，或者由<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.3/use/cloud_infer/converter_tool.html">模型转换工具</a>转换获得MindIR模型，并将其拷贝到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/runtime_cpp/model</span></code>目录，可以下载MobileNetV2模型文件<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a>。</p></li>
<li><p>从<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.3/use/downloads.html">官网</a>下载Ascend、Nvidia GPU、CPU三合一的MindSpore Lite云侧推理包<code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-linux-{arch}.tar.gz</span></code>，并存放到<code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/runtime_cpp</span></code>目录。</p></li>
</ol>
</section>
<section id="创建配置上下文">
<h2>创建配置上下文<a class="headerlink" href="#创建配置上下文" title="永久链接至标题"></a></h2>
<p>上下文会保存一些所需的基本配置参数，用于指导模型编译和模型执行。</p>
<p>下面示例代码演示了如何创建Context。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#mutabledeviceinfo">MutableDeviceInfo</a>返回后端信息列表的引用，指定运行的设备。<code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code>中支持用户设置设备信息，包括<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#gpudeviceinfo">GPUDeviceInfo</a>、<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#ascenddeviceinfo">AscendDeviceInfo</a>。设置的设备个数当前只能为其中一个。</p>
<section id="配置使用cpu后端">
<h3>配置使用CPU后端<a class="headerlink" href="#配置使用cpu后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为CPU时，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#cpudeviceinfo">CPUDeviceInfo</a>为推理后端。通过<code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>使能Float16推理。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>可选择性地额外设置线程数、线程亲和性、并行策略等特性。</p>
<ol class="arabic">
<li><p>配置线程数</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setthreadnum">SetThreadNum</a>配置线程数：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>配置线程亲和性</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setthreadaffinity-1">SetThreadAffinity</a>配置线程和CPU绑定。
通过参数<code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">std::vector&lt;int&gt;</span> <span class="pre">&amp;core_list</span></code>设置绑核列表。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the thread to be bound to the core list.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadAffinity</span><span class="p">({</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">});</span>
</pre></div>
</div>
</li>
<li><p>配置并行策略</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#context">Context</a>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setinteropparallelnum">SetInterOpParallelNum</a>设置运行时的算子并行推理数目。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the inference supports parallel.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetInterOpParallelNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="配置使用gpu后端">
<h3>配置使用GPU后端<a class="headerlink" href="#配置使用gpu后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为GPU时，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#gpudeviceinfo">GPUDeviceInfo</a>为推理后端。其中GPUDeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>来设置设备ID，通过<code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>或者<code class="docutils literal notranslate"><span class="pre">SetPrecisionMode</span></code>使能Float16推理。</p>
<p>下面示例代码演示如何创建GPU推理后端，同时设备ID设置为0：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New GPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set NVIDIA device id.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// GPU use float16 operator as priority.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="c1">// The GPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>属性是否设置成功取决于当前设备的<a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix">CUDA计算能力</a>。</p>
<p>用户可通过调用 <code class="docutils literal notranslate"><span class="pre">SetPrecisionMode()</span></code>接口配置精度模式，设置 <code class="docutils literal notranslate"><span class="pre">SetPrecisionMode(&quot;preferred_fp16&quot;)</span></code> 时，同时 <code class="docutils literal notranslate"><span class="pre">SetEnableFP16(true)</span></code> 会自动设置，反之亦然。</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>SetPrecisionMode()</p></th>
<th class="head"><p>SetEnableFP16()</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enforce_fp32</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-odd"><td><p>preferred_fp16</p></td>
<td><p>true</p></td>
</tr>
</tbody>
</table>
</section>
<section id="配置使用ascend后端">
<h3>配置使用Ascend后端<a class="headerlink" href="#配置使用ascend后端" title="永久链接至标题"></a></h3>
<p>当需要执行的后端为Ascend时(目前支持Atlas 200/300/500推理产品、Atlas推理系列产品（配置Ascend310P AI 处理器）、Atlas训练系列产品)，需要设置<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#ascenddeviceinfo">AscendDeviceInfo</a>为推理后端。其中AscendDeviceInfo通过<code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>来设置设备ID。Ascend默认使能Float16精度，可通过<code class="docutils literal notranslate"><span class="pre">AscendDeviceInfo.SetPrecisionMode</span></code>更改精度模式。</p>
<p>下面示例代码演示如何创建Ascend推理后端，同时设备ID设置为0：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// for Atlas 200/300/500 inference product, Atlas inference series (with Ascend 310P AI processor), Atlas training series</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set Atlas 200/300/500 inference product, Atlas inference series (with Ascend 310P AI processor), Atlas training series device id.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="c1">// The Ascend device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>在Ascend弹性加速服务（拉远模式）环境运行推理：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Set the provider to ge.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>用户可通过调用 <code class="docutils literal notranslate"><span class="pre">SetPrecisionMode()</span></code>接口配置精度模式，使用场景如下表所示：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>用户配置precision mode参数</p></th>
<th class="head"><p>ACL实际获取precision mode参数</p></th>
<th class="head"><p>ACL使用场景说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enforce_fp32</p></td>
<td><p>force_fp32</p></td>
<td><p>强制使用 fp32</p></td>
</tr>
<tr class="row-odd"><td><p>preferred_fp32</p></td>
<td><p>allow_fp32_to_fp16</p></td>
<td><p>优先使用 fp32</p></td>
</tr>
<tr class="row-even"><td><p>enforce_fp16</p></td>
<td><p>force_fp16</p></td>
<td><p>强制使用 fp16</p></td>
</tr>
<tr class="row-odd"><td><p>enforce_origin</p></td>
<td><p>must_keep_origin_dtype</p></td>
<td><p>强制使用 初始类型</p></td>
</tr>
<tr class="row-even"><td><p>preferred_optimal</p></td>
<td><p>allow_mix_precision</p></td>
<td><p>优先使用 fp16+精度权衡</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="模型创建加载与编译">
<h2>模型创建加载与编译<a class="headerlink" href="#模型创建加载与编译" title="永久链接至标题"></a></h2>
<p>使用MindSpore Lite执行推理时，<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#model">Model</a>是推理的主入口，通过Model可以实现模型加载、模型编译和模型执行。采用上一步创建得到的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#context">Context</a>，调用Model的复合<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#build-3">Build</a>接口来实现模型加载与模型编译。</p>
<p>下面示例代码演示了Model创建、加载与编译的过程：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="w"> </span><span class="n">BuildModel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_type</span><span class="p">,</span>
<span class="w">                                             </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">device_id</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">DeviceInfoContext</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;CPU&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;GPU&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateGPUDeviceInfo</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Ascend&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateAscendDeviceInfo</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">device_type</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;DeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>针对大模型，使用model buffer进行加载编译的时候需要单独设置权重文件的路径，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#loadconfig">LoadConfig</a>或<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#updateconfig">UpdateConfig</a>接口设置模型路径，其中<code class="docutils literal notranslate"><span class="pre">section</span></code>为<code class="docutils literal notranslate"><span class="pre">model_file</span></code>，<code class="docutils literal notranslate"><span class="pre">key</span></code>为<code class="docutils literal notranslate"><span class="pre">mindir_path</span></code>；使用model path进行加载编译的时候不需要设置其他参数，会自动读取权重参数。</p>
</div></blockquote>
</section>
<section id="输入数据">
<h2>输入数据<a class="headerlink" href="#输入数据" title="永久链接至标题"></a></h2>
<p>在模型执行前，需要设置输入数据，使用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#getinputs">GetInputs</a>方法，直接获取所有的模型输入Tensor的vector。可以通过MSTensor的<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#datasize">DataSize</a>方法来获取Tensor应该填入的数据大小，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#datatype">DataType</a>方法来获取Tensor的数据类型，通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setdata-1">SetData</a>方法设置输入host数据。</p>
<p>当前有两种指定输入数据的方式：</p>
<ol class="arabic">
<li><p>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setdata-1">SetData</a>设置输入数据，可以避免host之间的拷贝，输入数据将最终直接拷贝到推理设备上。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SetTensorHostData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MemBuffer</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">tensors</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Argument tensors or buffers cannot be nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffers</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensors size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; != &quot;</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; buffers size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">buffers</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">buffers</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Tensor data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; != buffer size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// set tensor data, and the memory should be freed by user</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="n">buffer</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Set the input data of the model, this inference input will be copied directly to the device.</span>
<span class="w">  </span><span class="n">SetTensorHostData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>将输入数据拷贝到<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#mutabledata">MutableData</a>返回的Tensor缓存中。注意，如果已通过<code class="docutils literal notranslate"><span class="pre">SetData</span></code>设置过数据地址，则<code class="docutils literal notranslate"><span class="pre">MutableData</span></code>返回的将是<code class="docutils literal notranslate"><span class="pre">SetData</span></code>的数据地址，此时需要先调用<code class="docutils literal notranslate"><span class="pre">SetData(nullptr)</span></code>。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CopyTensorHostData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MemBuffer</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">buffers</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Tensor data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; != buffer size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">dst_mem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">dst_mem</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Tensor MutableData return nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">memcpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">(),</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Set the input data of the model, copy data to the tensor buffer of Model.GetInputs.</span>
<span class="w">  </span><span class="n">CopyTensorHostData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="执行推理">
<h2>执行推理<a class="headerlink" href="#执行推理" title="永久链接至标题"></a></h2>
<p>调用<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#predict">Model.Predict</a>接口执行推理，并对返回的输出结果进行后续处理。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SpecifyInputDataExample</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">device_id</span><span class="p">,</span>
<span class="w">                            </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BuildModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="n">device_id</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create and build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// InferenceApp is user-defined code. Users need to obtain inputs and process outputs based on</span>
<span class="w">  </span><span class="c1">// the actual situation.</span>
<span class="w">  </span><span class="n">InferenceApp</span><span class="w"> </span><span class="n">app</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Obtain inputs. The input data for inference may come from the preprocessing result.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">app</span><span class="p">.</span><span class="n">GetInferenceInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_buffer</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Set the input data of the model, this inference input will be copied directly to the device.</span>
<span class="w">  </span><span class="n">SetTensorHostData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="p">);</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Process outputs.</span>
<span class="w">  </span><span class="n">app</span><span class="p">.</span><span class="n">OnInferenceResult</span><span class="p">(</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="编译和执行">
<h2>编译和执行<a class="headerlink" href="#编译和执行" title="永久链接至标题"></a></h2>
<p>按照<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.3/quick_start/one_hour_introduction_cloud.html#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">快速入门环境变量</a>一节所述，设置环境变量。接着按如下方式编译程序：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>../
make
</pre></div>
</div>
<p>在编译成功后，可以在<code class="docutils literal notranslate"><span class="pre">build</span></code>目录下得到<code class="docutils literal notranslate"><span class="pre">runtime_cpp</span></code>可执行程序。执行程序<code class="docutils literal notranslate"><span class="pre">runtime_cpp</span></code>运行样例：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./runtime_cpp<span class="w"> </span>--model_path<span class="o">=</span>../model/mobilenetv2.mindir<span class="w"> </span>--device_type<span class="o">=</span>CPU
</pre></div>
</div>
</section>
<section id="高级用法">
<h2>高级用法<a class="headerlink" href="#高级用法" title="永久链接至标题"></a></h2>
<section id="动态shape输入">
<h3>动态shape输入<a class="headerlink" href="#动态shape输入" title="永久链接至标题"></a></h3>
<p>Lite云侧推理框架支持动态shape输入的模型，GPU和Ascend硬件后端，需要在模型转换和模型推理时配置动态输入信息。</p>
<p>动态输入信息的配置与离线和在线场景有关。离线场景，模型转换工具参数<code class="docutils literal notranslate"><span class="pre">--optimize=general</span></code>，<code class="docutils literal notranslate"><span class="pre">--optimize=gpu_oriented</span></code>或<code class="docutils literal notranslate"><span class="pre">--optimize=ascend_oriented</span></code>，即经历和硬件相关的融合和优化，产生的MindIR模型仅能在对应硬件后端上运行，比如，在Atlas 200/300/500推理产品环境上，模型转换工具指定<code class="docutils literal notranslate"><span class="pre">--optimize=ascend_oriented</span></code>，则产生的模型仅支持在Atlas 200/300/500推理产品上运行，如果指定<code class="docutils literal notranslate"><span class="pre">--optimize=general</span></code>，则支持在GPU和CPU上运行。在线场景，加载的MindIR没有经历和硬件相关的融合和优化，支持在Ascend、GPU和CPU上运行，模型转换工具参数<code class="docutils literal notranslate"><span class="pre">--optimize=none</span></code>，或MindSpore导出的MindIR模型没有经过转换工具处理。</p>
<p>Ascend硬件后端离线场景下，需要在模型转换阶段配置动态输入信息。Ascend硬件后端在线场景下，以及GPU硬件后端离线和在线场景下，需要在模型加载阶段通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#loadconfig">LoadConfig</a>接口配置动态输入信息。</p>
<p>通过<code class="docutils literal notranslate"><span class="pre">LoadConfig</span></code>加载的配置文件示例如下所示：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ascend_context]</span>
<span class="na">input_shape</span><span class="o">=</span><span class="s">input_1:[-1,3,224,224]</span>
<span class="na">dynamic_dims</span><span class="o">=</span><span class="s">[1~4],[8],[16]</span>

<span class="k">[gpu_context]</span>
<span class="na">input_shape</span><span class="o">=</span><span class="s">input_1:[-1,3,224,224]</span>
<span class="na">dynamic_dims</span><span class="o">=</span><span class="s">[1~16]</span>
<span class="na">opt_dims</span><span class="o">=</span><span class="s">[1]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code>和<code class="docutils literal notranslate"><span class="pre">[gpu_context]</span></code>分别作用于Ascend和GPU硬件后端。</p>
<ol class="arabic simple">
<li><p>Ascend和GPU硬件后端需要通过动态输入信息进行图的编译和优化，CPU硬件后端不需要配置动态维度信息。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code>用于指示输入shape信息，格式为<code class="docutils literal notranslate"><span class="pre">input_name1:[shape1];input_name2:[shape2]</span></code>，如果有动态输入，则需要将相应的维度设定为-1，多个输入通过英文分号<code class="docutils literal notranslate"><span class="pre">;</span></code>隔开。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code>用于指示动态维度的值范围，多个非连续的值范围通过英文逗号<code class="docutils literal notranslate"><span class="pre">,</span></code>隔开。上例子中，Ascend的batch维度值范围为<code class="docutils literal notranslate"><span class="pre">1,2,3,4,8,16</span></code>，GPU的batch维度值范围为1到16。Ascend硬件后端，动态输入为多档模式，动态输入范围越大，模型编译时间越长。</p></li>
<li><p>对于GPU硬件后端，需要额外配置<code class="docutils literal notranslate"><span class="pre">opt_dims</span></code>用于指示<code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code>范围中最优的值。</p></li>
<li><p>如果<code class="docutils literal notranslate"><span class="pre">input_shape</span></code>配置的为静态shape，则不需要配置<code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code>和<code class="docutils literal notranslate"><span class="pre">opt_dims</span></code>。</p></li>
</ol>
<p>在模型<code class="docutils literal notranslate"><span class="pre">Build</span></code>前，通过<code class="docutils literal notranslate"><span class="pre">LoadConfig</span></code>加载配置文件信息：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">config_file</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<p>在模型推理时，如果模型的输入是动态的，通过<code class="docutils literal notranslate"><span class="pre">GetInputs</span></code>和<code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code>返回的输入输出shape可能包括-1，即为动态shape，则需要通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#resize">Resize</a>接口指定输入shape。如果输入Shape需要发生变化，比如<code class="docutils literal notranslate"><span class="pre">batch</span></code>维度发生变化，则需要重新调用<code class="docutils literal notranslate"><span class="pre">Resize</span></code>接口调整输入shape。</p>
<p>调用<code class="docutils literal notranslate"><span class="pre">Resize</span></code>接口后，已调用和后续调用的<code class="docutils literal notranslate"><span class="pre">GetInputs</span></code>和<code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code>中的Tensor的shape将发生变化。</p>
<p>下面示例代码演示如何对MindSpore Lite的输入Tensor进行<code class="docutils literal notranslate"><span class="pre">Resize</span></code>：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">ResizeModel</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span>
<span class="w">    </span><span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">shape</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to resize to batch size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="指定输入输出host内存">
<h3>指定输入输出host内存<a class="headerlink" href="#指定输入输出host内存" title="永久链接至标题"></a></h3>
<p>指定设备内存支持CPU、Asend和GPU硬件后端。指定的输入host内存，缓存中的数据将直接拷贝到设备（device）内存上，指定的输出host内存，设备（device）内存的数据将直接拷贝到这块缓存中。避免了额外的host之间的数据拷贝，提升推理性能。</p>
<p>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setdata-1">SetData</a>可单独或者同时指定输入和输出host内存。建议参数<code class="docutils literal notranslate"><span class="pre">own_data</span></code>为false，当<code class="docutils literal notranslate"><span class="pre">own_data</span></code>为false，用户需要维护host内存的生命周期，负责host内存的申请和释放。当参数<code class="docutils literal notranslate"><span class="pre">own_data</span></code>为true时，在MSTensor析构时释放指定的内存。</p>
<ol class="arabic">
<li><p>指定输入host内存</p>
<p>输入host内存的值，一般来源于host侧的C++、Python等预处理的结果。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">host_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// ... get host buffer from preprocessing etc.</span>
<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">host_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">host_buffers</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="n">host_data</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>指定输出host内存</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Get Output from model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">output_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="n">ResourceGuard</span><span class="w"> </span><span class="nf">output_device_rel</span><span class="p">([</span><span class="o">&amp;</span><span class="n">output_buffers</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">item</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">output_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">free</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">    </span><span class="n">output_buffers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span><span class="w"> </span><span class="c1">// for free</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="指定输入输出设备device内存">
<h3>指定输入输出设备（device）内存<a class="headerlink" href="#指定输入输出设备device内存" title="永久链接至标题"></a></h3>
<p>指定设备内存支持Asend和GPU硬件后端。指定输入输出设备内存可以避免device到host内存之间的相互拷贝，比如经过芯片dvpp预处理产生的device内存输入直接作为模型推理的输入，避免预处理结果从device内存拷贝到host内存，host结果作为模型推理输入，推理前重新拷贝到device上。</p>
<p>指定输入输出设备内存样例可参考<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3/mindspore/lite/examples/cloud_infer/device_example_cpp">设备内存样例</a>。</p>
<p>通过<a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/api_cpp/mindspore.html#setdevicedata">SetDeviceData</a>可单独或者同时指定输入和输出设备内存。用户需要维护设备内存的生命周期，负责设备内存的申请和释放。</p>
<ol class="arabic">
<li><p>指定输入设备内存</p>
<p>样例中，输入设备内存的值拷贝自host，一般设备内存的值来自于芯片预处理的结果或另一个模型的输出。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SetDeviceData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">host_data_buffer</span><span class="p">,</span>
<span class="w">                  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">host_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">host_data_buffer</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data_size</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data size cannot be 0, tensor shape: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ShapeToString</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Shape</span><span class="p">())</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">device_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MallocDeviceMemory</span><span class="p">(</span><span class="n">data_size</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to alloc device data, data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">device_buffers</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">CopyMemoryHost2Device</span><span class="p">(</span><span class="n">device_data</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">,</span><span class="w"> </span><span class="n">host_data</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to copy data to device, data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">device_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="n">ResourceGuard</span><span class="w"> </span><span class="nf">device_rel</span><span class="p">([</span><span class="o">&amp;</span><span class="n">device_buffers</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">item</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">FreeDeviceMemory</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="w">  </span><span class="n">SetDeviceData</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">host_buffers</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_buffers</span><span class="p">);</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">Predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>指定输出设备内存</p>
<p>样例中，输出设备内存拷贝到host打印输出，一般输出设备内存可作为其他模型的输入。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SetOutputDeviceData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data_size</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data size cannot be 0, tensor shape: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ShapeToString</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Shape</span><span class="p">())</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">device_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MallocDeviceMemory</span><span class="p">(</span><span class="n">data_size</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to alloc device data, data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">device_buffers</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Output from model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">output_device_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="n">ResourceGuard</span><span class="w"> </span><span class="nf">output_device_rel</span><span class="p">([</span><span class="o">&amp;</span><span class="n">output_device_buffers</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">item</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">output_device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">FreeDeviceMemory</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">SetOutputDeviceData</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output_device_buffers</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to set output device data&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">Predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="ascend后端ge推理">
<h3>Ascend后端GE推理<a class="headerlink" href="#ascend后端ge推理" title="永久链接至标题"></a></h3>
<p>Ascend推理当前有两种对接方式。</p>
<p>一种为默认的ACL推理，ACL接口仅有全局和模型（图）级别的选项配置，多个图无法指示关联关系，多个图之间相对独立，不可共享权重（包括常量和变量）。如果模型存在可以变更的权重，即变量，变量需要先执行初始化，需要额外构建和执行初始化图，与计算图共享变量，由于多个图相对独立，使用默认的ACL推理时，模型不能存在变量。</p>
<p>ACL接口支持提前构建模型，加载时使用已构建的模型。</p>
<p>另一种为GE推理，GE接口存在全局、Session和模型（图）级别的选项配置，多个图可以在同一个Session中，在同一个Session中的图可以使能共享权重。在同一个Session中，可针对变量创建初始化图，与计算图共享变量。使用默认的GE推理时，模型可以存在变量。</p>
<p>当前GE接口不支持提前构建模型，加载时需要构建模型。</p>
<p>可以通过指定 <code class="docutils literal notranslate"><span class="pre">provider</span></code> 为 <code class="docutils literal notranslate"><span class="pre">ge</span></code> 使能GE。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">rank_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="s2">&quot;seq_1024.mindir&quot;</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="s2">&quot;config.ini&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set Atlas training series device id, rank id and provider.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetRankID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
<span class="c1">// Device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="s">&quot;config.ini&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;config.ini&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="s">&quot;seq_1024.mindir&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>在配置文件中，来自 <code class="docutils literal notranslate"><span class="pre">[ge_global_options]</span></code> 、 <code class="docutils literal notranslate"><span class="pre">[ge_sesion_options]</span></code> 和 <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> 中的选项将作为GE接口的全局、Session和模型（图）级别的选项，详情可参考<a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/infacldevg/graphdevg/atlasgeapi_07_0112.html">GE选项</a>。比如：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ge_global_options]</span>
<span class="na">ge.opSelectImplmode</span><span class="o">=</span><span class="s">high_precision</span>

<span class="k">[ge_session_options]</span>
<span class="na">ge.externalWeight</span><span class="o">=</span><span class="s">1</span>

<span class="k">[ge_graph_options]</span>
<span class="na">ge.exec.precision_mode</span><span class="o">=</span><span class="s">allow_fp32_to_fp16</span>
<span class="na">ge.inputShape</span><span class="o">=</span><span class="s">x1:-1,3,224,224</span><span class="c1">;x2:-1,3,1024,1024</span>
<span class="na">ge.dynamicDims</span><span class="o">=</span><span class="s">1,1</span><span class="c1">;2,2;3,3;4,4</span>
<span class="na">ge.dynamicNodeType</span><span class="o">=</span><span class="s">1</span>
</pre></div>
</div>
</section>
<section id="多线程加载模型">
<h3>多线程加载模型<a class="headerlink" href="#多线程加载模型" title="永久链接至标题"></a></h3>
<p>硬件后端为Ascend，provider为默认时，支持多线程并发加载多个Ascend优化后模型，以提升模型加载性能。使用<a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.3/use/converter_tool.html">模型转换工具</a>，指定 <code class="docutils literal notranslate"><span class="pre">--optimize=ascend_oriented</span></code> 可将MindSpore导出的 <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> 模型、TensorFlow和ONNX等第三方框架模型转换为Ascend优化后模型。MindSpore导出的 <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> 模型未进行Ascend优化，对于第三方框架模型，转换工具中如果指定 <code class="docutils literal notranslate"><span class="pre">--optimize=none</span></code> 产生的 <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> 模型也未进行Ascend优化。</p>
</section>
<section id="多模型共享权重">
<h3>多模型共享权重<a class="headerlink" href="#多模型共享权重" title="永久链接至标题"></a></h3>
<p>Ascend推理时，运行时指定 <code class="docutils literal notranslate"><span class="pre">provider</span></code> 为 <code class="docutils literal notranslate"><span class="pre">ge</span></code> 时，支持部署到同一张卡的多个模型共享权重，支持模型中存在可以被更新的权重。</p>
<p>针对相同的模型脚本，不同的条件分支或者不同的输入shape，使用相同的权重，可以导出不同的模型。多个模型共享权重时，在推理过程中，部分权重可以不再更新，我们将解析为常量，多个模型将拥有相同的常量权重。部分权重也可以发生变化，我们解析为变量，其中一个模型修改权重，本模型下次推理或其他模型推理可以使用和更新修改后的权重。</p>
<p>可以通过 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.3/mindspore_lite/mindspore_lite.ModelGroup.html#mindspore_lite.ModelGroup">ModelGroup</a> 接口关联多个模型的共享权重的关系。</p>
<p>Python实现：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_path0</span><span class="p">,</span> <span class="n">model_path1</span><span class="p">,</span> <span class="n">config_file_0</span><span class="p">,</span> <span class="n">config_file_1</span><span class="p">,</span> <span class="n">rank_id</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">device_id</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">rank_id</span> <span class="o">=</span> <span class="n">rank_id</span>  <span class="c1"># for distributed model</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
    <span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
    <span class="n">model0</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
    <span class="n">model1</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>

    <span class="n">model_group</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelGroup</span><span class="p">(</span><span class="n">mslite</span><span class="o">.</span><span class="n">ModelGroupFlag</span><span class="o">.</span><span class="n">SHARE_WEIGHT</span><span class="p">)</span>
    <span class="n">model_group</span><span class="o">.</span><span class="n">add_model</span><span class="p">([</span><span class="n">model0</span><span class="p">,</span> <span class="n">model1</span><span class="p">])</span>

    <span class="n">model0</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path0</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">config_file_0</span><span class="p">)</span>
    <span class="n">model1</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path1</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">config_file_1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model0</span><span class="p">,</span> <span class="n">model1</span>
</pre></div>
</div>
<p>C++实现：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Model</span><span class="o">&gt;</span><span class="w"> </span><span class="n">LoadModel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path0</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path1</span><span class="p">,</span>
<span class="w">                             </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">config_file_0</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">config_file_1</span><span class="p">,</span>
<span class="w">                             </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">rank_id</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">device_id</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="w">    </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetRankID</span><span class="p">(</span><span class="n">rank_id</span><span class="p">);</span>
<span class="w">    </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model0</span><span class="p">;</span>
<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model1</span><span class="p">;</span>
<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelGroup</span><span class="w"> </span><span class="nf">model_group</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelGroupFlag</span><span class="o">::</span><span class="n">kShareWeight</span><span class="p">);</span>
<span class="w">    </span><span class="n">model_group</span><span class="p">.</span><span class="n">AddModel</span><span class="p">({</span><span class="n">model0</span><span class="p">,</span><span class="w"> </span><span class="n">model1</span><span class="p">});</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model0</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_0</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_file_0</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model0</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path0</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load model &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">model_path0</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model1</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_1</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_file_1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model1</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path1</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load model &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">model_path1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">model0</span><span class="p">,</span><span class="w"> </span><span class="n">model1</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
<p>上述配置的默认情况下多个模型仅共享了变量，共享常量时，需要在配置文件中配置权重外置选项。配置文件即上述例子的 <code class="docutils literal notranslate"><span class="pre">config_file_0</span></code> 和 <code class="docutils literal notranslate"><span class="pre">config_file_1</span></code> 。</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ge_session_options]</span>
<span class="na">ge.externalWeight</span><span class="o">=</span><span class="s">1</span>
</pre></div>
</div>
</section>
</section>
<section id="实验特性">
<h2>实验特性<a class="headerlink" href="#实验特性" title="永久链接至标题"></a></h2>
<section id="多后端异构能力">
<h3>多后端异构能力<a class="headerlink" href="#多后端异构能力" title="永久链接至标题"></a></h3>
<p>MindSpore Lite云侧推理正在支持多后端异构场景，可以通过在运行期间指定环境变量‘export ENABLE_MULTI_BACKEND_RUNTIME=on’来使能该特性，其他接口的使用方式与原流程一致。当前该特性为实验特性，不保证特性的正确性，稳定性和后续的兼容性。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime.html" class="btn btn-neutral float-left" title="基础推理" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="runtime_java.html" class="btn btn-neutral float-right" title="使用Java接口执行云侧推理" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>