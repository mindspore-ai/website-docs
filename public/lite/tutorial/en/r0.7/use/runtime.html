<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Runtime User Guide &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Benchmark Tool" href="benchmark_tool.html" />
    <link rel="prev" title="Converter Tool" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Use</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converter Tool</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Runtime User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reading-models">Reading Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#session-creation">Session Creation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#creating-contexts">Creating Contexts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-sessions">Creating Sessions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#graph-compilation">Graph Compilation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#variable-dimension">Variable Dimension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-graphs">Compiling Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-input">Data Input</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-input-tensors">Obtaining Input Tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#copying-data">Copying Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#graph-execution">Graph Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#executing-sessions">Executing Sessions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#core-binding">Core Binding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#callback-running">Callback Running</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#obtaining-outputs">Obtaining Outputs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-output-tensors">Obtaining Output Tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#obtaining-version-string">Obtaining Version String</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id6">Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_tool.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeprofiler_tool.html">TimeProfiler Tool</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Runtime User Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="runtime-user-guide">
<h1>Runtime User Guide<a class="headerlink" href="#runtime-user-guide" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r0.7/lite/tutorials/source_en/use/runtime.md"><img alt="View Source On Gitee" src="../_images/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>After model conversion using MindSpore Lite, the model inference process needs to be completed in Runtime.</p>
<p>The procedure for using Runtime is shown in the following figure:</p>
<p><img alt="img" src="../_images/side_infer_process.png" /></p>
<p>Its components and their functions are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Model</span></code>: model used by MindSpore Lite, which instantiates the list of operator prototypes through image composition or direct network loading.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Lite</span> <span class="pre">Session</span></code>: provides the graph compilation function and calls the graph executor for inference.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>: operator heterogeneous scheduler. It can select a proper kernel for each operator based on the heterogeneous scheduling policy, construct a kernel list, and split a graph into subgraphs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Executor</span></code>: graph executor, which executes the kernel list to dynamically allocate and release tensors.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Operator</span></code>: operator prototype, including operator attributes and methods for inferring the shape, data type, and format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Kernel</span></code>: operator, which provides specific operator implementation and the operator forwarding function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tensor</span></code>: tensor used by MindSpore Lite, which provides functions and APIs for tensor memory operations.</p></li>
</ul>
</section>
<section id="reading-models">
<h2>Reading Models<a class="headerlink" href="#reading-models" title="Permalink to this headline"></a></h2>
<p>In MindSpore Lite, a model file is an <code class="docutils literal notranslate"><span class="pre">.ms</span></code> file converted using the model conversion tool. During model inference, the model needs to be loaded from the file system and parsed. Related operations are mainly implemented in the Model component. The Model component holds model data such as weight data and operator attributes.</p>
<p>A model is created based on memory data using the static <code class="docutils literal notranslate"><span class="pre">Import</span></code> method of the Model class. The <code class="docutils literal notranslate"><span class="pre">Model</span></code> instance returned by the function is a pointer, which is created by using <code class="docutils literal notranslate"><span class="pre">new</span></code>. If the pointer is not required, you need to release it by using <code class="docutils literal notranslate"><span class="pre">delete</span></code>.</p>
</section>
<section id="session-creation">
<h2>Session Creation<a class="headerlink" href="#session-creation" title="Permalink to this headline"></a></h2>
<p>When MindSpore Lite is used for inference, sessions are the main entrance of inference. You can compile and execute graphs through sessions.</p>
<section id="creating-contexts">
<h3>Creating Contexts<a class="headerlink" href="#creating-contexts" title="Permalink to this headline"></a></h3>
<p>Contexts save some basic configuration parameters required by sessions to guide graph compilation and execution. The definition of context is as follows:</p>
<p>MindSpore Lite supports heterogeneous inference. The preferred backend for inference is specified by <code class="docutils literal notranslate"><span class="pre">device_ctx_</span></code> in <code class="docutils literal notranslate"><span class="pre">Context</span></code> and is CPU by default. During graph compilation, operator selection and scheduling are performed based on the preferred backend.</p>
<p>MindSpore Lite has a built-in thread pool shared by processes. During inference, <code class="docutils literal notranslate"><span class="pre">thread_num_</span></code> is used to specify the maximum number of threads in the thread pool. The default maximum number is 2. It is recommended that the maximum number be no more than 4. Otherwise, the performance may be affected.</p>
<p>MindSpore Lite supports dynamic memory allocation and release. If <code class="docutils literal notranslate"><span class="pre">allocator</span></code> is not specified, a default <code class="docutils literal notranslate"><span class="pre">allocator</span></code> is generated during inference. You can also use the <code class="docutils literal notranslate"><span class="pre">Context</span></code> method to allow multiple <code class="docutils literal notranslate"><span class="pre">Context</span></code> to share the memory allocator.</p>
</section>
<section id="creating-sessions">
<h3>Creating Sessions<a class="headerlink" href="#creating-sessions" title="Permalink to this headline"></a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">Context</span></code> created in the previous step to call the static <code class="docutils literal notranslate"><span class="pre">CreateSession</span></code> method of LiteSession to create <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code>. The <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> instance returned by the function is a pointer, which is created by using <code class="docutils literal notranslate"><span class="pre">new</span></code>. If the pointer is not required, you need to release it by using <code class="docutils literal notranslate"><span class="pre">delete</span></code>.</p>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline"></a></h3>
<p>The following sample code demonstrates how to create a <code class="docutils literal notranslate"><span class="pre">Context</span></code> and how to allow two <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> to share a memory pool.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="p">;</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed while running %s&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">modelName</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// The preferred backend is GPU, which means, if there is a GPU operator, it will run on the GPU first, otherwise it will run on the CPU.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">device_ctx_</span><span class="p">.</span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">DT_GPU</span><span class="p">;</span>
<span class="c1">// The medium core takes priority in thread and core binding methods. This parameter will work in the BindThread interface. For specific binding effect, see the &quot;Run Graph&quot; section.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">cpu_bind_mode_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MID_CPU</span><span class="p">;</span>
<span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread. </span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">thread_num_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="c1">// Allocators can be shared across multiple Contexts.</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">context2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Context</span><span class="p">(</span><span class="n">context</span><span class="o">-&gt;</span><span class="n">thread_num_</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">allocator</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">device_ctx_</span><span class="p">);</span>
<span class="n">context2</span><span class="o">-&gt;</span><span class="n">cpu_bind_mode_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">cpu_bind_mode_</span><span class="p">;</span>
<span class="c1">// Use Context to create Session.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">session1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
<span class="c1">// After the LiteSession is created, the Context can be released.</span>
<span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">session1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CreateSession failed while running %s&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">modelName</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// session1 and session2 can share one memory pool.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">session2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context2</span><span class="p">);</span>
<span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">context2</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CreateSession failed while running %s&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">modelName</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="graph-compilation">
<h2>Graph Compilation<a class="headerlink" href="#graph-compilation" title="Permalink to this headline"></a></h2>
<section id="variable-dimension">
<h3>Variable Dimension<a class="headerlink" href="#variable-dimension" title="Permalink to this headline"></a></h3>
<p>When using MindSpore Lite for inference, after the session creation and graph compilation have been completed, if you need to resize the input shape, you can reset the shape of the input tensor, and then call the session’s Resize() interface.</p>
</section>
<section id="id1">
<h3>Example<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>The following code demonstrates how to resize the input of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">};</span>
<span class="c1">// Assume the model has only one input,resize input shape to [1, 128, 128, 3]</span>
<span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">set_shape</span><span class="p">(</span><span class="n">resize_shape</span><span class="p">);</span>
<span class="n">session</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="compiling-graphs">
<h3>Compiling Graphs<a class="headerlink" href="#compiling-graphs" title="Permalink to this headline"></a></h3>
<p>Before graph execution, call the <code class="docutils literal notranslate"><span class="pre">CompileGraph</span></code> API of the <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> to compile graphs and further parse the Model instance loaded from the file, mainly for subgraph split and operator selection and scheduling. This process takes a long time. Therefore, it is recommended that <code class="docutils literal notranslate"><span class="pre">ListSession</span></code> achieve multiple executions with one creation and one compilation.</p>
</section>
<section id="id2">
<h3>Example<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>The following code demonstrates how to compile graph of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session and a Model instance named model before.</span>
<span class="c1">// The methods of creating model and session can refer to &quot;Import Model&quot; and &quot;Create Session&quot; two sections.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CompileGraph failed&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="c1">// session and model need to be released by users manually.</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="p">);</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="data-input">
<h2>Data Input<a class="headerlink" href="#data-input" title="Permalink to this headline"></a></h2>
<section id="obtaining-input-tensors">
<h3>Obtaining Input Tensors<a class="headerlink" href="#obtaining-input-tensors" title="Permalink to this headline"></a></h3>
<p>Before graph execution, you need to copy the input data to model input tensors.</p>
<p>MindSpore Lite provides the following methods to obtain model input tensors.</p>
<ol class="arabic simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetInputsByName</span></code> method to obtain vectors of the model input tensors that are connected to the model input node based on the node name.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> method to directly obtain the vectors of all model input tensors.</p></li>
</ol>
</section>
<section id="copying-data">
<h3>Copying Data<a class="headerlink" href="#copying-data" title="Permalink to this headline"></a></h3>
<p>After model input tensors are obtained, you need to enter data into the tensors. Use the <code class="docutils literal notranslate"><span class="pre">Size</span></code> method of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> to obtain the size of the data to be entered into tensors, use the <code class="docutils literal notranslate"><span class="pre">data_type</span></code> method to obtain the data type of tensors, and use the <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> method of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> to obtain the writable pointer.</p>
</section>
<section id="id3">
<h3>Example<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>The following sample code shows how to obtain the entire graph input <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> and enter the model input data to <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one input tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// It is omitted that users have read the model input file and generated a section of memory buffer: input_buf, as well as the byte size of input_buf: data_size.</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">data_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input data size is not suit for model input&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of in_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
<span class="c1">// The elements in the inputs are managed by MindSpore Lite so that users do not need to free inputs.</span>
</pre></div>
</div>
<p>Note:</p>
<ul class="simple">
<li><p>The data layout in the model input tensors of MindSpore Lite must be NHWC.</p></li>
<li><p>The model input <code class="docutils literal notranslate"><span class="pre">input_buf</span></code> is read from disks. After it is copied to model input tensors, you need to release <code class="docutils literal notranslate"><span class="pre">input_buf</span></code>.</p></li>
<li><p>Vectors returned by using the <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> and <code class="docutils literal notranslate"><span class="pre">GetInputsByName</span></code> methods do not need to be released by users.</p></li>
</ul>
</section>
</section>
<section id="graph-execution">
<h2>Graph Execution<a class="headerlink" href="#graph-execution" title="Permalink to this headline"></a></h2>
<section id="executing-sessions">
<h3>Executing Sessions<a class="headerlink" href="#executing-sessions" title="Permalink to this headline"></a></h3>
<p>After a MindSpore Lite session performs graph compilation, you can use <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code> of <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> for model inference.</p>
</section>
<section id="core-binding">
<h3>Core Binding<a class="headerlink" href="#core-binding" title="Permalink to this headline"></a></h3>
<p>The built-in thread pool of MindSpore Lite supports core binding and unbinding. By calling the <code class="docutils literal notranslate"><span class="pre">BindThread</span></code> API, you can bind working threads in the thread pool to specified CPU cores for performance analysis. The core binding operation is related to the context specified when <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> is created. The core binding operation sets the affinity between a thread and CPU based on the core binding policy in the context.</p>
<p>Note that core binding is an affinity operation, which is affected by system scheduling. Therefore, successful binding to the specified CPU core cannot be ensured. After executing the code of core binding, you need to perform the unbinding operation. The following is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="n">session</span><span class="o">-&gt;</span><span class="n">BindThread</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;RunGraph failed&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">session</span><span class="o">-&gt;</span><span class="n">BindThread</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>Core binding parameters can be used to bind big cores first or middle cores first.<br />
The rule for determining big core or middle core is based on the CPU core frequency instead of CPU architecture. For the CPU architecture where big, middle, and little cores are not distinguished, this rule can be used.<br />
Big core first indicates that threads in the thread pool are bound to cores according to core frequency. The first thread is bound to the core with the highest frequency, and the second thread is bound to the core with the second highest frequency. This rule also applies to other threads.<br />
Middle cores are defined based on experience. By default, middle cores are cores with the third and fourth highest frequency. Middle core first indicates that threads are bound to middle cores preferentially. When there are no available middle cores, threads are bound to little cores.</p>
</div></blockquote>
</section>
<section id="callback-running">
<h3>Callback Running<a class="headerlink" href="#callback-running" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite can transfer two <code class="docutils literal notranslate"><span class="pre">KernelCallBack</span></code> function pointers to call back the inference model when calling <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code>. Compared with common graph execution, callback running can obtain extra information during the running process to help developers analyze performance and fix bugs. The extra information includes:</p>
<ul class="simple">
<li><p>Name of the running node</p></li>
<li><p>Input and output tensors before inference of the current node</p></li>
<li><p>Input and output tensors after inference of the current node</p></li>
</ul>
</section>
<section id="id4">
<h3>Example<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>The following sample code demonstrates how to use <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> to compile a graph, defines two callback functions as the before-callback pointer and after-callback pointer, transfers them to the <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code> API for callback inference, and demonstrates the scenario of multiple graph executions with one graph compilation.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session and a Model instance named model before.</span>
<span class="c1">// The methods of creating model and session can refer to &quot;Import Model&quot; and &quot;Create Session&quot; two sections.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CompileGraph failed&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="c1">// session and model need to be released by users manually.</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="p">);</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Copy input data into the input tensor. Users can refer to the &quot;Input Data&quot; section. We uses random data here.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// When calling the MutableData method, if the data in MSTensor is not allocated, it will be malloced. After allocation, the data in MSTensor can be considered as random data.</span>
<span class="w">    </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="w"> </span><span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="p">}</span>
<span class="c1">// Definition of callback function before forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">before_call_back_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_inputs</span><span class="p">,</span>
<span class="w">                             </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_outputs</span><span class="p">,</span>
<span class="w">                             </span><span class="k">const</span><span class="w"> </span><span class="n">session</span><span class="o">::</span><span class="n">CallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Before forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">name_callback_param</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Definition of callback function after forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">after_call_back_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_inputs</span><span class="p">,</span>
<span class="w">                            </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_outputs</span><span class="p">,</span>
<span class="w">                            </span><span class="k">const</span><span class="w"> </span><span class="n">session</span><span class="o">::</span><span class="n">CallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;After forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">name_callback_param</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Call the callback function when performing the model inference process.</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session_</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">(</span><span class="n">before_call_back_</span><span class="p">,</span><span class="w"> </span><span class="n">after_call_back_</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Run graph failed.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CompileGraph would cost much time, a better solution is calling CompileGraph only once and RunGraph much more times.</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session_</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Run graph failed.&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
<span class="c1">// session and model needs to be released by users manually.</span>
<span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="p">);</span>
<span class="k">delete</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="obtaining-outputs">
<h2>Obtaining Outputs<a class="headerlink" href="#obtaining-outputs" title="Permalink to this headline"></a></h2>
<section id="obtaining-output-tensors">
<h3>Obtaining Output Tensors<a class="headerlink" href="#obtaining-output-tensors" title="Permalink to this headline"></a></h3>
<p>After performing inference, MindSpore Lite can obtain the model inference result.</p>
<p>MindSpore Lite provides the following methods to obtain the model output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<ol class="arabic simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputsByName</span></code> method to obtain vectors of the model output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> that is connected to the model output node based on the node name.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputMapByNode</span></code> method to directly obtain the mapping between the names of all model output nodes and the model output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> connected to the nodes.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> method to obtain the model output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> based on the tensor name.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputMapByTensor</span></code> method to directly obtain the mapping between the names of all model output tensors and the model output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p></li>
</ol>
<p>After model output tensors are obtained, you need to enter data into the tensors. Use the <code class="docutils literal notranslate"><span class="pre">Size</span></code> method of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> to obtain the size of the data to be entered into tensors, use the <code class="docutils literal notranslate"><span class="pre">data_type</span></code> method to obtain the data type of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>, and use the <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> method of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> to obtain the writable pointer.</p>
</section>
<section id="id5">
<h3>Example<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputMapByNode</span></code> method and print the first ten data or all data records of each output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session before.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputMapByNode</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one output node.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_node_iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_map</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out_node_iter</span><span class="o">-&gt;</span><span class="n">first</span><span class="p">;</span>
<span class="c1">// Assume that the unique output node has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out_node_iter</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Assume that the data format of output data is float 32.</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">data_type</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">TypeId</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output of lenet should in float32&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">());</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of out_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Print the first 10 float data or all output data of the output tensor. </span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output data: &quot;</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="c1">// The elements in outputs do not need to be free by users, because outputs are managed by the MindSpore Lite.</span>
</pre></div>
</div>
<p>Note that the vectors or map returned by the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code>, <code class="docutils literal notranslate"><span class="pre">GetOutputMapByNode</span></code>, <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> and <code class="docutils literal notranslate"><span class="pre">GetOutputMapByTensor</span></code> methods do not need to be released by users.</p>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session before.</span>
<span class="c1">// Assume that model has a output node named output_node_name_0.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_vec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;output_node_name_0&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named output_node_name_0 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputMapByTensor</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session before.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputMapByTensor</span><span class="p">();</span>
<span class="c1">// Assume that output node named output_node_name_0 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// We can use GetOutputTensorNames method to get all name of output tensor of model which is in order.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Assume we have created a LiteSession instance named session before.</span>
<span class="c1">// Use output tensor name returned by GetOutputTensorNames as key</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor_name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tensor_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="obtaining-version-string">
<h2>Obtaining Version String<a class="headerlink" href="#obtaining-version-string" title="Permalink to this headline"></a></h2>
<section id="id6">
<h3>Example<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<p>The following sample code shows how to obtain version string using <code class="docutils literal notranslate"><span class="pre">Version</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/version.h&quot;</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Version</span><span class="p">();</span><span class="w"> </span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="Converter Tool" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="benchmark_tool.html" class="btn btn-neutral float-right" title="Benchmark Tool" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>