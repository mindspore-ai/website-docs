<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overall Security and Trustworthiness Design &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Differential Privacy Design" href="differential_privacy_design.html" />
    <link rel="prev" title="mindarmour.utils" href="mindarmour.utils.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindarmour_install.html">MindSpore Armour Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Security</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="improve_model_security_nad.html">Improving Model Security with NAD Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_fuzzing.html">Testing Model Security Using Fuzz Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation_of_CNNCTC.html">Evaluating the Robustness of the OCR Model CNN-CTC</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_obfuscation_protection.html">Dynamic Model Obfuscation</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_encrypt_protection.html">Model Encryption Protection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_differential_privacy.html">Protecting User Privacy with Differential Privacy Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_suppress_privacy.html">Protecting User Privacy with Suppress Privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_membership_inference.html">Using Membership Inference to Test Model Security</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Reliability</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="concept_drift_time_series.html">Implementing the Concept Drift Detection Application of Time Series Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="concept_drift_images.html">Implementing the Concept Drift Detection Application of Image Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_injection.html">Implementing the Model Fault Injection and Evaluation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.natural_robustness.transform.image.html">mindarmour.natural_robustness.transform.image</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.privacy.sup_privacy.html">mindarmour.privacy.sup_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.reliability.html">mindarmour.reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overall Security and Trustworthiness Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overall-architecture">Overall Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#design-guidelines">Design Guidelines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-robustness">Model Robustness</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ai-fuzzer">AI Fuzzer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-asset-security">Model Asset Security</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-encryption">Model Encryption</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-obfuscation">Model Obfuscation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#privacy">Privacy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-masking">Data Masking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#differential-privacy-training">Differential Privacy Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#privacy-leakage-assessment">Privacy Leakage Assessment</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="differential_privacy_design.html">Differential Privacy Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="fuzzer_design.html">AI Model Security Testing Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="security_and_privacy.html">MindSpore Armour Module Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Overall Security and Trustworthiness Design</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/design.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="overall-security-and-trustworthiness-design">
<h1>Overall Security and Trustworthiness Design<a class="headerlink" href="#overall-security-and-trustworthiness-design" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/mindarmour/docs/source_en/design.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<section id="overall-architecture">
<h2>Overall Architecture<a class="headerlink" href="#overall-architecture" title="Permalink to this headline"></a></h2>
<p>As a general technology, AI brings great opportunities and benefits, and faces new security and privacy protection challenges. MindSpore Armour focuses on common security and privacy issues in AI application scenarios: AI models are vulnerable to adversarial example spoofing, sensitive privacy data may be collected in the data collection process, privacy data may be leaked during model use, model assets have a risk of being stolen, and models may become invalid due to data drift. MindSpore Armour provides corresponding security and privacy protection capabilities.</p>
<p>The following figure shows the overall MindSpore Armour architecture. The following describes three parts: model robustness, model asset security, and privacy protection.</p>
<ul class="simple">
<li><p>Model robustness: MindSpore Armour model robustness focuses on the robustness of AI models for natural and adversarial perturbation examples. It covers natural perturbation example generation, black-box and white-box adversarial perturbation example generation, adversarial example detection, adversarial training, and AI Fuzzer, helping security personnel quickly and efficiently evaluate the robustness of AI models and improve their anti-attack capabilities.</p></li>
<li><p>Model asset security: The structure and weight of AI models are key assets. To prevent models from being stolen during transmission, deployment, and running, MindSpore Armour provides encryption-based model file protection and lightweight model protection based on structure parameter obfuscation. In federated learning scenarios, secure multi-party computing capabilities with lossless precision are provided to prevent model theft.</p></li>
<li><p>Privacy leakage measurement and protection: In the data collection phase, the data masking capability is provided to prevent user privacy data from being collected. In the model training phase, differential privacy and privacy suppression mechanisms are provided to reduce model privacy leakage risks. In the model use phase, privacy leakage assessment technologies based on membership inference and inversion attacks are provided to evaluate the risks of model privacy leakage.</p></li>
</ul>
<p><img alt="architecture" src="_images/ma_arch.png" /></p>
</section>
<section id="design-guidelines">
<h2>Design Guidelines<a class="headerlink" href="#design-guidelines" title="Permalink to this headline"></a></h2>
<section id="model-robustness">
<h3>Model Robustness<a class="headerlink" href="#model-robustness" title="Permalink to this headline"></a></h3>
<p>Focus on the robustness of AI models for natural perturbation examples and adversarial examples.</p>
<p><strong>Natural Perturbation Examples</strong></p>
<p>Simulate common perturbations in real life, such as focus blur, motion blur, overexposure, rotation, translation, scaling, shearing, perspective transformation, uniform noise, and natural noise etc.</p>
<p><strong>Adversarial Examples</strong></p>
<p>An attacker adds small perturbations that are not easily perceived by human to the original example, causing deep learning model misjudgment. This is called an adversarial example attack.</p>
<p>MindSpore Armour model security provides functions such as natural perturbation example generation, adversarial example generation, adversarial example detection, model defense, and attack defense effect evaluation, providing important support for AI model security research and AI application security.</p>
<p><img alt="robustness" src="_images/robustness.png" /></p>
<ul class="simple">
<li><p>Perturbation example generation module: provides over 10 white-box adversarial example generation methods, over 5 black-box adversarial example generation methods, and over 15 natural perturbation example generation methods. Security engineers can quickly and efficiently generate adversarial examples based on different requirements to attack AI models.</p></li>
<li><p>Detection module: provides more than 5 adversarial example detection methods to determine whether the input examples are adversarial and identify attack data and attacks before model inference.</p></li>
<li><p>Defense module: provides defense methods for adversarial training to improve the attack defense capability of the model.</p></li>
<li><p>Evaluation module: provides multi-dimensional evaluation indicators to comprehensively evaluate the robustness of the model.</p></li>
</ul>
<section id="ai-fuzzer">
<h4>AI Fuzzer<a class="headerlink" href="#ai-fuzzer" title="Permalink to this headline"></a></h4>
<p>Different from traditional fuzzing, AI Fuzzer uses the neuron coverage rate as the test evaluation criterion based on the characteristics of neural networks. Neuron coverage refers to the range of the number of activated neurons and the output value of neurons observed through a group of inputs. The neuron coverage gain is used to guide input variation so that the input can activate more neurons and the neuron values can be distributed in a wider range. In this way, the output results and incorrect behaviors of different types of models can be explored to evaluate the robustness of the models.</p>
<p><img alt="fuzz_architecture" src="_images/fuzz_arch.png" /></p>
<p>AI Fuzzer consists of three modules:</p>
<ol class="arabic">
<li><p>Natural Threat/Adversarial Example Generator (data mutation module)</p>
<p>Randomly selects a mutation method to mutate seed data to generate multiple variants. Mutation policies for multiple types of examples are supported, including:</p>
<ol class="arabic simple">
<li><p>Method for generating natural perturbation examples:</p></li>
</ol>
<ul class="simple">
<li><p>Affine transformation: translate, scale, shear, rotate, perspective, and curve.</p></li>
<li><p>Blur: GaussianBlur, MotionBlur, and GradientBlur.</p></li>
<li><p>Luminance adjustment: Contrast and GradientLuminance.</p></li>
<li><p>Noise addition: UniformNoise, GaussianNoise, SaltAndPepperNoise, and NaturalNoise.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Methods for generating white-box and black-box adversarial examples based on adversarial attacks: fast gradient sign method (FGSM), projected gradient descent (PGD), and momentum diverse input iterative method (MDIIM).</p></li>
</ol>
</li>
<li><p>Fuzzer moduler (mutation guidance module):</p>
<p>Performs fuzzing on the data generated by mutation and observes the change of the neuron coverage rate. If the generated data increases the neuron coverage rate, it adds the data to the mutation seed queue for the next round of data mutation. Currently, the following neuron coverage rate metrics are supported: NC, TKNC, KMNC, NBC, and SNAC.</p>
</li>
<li><p>Evaluation (evaluation module):</p>
<p>Evaluates the fuzzer effect, quality of generated data, and strength of mutation methods. Three types of five metrics are supported, including common evaluation metrics such as accuracy, precision, and recall rate, neuron coverage, and attack success rate.</p>
</li>
</ol>
</section>
</section>
<section id="model-asset-security">
<h3>Model Asset Security<a class="headerlink" href="#model-asset-security" title="Permalink to this headline"></a></h3>
<p>Deep learning models have high business value and knowledge attributes. To protect model asset security and prevent models from being illegally copied, redistributed, and abused during transmission, deployment, and running, MindSpore Armour provides encryption-based model file protection and structure parameter obfuscation-based lightweight model protection.</p>
<section id="model-encryption">
<h4>Model Encryption<a class="headerlink" href="#model-encryption" title="Permalink to this headline"></a></h4>
<p>To ensure the security of model flushing to disks, MindSpore Armour integrates the CKPT and MINDIR encryption and decryption functions in the framework. Developers can encrypt models and load ciphertext models before flushing models to disks. In the training phase, you can transfer the encryption key and encryption mode to the framework to enable the model encryption function and generate a ciphertext model. When the inference service is deployed, the same encryption key and mode are transferred to the framework during encryption and export to enable decryption during running.</p>
<p><img alt="encrypted" src="_images/encrypted.png" /></p>
</section>
<section id="model-obfuscation">
<h4>Model Obfuscation<a class="headerlink" href="#model-obfuscation" title="Permalink to this headline"></a></h4>
<p>Model obfuscation scrambles the computation logic of a model without changing the model function, which greatly reduces the readability of the model. In this way, the reverse cost exceeds the benefit brought by the reverse, and the model is available but incomprehensible. It is lightweight and independent of specific hardware.</p>
<p><img alt="1631006317050" src="_images/obfuscation.png" /></p>
</section>
</section>
<section id="privacy">
<h3>Privacy<a class="headerlink" href="#privacy" title="Permalink to this headline"></a></h3>
<p>Protecting user privacy and security is an important corporate responsibility. MindSpore Armour provides privacy protection capabilities throughout the AI lifecycle. In the data collection phase, the data masking capability is provided to prevent user privacy data from being collected. In the model training phase, differential privacy and privacy suppression mechanisms are provided to reduce model privacy leakage risks. In the model use phase, privacy leakage assessment technologies based on membership inference and inversion attacks are provided to evaluate the risks of model privacy leakage.</p>
</section>
<section id="data-masking">
<h3>Data Masking<a class="headerlink" href="#data-masking" title="Permalink to this headline"></a></h3>
<p>Predefined privacy elements: The most common scenario is to anonymize street view data of autonomous driving, identify and mask specific content. The recall rate must be high, that is, the number of manual operations must be reduced as much as possible.</p>
<p><img alt="data_masking.png" src="_images/data_masking.png" /></p>
<ul class="simple">
<li><p>Objects: faces, license plates, and specific text.</p></li>
<li><p>Purposes: Protect ID privacy and maximize data availability while ensuring a high recall rate.</p></li>
<li><p>Process:</p>
<ul>
<li><p>Detect coarse-grained objects in which predefined fine-grained elements are located, and crop and save areas in which the coarse-grained objects are located.</p></li>
<li><p>Then, the fine-grained detection model is called to accurately locate privacy elements. If no privacy element is found in the coarse-grained detection area, the guarantee policy should be used to roughly locate the coarse-grained area to prevent privacy leakage caused by missing masking.</p></li>
<li><p>Finally, summarize all areas to be masked and the corresponding classes, and call the masking API to perform personalized masking.</p></li>
</ul>
</li>
</ul>
<section id="differential-privacy-training">
<h4>Differential Privacy Training<a class="headerlink" href="#differential-privacy-training" title="Permalink to this headline"></a></h4>
<p>The differential-privacy module of MindSpore Armour implements differential privacy training. Model training consists of building a training dataset, computing the loss, computing the gradient, and updating model parameters. Currently, the differential privacy training of MindSpore Armour focuses on the gradient computation process, that is, cropping and noise adding on the gradient using corresponding algorithms. In this way, user data privacy is protected.</p>
<p><img alt="dp_arch" src="_images/dp_arch.png" /></p>
<ul class="simple">
<li><p>DP optimizer inherits capabilities of the MindSpore optimizer and uses the DP mechanisms to scramble and protect gradients. Currently, MindSpore Armour provides three types of differential privacy optimizers: constant Gaussian optimizer, adaptive Gaussian optimizer, and adaptive clipping optimizer. Each type of differential privacy optimizer improves the differential privacy protection capabilities for common optimizers, such as SGD and Momentum, from different perspectives.</p></li>
<li><p>Differential privacy (DP) noise mechanisms are the basis for building the differential privacy training capability. Different noise mechanisms meet the requirements of different differential privacy optimizers, including constant Gaussian noise, adaptive Gaussian noise, adaptive Gaussian noise clipping, and Laplacian-based noise mechanism.</p></li>
<li><p>The privacy monitor provides callback functions such as Rényi differential privacy (RDP) and zero-concentrated differential privacy (ZCDP) to monitor the differential privacy budget of the model.</p></li>
</ul>
</section>
<section id="privacy-leakage-assessment">
<h4>Privacy Leakage Assessment<a class="headerlink" href="#privacy-leakage-assessment" title="Permalink to this headline"></a></h4>
<p>MindSpore Armour uses algorithms such as membership inference attack and model inversion attack to evaluate the risk of model privacy leakage.</p>
<ul class="simple">
<li><p>Membership inference: If an attacker has some access permissions (black box, gray box, or white box) of a model to obtain some or all information about the model output, structure, or parameters, they can determine whether a sample belongs to a training set of a model. In this case, we can use membership inference to evaluate the privacy data security of machine learning and deep learning models.</p></li>
<li><p>Inversion attack: Attackers use model gradients to inversely infer user data. The attacker first randomly generates a pair of input data and labels, exports the virtual gradient, and updates the input data and labels during the optimization process to reduce the difference between the virtual gradient and the real gradient, thereby obtaining the privacy input data. With inversion attacks, we can evaluate the risk of leaking training data by deep learning models.</p></li>
</ul>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindarmour.utils.html" class="btn btn-neutral float-left" title="mindarmour.utils" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="differential_privacy_design.html" class="btn btn-neutral float-right" title="Differential Privacy Design" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>