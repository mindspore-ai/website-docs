<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindflow.cell.neural_operators.percnn &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow_install.html">安装MindSpore Flow</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">特性</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../features/solve_pinns_by_mindflow.html">基于MindSpore Flow求解PINNs问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">物理驱动</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../physics_driven/burgers1D.html">一维Burgers问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../physics_driven/darcy2D.html">二维定常达西问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../physics_driven/navier_stokes2D.html">二维圆柱绕流</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../physics_driven/poisson_geometry.html">二维&amp;三维Poisson问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../physics_driven/taylor_green2D.html">二维Taylor-Green涡流动</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../physics_driven/navier_stokes_inverse.html">Navier-Stokes方程反问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据驱动</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../data_driven/2D_steady.html">AI工业流体仿真模型——东方御风</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data_driven/burgers_FNO1D.html">基于FNO求解一维Burgers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data_driven/navier_stokes_FNO2D.html">基于FNO求解二维Navier-Stokes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data_driven/burgers_KNO1D.html">基于KNO求解一维Burgers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data_driven/navier_stokes_KNO2D.html">基于KNO求解二维Navier-Stokes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据机理融合</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../data_mechanism_fusion/pde_net.html">PDE-Net求解对流扩散方程</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">可微分CFD求解器</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cfd_solver/lax_tube.html">一维Lax激波管</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cfd_solver/sod_tube.html">一维Sod激波管</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cfd_solver/couette.html">二维库埃特流</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cfd_solver/riemann2d.html">二维黎曼问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.cell.html">mindflow.cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.cfd.html">mindflow.cfd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.common.html">mindflow.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.data.html">mindflow.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.geometry.html">mindflow.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.loss.html">mindflow.loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.operators.html">mindflow.operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindflow.pde.html">mindflow.pde</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindflow.cell.neural_operators.percnn</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindflow.cell.neural_operators.percnn 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;physical informed recurrent convolutional nerual network&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span><span class="p">,</span> <span class="n">lazy_inline</span>

<span class="kn">from</span> <span class="nn">...utils.check_func</span> <span class="kn">import</span> <span class="n">check_param_type</span><span class="p">,</span> <span class="n">check_param_type_value</span>

<span class="k">def</span> <span class="nf">lazy_inline_wrapper</span><span class="p">(</span><span class="n">backend</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;lazy inline wrapper&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">deco</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">lazy_inline</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">f</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">deco</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">f</span>
    <span class="k">return</span> <span class="n">deco</span>


<span class="k">class</span> <span class="nc">PeriodicPadding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    PeriodicPadding pads input tensor according to given kernel size.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): The physical dimension of input. Length of the shape of a 2D input is 4, of a</span>
<span class="sd">                    3D input is 5. Data follows `NCHW` or `NCDHW` format.</span>
<span class="sd">        kernel_size (int, tuple): Specifies the convolution kernel. If type of kernel_size is int,</span>
<span class="sd">                    last 2 or 3 dimension will be padded. If type of kernel_size is tuple, its</span>
<span class="sd">                    sequence should be (depth, height, width) for 3D and (height, width) for 2D.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        **input** (Tensor) - Tensor of shape :math:`(batch\_size, channels, depth, height, width)` for 3D.</span>
<span class="sd">                               Tensor of shape :math:`(batch\_size, channels, height, width)` for 2D.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dim` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size` is not a int/tuple/list.</span>
<span class="sd">        ValueError: If length of `kernel_size` is not the same as the value of `dim`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, nn</span>
<span class="sd">        &gt;&gt;&gt; x= np.zeros((1,2, 48, 48, 48))</span>
<span class="sd">        &gt;&gt;&gt; pd = PeriodicPadding(3, (3,3,5))</span>
<span class="sd">        &gt;&gt;&gt; x = pd(Tensor(x))</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>

<span class="sd">        (1, 2, 50, 50, 52)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">((</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;length of kernel size must be the same as dim&quot;</span><span class="p">)</span>

            <span class="n">kernel_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="n">kernel_size</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">kernel_size</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;circular&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<div class="viewcode-block" id="PeRCNN"><a class="viewcode-back" href="../../../../cell/mindflow.cell.PeRCNN.html#mindflow.cell.PeRCNN">[文档]</a><span class="k">class</span> <span class="nc">PeRCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Physics-embedded Recurrent Convolutional Neural Network (PeRCNN) Cell. It forcibly encodes given</span>
<span class="sd">    physics structure to facilitate the learning of the spatiotemporal dynamics in sparse data regimes.</span>
<span class="sd">    PeRCNN can be applied to a variety of problems regarding the PDE system, including forward and</span>
<span class="sd">    inverse analysis, data-driven modeling, and discovery of PDEs.</span>

<span class="sd">    For more details, please refers to the paper `Encoding physics to learn reaction–diffusion processes</span>
<span class="sd">    &lt;https://www.nature.com/articles/s42256-023-00685-7&gt;`_ .</span>

<span class="sd">    lazy_inline is used to accelerate the compile stage, but now it only functions in Ascend backends.</span>
<span class="sd">    PeRCNN currently supports input with two physical components. For inputs with different shape, users</span>
<span class="sd">    must manually add or remove corresponding parameters and pi_blocks.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): The physical dimension of input. Length of the shape of a 2D input is 4, of a</span>
<span class="sd">                    3D input is 5. Data follows `NCHW` or `NCDHW` format.</span>
<span class="sd">        in_channels (int): The number of channels in the input space.</span>
<span class="sd">        hidden_channels (int): Number of channels in the output space of parallel convolution layers.</span>
<span class="sd">        kernel_size (int): Specifies the convolution kernel for parallel convolution layers.</span>
<span class="sd">        dt (Union[int, float]): The time step of PeRCNN.</span>
<span class="sd">        nu (Union[int, float]): The coefficient of diffusion term.</span>
<span class="sd">        laplace_kernel (mindspore.Tensor): For 3D, Set size of kernel is :math:`(\text{kernel_size[0]},</span>
<span class="sd">            \text{kernel_size[1]}, \text{kernel_size[2]})`, then the shape is :math:`(C_{out}, C_{in},</span>
<span class="sd">            \text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[1]})`. For 2D, Tensor of shape</span>
<span class="sd">            :math:`(N, C_{in} / \text{groups}, \text{kernel_size[0]}, \text{kernel_size[1]})`, then the size of kernel</span>
<span class="sd">            is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`.</span>
<span class="sd">        conv_layers_num (int): Number of parallel convolution layers. Default: ``3``.</span>
<span class="sd">        padding (str): Boundary padding, currently only periodic padding supported. Default: ``periodic``.</span>
<span class="sd">        compute_dtype (dtype.Number): The data type of PeRCNN. Default: ``mindspore.float32``.</span>
<span class="sd">                Should be ``mindspore.float16`` or ``mindspore.float32``.</span>
<span class="sd">                mindspore.float32 is recommended for GPU backends,</span>
<span class="sd">                mindspore.float16 is recommended for Ascend backends.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(batch\_size, channels, depth, height, width)` for 3D.</span>
<span class="sd">          Tensor of shape :math:`(batch\_size, channels, height, width)` for 2D.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dim`, `in_channels`, `hidden_channels`, `kernel_size` is not an int.</span>
<span class="sd">        TypeError: If `dt` and `nu` is not an int nor a float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindflow.cell import PeRCNN</span>
<span class="sd">        &gt;&gt;&gt; laplace_3d = [[[[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, -0.08333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]],</span>
<span class="sd">        ...                 [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 1.3333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]],</span>
<span class="sd">        ...                 [[0.0, 0.0, -0.08333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 1.3333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [-0.08333333333333333, 1.3333333333333333, -7.5, 1.3333333333333333, -0.08333333333333333],</span>
<span class="sd">        ...                 [0.0, 0.0, 1.3333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, -0.08333333333333333, 0.0, 0.0]],</span>
<span class="sd">        ...                 [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 1.3333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]],</span>
<span class="sd">        ...                 [[0.0, 0.0, 0.0, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.08333333333333333, 0.0, 0.0],</span>
<span class="sd">        ...                 [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]]]]</span>
<span class="sd">        &gt;&gt;&gt; laplace = np.array(laplace_3d)</span>
<span class="sd">        &gt;&gt;&gt; grid_size = 48</span>
<span class="sd">        &gt;&gt;&gt; field = 100</span>
<span class="sd">        &gt;&gt;&gt; dx_3d = field / grid_size</span>
<span class="sd">        &gt;&gt;&gt; laplace_3d_kernel = ms.Tensor(1 / dx_3d**2 * laplace, dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; rcnn_ms = PeRCNN(</span>
<span class="sd">        ...     dim=3,</span>
<span class="sd">        ...     in_channels=2,</span>
<span class="sd">        ...     hidden_channels=2,</span>
<span class="sd">        ...     kernel_size=1,</span>
<span class="sd">        ...     dt=0.5,</span>
<span class="sd">        ...     nu=0.274,</span>
<span class="sd">        ...     laplace_kernel=laplace_3d_kernel,</span>
<span class="sd">        ...     conv_layers_num=3,</span>
<span class="sd">        ...     compute_dtype=ms.float32,</span>
<span class="sd">        ...   )</span>
<span class="sd">        &gt;&gt;&gt; input = np.random.randn(1, 2, 48, 48, 48)</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(input, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = rcnn_ms(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 2, 48, 48, 48)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@lazy_inline_wrapper</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="n">attr_key</span><span class="o">=</span><span class="s2">&quot;device_target&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span>
            <span class="n">laplace_kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">conv_layers_num</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;periodic&quot;</span><span class="p">,</span> <span class="n">compute_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">check_param_type_value</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">valid_value</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">data_type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="s2">&quot;in_channels&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="s2">&quot;hidden_channels&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">conv_layers_num</span><span class="p">,</span> <span class="s2">&quot;conv_layers_num&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="s2">&quot;dt&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">check_param_type</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="s2">&quot;nu&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="n">exclude_type</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_channels</span> <span class="o">=</span> <span class="n">hidden_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_stride</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dt</span> <span class="o">=</span> <span class="n">dt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nu</span> <span class="o">=</span> <span class="n">nu</span>  <span class="c1"># nu from 0 to upper bound (two times the estimate)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_nn_conv_table</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ops_conv_table</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">conv3d</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">laplace_kernel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_laplace</span> <span class="o">=</span> <span class="n">laplace_kernel</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_u</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">u_pi_block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_pi_block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
        <span class="c1"># Parallel conv layer for u</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conv_layers_num</span><span class="p">):</span>
            <span class="n">u_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nn_conv_table</span><span class="p">[</span><span class="n">dim</span><span class="p">](</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_stride</span><span class="p">,</span>
                <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u_pi_block</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u_conv_layer</span><span class="p">)</span>
            <span class="n">u_conv_layer</span><span class="o">.</span><span class="n">update_parameters_name</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;u_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="c1"># 1x1 layer conv for u</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">u_conv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nn_conv_table</span><span class="p">[</span><span class="n">dim</span><span class="p">](</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="c1"># Parallel conv layer for v</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conv_layers_num</span><span class="p">):</span>
            <span class="n">v_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nn_conv_table</span><span class="p">[</span><span class="n">dim</span><span class="p">](</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_stride</span><span class="p">,</span>
                <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v_pi_block</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v_conv_layer</span><span class="p">)</span>
            <span class="n">v_conv_layer</span><span class="o">.</span><span class="n">update_parameters_name</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;v_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="c1"># 1x1 layer conv for v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_conv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nn_conv_table</span><span class="p">[</span><span class="n">dim</span><span class="p">](</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;construct function of PeRCNN&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;periodic&quot;</span><span class="p">:</span>
            <span class="n">conv_padding</span> <span class="o">=</span> <span class="n">PeriodicPadding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;unsupported padding type&quot;</span><span class="p">)</span>
        <span class="n">h_conv</span> <span class="o">=</span> <span class="n">conv_padding</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">u_res</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">v_res</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">u_pi_block</span><span class="p">:</span>
            <span class="n">u_res</span> <span class="o">=</span> <span class="n">u_res</span> <span class="o">*</span> <span class="n">conv</span><span class="p">(</span><span class="n">h_conv</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_pi_block</span><span class="p">:</span>
            <span class="n">v_res</span> <span class="o">=</span> <span class="n">v_res</span> <span class="o">*</span> <span class="n">conv</span><span class="p">(</span><span class="n">h_conv</span><span class="p">)</span>
        <span class="n">u_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">u_conv</span><span class="p">(</span><span class="n">u_res</span><span class="p">)</span>
        <span class="n">v_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_conv</span><span class="p">(</span><span class="n">v_res</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_laplace</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">laplace_padding</span> <span class="o">=</span> <span class="n">PeriodicPadding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_laplace</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
            <span class="n">h_lap</span> <span class="o">=</span> <span class="n">laplace_padding</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

            <span class="n">u_pad</span> <span class="o">=</span> <span class="n">h_lap</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
            <span class="n">v_pad</span> <span class="o">=</span> <span class="n">h_lap</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
            <span class="n">u_res</span> <span class="o">=</span> <span class="n">u_res</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_u</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ops_conv_table</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dim</span>
            <span class="p">](</span><span class="n">u_pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_laplace</span><span class="p">)</span>
            <span class="n">v_res</span> <span class="o">=</span> <span class="n">v_res</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">nu</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_v</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ops_conv_table</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dim</span>
            <span class="p">](</span><span class="n">v_pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_laplace</span><span class="p">)</span>

        <span class="c1"># previous state</span>
        <span class="k">if</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input field should have at least two physical components&quot;</span><span class="p">)</span>
        <span class="n">u_prev</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
        <span class="n">v_prev</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

        <span class="n">u_next</span> <span class="o">=</span> <span class="n">u_prev</span> <span class="o">+</span> <span class="n">u_res</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
        <span class="n">v_next</span> <span class="o">=</span> <span class="n">v_prev</span> <span class="o">+</span> <span class="n">v_res</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dt</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">u_next</span><span class="p">,</span> <span class="n">v_next</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>