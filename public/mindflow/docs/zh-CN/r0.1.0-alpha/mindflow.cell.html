

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindflow.cell &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/translations.js"></script>
        
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="mindflow.cfd" href="mindflow.cfd.html" />
    <link rel="prev" title="PDE-Net求解微分方程反问题并实现长期预测" href="physics_plus_data_driven/pdenet.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindflow_install.html">安装MindFlow</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">物理驱动</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="physics_driven/burgers1D.html">一维Burgers问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="physics_driven/navier_stokes2D.html">二维圆柱绕流</a></li>
<li class="toctree-l1"><a class="reference internal" href="physics_driven/poisson_ring.html">作用于圆环的二维Poisson问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="physics_driven/sympy_pde_definition.html">基于MindFlow定义符号化偏微分方程</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据驱动</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data_driven/dfyf.html">AI工业流体仿真模型——东方·御风</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_driven/FNO1D.html">基于FNO求解一维Burgers</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_driven/FNO2D.html">基于FNO求解二维Navier-Stokes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">可微分CFD求解器</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cfd/lax_tube.html">一维Lax激波管</a></li>
<li class="toctree-l1"><a class="reference internal" href="cfd/sod_tube.html">一维Sod激波管</a></li>
<li class="toctree-l1"><a class="reference internal" href="cfd/couette.html">二维库埃特流</a></li>
<li class="toctree-l1"><a class="reference internal" href="cfd/riemann2d.html">二维黎曼问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据机理融合</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="physics_plus_data_driven/pdenet.html">PDE-Net求解微分方程反问题并实现长期预测</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API参考</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindflow.cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.cfd.html">mindflow.cfd</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.common.html">mindflow.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.data.html">mindflow.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.geometry.html">mindflow.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.loss.html">mindflow.loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.operators.html">mindflow.operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.pde.html">mindflow.pde</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindflow.solver.html">mindflow.solver</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindflow.cell</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/mindflow.cell.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindflow-cell">
<h1>mindflow.cell<a class="headerlink" href="#mindflow-cell" title="永久链接至标题"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.FCSequential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">FCSequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neurons</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sin'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/basic_block.html#FCSequential"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.FCSequential" title="打开链接"></a></dt>
<dd><p>全连接层的一个时序容器，按序放入全连接层。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入中的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出中的通道数。</p></li>
<li><p><strong>layers</strong> (int) - 层的总数，包括输入/隐藏/输出层。</p></li>
<li><p><strong>neurons</strong> (int) - 隐藏层的神经元数量。</p></li>
<li><p><strong>residual</strong> (bool) - 隐藏层是否使用残差网络模块。若为True，使用残差网络模块。若为False，使用线性模块。默认值：True。</p></li>
<li><p><strong>act</strong> (Union[str, Cell, Primitive, None]) - 激活应用于全连接层输出的函数，例如”ReLU”。默认值：”sin”。</p></li>
<li><p><strong>weight_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始权重值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：’normal’。</p></li>
<li><p><strong>has_bias</strong> (bool) - 指定图层是否使用偏置向量。默认值：True。</p></li>
<li><p><strong>bias_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始偏差值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：’default’。</p></li>
<li><p><strong>weight_norm</strong> (bool) - 是否计算权重的平方和。默认值：False。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((*, in\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>shape为 <span class="math notranslate nohighlight">\((*, out\_channels)\)</span> 的Tensor。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>layers</cite> 不是int类型。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>neurons</cite> 不是int类型。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>residual</cite> 不是bool类型。</p></li>
<li><p><strong>ValueError</strong> - 如果 <cite>layers</cite> 小于3。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell</span> <span class="kn">import</span> <span class="n">FCSequential</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">FCSequential</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span> <span class="n">bias_init</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(16, 3)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.FNO1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">FNO1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resolution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/neural_operators/fno1d.html#FNO1D"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.FNO1D" title="打开链接"></a></dt>
<dd><p>一维傅里叶神经算子（FNO1D）包含一个提升层、多个傅里叶层和一个解码器层。
有关更多详细信息，请参考论文 <a class="reference external" href="https://arxiv.org/pdf/2010.08895.pdf">Fourier Neural Operator for Parametric Partial Differential Equations</a> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入中的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出中的通道数。</p></li>
<li><p><strong>resolution</strong> (int) - 输入的分辨率。</p></li>
<li><p><strong>modes</strong> (int) - 要保留的低频分量的数量。</p></li>
<li><p><strong>channels</strong> (int) - 输入提升尺寸后的通道数。默认值：20。</p></li>
<li><p><strong>depths</strong> (int) - FNO层的数量。默认值：4。</p></li>
<li><p><strong>mlp_ratio</strong> (int) - 解码器层的通道数提升比率。默认值：4。</p></li>
<li><p><strong>compute_dtype</strong> (dtype.Number) - 密集的计算类型。默认mstype.float16。支持以下数据类型：mstype.float32或mstype.float16。GPU后端建议使用mstype.float32，Ascend后端建议使用mstype.float16。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, resolution, in\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，FNO网络的输出。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, resolution, out\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>in_channels</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>out_channels</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>resolution</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>modes</cite> 不是int。</p></li>
<li><p><strong>ValueError</strong> - 如果 <cite>modes</cite> 小于1。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">Normal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell.neural_operators</span> <span class="kn">import</span> <span class="n">FNO1D</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span><span class="mi">1024</span><span class="p">,</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(),</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">FNO1D</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">modes</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(32, 1024, 1)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.FNO2D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">FNO2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resolution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/neural_operators/fno2d.html#FNO2D"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.FNO2D" title="打开链接"></a></dt>
<dd><p>二维傅里叶神经算子（FNO2D）包含一个提升层、多个傅里叶层和一个解码器层。
有关更多详细信息，请参考论文 <a class="reference external" href="https://arxiv.org/pdf/2010.08895.pdf">Fourier Neural Operator for Parametric Partial Differential Equations</a> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入中的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出中的通道数。</p></li>
<li><p><strong>resolution</strong> (int) - 输入的分辨率。</p></li>
<li><p><strong>modes</strong> (int) - 要保留的低频分量的数量。</p></li>
<li><p><strong>channels</strong> (int) - 输入提升尺寸后的通道数。默认值：20。</p></li>
<li><p><strong>depths</strong> (int) - FNO层的数量。默认值：4。</p></li>
<li><p><strong>mlp_ratio</strong> (int) - 解码器层的通道数提升比率。默认值：4。</p></li>
<li><p><strong>compute_dtype</strong> (dtype.Number) - 密集的计算类型。默认mstype.float16。支持以下数据类型：mstype.float16或mstype.float32。GPU后端建议使用mstype.float32，Ascend后端建议使用mstype.float16。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, resolution, in\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，此FNO网络的输出。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, resolution, out\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>in_channels</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>out_channels</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>resolution</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>modes</cite> 不是int。</p></li>
<li><p><strong>ValueError</strong> - 如果 <cite>modes</cite> 小于1。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">Normal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell.neural_operators</span> <span class="kn">import</span> <span class="n">FNO2D</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(),</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">FNO2D</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">modes</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(32, 64, 64, 1)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.InputScaleNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">InputScaleNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/basic_block.html#InputScaleNet"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.InputScaleNet" title="打开链接"></a></dt>
<dd><p>将输入值缩放到指定的区域。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>input_scale</strong> (list) - 输入x/y/t的比例因子。</p></li>
<li><p><strong>input_center</strong> (Union[list, None]) - 坐标转换的中心位置。默认值：None。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((*, channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>shape为 <span class="math notranslate nohighlight">\((*, channels)\)</span> 的Tensor。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>input_scale</cite> 不是list类型。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>input_center</cite> 不是list或者None类型。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell</span> <span class="kn">import</span> <span class="n">InputScaleNet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="mf">3.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_scale</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_center</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">InputScaleNet</span><span class="p">(</span><span class="n">input_scale</span><span class="p">,</span> <span class="n">input_center</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">2.0</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.LinearBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">LinearBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/basic_block.html#LinearBlock"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.LinearBlock" title="打开链接"></a></dt>
<dd><p>连接层Block。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入中的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出中的通道数。</p></li>
<li><p><strong>weight_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始权重值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：”normal”。</p></li>
<li><p><strong>bias_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始偏差值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：”zeros”。</p></li>
<li><p><strong>has_bias</strong> (bool) - 指定图层是否使用偏置向量。默认值：True。</p></li>
<li><p><strong>activation</strong> (Union[str, Cell, Primitive, None]) - 应用于全连接输出的激活函数层。默认值：None。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((*, in\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>shape为 <span class="math notranslate nohighlight">\((*, out\_channels)\)</span> 的Tensor。</p>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindelec.architecture</span> <span class="kn">import</span> <span class="n">LinearBlock</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">234</span><span class="p">,</span> <span class="mi">154</span><span class="p">],</span> <span class="p">[</span><span class="mi">244</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">247</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LinearBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 4)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.MultiScaleFCCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">MultiScaleFCCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neurons</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sin'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_scales</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amp_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_center</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_vector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/basic_block.html#MultiScaleFCCell"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.MultiScaleFCCell" title="打开链接"></a></dt>
<dd><p>多尺度神经网络。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入中的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出中的通道数。</p></li>
<li><p><strong>layers</strong> (int) - 层总数，包括输入/隐藏/输出层。</p></li>
<li><p><strong>neurons</strong> (int) - 隐藏层的神经元数量。</p></li>
<li><p><strong>residual</strong> (bool) - 隐藏层的残差块的全连接。默认值：True。</p></li>
<li><p><strong>act</strong> (Union[str, Cell, Primitive, None]) - 激活应用于全连接层输出的函数，例如“ReLU”。默认值：sin。</p></li>
<li><p><strong>weight_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始权重值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：”normal”。</p></li>
<li><p><strong>weight_norm</strong> (bool) - 是否计算权重的平方和。默认值：False。</p></li>
<li><p><strong>has_bias</strong> (bool) - 指定图层是否使用偏置向量。默认值：True。</p></li>
<li><p><strong>bias_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始偏差值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：”default”。</p></li>
<li><p><strong>num_scales</strong> (int) - 多规模网络的子网号。默认值：4</p></li>
<li><p><strong>amp_factor</strong> (Union[int, float]) - 输入的放大系数。默认值：1.0</p></li>
<li><p><strong>scale_factor</strong> (Union[int, float]) - 基本比例因子。默认值：2.0</p></li>
<li><p><strong>input_scale</strong> (Union[list, None]) - 输入x/y/t的比例因子。如果不是None，则输入将在网络中设置之前缩放。默认值：None。</p></li>
<li><p><strong>input_center</strong> (Union[list, None]) - 坐标转换的中心位置。如果不是None，则输入将在网络中设置之前翻译。默认值：None。</p></li>
<li><p><strong>latent_vector</strong> (Union[Parameter, None]) - 将与采样输入连接的可训练的parameter并在训练期间更新。默认值：None。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((*, in\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>shape为 <span class="math notranslate nohighlight">\((*, out\_channels)\)</span> 的Tensor。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>num_scales</cite> 不是int类型。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>amp_factor</cite> 不是int及或者float类型。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>scale_factor</cite> 不是int及或者float类型。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>latent_vector</cite> 不是Parameter或者None类型。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell</span> <span class="kn">import</span> <span class="n">MultiScaleFCCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="mf">3.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_scenarios</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">latent_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">latent_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_scenarios</span><span class="p">,</span> <span class="n">latent_size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">latent_vector</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">latent_init</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_scale</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_center</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">MultiScaleFCCell</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span> <span class="n">bias_init</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">input_scale</span><span class="o">=</span><span class="n">input_scale</span><span class="p">,</span> <span class="n">input_center</span><span class="o">=</span><span class="n">input_center</span><span class="p">,</span> <span class="n">latent_vector</span><span class="o">=</span><span class="n">latent_vector</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(64, 3)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.PDENet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">PDENet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">height</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">width</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_order</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">periodic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_moment</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">if_fronzen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/neural_operators/pdenet.html#PDENet"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.PDENet" title="打开链接"></a></dt>
<dd><p>PDE-Net模型。
PDE-Net是一个前馈深度网络，可同时实现两个目标：准确预测复杂的系统，并揭示底层隐藏的PDE模型。基本思想是学习微分算子通过学习卷积核（过滤器），并将神经网络或其他机器学习方法应用于
近似未知非线性响应。PDE-Net的特殊性在于，卷积核受“矩”的约束，这使得模型能够轻松地识别PDE模型，同时仍保持网络的表达能力和预测能力。
这些约束通过充分利用微分算子的阶数与卷积核的关系得到的.一个重要的概念起源于小波理论。有关更多详细信息，请参考论文 <a class="reference external" href="https://arxiv.org/pdf/1710.09668.pdf">PDE-NET: LEARNING PDES FROM DATA</a> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>height</strong> (int) - PDE-Net输入和输出Tensor的高度。</p></li>
<li><p><strong>width</strong> (int) - PDE-Net输入和输出Tensor的宽度。</p></li>
<li><p><strong>channels</strong> (int) - PDE-Net输入和输出Tensor的通。</p></li>
<li><p><strong>kernel_size</strong> (int) - 指定2D卷积内核的高度和宽度。</p></li>
<li><p><strong>max_order</strong> (int) - PDE模型的最大顺序。</p></li>
<li><p><strong>step</strong> (int) - PDE-Net中使用的增量-T块的数量。</p></li>
<li><p><strong>dx</strong> (float) - x维的空间分辨率。默认值：0.01。</p></li>
<li><p><strong>dy</strong> (float) - y维的空间分辨率。默认值：0.01。</p></li>
<li><p><strong>dt</strong> (float) - PDE-Net的时间步长。默认值：0.01。</p></li>
<li><p><strong>periodic</strong> (bool) - 指定周期是否与卷积核一起使用。默认值：True。</p></li>
<li><p><strong>enable_moment</strong> (bool) - 指定卷积核是否受moment约束。默认值：True。</p></li>
<li><p><strong>if_fronzen</strong> (bool) - moment里的参数是否参与训练。默认值：False。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, channels, height, width)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，具有与 <cite>input</cite> 相同的形状，数据类型为float32。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>height</cite> 、 <cite>width</cite> 、 <cite>channels</cite> 、 <cite>kernel_size</cite> 、 <cite>max_order</cite> 或 <cite>step</cite> 不是int。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>periodic</cite> 、 <cite>enable_moment</cite> 、 <cite>if_fronzen</cite> 不是bool。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell.neural_operators</span> <span class="kn">import</span> <span class="n">PDENet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">PDENet</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1, 2, 16, 16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.ResBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">ResBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/basic_block.html#ResBlock"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.ResBlock" title="打开链接"></a></dt>
<dd><p>密集层的ResBlock。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>in_channels</strong> (int) - 输入中的通道数。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出中的通道数。</p></li>
<li><p><strong>weight_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始权重值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：”normal”。</p></li>
<li><p><strong>bias_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 可训练的初始偏差值。数据类型与输入 <cite>input</cite> 相同。str的值引用函数 <cite>initializer</cite> 。默认值：”zeros”。</p></li>
<li><p><strong>has_bias</strong> (bool) - 指定图层是否使用偏置向量。默认值：True。</p></li>
<li><p><strong>activation</strong> (Union[str, Cell, Primitive, None]) - 应用于密集层输出的激活函数。默认值：None。</p></li>
<li><p><strong>weight_norm</strong> (bool) - 是否计算权重的平方和。默认值：False。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((*, in\_channels)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>shape为 <span class="math notranslate nohighlight">\((*, out\_channels)\)</span> 的Tensor。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>ValueError</strong> - 如果 <cite>in_channels</cite> 不等于 <cite>out_channels</cite> 。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>activation</cite> 类型不是str或者Cell或者Primitive。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell</span> <span class="kn">import</span> <span class="n">ResBlock</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">234</span><span class="p">,</span> <span class="mi">154</span><span class="p">],</span> <span class="p">[</span><span class="mi">244</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">247</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">ResBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindflow.cell.ViT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">ViT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(192,</span> <span class="pre">384)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_depths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_embed_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_depths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_embed_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float16</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/transformer/vit.html#ViT"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.ViT" title="打开链接"></a></dt>
<dd><p>该模块基于ViT，包括encoder层、decoding_embedding层、decoder层和dense层。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>image_size</strong> (tuple[int]) - 输入的图像尺寸。默认值：（192,384）。</p></li>
<li><p><strong>in_channels</strong> (int) - 输入的输入特征维度。默认值：7。</p></li>
<li><p><strong>out_channels</strong> (int) - 输出的输出特征维度。默认值：3。</p></li>
<li><p><strong>patch_size</strong> (int) - 图像的path尺寸。默认值：16。</p></li>
<li><p><strong>encoder_depths</strong> (int) - encoder层的层数。默认值：12。</p></li>
<li><p><strong>encoder_embed_dim</strong> (int) - encoder层的编码器维度。默认值：768。</p></li>
<li><p><strong>encoder_num_heads</strong> (int) - encoder层的head数。默认值：12。</p></li>
<li><p><strong>decoder_depths</strong> (int) - decoder层的解码器深度。默认值：8。</p></li>
<li><p><strong>decoder_embed_dim</strong> (int) - decoder层的解码器维度。默认值：512。</p></li>
<li><p><strong>decoder_num_heads</strong> (int) - decoder层的head数。默认值：16。</p></li>
<li><p><strong>mlp_ratio</strong> (int) - mlp层的比例。默认值：4。</p></li>
<li><p><strong>dropout_rate</strong> (float) - dropout层的速率。默认值：1.0。</p></li>
<li><p><strong>compute_dtype</strong> (dtype) - encoder层、decoding_embedding层、decoder层和dense层的数据类型。默认值：mstype.float16。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, feature\_size, image\_height, image\_width)\)</span> 的Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><ul class="simple">
<li><p><strong>output</strong> (Tensor) - shape为 <span class="math notranslate nohighlight">\((batch\_size, patchify\_size, embed\_dim)\)</span> 的Tensor。其中，patchify_size = (image_height * image_width) / (patch_size * patch_size)</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell</span> <span class="kn">import</span> <span class="n">ViT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">384</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(32, 3, 192, 384)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">encoder_depths</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">encoder_embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">encoder_num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">decoder_depths</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(32, 288, 768)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindflow.cell.get_activation">
<span class="sig-prename descclassname"><span class="pre">mindflow.cell.</span></span><span class="sig-name descname"><span class="pre">get_activation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mindflow/cell/activation.html#get_activation"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindflow.cell.get_activation" title="打开链接"></a></dt>
<dd><p>获取激活函数。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>name</strong> (Union[str, None]) - 激活函数的名称。若输入为None，函数返回None。</p></li>
</ul>
</dd>
<dt>返回：</dt><dd><p>Function，激活函数。</p>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindflow.cell</span> <span class="kn">import</span> <span class="n">get_activation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">[[0.7685248  0.5249792 ]</span>
<span class="go">[0.54983395 0.96083426]]</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="physics_plus_data_driven/pdenet.html" class="btn btn-neutral float-left" title="PDE-Net求解微分方程反问题并实现长期预测" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="mindflow.cfd.html" class="btn btn-neutral float-right" title="mindflow.cfd" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>