

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindformers.models.t5.t5 &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Installation Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers_install.html">Confirm system environment information</a></li>
</ul>
<p class="caption"><span class="caption-text">BERT Fine adjustment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers_bert_finetune.html">Use BERT tuning in Mindformer</a></li>
</ul>
<p class="caption"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.html">mindformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.core.html">mindformers.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.dataset.html">mindformers.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.models.html">mindformers.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.modules.html">mindformers.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.pipeline.html">mindformers.pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.trainer.html">mindformers.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindformers.wrapper.html">mindformers.wrapper</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>mindformers.models.t5.t5</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindformers.models.t5.t5</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;T5 model.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>

<span class="kn">import</span> <span class="nn">mindspore.numpy</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">TruncatedNormal</span><span class="p">,</span> <span class="n">initializer</span>

<span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="n">_get_parallel_mode</span><span class="p">,</span> <span class="n">_is_sharding_propagation</span>

<span class="kn">from</span> <span class="nn">mindformers.modules.layers</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">_check_past_none_input_none</span><span class="p">,</span> <span class="n">_check_input_dtype</span>
<span class="kn">from</span> <span class="nn">mindformers.modules.transformer.moe</span> <span class="kn">import</span> <span class="n">MoE</span><span class="p">,</span> <span class="n">_check_moe_config</span>
<span class="kn">from</span> <span class="nn">mindformers.modules.transformer.transformer</span> <span class="kn">import</span> <span class="n">default_transformer_config</span><span class="p">,</span> <span class="n">default_moe_config</span><span class="p">,</span> \
    <span class="n">default_dpmp_config</span><span class="p">,</span> \
    <span class="n">EmbeddingOpParallelConfig</span><span class="p">,</span> <span class="n">OpParallelConfig</span>
<span class="kn">from</span> <span class="nn">mindformers.core.loss</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>
<span class="kn">from</span> <span class="nn">mindformers.modules</span> <span class="kn">import</span> <span class="n">VocabEmbedding</span>

<span class="kn">from</span> <span class="nn">.t5_config</span> <span class="kn">import</span> <span class="n">T5Config</span>

<span class="kn">from</span> <span class="nn">..base_model</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">...tools</span> <span class="kn">import</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">...tools.register</span> <span class="kn">import</span> <span class="n">MindFormerRegister</span><span class="p">,</span> <span class="n">MindFormerModuleType</span>
<span class="kn">from</span> <span class="nn">...mindformer_book</span> <span class="kn">import</span> <span class="n">MindFormerBook</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;T5ForConditionalGeneration&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        T5 layer norm nn.Cell</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param_init_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The type of parameter &#39;param_init_type&#39; should in [float32, float16], &quot;</span>
                            <span class="s2">&quot;but got the type : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param_init_type</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">param_init_type</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span>
                               <span class="n">parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">square</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">          x : batch x seq_length x d_model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">variance_eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">variance</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">variance_eps</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the shard for the layer norm. the strategy size should be equal to the inputs.</span>

<span class="sd">        Note:</span>
<span class="sd">            It is valid only in semi auto parallel or auto parallel mode.</span>
<span class="sd">            In other parallel modes, strategies set here will be ignored.</span>

<span class="sd">        Args:</span>
<span class="sd">            strategy (tuple): The strategy for the dropout. Should be the same shape as the inputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub2</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add2</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">T5FeedFoward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        T5 feedfoward cell with relu as hidden act</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">expert_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">expert_group_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">T5FeedFoward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span>
        <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">ep</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">expert_parallel</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ep</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># ffn use less dp than other ops when use_moe, due to there are ops use dp and ep.</span>
        <span class="n">dp</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">/</span> <span class="n">ep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5FeedFoward&#39;, the class variable &#39;ffn_hidden_size&#39; must be a multiple of the&quot;</span>
                             <span class="s2">&quot;num of model parallel, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> and the num of model &quot;</span>
                             <span class="s2">&quot;parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">mp</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5FeedFoward&#39;, the class variable &#39;d_model&#39; must be a multiple of the num of &quot;</span>
                             <span class="s2">&quot;model parallel, but got the d_model is </span><span class="si">{}</span><span class="s2"> and the num of model parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">mp</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5FeedFoward&#39;, the class variable &#39;dropout_rate&#39; must be in the range [0, 1.0), &quot;</span>
                             <span class="s2">&quot;but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>

        <span class="c1"># Project to ffn_hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                              <span class="n">out_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                              <span class="n">activation</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                              <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                              <span class="n">expert_group_size</span><span class="o">=</span><span class="n">expert_group_size</span><span class="p">,</span>
                              <span class="n">outer_batch</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                               <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                               <span class="n">strategy_activation</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                               <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,)),</span>
                               <span class="n">strategy_activation</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),))</span>
        <span class="c1"># Project back to hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                 <span class="n">out_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                                 <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">expert_group_size</span><span class="o">=</span><span class="n">expert_group_size</span><span class="p">,</span>
                                 <span class="n">outer_batch</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
                                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                  <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                  <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The forward function of FFN&quot;&quot;&quot;</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="c1"># returned shape is [bs, seq_length, ffn_hidden_size] or [bs * seq_length, ffn_hidden_size]</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># returned shape is [bs, seq_length, ffn_hidden_size] or [bs * seq_length, ffn_hidden_size]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="k">class</span> <span class="nc">RelaPosMatrixGenerator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The relative position index generator. The result of the cell should be feed into the bias embedding table.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_relative_position</span><span class="p">,</span> <span class="n">log_relative_distance</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RelaPosMatrixGenerator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_relative_position</span> <span class="o">=</span> <span class="n">max_relative_position</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_relative_position</span> <span class="o">=</span> <span class="o">-</span><span class="n">max_relative_position</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">range_mat</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expanddims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_relative_distance</span> <span class="o">=</span> <span class="n">log_relative_distance</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">relative_position</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_buckets</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The forward of the bias position&quot;&quot;&quot;</span>
        <span class="n">relative_bucket</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">relative_bucket</span> <span class="o">=</span> <span class="n">relative_bucket</span> <span class="o">+</span> <span class="p">(</span><span class="n">relative_position</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_buckets</span>
            <span class="n">relative_position</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()(</span><span class="n">relative_position</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">relative_position</span> <span class="o">=</span> <span class="o">-</span><span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()(</span><span class="n">relative_position</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()(</span><span class="n">relative_position</span><span class="p">))</span>

        <span class="n">max_exact</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">is_small</span> <span class="o">=</span> <span class="n">relative_position</span> <span class="o">&lt;</span> <span class="n">max_exact</span>
        <span class="n">relative_position_if_large</span> <span class="o">=</span> <span class="n">max_exact</span> <span class="o">+</span> <span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()(</span><span class="n">relative_position</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="n">max_exact</span><span class="p">)</span>
                                                  <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_relative_distance</span>
                                                  <span class="o">*</span> <span class="p">(</span><span class="n">num_buckets</span> <span class="o">-</span> <span class="n">max_exact</span><span class="p">))</span>
        <span class="n">relative_position_if_large</span> <span class="o">=</span> <span class="n">relative_position_if_large</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">relative_position_if_large</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()(</span><span class="n">relative_position_if_large</span><span class="p">,</span>
                                                 <span class="n">mindspore</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">relative_position_if_large</span><span class="p">,</span>
                                                                           <span class="n">num_buckets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">relative_bucket</span> <span class="o">+=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_small</span><span class="p">,</span> <span class="n">relative_position</span><span class="p">,</span> <span class="n">relative_position_if_large</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">relative_bucket</span>


<span class="k">class</span> <span class="nc">RelaPosEmbeddingsGenerator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The relative position embedding generator.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">depth</span><span class="p">,</span>
                 <span class="n">max_relative_position</span><span class="p">,</span>
                 <span class="n">initializer_range</span><span class="p">,</span>
                 <span class="n">is_decoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RelaPosEmbeddingsGenerator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">max_relative_position</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_table</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">TruncatedNormal</span><span class="p">(</span><span class="n">initializer_range</span><span class="p">),</span>
                                                      <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">OneHot</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_max_distance</span> <span class="o">=</span> <span class="mi">128</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">is_decoder</span>

        <span class="n">num_buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span>

        <span class="n">max_exact</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">max_exact</span> <span class="o">=</span> <span class="n">max_exact</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">num_buckets</span> <span class="o">//=</span> <span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_relative_distance</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_max_distance</span> <span class="o">/</span> <span class="n">max_exact</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_matrix</span> <span class="o">=</span> <span class="n">RelaPosMatrixGenerator</span><span class="p">(</span><span class="n">max_relative_position</span><span class="o">=</span><span class="n">max_relative_position</span><span class="p">,</span>
                                                               <span class="n">log_relative_distance</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_relative_distance</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The forward function&quot;&quot;&quot;</span>
        <span class="n">context_position</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">query_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">memory_position</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">key_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">relative_position</span> <span class="o">=</span> <span class="n">memory_position</span> <span class="o">-</span> <span class="n">context_position</span>
        <span class="n">relative_position_bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_matrix</span><span class="p">(</span>
            <span class="n">relative_position</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">),</span>
            <span class="n">num_buckets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_table</span><span class="p">,</span>
                                 <span class="n">relative_position_bucket</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span>


<span class="k">class</span> <span class="nc">T5MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        T5 multi head attention</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">kv_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">has_relative_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">is_decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">is_cross_atten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">T5MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_ascend</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_mode</span> <span class="o">=</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_bias</span> <span class="o">=</span> <span class="n">has_relative_bias</span>
        <span class="k">if</span> <span class="n">hidden_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">hidden_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5MultiHeadAttention&#39;, the class variable &#39;hidden_dropout_rate&#39; must be &quot;</span>
                             <span class="s2">&quot;in range [0, 1.0), but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_dropout_rate</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">attention_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">attention_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5MultiHeadAttention&#39;, the class variable &#39;attention_dropout_rate&#39; must be &quot;</span>
                             <span class="s2">&quot;in range [0, 1.0), but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attention_dropout_rate</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5MultiHeadAttention&#39;, the class variable &#39;d_model&#39; must be a multiple &quot;</span>
                             <span class="s2">&quot;of &#39;num_heads&#39;, but got the d_model is </span><span class="si">{}</span><span class="s2"> and the num_heads is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5MultiHeadAttention&#39;, the class variable &#39;num_heads&#39; must be a multiple of &quot;</span>
                             <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;and the parallel_config.model_parallel  is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_mode</span> <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;T5MultiHeadAttention&#39;, the class variable &#39;batch_size&#39; must be a multiple of &quot;</span>
                             <span class="s2">&quot;&#39;parallel_config.data_parallel&#39;, but got the batch_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;and the parallel_config.data_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">kv_size</span>
        <span class="c1"># Output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span><span class="p">,</span>
                                 <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                 <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
                              <span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                               <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="c1"># embedding size per head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">=</span> <span class="n">kv_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_k</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_v</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span>
            <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span>
        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="c1"># Normalize factor for attention, sqrt(dk) as widely used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

        <span class="c1"># Query</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span><span class="p">,</span>
                             <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                             <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                          <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>
        <span class="c1"># Key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span><span class="p">,</span>
                             <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                             <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                          <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>

        <span class="c1"># Value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span><span class="p">,</span>
                             <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                             <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                          <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">is_decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_bias</span> <span class="o">=</span> <span class="n">has_relative_bias</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_cross_atten</span> <span class="o">=</span> <span class="n">is_cross_atten</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cross_atten</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_generator</span> <span class="o">=</span> <span class="n">RelaPosEmbeddingsGenerator</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                                 <span class="n">max_relative_position</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                                                                 <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
                                                                 <span class="n">is_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cross_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;zero&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">]),</span>
                                            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cross_attention_bias&#39;</span><span class="p">,</span> <span class="n">parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># operators used for state reuse</span>
            <span class="n">seq_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">src_seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">range</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">seq_range</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">))),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">less</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">value_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward function for attention&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span>
                           <span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">ori_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_to_2d_tensor</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span>
                                                                                                   <span class="n">key_tensor</span><span class="p">,</span>
                                                                                                   <span class="n">value_tensor</span><span class="p">,</span>
                                                                                                   <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">ori_dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">query_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">key_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># multi head attention: query, key, value are derived from the same inputs</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="c1"># the returned shape is [bs, size_per_head, seq_length, num_heads]</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># the returned shape is [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="c1"># support input shape is [bs, seq, seq] or [bs, heads, seq, seq]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># expand attention mask from [bs, seq, seq] -&gt; [bs, 1, seq, seq]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># key and value for current token(s)</span>
        <span class="n">key_present</span> <span class="o">=</span> <span class="n">key</span>
        <span class="n">value_present</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># The first graph with the input size of (bs, seq_length)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
                <span class="c1"># Get the valid input length without padding</span>
                <span class="n">valid_length_vector</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="c1"># Cover the key and value numbers corresponding to the padding position</span>
                <span class="n">key_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">value_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="c1"># The second graph with the inpus size of (bs, 1)</span>
            <span class="c1"># the shape of query is (bs, num_heads, 1, size_per_head)</span>
            <span class="c1"># the shape of key is   (bs, num_heads, size_per_head, 1)</span>
            <span class="c1"># the shape of value is (bs, num_heads, 1, size_per_head)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Get the current token position index</span>
                <span class="n">valid_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                                                               <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                                                <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">),</span>
                                                                               <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                                    <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="n">valid_length</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">valid_length</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">valid_length_vector</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">valid_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="c1"># Pad the key and value to seq_length with only the position index not zero</span>
                <span class="n">current_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)),</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">current_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="c1"># Concat the previous saved state and current state</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="n">current_key</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value_past</span><span class="p">,</span> <span class="n">current_value</span><span class="p">)</span>
                <span class="c1"># Update key_present and value_present for state update</span>
                <span class="n">key_present</span> <span class="o">=</span> <span class="n">key</span>
                <span class="n">value_present</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">layer_present</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
        <span class="c1"># multi head attention considering attention mask</span>
        <span class="c1"># the return shape is [bs * seq_length, d_model]</span>
        <span class="n">attention</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="c1"># Output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ori_shape</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ori_dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">bias</span>

    <span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">value_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">key_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">value_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value_past</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">key_is_default</span> <span class="o">=</span> <span class="n">key_past</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">value_is_default</span> <span class="o">=</span> <span class="n">value_past</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">key_is_tensor</span><span class="p">,</span>
                                    <span class="n">key_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">value_is_tensor</span><span class="p">,</span>
                                    <span class="n">value_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_convert_to_2d_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;convert a nd tensor to a 2d tensor&quot;&quot;&quot;</span>
        <span class="n">query_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">query_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">query_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">key_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">key_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">key_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">value_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">)</span>
        <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">value_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">query_shape</span>

    <span class="k">def</span> <span class="nf">_merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        convert a 4d input to a 2d output</span>

<span class="sd">        Inputs:</span>
<span class="sd">            x: input tensor</span>

<span class="sd">        Output:</span>
<span class="sd">            x_merge: the 2d output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># bs, seq_length, head, size_per_head</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_merge</span>

    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_scores</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For the consideration of the performance, do softmax according to different situations</span>
<span class="sd">        :param attention_scores: a 3d tensor before softmax</span>
<span class="sd">        :return: the attention scores.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_ascend</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_ascend</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
            <span class="c1"># attention probs</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span><span class="p">(</span>
                <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span>
                          <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_probs</span>

    <span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the weighted score along the seq_length</span>

<span class="sd">        Inputs:</span>
<span class="sd">            query: the query matrix</span>
<span class="sd">            key: the key matrix</span>
<span class="sd">            value: the value matrix</span>
<span class="sd">            attention_mask: the attention mask matrix with shape (batch_size,</span>
<span class="sd">            1, seq_length, seq_length)</span>
<span class="sd">        Outputs:</span>
<span class="sd">            weighted_values: Tensor, the weighted sum scores</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Normalize query and key before MatMul, default off</span>
        <span class="c1"># Attention score [bs, num_heads, seq_length, seq_length]</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

        <span class="n">ori_dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span><span class="p">)</span>

        <span class="c1"># for input size of (bs, 1) namely the second graph,</span>
        <span class="c1"># the shape of attention_mask matrix should be (bs, 1, 1, seq_length)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
            <span class="c1"># Calculate the current total token</span>
            <span class="n">current_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                                                            <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">),</span>
                                                                            <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                                 <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="c1"># Get the precise position index</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">current_index</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="c1"># Calculate the attention_mask matrix via the position index</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">,</span> <span class="n">index</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cross_atten</span><span class="p">:</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_generator</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">score</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">score</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">cross_bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="c1"># Minus 10000 for the position where masked to exclude them from softmax</span>
        <span class="n">multiplu_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
            <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">F</span><span class="o">.</span><span class="n">tuple_to_array</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,)),</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)),</span>
            <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)))</span>

        <span class="n">adder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">multiplu_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span><span class="p">)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">adder</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
        <span class="c1"># attention probs</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">ori_dtype</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
        <span class="c1"># Weighted sum output [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">weighted_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">weighted_values</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_merge</span><span class="p">,</span> <span class="n">bias</span>


<span class="k">class</span> <span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Encoder Layer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="p">,</span>
                 <span class="n">kv_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;num_heads&#39; must be divisibled by the &quot;</span>
                <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;d_model&#39; must be divisibled by &quot;</span>
                <span class="s2">&quot;the &#39;parallel_config.model_parallel&#39;, but got the d_model is </span><span class="si">{}</span><span class="s2"> and parallel_config.&quot;</span>
                <span class="s2">&quot; model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;ffn_hidden_size&#39; must be divisibled &quot;</span>
                <span class="s2">&quot;by the &#39;parallel_config.model_parallel&#39;, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;and parallel_config. model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                                    <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">T5MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                              <span class="n">src_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                              <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                              <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                              <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                              <span class="n">kv_size</span><span class="o">=</span><span class="n">kv_size</span><span class="p">,</span>
                                              <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                              <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                              <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                              <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                              <span class="n">is_decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                              <span class="n">has_relative_bias</span><span class="o">=</span><span class="n">has_bias</span><span class="p">,</span>
                                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dpmp</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span>
                                              <span class="k">else</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                              <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                              <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                              <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Feed Forward Network, FFN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">T5FeedFoward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                       <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                       <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                       <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                       <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                       <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># operator used for state reuse</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="n">size_per_head</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
            <span class="c1"># parameters saving key and value states</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward function of the EncoderLayer&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># indicate whether reset saved states</span>
        <span class="n">key_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">value_reset</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># reset states, init_reset True for reuse and False for reset</span>
            <span class="n">key_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">value_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="n">attention</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span>
                                                        <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># For post-layernorm the inputs for residual path are output of self-attention and output of layernorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># For pre-layernorm the inputs for residual path are output of self-attention and input of this layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>

        <span class="n">output_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">aux_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">mlp_logit</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>

        <span class="n">value_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">key_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># current key and value</span>
            <span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span> <span class="o">=</span> <span class="n">layer_present</span>
            <span class="c1"># update key and value calculated this step</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">key_present</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">key_update</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">value_update</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="c1"># add dependency for desired execution order</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">value_update</span><span class="p">)</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">key_update</span><span class="p">)</span>

        <span class="c1"># if shape is 3d, we reshape the inputs of the add</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">aux_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">bias</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">init_reset_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">init_reset_is_default</span> <span class="o">=</span> <span class="n">init_reset</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_reset_is_tensor</span><span class="p">,</span>
                                    <span class="n">init_reset_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="k">class</span> <span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The Transformer Decoder Layer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">kv_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;num_heads&#39; must be divisibled by &quot;</span>
                             <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                             <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span>
                                                                            <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;d_model&#39; must be divisibled by &quot;</span>
                <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the d_model is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;ffn_hidden_size&#39; must be &quot;</span>
                             <span class="s2">&quot;divisibled by &#39;parallel_config.model_parallel&#39;, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;and parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_past</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support use_past=True.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_compute_type</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">T5MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                              <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                              <span class="n">kv_size</span><span class="o">=</span><span class="n">kv_size</span><span class="p">,</span>
                                              <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                              <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                              <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                              <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                              <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                              <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                              <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                              <span class="n">has_relative_bias</span><span class="o">=</span><span class="n">has_bias</span><span class="p">,</span>
                                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dpmp</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span>
                                              <span class="k">else</span> <span class="n">parallel_config</span><span class="p">)</span>

        <span class="c1"># Cross attention with the output of encoder as memory tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">T5MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                    <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                    <span class="n">kv_size</span><span class="o">=</span><span class="n">kv_size</span><span class="p">,</span>
                                                    <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                    <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                    <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                    <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                    <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                    <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                    <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                    <span class="n">is_cross_atten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                    <span class="n">has_relative_bias</span><span class="o">=</span><span class="n">has_bias</span><span class="p">,</span>
                                                    <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                    <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dpmp</span>
                                                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="k">else</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span>
            <span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                              <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                              <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                              <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Feed Forward Network, FFN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">T5FeedFoward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                       <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                       <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                       <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                       <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                       <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># operator used for state reuse</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="n">size_per_head</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
            <span class="c1"># parameters saving key and value states</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_stats</span><span class="p">,</span>
                  <span class="n">decoder_mask</span><span class="p">,</span>
                  <span class="n">encoder_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">self_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">encoder_attention_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The forward function of the decoder layer&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="n">decoder_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, seq_length, embedding_size] or [bs * seq_length, embedding_size]</span>
        <span class="n">hidden_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">)</span>
        <span class="n">hidden_stats</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># indicate whether reset saved states</span>
        <span class="n">key_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">value_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># reset states, init_reset True for reuse and False for reset</span>
            <span class="n">key_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">value_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="n">attention</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">self_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">decoder_mask</span><span class="p">,</span> <span class="n">self_bias</span><span class="p">,</span>
                                                             <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span>
                                                             <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># For post-layernorm the inputs for residual path are output of self-attention and output of layernorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># For pre-layernorm the inputs for residual path are output of self-attention and input of this layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>

        <span class="n">middle_output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">encoder_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">middle_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">middle_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">cross_attn_out</span><span class="p">,</span> <span class="n">cross_layer_present</span><span class="p">,</span> <span class="n">encoder_attention_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span>
                                                                                               <span class="n">encoder_output</span><span class="p">,</span>
                                                                                               <span class="n">encoder_output</span><span class="p">,</span>
                                                                                               <span class="n">memory_mask</span><span class="p">,</span>
                                                                                               <span class="n">encoder_attention_bias</span><span class="p">,</span>
                                                                                               <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span>
                                                                                               <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span>
                                                                                               <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">layer_present</span> <span class="o">+=</span> <span class="n">cross_layer_present</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="n">cross_attn_out</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cross_attn_out</span><span class="p">)</span>

        <span class="n">output_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">aux_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">mlp_logit</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>

        <span class="n">value_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">key_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># current key and value</span>
            <span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span> <span class="o">=</span> <span class="n">layer_present</span>
            <span class="c1"># update key and value calculated this step</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">key_present</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">key_update</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">value_update</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="c1"># add dependency for desired execution order</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">value_update</span><span class="p">)</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">key_update</span><span class="p">)</span>

        <span class="c1"># if shape is 3d, we reshape the inputs of the add</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">aux_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">self_bias</span><span class="p">,</span> <span class="n">encoder_attention_bias</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">),</span> <span class="s2">&quot;encoder_output&quot;</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">memory_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">memory_mask</span><span class="p">),</span> <span class="s2">&quot;memory_mask&quot;</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">init_reset_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">init_reset_is_default</span> <span class="o">=</span> <span class="n">init_reset</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_reset_is_tensor</span><span class="p">,</span>
                                    <span class="n">init_reset_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_get_lambda_func</span><span class="p">(</span><span class="n">total_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper function of specifying pipeline stage and gradient aggregation fusion. If the total layer</span>
<span class="sd">    is not None, for example, set in the transformer model, the pipeline stage setting function will be</span>
<span class="sd">    `(layer_id + 0) // (total_layers / parallel_config.pipeline_stage)` for the encoder and,</span>
<span class="sd">    `(layer_id + offset) //</span>
<span class="sd">    (total_layers / parallel_config.pipeline_stage)` for the decoder, where `offset` is the layers in the encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_set_parallel_configure_for_layer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>

<span class="sd">        Args:</span>
<span class="sd">            network(Cell) - Represents the transformer block</span>
<span class="sd">            layer_id(int) - Means the layer index for the current module, counts from zero.</span>
<span class="sd">            offset(int) - Means the layer_index needs an offset, if there are other modules in the net.</span>
<span class="sd">            layers(int) - The total layers used for the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># override the layers</span>
        <span class="k">if</span> <span class="n">total_layer</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="n">total_layer</span>
        <span class="c1"># Used for the pipeline&#39;s stages setting</span>
        <span class="k">if</span> <span class="n">layers</span> <span class="o">&lt;</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layers </span><span class="si">{</span><span class="n">layers</span><span class="si">}</span><span class="s2"> must be larger than pipeline stage </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">pp_dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">layers</span> <span class="o">/</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># the pipeline stage must be in [0, parallel_config.pipeline_stage - 1]</span>
        <span class="n">pp_id</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_dis</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="n">pp_id</span>

        <span class="c1"># Used for optimizer&#39;s fusion tag</span>
        <span class="n">dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">layers</span> <span class="o">/</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">gradient_aggregation_group</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">dis</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Used for enabling recomputation of the block</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="p">:</span>
                <span class="n">network</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">recompute</span><span class="p">:</span>
                <span class="n">paralel_op_comm_compute</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">parallel_optimizer_comm_recompute</span>
                <span class="n">network</span><span class="o">.</span><span class="n">recompute</span><span class="p">(</span><span class="n">parallel_optimizer_comm_recompute</span><span class="o">=</span><span class="n">paralel_op_comm_compute</span><span class="p">,</span>
                                  <span class="n">mp_comm_recompute</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">mp_comm_recompute</span><span class="p">,</span>
                                  <span class="n">recompute_slice_activation</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">recompute_slice_activation</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_set_parallel_configure_for_layer</span>


<span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The TransformerEncoder Cell&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">kv_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                            <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                            <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                            <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                            <span class="n">kv_size</span><span class="o">=</span><span class="n">kv_size</span><span class="p">,</span>
                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                            <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                            <span class="n">has_bias</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
                                            <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">,</span>
                                            <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                            <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                            <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                            <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">moe_parallel_config</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span>
                                            <span class="k">else</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span><span class="p">)</span>
            <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

            <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                        <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For parallel mode, sharding propagation is recommended, you can use it by setting &quot;</span>
                           <span class="s2">&quot;&#39;set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL, &quot;</span>
                           <span class="s2">&quot;search_mode=</span><span class="se">\&quot;</span><span class="s2">sharding_propagation</span><span class="se">\&quot;</span><span class="s2">)&#39; and &quot;</span>
                           <span class="s2">&quot;&#39;set_algo_parameters(elementwise_op_strategy_follow=False, fully_use_devices=False)&#39;&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The forward process of the encoder&quot;&quot;&quot;</span>
        <span class="n">present_layer</span> <span class="o">=</span> <span class="p">()</span>
        <span class="n">attention_bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                  <span class="n">attention_mask</span><span class="p">,</span>
                                                                  <span class="n">init_reset</span><span class="p">,</span>
                                                                  <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span><span class="p">,</span> <span class="n">accum_loss</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">attention_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                    <span class="n">attention_mask</span><span class="p">,</span>
                                                                    <span class="n">attention_bias</span><span class="p">,</span>
                                                                    <span class="n">init_reset</span><span class="p">,</span>
                                                                    <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span>


<span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The TransformerDecoder cell&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">kv_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                            <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                            <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                            <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                            <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                            <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                            <span class="n">kv_size</span><span class="o">=</span><span class="n">kv_size</span><span class="p">,</span>
                                            <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                            <span class="n">has_bias</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
                                            <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                            <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                            <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="n">layer_norm_epsilon</span><span class="p">,</span>
                                            <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">moe_parallel_config</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span>
                                            <span class="k">else</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span><span class="p">)</span>
            <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

            <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                        <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For parallel mode, sharding propagation is recommended, you can use it by setting &quot;</span>
                           <span class="s2">&quot;&#39;set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL, &quot;</span>
                           <span class="s2">&quot;search_mode=</span><span class="se">\&quot;</span><span class="s2">sharding_propagation</span><span class="se">\&quot;</span><span class="s2">)&#39; and &quot;</span>
                           <span class="s2">&quot;&#39;set_algo_parameters(elementwise_op_strategy_follow=False, fully_use_devices=False)&#39;&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;For forward process of the decoder&quot;&quot;&quot;</span>
        <span class="n">present_layer</span> <span class="o">=</span> <span class="p">()</span>
        <span class="n">self_bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">encoder_decoder_bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                  <span class="n">attention_mask</span><span class="p">,</span>
                                                                  <span class="n">encoder_output</span><span class="p">,</span>
                                                                  <span class="n">memory_mask</span><span class="p">,</span>
                                                                  <span class="n">init_reset</span><span class="p">,</span>
                                                                  <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span><span class="p">,</span> <span class="n">accum_loss</span>

        <span class="c1"># Loop through each self-attention layer</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">self_bias</span><span class="p">,</span> <span class="n">encoder_decoder_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                                     <span class="n">attention_mask</span><span class="p">,</span>
                                                                                     <span class="n">encoder_output</span><span class="p">,</span>
                                                                                     <span class="n">memory_mask</span><span class="p">,</span>
                                                                                     <span class="n">self_bias</span><span class="p">,</span>
                                                                                     <span class="n">encoder_decoder_bias</span><span class="p">,</span>
                                                                                     <span class="n">init_reset</span><span class="p">,</span>
                                                                                     <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span>


<span class="k">def</span> <span class="nf">position_encoding</span><span class="p">(</span><span class="n">length</span><span class="p">,</span>
                      <span class="n">depth</span><span class="p">,</span>
                      <span class="n">min_timescale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">max_timescale</span><span class="o">=</span><span class="mf">1e4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create Tensor of sinusoids of different frequencies.</span>

<span class="sd">    Args:</span>
<span class="sd">        length (int): Length of the Tensor to create, i.e. Number of steps.</span>
<span class="sd">        depth (int): Hidden size.</span>
<span class="sd">        min_timescale (float): Default: 1.</span>
<span class="sd">        max_timescale (float): Default: 10000.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape (length, depth)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">log_timescale_increment</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_timescale</span> <span class="o">/</span> <span class="n">min_timescale</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">inv_timescales</span> <span class="o">=</span> <span class="n">min_timescale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">log_timescale_increment</span><span class="p">)</span>
    <span class="n">scaled_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inv_timescales</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">scaled_time</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">scaled_time</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">EmbeddingPostprocessor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Postprocessors apply positional embeddings to word embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        embedding_size (int): The size of each embedding vector.</span>
<span class="sd">        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.</span>
<span class="sd">        max_position_embeddings (int): Maximum length of sequences used in this</span>
<span class="sd">                                 model. Default: 128.</span>
<span class="sd">        dropout_prob (float): The dropout probability. Default: 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embedding_size</span><span class="p">,</span>
                 <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                 <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingPostprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scores_mul</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">))],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_table</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">),</span>
                                               <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_embeddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Postprocessors apply positional embeddings to word embeddings.&quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">word_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scores_mul</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="k">class</span> <span class="nc">CastWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cast wrapper.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CastWrapper</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_type</span> <span class="o">=</span> <span class="n">dst_type</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dst_type</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CreateAttentionMaskFromInputMask</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create attention mask according to input mask.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (:class:`TransformerConfig`): Configuration for Transformer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CreateAttentionMaskFromInputMask</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create attention mask according to input mask.&quot;&quot;&quot;</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="n">shape_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">shape_left</span> <span class="o">=</span> <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

        <span class="n">input_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">mask_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">shape_left</span><span class="p">)</span>
        <span class="n">mask_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">shape_right</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">mask_left</span><span class="p">,</span> <span class="n">mask_right</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attention_mask</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">convert_np_to_tensor_encoder</span><span class="p">(</span><span class="n">seq_length</span><span class="p">):</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">T5Head</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Head to get the logits of each token in the vocab</span>
<span class="sd">    Args:</span>
<span class="sd">        config(): the config of network</span>
<span class="sd">    Inputs:</span>
<span class="sd">        state: the output of the backbone</span>
<span class="sd">        embedding_table: the embedding table of the vocabulary</span>
<span class="sd">    Returns:</span>
<span class="sd">        logits: Tensor, the logits of the corresponding inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">T5Head</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span>
                <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">embed</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">state</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="c1"># output logits over vocabulary [bs*seq_length, vocab_size]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">logits</span>


<span class="k">class</span> <span class="nc">T5Model</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    T5Model with encoder and decoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (Class): Configuration for T5Model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">T5Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_decode_length</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_decode_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_output</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_output</span>
        <span class="n">embedding_config</span> <span class="o">=</span> <span class="n">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span>
                                                     <span class="n">model_parallel</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tfm_embedding_lookup</span> <span class="o">=</span> <span class="n">VocabEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
                                                   <span class="n">embedding_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
                                                   <span class="n">parallel_config</span><span class="o">=</span><span class="n">embedding_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tfm_embedding_postprocessor_for_encoder</span> <span class="o">=</span> <span class="n">EmbeddingPostprocessor</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span>
                                                                              <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
                                                                              <span class="n">max_position_embeddings</span><span class="o">=</span>
                                                                              <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
                                                                              <span class="n">dropout_prob</span><span class="o">=</span>
                                                                              <span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tfm_embedding_postprocessor_for_decoder</span> <span class="o">=</span> <span class="n">EmbeddingPostprocessor</span><span class="p">(</span>
            <span class="n">embedding_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
            <span class="n">dropout_prob</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tfm_encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span>
            <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span>
            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">,</span>
            <span class="n">kv_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">kv_size</span><span class="p">,</span>
            <span class="n">hidden_act</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tfm_decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">src_seq_length</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span>
            <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">max_decode_length</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span>
            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">,</span>
            <span class="n">kv_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">kv_size</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_decoder_layers</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">num_decoder_layers</span> <span class="k">else</span> <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">hidden_act</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">T5Head</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
                                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
                                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast_compute_type</span> <span class="o">=</span> <span class="n">CastWrapper</span><span class="p">(</span><span class="n">dst_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layernorm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,),</span>
                                           <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layernorm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,),</span>
                                           <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layernorm</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layernorm</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_create_attention_mask_from_input_mask</span> <span class="o">=</span> <span class="n">CreateAttentionMaskFromInputMask</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">source_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">target_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">encoder_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;T5Model with encoder and decoder.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">source_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">source_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">source_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">source_ids</span><span class="p">)</span>
            <span class="n">source_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_attention_mask_from_input_mask</span><span class="p">(</span><span class="n">source_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">source_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_forward</span><span class="p">(</span><span class="n">source_ids</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">encoder_cache</span>

        <span class="k">if</span> <span class="n">target_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">encoder_output</span>

        <span class="c1"># process target sentence</span>
        <span class="n">tgt_embedding_output</span><span class="p">,</span> <span class="n">embedding_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tfm_embedding_lookup</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)</span>
        <span class="c1"># attention mask [batch_size, seq_length, seq_length]</span>
        <span class="n">tgt_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">memory_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">memory_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_memory_mask</span><span class="p">(</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target_mask</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">future_mask</span> <span class="o">=</span> <span class="n">convert_np_to_tensor_encoder</span><span class="p">(</span><span class="n">tgt_length</span><span class="p">)</span>
            <span class="n">tgt_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_attention_mask_from_input_mask</span><span class="p">(</span><span class="n">target_mask</span><span class="p">)</span>
            <span class="n">tgt_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">tgt_attention_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">future_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_attention_mask</span> <span class="o">=</span> <span class="n">target_mask</span>

        <span class="c1"># transformer decoder</span>
        <span class="n">decoder_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tfm_decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cast_compute_type</span><span class="p">(</span><span class="n">tgt_embedding_output</span><span class="p">),</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">cast_compute_type</span><span class="p">(</span><span class="n">tgt_attention_mask</span><span class="p">),</span>
                                             <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
        <span class="n">decoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layernorm</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_output</span><span class="p">:</span>
            <span class="n">decoder_output</span> <span class="o">=</span> <span class="n">decoder_output</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="c1"># calculate logits and log_probs</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">embedding_table</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_probs</span>

    <span class="k">def</span> <span class="nf">encoder_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_ids</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute the forward process&quot;&quot;&quot;</span>
        <span class="c1"># process source sentence</span>
        <span class="n">src_embedding_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tfm_embedding_lookup</span><span class="p">(</span><span class="n">source_ids</span><span class="p">)</span>
        <span class="c1"># attention mask [batch_size, seq_length, seq_length]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_mask</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">enc_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_attention_mask_from_input_mask</span><span class="p">(</span><span class="n">source_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">enc_attention_mask</span> <span class="o">=</span> <span class="n">source_mask</span>
        <span class="c1"># transformer encoder</span>
        <span class="n">encoder_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tfm_encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cast_compute_type</span><span class="p">(</span><span class="n">src_embedding_output</span><span class="p">),</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">cast_compute_type</span><span class="p">(</span><span class="n">enc_attention_mask</span><span class="p">))</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layernorm</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">encoder_output</span>

    <span class="k">def</span> <span class="nf">create_memory_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">):</span>
        <span class="n">memory_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()((</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">memory_mask</span> <span class="o">=</span> <span class="n">memory_mask</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">memory_mask</span> <span class="o">=</span> <span class="n">memory_mask</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">target_mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">memory_mask</span>


<div class="viewcode-block" id="T5ForConditionalGeneration"><a class="viewcode-back" href="../../../../models/mindformers.models.t5.T5ForConditionalGeneration.html#mindformers.models.t5.T5ForConditionalGeneration">[docs]</a><span class="nd">@MindFormerRegister</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">MindFormerModuleType</span><span class="o">.</span><span class="n">MODELS</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">T5ForConditionalGeneration</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A T5 model with the loss added.</span>

<span class="sd">    Args:</span>
<span class="sd">        config(T5Config) : The network of the transformer.</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindformers import T5ForConditionalGeneration, T5Tokenizer</span>
<span class="sd">        &gt;&gt;&gt; model = T5ForConditionalGeneration.from_pretrained(&#39;t5_small&#39;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = T5Tokenizer.from_pretrained(&#39;t5_small&#39;)</span>
<span class="sd">        &gt;&gt;&gt; src_output = tokenizer([&quot;hello world&quot;], padding=&#39;max_length&#39;, max_length=model.config.seq_length,</span>
<span class="sd">        ...                        return_tensors=&#39;ms&#39;)</span>
<span class="sd">        &gt;&gt;&gt; model_input = tokenizer([&quot;So happy to see you!&quot;], padding=&#39;max_length&#39;,</span>
<span class="sd">        ...                         max_length=model.config.max_decode_length,</span>
<span class="sd">        ...                         return_tensors=&#39;ms&#39;)[&quot;input_ids&quot;]</span>
<span class="sd">        &gt;&gt;&gt; input_ids = src_output[&#39;input_ids&#39;]</span>
<span class="sd">        &gt;&gt;&gt; attention_mask = src_output[&#39;attention_mask&#39;]</span>
<span class="sd">        &gt;&gt;&gt; output = model(input_ids, attention_mask, model_input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5.64458]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_support_list</span> <span class="o">=</span> <span class="n">MindFormerBook</span><span class="o">.</span><span class="n">get_model_support_list</span><span class="p">()[</span><span class="s1">&#39;t5&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">T5Config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">T5ForConditionalGeneration</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">parallel_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t5_model</span> <span class="o">=</span> <span class="n">T5Model</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">=</span><span class="n">OpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span>
                                                                      <span class="n">model_parallel</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>

        <span class="c1"># The value of start and end should get from the tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_token</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eod_token</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>

        <span class="c1"># disable the bias</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;bias&#39;</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span> <span class="ow">or</span> <span class="s1">&#39;beta&#39;</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="s1">&#39;relative&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_add_start_to_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_ids</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;concat the start id to the decoder inputs&quot;&quot;&quot;</span>
        <span class="n">start_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">start_token</span><span class="p">,</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">decoder_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">start_token</span><span class="p">,</span> <span class="n">target_ids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">decoder_inputs</span>

    <span class="k">def</span> <span class="nf">_add_eos_to_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_ids</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;concat the eos id to the end of the decoder inputs&quot;&quot;&quot;</span>
        <span class="n">eod_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eod_token</span><span class="p">,</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">inputs_with_eos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">target_ids</span><span class="p">,</span> <span class="n">eod_token</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">inputs_with_eos</span>

<div class="viewcode-block" id="T5ForConditionalGeneration.encoder_forward"><a class="viewcode-back" href="../../../../models/mindformers.models.t5.T5ForConditionalGeneration.html#mindformers.models.t5.T5ForConditionalGeneration.encoder_forward">[docs]</a>    <span class="k">def</span> <span class="nf">encoder_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_ids</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute the encoder forward process&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">t5_model</span><span class="o">.</span><span class="n">encoder_forward</span><span class="p">(</span><span class="n">source_ids</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">input_ids</span><span class="p">,</span>
                  <span class="n">attention_mask</span><span class="p">,</span>
                  <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">return_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;t5_model network with loss.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">decoder_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_start_to_inputs</span><span class="p">(</span><span class="n">labels</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t5_model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">decoder_input_ids</span><span class="p">,</span> <span class="n">decoder_attention_mask</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span>
                               <span class="n">encoder_cache</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">label_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">label_weights</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">decoder_attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">label_weights</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">total_loss</span>

        <span class="k">if</span> <span class="n">return_loss</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">total_loss</span>

        <span class="k">return</span> <span class="n">logits</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>