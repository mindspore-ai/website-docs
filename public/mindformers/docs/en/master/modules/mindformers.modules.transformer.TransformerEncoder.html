

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindformers.modules.transformer.TransformerEncoder &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindformers.modules.transformer.TransformerEncoderLayer" href="mindformers.modules.transformer.TransformerEncoderLayer.html" />
    <link rel="prev" title="mindformers.modules.transformer.TransformerDecoderLayer" href="mindformers.modules.transformer.TransformerDecoderLayer.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Installation Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_install.html">Confirm system environment information</a></li>
</ul>
<p class="caption"><span class="caption-text">BERT Fine adjustment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_bert_finetune.html">Use BERT tuning in Mindformer</a></li>
</ul>
<p class="caption"><span class="caption-text">API References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindformers.html">mindformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.core.html">mindformers.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.dataset.html">mindformers.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.models.html">mindformers.models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindformers.modules.html">mindformers.modules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindformers.modules.html#mindformers-modules-layers">mindformers.modules.layers</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindformers.modules.html#mindformers-modules-transformer">mindformers.modules.transformer</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.AttentionMask.html">mindformers.modules.transformer.AttentionMask</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.EmbeddingOpParallelConfig.html">mindformers.modules.transformer.EmbeddingOpParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.FeedForward.html">mindformers.modules.transformer.FeedForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.MoEConfig.html">mindformers.modules.transformer.MoEConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.MultiHeadAttention.html">mindformers.modules.transformer.MultiHeadAttention</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.OpParallelConfig.html">mindformers.modules.transformer.OpParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.Transformer.html">mindformers.modules.transformer.Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerDecoder.html">mindformers.modules.transformer.TransformerDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerDecoderLayer.html">mindformers.modules.transformer.TransformerDecoderLayer</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindformers.modules.transformer.TransformerEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerEncoderLayer.html">mindformers.modules.transformer.TransformerEncoderLayer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerOpParallelConfig.html">mindformers.modules.transformer.TransformerOpParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerRecomputeConfig.html">mindformers.modules.transformer.TransformerRecomputeConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.VocabEmbedding.html">mindformers.modules.transformer.VocabEmbedding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.pipeline.html">mindformers.pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.trainer.html">mindformers.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.wrapper.html">mindformers.wrapper</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindformers.modules.html">mindformers.modules</a> &raquo;</li>
        
      <li>mindformers.modules.transformer.TransformerEncoder</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/modules/mindformers.modules.transformer.TransformerEncoder.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindformers-modules-transformer-transformerencoder">
<h1>mindformers.modules.transformer.TransformerEncoder<a class="headerlink" href="#mindformers-modules-transformer-transformerencoder" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindformers.modules.transformer.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">mindformers.modules.transformer.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/modules/transformer/transformer.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.modules.transformer.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Encoder module with multi-layer stacked of <cite>TransformerEncoderLayer</cite>, including multihead self
attention and feedforward layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor when do increnmental prediction. Should be a positive
value. When do training or prediction, the argument will not work and the user can just pass None to
the argument.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The layers of the <cite>TransformerEncoderLayer</cite></p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of bottleneck in the feedforward layer.</p></li>
<li><p><strong>seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The seq_length of the input tensor.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default: 0.1.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>nn.Cell</em>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. User can provide custom activition to the argument.
If user wants to run the net in the parallel mode, the custom activation must also provide
the <cite>activation_shard</cite> function. Please see the examples of the
class:<cite>mindformers.modules.transformer.FeedForward</cite>. Default: gelu.</p></li>
<li><p><strong>post_layernorm_residual</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Do residuals adds before the layernorm. Default False.</p></li>
<li><p><strong>layernorm_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the layernorm.
Should be mstype.float32 or mstype.float16. Default mstype.float32.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the softmax in the attention.
Should be mstype.float32 or mstype.float16. Default: mstype.float32.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module.
Should be mstype.float32 or mstype.float16. Default: mstype.float32.</p></li>
<li><p><strong>lambda_func</strong> (<em>function</em>) – A function can determine the fusion index,
pipeline stages and recompute attribute. If the
user wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a
function that accepts <cite>network</cite>, <cite>layer_id</cite>, <cite>offset</cite>, <cite>parallel_config</cite>, <cite>layers</cite>. The <cite>network(Cell)</cite>
represents the transformer block, <cite>layer_id(int)</cite> means the layer index for the current module, counts
from zero, <cite>offset(int)</cite> means the layer_index needs an offset, if there are other modules in the net.
The default setting for the pipeline is: <cite>(layer_id + offset) // (layers / pipeline_stage)</cite>.
Default: None.</p></li>
<li><p><strong>offset</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The initial layer index for the <cite>encoder</cite>. Used for setting the fusion id and stage id, to not
overlap with the encoder layer. Default 0.</p></li>
<li><p><strong>use_past</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use the past state to compute, used for incremental prediction. For example, if we have two
words and want to generate the ten more words. We just need to compute the two words’ state only once,
and generate the next word one by one. When use_past is True, there are two steps to run the prediction.
In the first step, set the is_first_iteration to be True by
<cite>model.add_flags_recursive(is_first_iteration=True)</cite>, and pass the full inputs. Then, set the
is_first_iteration to be False by <cite>model.add_flags_recursive(is_first_iteration=False)</cite>. At this moment,
pass the single step’s input tensor, and loop it. Default: False.</p></li>
<li><p><strong>moe_config</strong> (<a class="reference internal" href="mindformers.modules.transformer.MoEConfig.html#mindformers.modules.transformer.MoEConfig" title="mindformers.modules.transformer.MoEConfig"><em>MoEConfig</em></a>) – The configuration of MoE (Mixture of Expert). Default is an instance of MoEConfig
with default values. Please see <cite>MoEConfig</cite>.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="mindformers.modules.transformer.TransformerOpParallelConfig.html#mindformers.modules.transformer.TransformerOpParallelConfig" title="mindformers.modules.transformer.TransformerOpParallelConfig"><em>TransformerOpParallelConfig</em></a>) – The parallel configure. Default <cite>default_transformer_config</cite>,
an instance of <cite>TransformerOpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>hidden_states</strong> (Tensor) - Tensor, shape should be [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,
should be [batch_size, 1, hidden_size].</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - Float Tensor, If the use_past is False or is_first_iteration=True,
the attention mask matrix should ba [batch_size, seq_length, seq_length], or None. None means there will
be no mask in softmax computation. Otherwise, should be [batch_size, 1, hidden_size]</p></li>
<li><p><strong>init_reset</strong> (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.
Used for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - The float tensor of the output of the layer with
shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is
False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</p></li>
<li><p><strong>layer_present</strong> (Tuple) - A tuple with size of num_layers, where each tuple contains the Tensor the
projected key and value vector with shape ((batch_size, num_heads, size_per_head, seq_length),
and (batch_size, num_heads, seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.modules.transformer</span> <span class="kn">import</span> <span class="n">TransformerEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">past</span><span class="p">))</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the full sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindformers.modules.transformer.TransformerEncoderLayer.html" class="btn btn-neutral float-right" title="mindformers.modules.transformer.TransformerEncoderLayer" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindformers.modules.transformer.TransformerDecoderLayer.html" class="btn btn-neutral float-left" title="mindformers.modules.transformer.TransformerDecoderLayer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>