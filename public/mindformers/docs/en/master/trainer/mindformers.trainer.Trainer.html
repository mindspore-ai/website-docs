

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindformers.trainer.Trainer &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindformers.trainer.TrainingArguments" href="mindformers.trainer.TrainingArguments.html" />
    <link rel="prev" title="mindformers.trainer.TokenClassificationTrainer" href="mindformers.trainer.TokenClassificationTrainer.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Installation Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_install.html">Confirm system environment information</a></li>
</ul>
<p class="caption"><span class="caption-text">BERT Fine adjustment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_bert_finetune.html">Use BERT tuning in Mindformer</a></li>
</ul>
<p class="caption"><span class="caption-text">API References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindformers.html">mindformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.core.html">mindformers.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.dataset.html">mindformers.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.models.html">mindformers.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.modules.html">mindformers.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.pipeline.html">mindformers.pipeline</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindformers.trainer.html">mindformers.trainer</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.BaseArgsConfig.html">mindformers.trainer.BaseArgsConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.CheckpointConfig.html">mindformers.trainer.CheckpointConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.CloudConfig.html">mindformers.trainer.CloudConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.ConfigArguments.html">mindformers.trainer.ConfigArguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.ContextConfig.html">mindformers.trainer.ContextConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.ContrastiveLanguageImagePretrainTrainer.html">mindformers.trainer.ContrastiveLanguageImagePretrainTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.DataLoaderConfig.html">mindformers.trainer.DataLoaderConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.DatasetConfig.html">mindformers.trainer.DatasetConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.ImageClassificationTrainer.html">mindformers.trainer.ImageClassificationTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.LRConfig.html">mindformers.trainer.LRConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.MaskedImageModelingTrainer.html">mindformers.trainer.MaskedImageModelingTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.MaskedLanguageModelingTrainer.html">mindformers.trainer.MaskedLanguageModelingTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.OptimizerConfig.html">mindformers.trainer.OptimizerConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.ParallelContextConfig.html">mindformers.trainer.ParallelContextConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.QuestionAnsweringTrainer.html">mindformers.trainer.QuestionAnsweringTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.RunnerConfig.html">mindformers.trainer.RunnerConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.TextClassificationTrainer.html">mindformers.trainer.TextClassificationTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.TokenClassificationTrainer.html">mindformers.trainer.TokenClassificationTrainer</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">mindformers.trainer.Trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.TrainingArguments.html">mindformers.trainer.TrainingArguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.TranslationTrainer.html">mindformers.trainer.TranslationTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.WrapperConfig.html">mindformers.trainer.WrapperConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindformers.trainer.ZeroShotImageClassificationTrainer.html">mindformers.trainer.ZeroShotImageClassificationTrainer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.wrapper.html">mindformers.wrapper</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindformers.trainer.html">mindformers.trainer</a> &raquo;</li>
        
      <li>mindformers.trainer.Trainer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/trainer/mindformers.trainer.Trainer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindformers-trainer-trainer">
<h1>mindformers.trainer.Trainer<a class="headerlink" href="#mindformers-trainer-trainer" title="Permalink to this headline">Â¶</a></h1>
<dl class="class">
<dt id="mindformers.trainer.Trainer">
<em class="property">class </em><code class="sig-prename descclassname">mindformers.trainer.</code><code class="sig-name descname">Trainer</code><span class="sig-paren">(</span><em class="sig-param">args: Optional[Union[str</em>, <em class="sig-param">dict</em>, <em class="sig-param">ConfigArguments</em>, <em class="sig-param">TrainingArguments]] = None</em>, <em class="sig-param">task: Optional[str] = &quot;general&quot;</em>, <em class="sig-param">model: Optional[Union[str</em>, <em class="sig-param">Cell</em>, <em class="sig-param">BaseModel]] = None</em>, <em class="sig-param">train_dataset: Optional[Union[str</em>, <em class="sig-param">BaseDataset]] = None</em>, <em class="sig-param">eval_dataset: Optional[Union[str</em>, <em class="sig-param">BaseDataset]] = None</em>, <em class="sig-param">tokenizer: Optional[BaseTokenizer] = None</em>, <em class="sig-param">image_processor: Optional[BaseImageProcessor] = None</em>, <em class="sig-param">audio_processor: Optional[BaseAudioProcessor] = None</em>, <em class="sig-param">optimizers: Optional[Optimizer] = None</em>, <em class="sig-param">wrapper: Optional[TrainOneStepCell] = None</em>, <em class="sig-param">callbacks: Optional[Union[Callback</em>, <em class="sig-param">List[Callback]]] = None</em>, <em class="sig-param">eval_callbacks: Optional[Union[Callback</em>, <em class="sig-param">List[Callback]]] = None</em>, <em class="sig-param">compute_metrics: Optional[Union[dict</em>, <em class="sig-param">set]] = None</em>, <em class="sig-param">save_config: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Trainer package to trainevaluatepredict class.</p>
<p>The trainer interface is used to quickly start training, evaluation and predict
for integrated tasks. It also allows users to customize the model, optimizer, dataset,
tokenizer, processor, train_one_step, callback, and metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>, </em><a class="reference internal" href="mindformers.trainer.ConfigArguments.html#mindformers.trainer.ConfigArguments" title="mindformers.trainer.ConfigArguments"><em>ConfigArguments</em></a><em>, </em><a class="reference internal" href="mindformers.trainer.TrainingArguments.html#mindformers.trainer.TrainingArguments" title="mindformers.trainer.TrainingArguments"><em>TrainingArguments</em></a><em>]</em><em>]</em>) â The task config which is used to
configure the dataset, the hyper-parameter, optimizer, etc. It support yaml path or
config dict or ConfigArguments class.
Default: None.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) â The task name supported.
Please refer to <a class="reference external" href="https://gitee.com/mindspore/transformer#%E4%BB%8B%E7%BB%8D">https://gitee.com/mindspore/transformer#%E4%BB%8B%E7%BB%8D</a>.
Default: âgeneralâ.</p></li>
<li><p><strong>model</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>Cell</em><em>, </em><a class="reference internal" href="../models/mindformers.models.BaseModel.html#mindformers.models.BaseModel" title="mindformers.models.BaseModel"><em>BaseModel</em></a><em>]</em><em>]</em>) â The network for trainer.
It support model name supported or BaseModel or MindSpore Cell class.
Supported model name can refer to <a class="reference external" href="https://gitee.com/mindspore/transformer#%E4%BB%8B%E7%BB%8D">https://gitee.com/mindspore/transformer#%E4%BB%8B%E7%BB%8D</a>.
Default: None.</p></li>
<li><p><strong>train_dataset</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="../dataset/mindformers.dataset.BaseDataset.html#mindformers.dataset.BaseDataset" title="mindformers.dataset.BaseDataset"><em>BaseDataset</em></a><em>]</em><em>]</em>) â The training dataset. It support real dataset path or
BaseDateset class or MindSpore Dataset class.
Default: None.</p></li>
<li><p><strong>eval_dataset</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="../dataset/mindformers.dataset.BaseDataset.html#mindformers.dataset.BaseDataset" title="mindformers.dataset.BaseDataset"><em>BaseDataset</em></a><em>]</em><em>]</em>) â The evaluate dataset. It support real dataset path or
BaseDateset class or MindSpore Dataset class.
Default: None.</p></li>
<li><p><strong>tokenizer</strong> (<em>Optional</em><em>[</em><em>BaseTokenizer</em><em>]</em>) â The tokenizer for text preprocessing. It support BaseTokenizer class.
Default: None.</p></li>
<li><p><strong>image_processor</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../models/mindformers.models.BaseImageProcessor.html#mindformers.models.BaseImageProcessor" title="mindformers.models.BaseImageProcessor"><em>BaseImageProcessor</em></a><em>]</em>) â The processor for image preprocessing.
It support BaseImageProcessor class.
Default: None.</p></li>
<li><p><strong>audio_processor</strong> (<em>Optional</em><em>[</em><em>BaseAudioProcessor</em><em>]</em>) â The processor for audio preprocessing.
It support BaseAudioProcessor class.
Default: None.</p></li>
<li><p><strong>optimizers</strong> (<em>Optional</em><em>[</em><em>Optimizer</em><em>]</em>) â The training networkâs optimizer. It support Optimizer class of MindSpore.
Default: None.</p></li>
<li><p><strong>wrapper</strong> (<em>Optional</em><em>[</em><em>TrainOneStepCell</em><em>]</em>) â Wraps the <cite>network</cite> with the <cite>optimizer</cite>.
It support TrainOneStepCell class of MindSpore.
Default: None.</p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callback</em><em>, </em><em>List</em><em>[</em><em>Callback</em><em>]</em><em>]</em><em>]</em>) â The training callback function.
It support CallBack or CallBack List of MindSpore.
Default: None.</p></li>
<li><p><strong>eval_callbacks</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callback</em><em>, </em><em>List</em><em>[</em><em>Callback</em><em>]</em><em>]</em><em>]</em>) â The evaluate callback function.
It support CallBack or CallBack List of MindSpore.
Default: None.</p></li>
<li><p><strong>compute_metrics</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#set" title="(in Python v3.8)"><em>set</em></a><em>]</em><em>]</em>) â The metric of evaluating.
It support dict or set in MindSporeâs Metric class.
Default: None.</p></li>
<li><p><strong>save_config</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Save current the config of task. Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#KeyError" title="(in Python v3.8)"><strong>KeyError</strong></a> â If âtaskâ or âmodelâ not in supported trainer.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.dataset</span> <span class="kn">import</span> <span class="n">GeneratorDataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyDataLoader</span><span class="p">:</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">)]</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) input task name and model name to init trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">train_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) input config to init trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.trainer.config_args</span> <span class="kn">import</span> <span class="n">ConfigArguments</span><span class="p">,</span> <span class="n">OptimizerConfig</span><span class="p">,</span> \
<span class="gp">... </span>    <span class="n">RunnerConfig</span><span class="p">,</span> <span class="n">LRConfig</span><span class="p">,</span> <span class="n">WrapperConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">AdamWeightDecay</span><span class="p">,</span> <span class="n">WarmUpLR</span><span class="p">,</span> \
<span class="gp">... </span>    <span class="n">DynamicLossScaleUpdateCell</span><span class="p">,</span> <span class="n">TrainOneStepWithLossScaleCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">runner_config</span> <span class="o">=</span> <span class="n">RunnerConfig</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr_schedule_config</span> <span class="o">=</span> <span class="n">LRConfig</span><span class="p">(</span><span class="n">lr_type</span><span class="o">=</span><span class="s1">&#39;WarmUpLR&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim_config</span> <span class="o">=</span> <span class="n">OptimizerConfig</span><span class="p">(</span><span class="n">optim_type</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.009</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr_schedule_config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale</span> <span class="o">=</span> <span class="n">DynamicLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_window</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapper_config</span> <span class="o">=</span> <span class="n">WrapperConfig</span><span class="p">(</span><span class="n">wrapper_type</span><span class="o">=</span><span class="s1">&#39;TrainOneStepWithLossScaleCell&#39;</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">MyDataLoader</span><span class="p">(),</span> <span class="n">column_names</span><span class="o">=</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ConfigArguments</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2022</span><span class="p">,</span> <span class="n">runner_config</span><span class="o">=</span><span class="n">runner_config</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="n">optimizer</span><span class="o">=</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">runner_wrapper</span><span class="o">=</span><span class="n">wrapper_config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">args</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#3) input instance to init trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.models</span> <span class="kn">import</span> <span class="n">ViTForImageClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vit_model_with_loss</span> <span class="o">=</span> <span class="n">ViTForImageClassification</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">WarmUpLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">beta1</span><span class="o">=</span><span class="mf">0.009</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr_schedule</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">params</span><span class="o">=</span><span class="n">vit_model_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">(</span><span class="n">per_print_times</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">loss_cb</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="n">vit_model_with_loss</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">args</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">optimizers</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindformers.trainer.Trainer.build_network">
<code class="sig-name descname">build_network</code><span class="sig-paren">(</span><em class="sig-param">input_checkpoint: Optional[Union[str</em>, <em class="sig-param">bool]] = None</em>, <em class="sig-param">is_train: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.build_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.build_network" title="Permalink to this definition">Â¶</a></dt>
<dd><p>build network for trainer.</p>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">eval_checkpoint: Optional[Union[str</em>, <em class="sig-param">bool]] = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.evaluate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Evaluate task for Trainer.
This function is used to evaluate the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_checkpoint</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>]</em><em>]</em>) â Used to evaluate the weight of the network.
It support real checkpoint path or valid model name of mindformers or bool value.
if itâs true, the last checkpoint file saved from the previous training round is automatically used.
Default: False.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> â if eval_checkpoint is not bool or str type.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">eval_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1) default evaluate task to test model.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2) evaluate task to auto load the last checkpoint.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 3) evaluate task according to checkpoint path.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_checkpoint</span><span class="o">=</span><span class="s1">&#39;./output/rank_0/checkpoint/mindformers.ckpt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.get_eval_dataloader">
<code class="sig-name descname">get_eval_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.get_eval_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.get_eval_dataloader" title="Permalink to this definition">Â¶</a></dt>
<dd><p>get eval dataloader of mindspore.</p>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.get_last_checkpoint">
<code class="sig-name descname">get_last_checkpoint</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.get_last_checkpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.get_last_checkpoint" title="Permalink to this definition">Â¶</a></dt>
<dd><p>get last checkpoint for resuming or finetune.</p>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.get_train_dataloader">
<code class="sig-name descname">get_train_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.get_train_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.get_train_dataloader" title="Permalink to this definition">Â¶</a></dt>
<dd><p>get train dataloader of mindspore.</p>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">predict_checkpoint: Optional[Union[str</em>, <em class="sig-param">bool]] = None</em>, <em class="sig-param">input_data: Optional[Union[GeneratorDataset</em>, <em class="sig-param">Tensor</em>, <em class="sig-param">np.ndarray</em>, <em class="sig-param">Image</em>, <em class="sig-param">str</em>, <em class="sig-param">list]] = None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.predict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Predict task for Trainer.
This function is used to predict the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_checkpoint</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>]</em><em>]</em>) â Used to predict the weight of the network.
It support real checkpoint path or valid model name of mindformers or bool value.
if itâs true, the last checkpoint file saved from the previous training round is automatically used.
Default: False.</p></li>
<li><p><strong>input_data</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Tensor</em><em>, </em><em>np.ndarray</em><em>, </em><em>Image</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em><em>]</em>) â The predict data. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>predict result (dict).</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> â if predict_checkpoint is not bool or str type.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> â if input_data is not Tensor or np.ndarray or Image or str or list.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="s2">&quot;./sunflower.png&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1) predict task to auto load the last checkpoint.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predict_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2) predict task according to checkpoint path.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predict_checkpoint</span><span class="o">=</span><span class="s1">&#39;./output/rank_0/checkpoint/mindformers.ckpt&#39;</span><span class="p">,</span>
<span class="gp">... </span>                     <span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 3) download and auto load the checkpoint on obs and predict.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.save_config_to_yaml">
<code class="sig-name descname">save_config_to_yaml</code><span class="sig-paren">(</span><em class="sig-param">config: dict = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.save_config_to_yaml"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.save_config_to_yaml" title="Permalink to this definition">Â¶</a></dt>
<dd><p>save now config file to yaml file.</p>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.set_moe_config">
<code class="sig-name descname">set_moe_config</code><span class="sig-paren">(</span><em class="sig-param">expert_num=1</em>, <em class="sig-param">capacity_factor=1.1</em>, <em class="sig-param">aux_loss_factor=0.05</em>, <em class="sig-param">num_experts_chosen=1</em>, <em class="sig-param">expert_group_size=None</em>, <em class="sig-param">group_wise_a2a=False</em>, <em class="sig-param">comp_comm_parallel=False</em>, <em class="sig-param">comp_comm_parallel_degree=2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.set_moe_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.set_moe_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The configuration of MoE (Mixture of Expert).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expert_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The number of experts employed. Default: 1</p></li>
<li><p><strong>capacity_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) â The factor is used to indicate how much to expand expert capacity,
which is &gt;=1.0. Default: 1.1.</p></li>
<li><p><strong>aux_loss_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) â The factor is used to indicate how much the load balance loss (produced by the
router) to be added to the entire model loss, which is &lt; 1.0. Default: 0.05.</p></li>
<li><p><strong>num_experts_chosen</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The number of experts is chosen by each token and it should not be larger
than expert_num. Default: 1.</p></li>
<li><p><strong>expert_group_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The number of tokens in each data parallel group. Default: None. This parameter is
effective only when in AUTO_PARALLEL mode, and NOT SHARDING_PROPAGATION.</p></li>
<li><p><strong>group_wise_a2a</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Whether to enable group-wise alltoall communication, which can reduce communication
time by converting part of inter communication into intra communication. Default: False. This parameter
is effective only when model parallel &gt; 1 and data_parallel equal to expert parallel.</p></li>
<li><p><strong>comp_comm_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Whether to enable ffn compute and communication parallel, which can reduce pure
communicattion time by splitting and overlapping compute and communication. Default: False.</p></li>
<li><p><strong>comp_comm_parallel_degree</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The split number of compute and communication. The larger the numbers,
the more overlap there will be but will consume more memory. Default: 2. This parameter is effective
only when comp_comm_parallel enable.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">train_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">eval_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">set_moe_config</span><span class="p">(</span><span class="n">expert_num</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">capacity_factor</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">aux_loss_factor</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.set_parallel_config">
<code class="sig-name descname">set_parallel_config</code><span class="sig-paren">(</span><em class="sig-param">data_parallel=1</em>, <em class="sig-param">model_parallel=1</em>, <em class="sig-param">expert_parallel=1</em>, <em class="sig-param">pipeline_stage=1</em>, <em class="sig-param">micro_batch_num=1</em>, <em class="sig-param">optimizer_shard=False</em>, <em class="sig-param">gradient_aggregation_group=4</em>, <em class="sig-param">vocab_emb_dp=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.set_parallel_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.set_parallel_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>set_parallel_config for the setting global data parallel, model parallel and fusion group.
The parallel configure setting for Trainer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The data parallel way. The input data will be sliced into n parts for each layer
according to the data parallel way. Default: 1.</p></li>
<li><p><strong>model_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The model parallel way. The parameters of dense layers in MultiheadAttention and
FeedForward layer will be sliced according to the model parallel way. Default: 1.</p></li>
<li><p><strong>expert_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The expert parallel way. This is effective only when MoE (Mixture of Experts)
is applied. This value specifies the number of partitions to split the experts into.</p></li>
<li><p><strong>pipeline_stage</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The number of the pipeline stage. Should be a positive value. Default: 1.</p></li>
<li><p><strong>micro_batch_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The micro size of the batches for the pipeline training. Default: 1.</p></li>
<li><p><strong>optimizer_shard</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Whether to enable optimizer shard. Default False.</p></li>
<li><p><strong>gradient_aggregation_group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â The fusion group size of the optimizer state sharding. Default: 4.</p></li>
<li><p><strong>vocab_emb_dp</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Shard embedding in model parallel or data parallel. Default: True.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">train_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">eval_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">set_parallel_config</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.set_recompute_config">
<code class="sig-name descname">set_recompute_config</code><span class="sig-paren">(</span><em class="sig-param">recompute=False</em>, <em class="sig-param">parallel_optimizer_comm_recompute=False</em>, <em class="sig-param">mp_comm_recompute=True</em>, <em class="sig-param">recompute_slice_activation=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.set_recompute_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.set_recompute_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Set recompute config.
TransformerRecomputeConfig for the setting recompute attributes for encoder/decoder layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recompute</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Enable recomputation of the transformer block or not. Default: False.</p></li>
<li><p><strong>parallel_optimizer_comm_recompute</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Specifies whether the communication operator allgathers
introduced by optimizer shard are recomputed in auto parallel or semi auto parallel mode.
Default: False.</p></li>
<li><p><strong>mp_comm_recompute</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Specifies whether the model parallel communication operators
in the cell are recomputed in auto parallel or semi auto parallel mode. Default: True.</p></li>
<li><p><strong>recompute_slice_activation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Slice the cell output which would remains in memory. Default: False.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">train_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">eval_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">set_recompute_config</span><span class="p">(</span><span class="n">recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindformers.trainer.Trainer.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">resume_or_finetune_from_checkpoint: Optional[Union[str</em>, <em class="sig-param">bool]] = False</em>, <em class="sig-param">initial_epoch: int = 0</em>, <em class="sig-param">do_eval: bool = False</em>, <em class="sig-param">do_finetune: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/trainer/trainer.html#Trainer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindformers.trainer.Trainer.train" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Train task for Trainer.
This function is used to train or fine-tune the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>resume_or_finetune_from_checkpoint</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>]</em><em>]</em>) â Used to restore training or fine-tune the weight of the network.
It support real checkpoint path or valid model name of mindformers or bool value.
if itâs true, the last checkpoint file saved from the previous training round is automatically used.
if do_finetune is true, this checkpoint will be used to finetune the network.
Default: False.</p></li>
<li><p><strong>initial_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) â Epoch at which to start train, it used for resuming a previous training run.
Default: 0.</p></li>
<li><p><strong>do_eval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) â Whether evaluations are performed during training. Default: False.</p></li>
<li><p><strong>do_finetune</strong> â Whether to finetune network. When itâs true, resume_or_finetune_from_checkpoint must be input.
Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> â if resume_or_finetune_from_checkpoint is not bool or str type.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;image_classification&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">model</span><span class="o">=</span><span class="s1">&#39;vit_base_p16&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">train_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">eval_dataset</span><span class="o">=</span><span class="s1">&#39;data/imagenet/train&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1) default train task to reproduce model.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2) eval network when train task to reproduce model.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 3) resume train task to auto load the last checkpoint, if training break after 10 epochs.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_or_finetune_from_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initial_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 4) resume train task according to checkpoint path, if training break after 10 epochs.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">resume_or_finetune_from_checkpoint</span><span class="o">=</span><span class="s1">&#39;./output/rank_0/checkpoint/mindformers.ckpt&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">initial_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 5) finetune train task according to resume_or_finetune_from_checkpoint.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">task_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_or_finetune_from_checkpoint</span><span class="o">=</span><span class="s1">&#39;mae_vit_base_p16&#39;</span><span class="p">,</span> <span class="n">do_finetune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindformers.trainer.TrainingArguments.html" class="btn btn-neutral float-right" title="mindformers.trainer.TrainingArguments" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindformers.trainer.TokenClassificationTrainer.html" class="btn btn-neutral float-left" title="mindformers.trainer.TokenClassificationTrainer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>