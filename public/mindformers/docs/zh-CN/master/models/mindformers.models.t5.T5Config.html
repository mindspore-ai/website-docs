

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindformers.models.t5.T5Config &mdash; MindSpore master 文档</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/translations.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="mindformers.models.t5.T5ForConditionalGeneration" href="mindformers.models.t5.T5ForConditionalGeneration.html" />
    <link rel="prev" title="mindformers.models.bert.BertModel" href="mindformers.models.bert.BertModel.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_install.html">确认系统环境信息</a></li>
</ul>
<p class="caption"><span class="caption-text">BERT微调</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_bert_finetune.html">使用mindformers中的BERT微调</a></li>
</ul>
<p class="caption"><span class="caption-text">API参考</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindformers.html">mindformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.core.html">mindformers.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.dataset.html">mindformers.dataset</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindformers.models.html">mindformers.models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindformers.models.html#id1">mindformers.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindformers.models.html#mindformers-models-bert">mindformers.models.bert</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindformers.models.html#mindformers-models-t5">mindformers.models.t5</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindformers.models.t5.T5Config</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.models.t5.T5ForConditionalGeneration.html">mindformers.models.t5.T5ForConditionalGeneration</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.models.t5.T5Processor.html">mindformers.models.t5.T5Processor</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.models.t5.T5Tokenizer.html">mindformers.models.t5.T5Tokenizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindformers.models.html#mindformers-models-clip">mindformers.models.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindformers.models.html#mindformers-models-mae">mindformers.models.mae</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindformers.models.html#mindformers-models-swin">mindformers.models.swin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindformers.models.html#mindformers-models-vit">mindformers.models.vit</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.modules.html">mindformers.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.pipeline.html">mindformers.pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.trainer.html">mindformers.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.wrapper.html">mindformers.wrapper</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindformers.models.html">mindformers.models</a> &raquo;</li>
        
      <li>mindformers.models.t5.T5Config</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/models/mindformers.models.t5.T5Config.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindformers-models-t5-t5config">
<h1>mindformers.models.t5.T5Config<a class="headerlink" href="#mindformers-models-t5-t5config" title="永久链接至标题">¶</a></h1>
<dl class="class">
<dt id="mindformers.models.t5.T5Config">
<em class="property">class </em><code class="sig-prename descclassname">mindformers.models.t5.</code><code class="sig-name descname">T5Config</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int = 32128</em>, <em class="sig-param">d_model: int = 512</em>, <em class="sig-param">d_kv: int = 64</em>, <em class="sig-param">d_ff: int = 2048</em>, <em class="sig-param">num_layers: int = 6</em>, <em class="sig-param">num_decoder_layers=None</em>, <em class="sig-param">num_heads: int = 8</em>, <em class="sig-param">relative_attention_num_buckets: int = 32</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">layer_norm_epsilon: float = 1e-6</em>, <em class="sig-param">initializer_factor: float = 1.0</em>, <em class="sig-param">feed_froward_proj: str = &quot;relu&quot;</em>, <em class="sig-param">is_encoder_decoder=True</em>, <em class="sig-param">use_cache: bool = True</em>, <em class="sig-param">pad_token_id: int = 0</em>, <em class="sig-param">eos_token_id: int = 1</em>, <em class="sig-param"># The following are mindformers parameters                 batch_size: int = 1</em>, <em class="sig-param">seq_length: int = 1024</em>, <em class="sig-param">max_position_embeddings: int = 1024</em>, <em class="sig-param">initializer_range: float = 0.02</em>, <em class="sig-param">max_decode_length: int = 128</em>, <em class="sig-param">length_penalty_weight: float = 1.0</em>, <em class="sig-param">dtype: str = &quot;float32&quot;</em>, <em class="sig-param">compute_dtype: str = &quot;float32&quot;</em>, <em class="sig-param">has_relative_bias: bool = True</em>, <em class="sig-param">scale_output: bool = True</em>, <em class="sig-param">parallel_config: TransformerOpParallelConfig = None</em>, <em class="sig-param">checkpoint_name_or_path: str = None</em>, <em class="sig-param">top_p=0.95</em>, <em class="sig-param">top_k=1</em>, <em class="sig-param">repetition_penalty=1</em>, <em class="sig-param">max_length=20</em>, <em class="sig-param">do_sample=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/models/t5/t5_config.html#T5Config"><span class="viewcode-link">[源代码]</span></a><a class="headerlink" href="#mindformers.models.t5.T5Config" title="打开链接">¶</a></dt>
<dd><p>Config For T5 model</p>
<dl class="field-list simple">
<dt class="field-odd">参数</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The vocabuary size, it determines the size of embedding table and the final projection size.</p></li>
<li><p><strong>d_model</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The hidden size of the model. Default 512.</p></li>
<li><p><strong>d_kv</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The internal hidden size of the attention head. Default 64.</p></li>
<li><p><strong>d_ff</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The intermediate size of the T5 FFN layer. Default 2048.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The layers of the encoder and decoder parts. Default 6.</p></li>
<li><p><strong>num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The layers of the decoder. If not set, the value will
copy from num_layers. Default None.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The number of attention heads. Default 8.</p></li>
<li><p><strong>relative_attention_num_buckets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The size of relative attention buckets. Default 32.</p></li>
<li><p><strong>dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – The dropout rate of the hidden state. Default 0.1.</p></li>
<li><p><strong>layer_norm_epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – The layernorm epsilon. Default 1e-6.</p></li>
<li><p><strong>initializer_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – The factor of the weight initialization. Default 1.0.</p></li>
<li><p><strong>feed_froward_proj</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(在 Python v3.8)"><em>str</em></a>) – The type of activation. Default <cite>relu</cite>.</p></li>
<li><p><strong>is_encoder_decoder</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(在 Python v3.8)"><em>bool</em></a>) – Whether the current model is encdeor-decoder. Default True.</p></li>
<li><p><strong>use_cache</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(在 Python v3.8)"><em>bool</em></a>) – If use cache or not for inference. Default True.</p></li>
<li><p><strong>pad_token_id</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The pad token id. Default 0.</p></li>
<li><p><strong>eos_token_id</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The end of sentence token id. Default 1.</p></li>
<li><p><strong>max_position_embeddings</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The length of the position embedding. Default 1024.</p></li>
<li><p><strong>max_decode_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The sequence length of the decoder part. Default 128.</p></li>
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(在 Python v3.8)"><em>str</em></a>) – The initialization type of the parameters in T5 model. Default “float32”.</p></li>
<li><p><strong>compute_dtype</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(在 Python v3.8)"><em>str</em></a>) – The computation type of the dense layer in the T5 model. Default “float32”.</p></li>
<li><p><strong>has_relative_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(在 Python v3.8)"><em>bool</em></a>) – Whether add relative attention bias. Default True.</p></li>
<li><p><strong>scale_output</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(在 Python v3.8)"><em>bool</em></a>) – Whether to scale the output. Default True.</p></li>
<li><p><strong>seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The sequence length of the encoder part. Default 1024.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The batch size of the model. Default 1.</p></li>
<li><p><strong>checkpoint_name_or_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(在 Python v3.8)"><em>str</em></a>) – The path to the checkpoint. If set, it will load the checkpoint from the given
path.</p></li>
<li><p><strong>top_p</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – Used in the <cite>generate</cite> method of the BaseModel. Default 0.95. The accumulation probability of
the candidate token ids below the top_p will be select as the condaite ids. The validate the value of
top_p is between (0, 1]. If the value is larger than 1,</p></li>
<li><p><strong>top_k</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – Used in the <cite>generate</cite> method of the BaseModel. Determine the topK numbers token id as candidate.
This should be a positive number. Default 1.</p></li>
<li><p><strong>repetition_penalty</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – The penalty of the repeated words when call <cite>generate</cite> of the BaseModel.</p></li>
<li><p><strong>max_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The maximum length of the generated words. If set None, it follow the setting in the
configureation in the model. Default 20.</p></li>
<li><p><strong>do_sample</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(在 Python v3.8)"><em>bool</em></a>) – Do sampling on the candidate ids. If set True it will be enabled, and set it to be
False to disable the sampling, equivalent to topk 1. If set None, it follow the setting in the
configureation in the model. Default False.</p></li>
<li><p><strong>**kwargs</strong> – </p></li>
</ul>
</dd>
</dl>
<p class="rubric">样例</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">T5Config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">T5Config</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">40000</span><span class="p">)</span>
<span class="go">    {&#39;d_model&#39;: 256, &#39;vocab_size&#39;: 40000,</span>
<span class="go">     &#39;max_position_embeddings&#39;: 77, &#39;num_layers&#39;: 12}</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindformers.models.t5.T5ForConditionalGeneration.html" class="btn btn-neutral float-right" title="mindformers.models.t5.T5ForConditionalGeneration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindformers.models.bert.BertModel.html" class="btn btn-neutral float-left" title="mindformers.models.bert.BertModel" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>