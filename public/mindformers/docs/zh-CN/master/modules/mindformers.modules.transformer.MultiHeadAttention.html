

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindformers.modules.transformer.MultiHeadAttention &mdash; MindSpore master 文档</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/translations.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="mindformers.modules.transformer.OpParallelConfig" href="mindformers.modules.transformer.OpParallelConfig.html" />
    <link rel="prev" title="mindformers.modules.transformer.MoEConfig" href="mindformers.modules.transformer.MoEConfig.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_install.html">确认系统环境信息</a></li>
</ul>
<p class="caption"><span class="caption-text">BERT微调</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindformers_bert_finetune.html">使用mindformers中的BERT微调</a></li>
</ul>
<p class="caption"><span class="caption-text">API参考</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindformers.html">mindformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.core.html">mindformers.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.dataset.html">mindformers.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.models.html">mindformers.models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindformers.modules.html">mindformers.modules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindformers.modules.html#mindformers-modules-layers">mindformers.modules.layers</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindformers.modules.html#mindformers-modules-transformer">mindformers.modules.transformer</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.AttentionMask.html">mindformers.modules.transformer.AttentionMask</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.EmbeddingOpParallelConfig.html">mindformers.modules.transformer.EmbeddingOpParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.FeedForward.html">mindformers.modules.transformer.FeedForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.MoEConfig.html">mindformers.modules.transformer.MoEConfig</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindformers.modules.transformer.MultiHeadAttention</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.OpParallelConfig.html">mindformers.modules.transformer.OpParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.Transformer.html">mindformers.modules.transformer.Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerDecoder.html">mindformers.modules.transformer.TransformerDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerDecoderLayer.html">mindformers.modules.transformer.TransformerDecoderLayer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerEncoder.html">mindformers.modules.transformer.TransformerEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerEncoderLayer.html">mindformers.modules.transformer.TransformerEncoderLayer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerOpParallelConfig.html">mindformers.modules.transformer.TransformerOpParallelConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.TransformerRecomputeConfig.html">mindformers.modules.transformer.TransformerRecomputeConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindformers.modules.transformer.VocabEmbedding.html">mindformers.modules.transformer.VocabEmbedding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.pipeline.html">mindformers.pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.trainer.html">mindformers.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindformers.wrapper.html">mindformers.wrapper</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindformers.modules.html">mindformers.modules</a> &raquo;</li>
        
      <li>mindformers.modules.transformer.MultiHeadAttention</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/modules/mindformers.modules.transformer.MultiHeadAttention.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindformers-modules-transformer-multiheadattention">
<h1>mindformers.modules.transformer.MultiHeadAttention<a class="headerlink" href="#mindformers-modules-transformer-multiheadattention" title="永久链接至标题">¶</a></h1>
<dl class="class">
<dt id="mindformers.modules.transformer.MultiHeadAttention">
<em class="property">class </em><code class="sig-prename descclassname">mindformers.modules.transformer.</code><code class="sig-name descname">MultiHeadAttention</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindformers/modules/transformer/transformer.html#MultiHeadAttention"><span class="viewcode-link">[源代码]</span></a><a class="headerlink" href="#mindformers.modules.transformer.MultiHeadAttention" title="打开链接">¶</a></dt>
<dd><p>This is an implementation of multihead attention in the paper <a class="reference external" href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention is all you need</a>. Given the query vector with source length, and the
key and value vector with target length, the attention will be performed as the following</p>
<div class="math notranslate nohighlight">
\[MultiHeadAttention(query, key, vector) = Concat(head_1, \dots, head_h)W^O\]</div>
<p>where <span class="math notranslate nohighlight">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span>. The default is with a bias.</p>
<p>if query, key and value tensor is same, then it will be self attention.</p>
<dl class="field-list simple">
<dt class="field-odd">参数</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The batch size of the input tensor when do increnmental prediction. Should be a positive
value. When do training or prediction, the argument will not work and the user can just pass None to
the argument.</p></li>
<li><p><strong>src_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The sequence length of the query vector.</p></li>
<li><p><strong>tgt_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The sequence length of the key and value vector.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(在 Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(在 Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1.</p></li>
<li><p><strong>compute_dtype</strong> (<em>dtype.Number</em>) – The computation type of dense. Default mstype.float16.
Should be mstype.float32 or mstype.float16.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The type of softmax computation module. Default mstype.float32.
Should be mstype.float32 or mstype.float16.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module. Default mstype.float32.
Should be mstype.float32 or mstype.float16.</p></li>
<li><p><strong>use_past</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(在 Python v3.8)"><em>bool</em></a>) – Use the past state to compute, used for incremental prediction. For example, if we have two
words and want to generate the ten more words. We just need to compute the two words’ state only once,
and generate the next word one by one. When use_past is True, there are two steps to run the prediction.
In the first step, set the is_first_iteration to be True by
<cite>model.add_flags_recursive(is_first_iteration=True)</cite>, and pass the full inputs. Then, set the
is_first_iteration to be False by <cite>model.add_flags_recursive(is_first_iteration=False)</cite>. At this moment,
pass the single step’s input tensor, and loop it. Default False.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="mindformers.modules.transformer.OpParallelConfig.html#mindformers.modules.transformer.OpParallelConfig" title="mindformers.modules.transformer.OpParallelConfig"><em>OpParallelConfig</em></a>) – The parallel configure. Default <cite>default_dpmp_config</cite>,
an instance of <cite>OpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>query_tensor</strong> (Tensor) - The query vector with shape (batch_size, src_seq_length, hidden_size) or
(batch_size * src_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.
Otherwise, must be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>key_tensor</strong> (Tensor) - The key vector with shape (batch_size, tgt_seq_length, hidden_size) or
(batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.
Otherwise, must be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>value_tensor</strong> (Tensor) - The value vector with shape (batch_size, tgt_seq_length, hidden_size) or
(batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.
Otherwise, must be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - If the use_past is False or is_first_iteration=True, the attention mask
matrix should ba (batch_size, src_seq_length, tgt_seq_length), or None. None means there will be no mask
in softmax computation. Otherwise, the mask must be (batch_size, 1, tgt_seq_length)</p></li>
<li><p><strong>key_past</strong> (Tensor) - Float16 tensor with shape (batch_size, num_heads, size_per_head, tgt_seq_length).
The past calculated key vector. Used for incremental prediction when the use_past is True.
Default None.</p></li>
<li><p><strong>value_past</strong> (Tensor) - Float16 tensor with shape
(batch_size, num_heads, tgt_seq_length, size_per_head).
The past calculated value vector. Used for incremental prediction when the use_past is True.
Default None.</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape (batch_size,) the past calculated the index.
Used for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - Tensor, the float tensor of the output of the layer with
shape (batch_size, src_seq_length, hidden_size) or (batch_size * src_seq_length, hidden_size),
if the use_past is False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</p></li>
<li><p><strong>layer_present</strong> (Tuple) - A tuple of the Tensor of the projected key and value vector with
((batch_size, num_heads, size_per_head, tgt_seq_length),
(batch_size, num_heads, tgt_seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">样例</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindformers.modules.transformer</span> <span class="kn">import</span> <span class="n">MultiHeadAttention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">from_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We need to prepare the memory parameters for saving key and value states firstly.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span> <span class="n">value_past</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">from_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># full sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span> <span class="n">value_past</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindformers.modules.transformer.OpParallelConfig.html" class="btn btn-neutral float-right" title="mindformers.modules.transformer.OpParallelConfig" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindformers.modules.transformer.MoEConfig.html" class="btn btn-neutral float-left" title="mindformers.modules.transformer.MoEConfig" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>