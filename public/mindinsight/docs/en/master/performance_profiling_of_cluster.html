<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cluster Performance Profiling &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Debugger" href="debugger.html" />
    <link rel="prev" title="Performance Profiling (GPU)" href="performance_profiling_gpu.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_install.html">MindSpore Insight Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="summary_record.html">Collecting Summary Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="dashboard.html">Viewing Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="lineage_and_scalars_comparison.html">Viewing Lineage and Scalars Comparison</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="performance_profiling.html">Performance Profiling</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="performance_profiling_ascend.html">Performance Profiling (Ascend)</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_profiling_gpu.html">Performance Profiling (GPU)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Cluster Performance Profiling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operation-process">Operation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#collect-cluster-performance-data">Collect Cluster Performance Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launch-mindspore-insight">Launch MindSpore Insight</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-performance">Training Performance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cluster-iterative-trajectory-analysis">Cluster Iterative Trajectory Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-communication-and-computation-overlap-time-analysis">Cluster Communication and Computation Overlap Time Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-communication-performance-analysis">Cluster Communication Performance Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#specifications">specifications</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#resource-utilization">Resource Utilization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cluster-memory-analysis">Cluster Memory Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-flops-analysis">Cluster FLOPs Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#strategy-perception">Strategy Perception</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#graph-exploration-module">Graph Exploration Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operator-strategy-matrix">Operator Strategy Matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-pipeline">Training Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operator-stacking-and-edge-hiding">Operator Stacking and Edge Hiding</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#execution-overview">Execution Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#analysis-of-the-computational-graph">Analysis of the computational graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operators-execution-timeline-analysis-on-each-device">Operators execution timeline analysis on each device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#time-overview-of-each-device">Time overview of each device</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#specifications-1">Specifications</a></li>
<li class="toctree-l3"><a class="reference internal" href="#notices">Notices</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="debugger.html">Debugger</a></li>
<li class="toctree-l1"><a class="reference internal" href="landscape.html">Training Optimization Process Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_commands.html">MindSpore Insight Commands</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tuning Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="accuracy_problem_preliminary_location.html">Guide to Locating Accuracy Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="accuracy_optimization.html">Accuracy Problem Locating and Optimization Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="fixing_randomness.html">Fixed Randomness to Reproduce Run Results of Script</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">Performance Debugging Cases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindinsight.debugger.html">mindinsight.debugger</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="training_visual_design.html">Overall Design of Training Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_visual_design.html">Computational Graph Visualization Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_visual_design.html">Tensor Visualization Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler_design.html">Performance Profiling Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="performance_profiling.html">Performance Profiling</a> &raquo;</li>
      <li>Cluster Performance Profiling</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/performance_profiling_of_cluster.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="cluster-performance-profiling">
<h1>Cluster Performance Profiling<a class="headerlink" href="#cluster-performance-profiling" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindinsight/docs/source_en/performance_profiling_of_cluster.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This article describes how to use MindSpore Profiler for cluster performance debugging on Ascend/GPU AI processors, support for cluster training data collection is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Profiling Data</p></th>
<th class="head"><p>Support Device</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Cluster Iterative Trajectory</p></td>
<td><p>Ascend, GPU</p></td>
</tr>
<tr class="row-odd"><td><p>Cluster Communication and Computation Overlap Time</p></td>
<td><p>Ascend, GPU</p></td>
</tr>
<tr class="row-even"><td><p>Cluster Communication Performance</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>Resource Utilization</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-even"><td><p>Strategy Perception</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>Execution Analysis</p></td>
<td><p>Ascend</p></td>
</tr>
</tbody>
</table>
</section>
<section id="operation-process">
<h2>Operation Process<a class="headerlink" href="#operation-process" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Set up the distributed training environment, prepare a training script, add profiler APIs in the training script and run the training script.</p></li>
<li><p>Collect Cluster Performance Data.</p></li>
<li><p>Start MindSpore Insight and specify the summary-base-dir using startup parameters, note that summary-base-dir is the parent directory of the directory created by Profiler. For example, the directory created by Profiler is <code class="docutils literal notranslate"><span class="pre">/home/user/code/data/</span></code>, the summary-base-dir should be <code class="docutils literal notranslate"><span class="pre">/home/user/code</span></code>. After MindSpore Insight is started, access the visualization page based on the IP address and port number. The default access IP address is <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8080</span></code>.</p></li>
<li><p>Find the cluster training in the list, click the cluster performance profiling link and view the data on the web page.</p></li>
</ol>
<blockquote>
<div><p>The images in this article are from the Ascend AI processor, and the differences between devices will be explained separately.</p>
</div></blockquote>
</section>
<section id="collect-cluster-performance-data">
<h2>Collect Cluster Performance Data<a class="headerlink" href="#collect-cluster-performance-data" title="Permalink to this headline"></a></h2>
<p>In multi-server and multi-device training, after the cluster training, the performance data is distributed in each host node. To analyze the cluster performance, we need to collect the performance data of all host nodes to one host for analysis. Considering the complexity of the cluster running environment and the related permissions and login problems, a more reasonable way is to let users collect cluster performance data. The following is the process of using a script to collect performance data after a distributed cluster training. Users can refer to this script to collect cluster performance data.</p>
<p>Script program description: the script program first creates the cluster job folder, and then uses the SSHPass technology for non interactive remote copy (to avoid manual authentication, manually enter the password), copies the data of each host node in the cluster to the cluster job folder.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash collect_cluster_profiler_data.sh&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;for example: bash collect_cluster_profiler_data.sh cluster_hccl_config_path cluster_account_config_path cluster_train_id host_train_id is_absolute_path&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>

<span class="nv">SSH</span><span class="o">=</span><span class="s2">&quot;ssh -o StrictHostKeyChecking=no&quot;</span>
<span class="nv">SCP</span><span class="o">=</span><span class="s2">&quot;scp -o StrictHostKeyChecking=no&quot;</span>

<span class="c1"># Get the node list in the cluster.</span>
get_cluster_list<span class="o">()</span>
<span class="o">{</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">cluster_config</span><span class="o">=</span><span class="nv">$1</span>
<span class="w">        </span>cat<span class="w"> </span><span class="si">${</span><span class="nv">cluster_config</span><span class="si">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import sys,json;[print(node) for node in json.load(sys.stdin)[&quot;cluster&quot;].keys()]&#39;</span>
<span class="o">}</span>

<span class="c1"># Get the account number of node.</span>
get_node_user<span class="o">()</span>
<span class="o">{</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">cluster_config</span><span class="o">=</span><span class="nv">$1</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">node</span><span class="o">=</span><span class="nv">$2</span>
<span class="w">        </span>cat<span class="w"> </span><span class="si">${</span><span class="nv">cluster_config</span><span class="si">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import sys,json;print(json.load(sys.stdin)[&quot;cluster&quot;][&#39;</span><span class="se">\&quot;</span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="se">\&quot;</span><span class="s1">&#39;][&quot;user&quot;])&#39;</span>
<span class="o">}</span>

<span class="c1"># Get the password of node.</span>
get_node_passwd<span class="o">()</span>
<span class="o">{</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">cluster_config</span><span class="o">=</span><span class="nv">$1</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">node</span><span class="o">=</span><span class="nv">$2</span>
<span class="w">        </span>cat<span class="w"> </span><span class="si">${</span><span class="nv">cluster_config</span><span class="si">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import sys,json;print(json.load(sys.stdin)[&quot;cluster&quot;][&#39;</span><span class="se">\&quot;</span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="se">\&quot;</span><span class="s1">&#39;][&quot;passwd&quot;])&#39;</span>
<span class="o">}</span>

<span class="c1"># Copy data from remote node to local node.</span>
rscp_pass<span class="o">()</span>
<span class="o">{</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">node</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$1</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">user</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$2</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">passwd</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$3</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">src</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$4</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="nb">local</span><span class="w"> </span><span class="nv">target</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$5</span><span class="s2">&quot;</span>
<span class="w">        </span>sshpass<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">passwd</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="si">${</span><span class="nv">SCP</span><span class="si">}</span><span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">user</span><span class="si">}</span><span class="s2">&quot;</span>@<span class="s2">&quot;</span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="s2">&quot;</span>:<span class="s2">&quot;</span><span class="si">${</span><span class="nv">src</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">target</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>

<span class="nv">cluster_hccl_config_path</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">cluster_account_config_path</span><span class="o">=</span><span class="nv">$2</span>
<span class="nv">cluster_train_id</span><span class="o">=</span><span class="nv">$3</span>
<span class="nv">host_train_id</span><span class="o">=</span><span class="nv">$4</span>
<span class="nv">is_absolute_path</span><span class="o">=</span><span class="nv">$5</span>

<span class="nv">node_list</span><span class="o">=</span><span class="k">$(</span>get_cluster_list<span class="w"> </span><span class="si">${</span><span class="nv">cluster_account_config_path</span><span class="si">}</span><span class="k">)</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;-----begin----&quot;</span>

<span class="nv">target_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">cluster_train_id</span><span class="si">}</span>/profiler/
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">target_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
mkdir<span class="w"> </span>-p<span class="w"> </span><span class="si">${</span><span class="nv">target_dir</span><span class="si">}</span>
<span class="k">fi</span>

<span class="k">for</span><span class="w"> </span>node<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="si">${</span><span class="nv">node_list</span><span class="si">}</span>
<span class="k">do</span>
<span class="w"> </span><span class="nv">user</span><span class="o">=</span><span class="k">$(</span>get_node_user<span class="w"> </span><span class="si">${</span><span class="nv">cluster_account_config_path</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="k">)</span>
<span class="w"> </span><span class="nv">passwd</span><span class="o">=</span><span class="k">$(</span>get_node_passwd<span class="w"> </span><span class="si">${</span><span class="nv">cluster_account_config_path</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="k">)</span>
<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;------------------</span><span class="si">${</span><span class="nv">user</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="s2">---------------------&quot;</span>

<span class="w"> </span><span class="c1"># Eight devices data</span>
<span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$is_absolute_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;0&#39;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="w"> </span><span class="nv">device_regex</span><span class="o">=</span><span class="k">$(</span>basename<span class="w"> </span><span class="k">$(</span>dirname<span class="w"> </span><span class="nv">$host_train_id</span><span class="k">))</span>
<span class="w"> </span><span class="nv">output</span><span class="o">=</span><span class="k">$(</span>basename<span class="w"> </span><span class="nv">$host_train_id</span><span class="k">)</span>
<span class="w"> </span><span class="nv">grandfather_host_train_id</span><span class="o">=</span><span class="k">$(</span>dirname<span class="w"> </span><span class="k">$(</span>dirname<span class="w"> </span><span class="nv">$host_train_id</span><span class="k">))</span>
<span class="w"> </span><span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="w"> </span><span class="k">do</span>
<span class="w">   </span><span class="nv">src_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">grandfather_host_train_id</span><span class="si">}</span>/<span class="si">${</span><span class="nv">device_regex</span><span class="si">}${</span><span class="nv">i</span><span class="si">}</span>/<span class="si">${</span><span class="nv">output</span><span class="si">}</span>*/profiler/*.*
<span class="w">   </span><span class="k">$(</span>rscp_pass<span class="w"> </span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">user</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">passwd</span><span class="si">}</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">src_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="si">${</span><span class="nv">target_dir</span><span class="si">}</span><span class="k">)</span>
<span class="w"> </span><span class="k">done</span>
<span class="w"> </span><span class="k">elif</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$is_absolute_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;1&#39;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="w"> </span><span class="nv">src_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">host_train_id</span><span class="si">}</span>/profiler/*.*
<span class="w"> </span><span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="w"> </span><span class="k">do</span>
<span class="w">   </span><span class="k">$(</span>rscp_pass<span class="w"> </span><span class="si">${</span><span class="nv">node</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">user</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">passwd</span><span class="si">}</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">src_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="si">${</span><span class="nv">target_dir</span><span class="si">}</span><span class="k">)</span>
<span class="w"> </span><span class="k">done</span>
<span class="w"> </span><span class="k">else</span>
<span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;The value of is_absolute_path can only be 0 or 1.&quot;</span>
<span class="w"> </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w"> </span><span class="k">fi</span>
<span class="k">done</span>
</pre></div>
</div>
<p>Script Parameter Description:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">cluster_hccl_config_path</span></code> Network information file path in the multi-device environment. The content format is as follows：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;server_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;10.xxx.xxx.1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">}],</span>
<span class="w">         </span><span class="nt">&quot;host_nic_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;reserve&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">],</span>
<span class="nt">&quot;status&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;completed&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">cluster_account_config_path</span></code> Host node account password configuration file path, The content format is as follows：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;rank_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cluster&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;10.xxx.xxx.1&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;user&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;root&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;passwd&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxx&quot;</span>
<span class="w">                </span><span class="p">},</span>
<span class="w">                </span><span class="nt">&quot;10.xxx.xxx.2&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;user&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;root&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;passwd&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxx&quot;</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">              </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">cluster_train_id</span></code> Path to save cluster performance data summary. For example, <code class="docutils literal notranslate"><span class="pre">/home/summary/run1</span></code> and <code class="docutils literal notranslate"><span class="pre">/home/summary/run2</span></code>, where <code class="docutils literal notranslate"><span class="pre">run1</span></code> and <code class="docutils literal notranslate"><span class="pre">run2</span></code> respectively save the jobs of two cluster training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">host_train_id</span></code> In cluster training, the performance data saving path is set by the user. When the performance data save path is set to an absolute path, <code class="docutils literal notranslate"><span class="pre">host_train_id</span></code> is the value set by the user. For example, when the value is <code class="docutils literal notranslate"><span class="pre">/data/run</span></code>, multi devices performance data are saved in <code class="docutils literal notranslate"><span class="pre">/data/run/profiler</span></code> (<code class="docutils literal notranslate"><span class="pre">profiler</span></code>folder is automatically created by the program), the value of <code class="docutils literal notranslate"><span class="pre">host_train_id</span></code> should be set to <code class="docutils literal notranslate"><span class="pre">/data/run</span></code>. When the performance data saving path is set as a relative path, multi card performance data may be saved in different folders, such as <code class="docutils literal notranslate"><span class="pre">/data/run/device0/data/profiler</span></code>, <code class="docutils literal notranslate"><span class="pre">/data/run/device1/data/profiler</span></code>. Their common path is <code class="docutils literal notranslate"><span class="pre">/data/run/device/data/profiler</span></code>, and the performance data storage path of each device is <code class="docutils literal notranslate"><span class="pre">/data/run/device{device_id}/data/profiler</span></code>. The value of <code class="docutils literal notranslate"><span class="pre">host_train_id</span></code> should be set to <code class="docutils literal notranslate"><span class="pre">/data/run/device/data</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_absolute_path</span></code> In the cluster performance data to be collected, whether the single machine and multi devices data are saved in the same directory. If yes, set to 1; Not set to 0.</p></li>
</ul>
<blockquote>
<div><p>The collected cluster performance jobs need to conform to the directory structure, otherwise, they cannot be visualized with MindSpore Insight. It must contain the networking information file (the file name is optional) and host_ips_mapping.txt File (file name and suffix are unique).</p>
</div></blockquote>
<p>The directory structure of cluster performance folder collected by script is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>|-- run
    |-- profiler
        |-- step_trace_raw_{rank_id}_detail_time.csv
</pre></div>
</div>
<blockquote>
<div><ol class="arabic simple">
<li><p>The format of cluster directory and single device performance directory are unified.</p></li>
<li><p>In the cluster scenario, the initialization of the Profiler needs to be placed after the execution of the <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code> method.</p></li>
</ol>
</div></blockquote>
<p>In MindSpore Insight r1.3 and earlier versions, the cluster directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>|-- run
    |-- hccl.json
    |-- host_ips_mapping.txt
    |-- cluster_profiler
        |-- 1
        |   |-- profiler
        |       |-- step_trace_raw_0_detail_time.csv
</pre></div>
</div>
<p>Through the data conversion script, you can convert the cluster performance directory created by users using MindSpore Insight r1.3 and earlier versions into the currently supported cluster performance directory. You can download <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/sample_code/transform_cluster_profiler_data.py">Cluster directory conversion script</a> from the official website.</p>
<blockquote>
<div><p>This is the Ascend AI processor’s introduction to collecting cluster performance data. For GPU cluster training, see the GPU distributed training tutorial.</p>
</div></blockquote>
</section>
<section id="launch-mindspore-insight">
<h2>Launch MindSpore Insight<a class="headerlink" href="#launch-mindspore-insight" title="Permalink to this headline"></a></h2>
<p>The MindSpore Insight launch command can refer to <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/mindinsight_commands.html">MindSpore Insight Commands</a>.</p>
</section>
<section id="training-performance">
<h2>Training Performance<a class="headerlink" href="#training-performance" title="Permalink to this headline"></a></h2>
<p>The user can select the specified training from the training list, click performance debugging, and click the “cluster” tab to display the performance data of this training from the cluster perspective. Cluster training performance includes cluster iterative trajectory analysis and cluster communication performance analysis.</p>
<p><img alt="cluster_summary.png" src="_images/cluster_summary.png" /></p>
<p><em>Figure 1: overview of cluster training performance</em></p>
<p>Figure 1 is the overview of cluster training performance, which is the overall presentation of cluster iterative trajectory component and cluster communication performance component. The display contents of each component are as follows:</p>
<ul class="simple">
<li><p>Cluster iteration trajectory: The iterative trajectory information of all devices in the cluster is displayed; The overview page shows the cluster iteration trajectory performance.</p></li>
<li><p>Cluster communication performance: Show the communication performance of all devices in the cluster and the link performance of the whole network; The overview page shows the cluster communication performance.</p></li>
<li><p>Cluster performance helper: The helper on the left provides possible performance bottlenecks during training, and users can optimize performance according to the prompts.</p></li>
</ul>
<blockquote>
<div><p>Currently, only cluster iteration track can be displayed on GPU. Users can view the performance of cluster iteration track. The left assistant provides a document about cluster performance tuning, which users can click to learn more about.</p>
</div></blockquote>
<section id="cluster-iterative-trajectory-analysis">
<h3>Cluster Iterative Trajectory Analysis<a class="headerlink" href="#cluster-iterative-trajectory-analysis" title="Permalink to this headline"></a></h3>
<p>Using the cluster iterative trajectory analysis component, we can find out the slow host and slow device in cluster training. Cluster iteration trajectory analysis component shows the iteration information of all devices, including step interval, forward and backward, iteration trailing, and supports sorting operation. The step interval reflects the speed of the data processing stage, and the step interval time of the device can reflect the speed of the corresponding host processing data. The forward and backward time of the device reflects the computing power of the device. Iterative tailing reflects all_reduce time and parallelism.</p>
<p><img alt="cluster_iterative_trajectory.png" src="_images/cluster_iterative_trajectory.png" /></p>
<p><em>Figure 2: cluster iteration trajectory analysis</em></p>
<p>Figure 2 shows the cluster iteration trajectory analysis page. By default, it shows the average performance of the device. It supports querying the iteration trajectory information of the device under a specific step. By clicking the details link in the single device, you can also jump to the detailed performance display page of the single device to query the detailed performance data of the single device.</p>
<p><img alt="single_car_performance_overall.png" src="_images/single_car_performance_overall.png" /></p>
<p><em>Figure 3: single device details</em></p>
<p>Figure 3 shows the performance information of a single device in the cluster. Please refer to <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling_ascend.html">single device performance information</a> for the performance information of a single device.</p>
</section>
<section id="cluster-communication-and-computation-overlap-time-analysis">
<h3>Cluster Communication and Computation Overlap Time Analysis<a class="headerlink" href="#cluster-communication-and-computation-overlap-time-analysis" title="Permalink to this headline"></a></h3>
<p>Cluster communication and computational overlap time analysis components are used in pipeline parallel and model parallel mode to identify slow hosts and slow devices in cluster training.</p>
<p>The cluster communication and computation overlap time analysis components add five new indicators: Communication Time(including the receive operator only), Stage Time, Communication Time, Computation Time, Communication Time(not including the receive operator).</p>
<ul class="simple">
<li><p>Communication Time(including the receive operator only): only the point-to-point(receive) communication operator is executed, and the calculation operator does not execute the time period. This time period reflects the asynchronous situation between the parallel stages of the pipeline.</p></li>
<li><p>Stage Time: the time-consuming duration of each stage. This value is the duration of the step minus the duration of the receive communication operator in the step. Through this indicator, you can see which stage takes the longest time.</p></li>
<li><p>Communication Time: the time period when only the communication operator is executed, and the calculation operator is not executed. If this part takes a long time, it means that the communication time-consuming has a greater impact on performance.</p></li>
<li><p>Computation Time: the total execution time of AI Core operator, used to judge whether there is a slow card. The longer the time, the slower the execution speed of the corresponding card.</p></li>
<li><p>Communication Time(not including the receive operator): only the time period during which other communication operators except the receive communication operators are executed, and the computation operators does not execute. When this time period accounts for a large proportion, you need to consider whether the segmentation strategy of the operators in the stage can be adjusted to reduce the time-consuming duration of this time period.</p></li>
</ul>
<p><img alt="cluster_pipeline-parallel_analyse.png" src="_images/cluster_pipeline-parallel_analyse_en.png" /></p>
<p><em>Figure 4: pipeline parallel mode analysis</em></p>
<p>Figure 4 shows the information in pipeline parallel scene, showing the average value of all step by default. The page shows step interval time, pure receive time, stage time, pure communication time, calculation time, pure collection communication time. Because the computation graph of the whole network is divided into subgraph of multiple stages, the stage time can be used to locate the slow stage, and the device of the same stage can be filtered out by selecting the stage number, and the idea of model parallel mode can be used to locate the bottleneck within the stage.</p>
<p><img alt="cluster_model-parallel_analyse.png" src="_images/cluster_model-parallel_analyse_en.png" /></p>
<p><em>Figure 5: model parallel mode analysis</em></p>
<p>Figure 5 shows the information in model parallel scene(here refers to the in-layer model parallel), showing the average value of all step by default. The page shows step interval time, pure communication time, and calculation time. Computation time can be used to locate slow devices. If there is no slow device, observe the communication time and computation time ratio, if the communication time is relatively large, consider whether there is a slow link.</p>
</section>
<section id="cluster-communication-performance-analysis">
<h3>Cluster Communication Performance Analysis<a class="headerlink" href="#cluster-communication-performance-analysis" title="Permalink to this headline"></a></h3>
<p>The cluster communication performance component displays the cluster communication performance information from two dimensions: device granularity and whole network link.</p>
<p><img alt="cluster_communication_info.png" src="_images/cluster_communication_info.png" /></p>
<p><em>Figure 6: cluster communication performance analysis</em></p>
<p>Figure 6 shows the analysis page of cluster communication performance, including the communication performance of logic device and the link information of the whole network (all logic device link information).</p>
<p>Logic device communication performance tab page is mainly used to show the communication performance of logic device, including communication time, waiting time, operator details, logic device link information.</p>
<ul class="simple">
<li><p>Communication time: Represents the communication time of the communication operator. If the communication time is too long, there may be a problem with a link, and the specific link can be located through the link bandwidth. The calculation method of communication time is to count the total communication operator time of SDMA link (intra server communication) and RDMA link (inter server communication). If it is the SDMA link, the total time of <code class="docutils literal notranslate"><span class="pre">Reduce</span> <span class="pre">inline</span></code> and <code class="docutils literal notranslate"><span class="pre">Memcpy</span></code> operators is taken as the communication time; If it is the RDMA link, the total time of three consecutive operators <code class="docutils literal notranslate"><span class="pre">RDMASendPayload</span></code>, <code class="docutils literal notranslate"><span class="pre">RDMASendNotify</span></code>, <code class="docutils literal notranslate"><span class="pre">Notify</span> <span class="pre">Wait</span></code> is taken as the communication time.</p></li>
<li><p>Waiting time: Also called synchronization time. Before communication between devices, synchronization will be carried out first to ensure that the two devices are synchronized before communication. The waiting time is calculated by counting the total time consumption of all <code class="docutils literal notranslate"><span class="pre">Notify</span> <span class="pre">wait</span></code> operators and subtracting the time consumption of <code class="docutils literal notranslate"><span class="pre">Notify</span> <span class="pre">wait</span></code> operator in the communication time of RDMA link.</p></li>
<li><p>Operator details: Display the communication performance with operator granularity, including the communication duration, waiting duration and link information of the communication operator.</p></li>
<li><p>Logic device link information: Display the link information of the source device or the destination device. Link information includes communication time, traffic, bandwidth (traffic divided by communication time) and link type. The link types include SDMA link (intra server communication link) and RDMA link (inter server communication link). Click the details and display them by pop-up window.</p></li>
</ul>
<p>The node link graph shows the situation of devices and communication links. Each node in the figure represents a device, and the size of the node encodes the total communication time of the device. The color of the node encodes the proportion of the device’s communication time to the total communication and waiting time, namely: communication time/(communication time + waiting time). The darker the color of the node, the shorter the waiting time of the device. Devices with short waiting time may be slow nodes. Nodes can be dragged and moved, and the edges between nodes represent communication links.</p>
<p><img alt="communication_matrix.png" src="_images/communication_matrix.png" /></p>
<p><em>Figure 7: the communication adjacency matrix</em></p>
<p>Figure 7 shows the adjacency matrix presented after brushing to select some devices in the node link graph. The adjacency matrix shows the communication links between devices. In each grid, the first and third rows show the communication time and traffic statistics of the link. The box plot in the second, fourth, and fifth rows of the adjacency matrix reflect the distribution of all communication operators on the three metrics, communication time, traffic, and bandwidth in the link. The outliers in the red box in the figure indicate that the operator occupies most of the bandwidth of the link. Users can locate abnormal communication links and abnormal communication operators through the adjacency matrix.</p>
<p>Right-click anywhere in the adjacency matrix, users can return to the node link graph.</p>
<p><img alt="operator_performance.png" src="_images/operator_performance.png" /></p>
<p><em>Figure 8: Operator performance information</em></p>
<p><img alt="rank_id_link_info.png" src="_images/rank_id_link_info.png" /></p>
<p><em>Figure 9: link information of logic device</em></p>
<p>The whole network link information tab page displays the link information of all logic devices, and provides the selection of source device, destination device and link type.</p>
<p><img alt="rank_ids_link_info.png" src="_images/rank_ids_link_info.png" /></p>
<p><em>Figure 10: link information of the whole network</em></p>
<p>By default, communication performance data is not collected. You need to use the <code class="docutils literal notranslate"><span class="pre">profile_communication</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">mindspore.Profiler</span></code> like <code class="docutils literal notranslate"><span class="pre">Profiler(profile_communication=True)</span></code> to turn on the communication performance data switch. It should be noted that only multi devices training can generate communication operator performance data. Setting this parameter in single device training scenario does not work.</p>
<p>To use MindSpore Insight to visualize communication performance data, you need to install the communication performance data parsing WHL package provided by the supporting software package of Ascend 910 AI processor. The WHL package is released with the supporting software package. Refer to the following command to complete the installation.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>/usr/local/Ascend/latest/tools/hccl_parser-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl
</pre></div>
</div>
</section>
<section id="specifications">
<h3>specifications<a class="headerlink" href="#specifications" title="Permalink to this headline"></a></h3>
<p>For data parsing performance, the number of files generated when cluster communication is enabled is currently limited. Currently, the maximum number of original communication performance files (named with the suffix.trace) generated by MindSpore is 500. When the original communication data exceeds the upper limit, the step number of Cluster Communication may be inconsistent with the step number of cluster Step Trace.</p>
</section>
</section>
<section id="resource-utilization">
<h2>Resource Utilization<a class="headerlink" href="#resource-utilization" title="Permalink to this headline"></a></h2>
<section id="cluster-memory-analysis">
<h3>Cluster Memory Analysis<a class="headerlink" href="#cluster-memory-analysis" title="Permalink to this headline"></a></h3>
<p>This page shows the memory usage of the model on the <strong>device side</strong> in the parallel mode, which is an ideal prediction <strong>based on the theoretical value</strong>. The content of the page includes:</p>
<ul class="simple">
<li><p>The distribution of cluster devices, which servers and which devices are used.</p></li>
<li><p>The peak memory of cluster devices, which is the ratio of peak memory to available memory.</p></li>
<li><p>Click a device to jump to the memory details page of the device.</p></li>
</ul>
<blockquote>
<div><p>Memory analysis does not support heterogeneous training currently.</p>
</div></blockquote>
<p><img alt="cluster_memory.png" src="_images/cluster_memory.png" /></p>
<p><em>Figure 11: The page of cluster memory analysis</em></p>
</section>
<section id="cluster-flops-analysis">
<h3>Cluster FLOPs Analysis<a class="headerlink" href="#cluster-flops-analysis" title="Permalink to this headline"></a></h3>
<p>This page shows the FLOPs data for each device in the parallel mode. The content of the page includes:</p>
<ul class="simple">
<li><p>The distribution of cluster devices, which servers and which devices are used.</p></li>
<li><p>The relative size of FLOPs among cluster devices. The color of the corresponding rectangular block of each device represents the ratio of FLOPs of the current device to the maximum FLOPs of all devices.</p></li>
<li><p>Click on a device to jump to the operator time-consuming details page of the device, which contains detailed data for FLOPs.</p></li>
</ul>
<p><img alt="cluster_flops.png" src="_images/cluster_flops.png" /></p>
<p><em>Figure 12: The page of cluster FLOPs analysis</em></p>
</section>
</section>
<section id="strategy-perception">
<h2>Strategy Perception<a class="headerlink" href="#strategy-perception" title="Permalink to this headline"></a></h2>
<p>Strategy Perception includes Computational Graph Exploration module, Parallel Strategy Analysis module, etc.</p>
<section id="graph-exploration-module">
<h3>Graph Exploration Module<a class="headerlink" href="#graph-exploration-module" title="Permalink to this headline"></a></h3>
<section id="general-introduction">
<h4>General Introduction<a class="headerlink" href="#general-introduction" title="Permalink to this headline"></a></h4>
<p><img alt="image-20211118132511452" src="_images/profiler_strategy_graph_en.png" /></p>
<p><em>Figure 13: The Page of Strategy Perception</em></p>
<p>The upper right corner of the page will show the parallel mode of this training. The figure above shows that the parallel mode in the current training is auto parallel.</p>
<p>Users can choose computational graphs of different stages to explore. Users can also use the graph selector to extract communication nodes from specific parts of the computational graph (feed-forward graph, back-propagation graph, and recompute graph).</p>
<p>The pipeline parallel view is at the upper left corner of the page. When training parallel mode is pipeline parallel, this view shows the data sending and receiving relationship between stages. Click the operator to jump to the graph.</p>
<p>The computational graph is displayed in the middle of the page. The node of the square is the aggregation node, which can be double-clicked to open or close. The ellipse is the common operator.</p>
<p>When the aggregation node is not expanded, it will show statistics of different special operators, like operators with strategy, operators for redistribution, and operators for gradient aggregation. If the output of the previous operator cannot be calculated with the output of the next operator, a redistribution operator is automatically inserted between two operators to implement the arrangement transformation. For more details, please refer to the chapter of distributed training design in the design document.</p>
<p>By clicking a certain node (operator or aggregation node), the node attributes panel will show the inputs and outputs of the node and its shard methods. The input and output nodes can be tracked by clicking.</p>
</section>
</section>
<section id="operator-strategy-matrix">
<h3>Operator Strategy Matrix<a class="headerlink" href="#operator-strategy-matrix" title="Permalink to this headline"></a></h3>
<p><img alt="image-20211118133144763" src="_images/profiler_strategy_graph_stack.png" /></p>
<p><em>Figure 14: Operator Strategy Matrix</em></p>
<p>If an input node of the operator has shard methods, a strategy matrix will be presented below the operator. One row represents the shard method of a certain input node. The number in the small grid cell represents the number of slices of the input node in the corresponding dimension.</p>
<p>The corresponding input edges will be highlighted when hovering on the strategy matrix. Along with the input and output locating feature, users can analyze the rationality of the operator’s shard method and adjust accordingly if needed.</p>
<p>It is important to note that constants are not plotted in the computational graph, so the shard strategy for constants is not reflected in the graph.</p>
</section>
<section id="training-pipeline">
<h3>Training Pipeline<a class="headerlink" href="#training-pipeline" title="Permalink to this headline"></a></h3>
<p><img alt="image-20211122180619886" src="_images/profiler_strategy_graph_pipeline.png" /></p>
<p><em>Figure 15: Training Pipeline</em></p>
<p>When the pipeline parallel strategy is adopted, click the button in the upper left corner to expand the training pipeline panel. This panel shows the send operators (red rectangles) and receive operators (green rectangles) in each stage and their correspondences between different stages. The rectangles (operators) can be clicked and the corresponding operator will be focused in the computational graph.</p>
<p>With the training pipeline panel, users can evaluate the rationality of stage segmentation and analyze the design space of pipeline parallel strategy, the number of micro-batches, etc.</p>
</section>
<section id="operator-stacking-and-edge-hiding">
<h3>Operator Stacking and Edge Hiding<a class="headerlink" href="#operator-stacking-and-edge-hiding" title="Permalink to this headline"></a></h3>
<p><img alt="image-20211118125032089" src="_images/profiler_strategy_graph_stack.png" /></p>
<p><em>Figure 16: Operator Stacking</em></p>
<p>In the computational graph, if there are too many operators of the same type in the aggregation node, they will be stacked and displayed. Double-click to expand the operator.</p>
<p><img alt="image-20211118125032089" src="_images/profiler_strategy_hideline.png" /></p>
<p><em>Figure 17: View Hidden Edges</em></p>
<p>In order to prevent the lines from being too messy and some unimportant edges will be hidden, move the mouse over the circle of the aggregation node to see the hidden edges.</p>
</section>
</section>
<section id="execution-overview">
<h2>Execution Overview<a class="headerlink" href="#execution-overview" title="Permalink to this headline"></a></h2>
<p>The user can select the specified training from the training list, click performance debugging, and click the <code class="docutils literal notranslate"><span class="pre">cluster</span></code> tab to display the performance data of this training from the cluster perspective.
The execution overview tab includes the analysis of the execution sequence of the operators in the computational graph, the analysis of the execution timeline of the operators on each device, and a time overview displaying the time information of each step and each device.</p>
<p><img alt="execution_overview.png" src="_images/execution_overview.png" /></p>
<p><em>Figure 18：execution overview of a cluster</em></p>
<section id="analysis-of-the-computational-graph">
<h3>Analysis of the computational graph<a class="headerlink" href="#analysis-of-the-computational-graph" title="Permalink to this headline"></a></h3>
<p>The parallel strategy view is displayed on the top of the page.</p>
<p><img alt="parallel_strategy_view.png" src="_images/parallel_strategy_view.png" /></p>
<p><em>Figure 19：parallel strategy view</em></p>
<p>In this computational graph, the operators are laid out from left to right according to the sequence of execution. The canvas can be dragged, scaled to observe. Each type of operator is distinguished with different colors, with a legend at the top of the view.</p>
<p>On the left is the namespace selector. After checking the namespace, the corresponding operators will be enveloped with a certain color.</p>
<p>When pipeline parallel strategy is adopted, computational graphs of all stages are displayed. Each computational graph is arranged horizontally according to the correspondence of <code class="docutils literal notranslate"><span class="pre">Send</span></code> and <code class="docutils literal notranslate"><span class="pre">Receive</span></code> operators. Users can get an overall perception of the parallel execution process.</p>
<p><img alt="timeline_minimap.png" src="_images/timeline_minimap.png" /></p>
<p><em>Figure 20：timeline minimap</em></p>
<p>The right side is the special nodes counting panel and the node attribute panel.
There are three types of special operators: operators with strategy, operators for redistribution, and operators for gradient aggregation.</p>
</section>
<section id="operators-execution-timeline-analysis-on-each-device">
<h3>Operators execution timeline analysis on each device<a class="headerlink" href="#operators-execution-timeline-analysis-on-each-device" title="Permalink to this headline"></a></h3>
<p>The Marey view is displayed in the middle of the page.</p>
<p><img alt="marey_graph.png" src="_images/marey_graph.png" /></p>
<p><em>Figure 21：Marey view</em></p>
<p>In the Marey view, each device has three color blocks and a timeline. The three color blocks show each device’s FLOPs(the number of floating-point operations, used to measure model/algorithm complexity), FLOPS(the number of floating-point operations per second, used to measure hardware performance) and PeakMem(peak memory). For FLOPs and FLOPS, the shades represents the ratio of the value(the value of the current device / max value of all devices). For PeakMem, the shades represents the memory utilization(peak memory / memory capacity).</p>
<p><img alt="marey_timeline.png" src="_images/marey_timeline.png" /></p>
<p><em>Figure 22：Marey timeline</em></p>
<p>As show in Figure 22, green block represents computation operator and orange block represents communication operator. The operators executed on devices in the same pipeline stage are basically the same. Each device has a timeline. We mark the start and end time of operator execution on the timeline, then connect polygons and fill in colors. This view can help locate the following two types of problems:</p>
<p><img alt="marey_exception.png" src="_images/marey_exception.png" /></p>
<p><em>Figure 23：Marey timeline pattern of exceptions</em></p>
<p>As shown in Figure 23(a), when the execution time of an operator on each device is significantly longer than that of other operators, it may be that the parameter <code class="docutils literal notranslate"><span class="pre">all_reduce_fusion_config</span></code> is unreasonable.
As shown in Figure 23(b), when the execution time of an operator on one device is significantly longer than that on other devices, there may be a slow node.</p>
<p>The marey timeline supports the following interactions: brush to zoom in, double-click to zoom out, and hover to show the corresponding operators.</p>
<p>The stage tree on the left side can be aggregated or expanded as needed. The timeline of the stage shows the union of the execution time of the same operator on all devices in the stage.</p>
<p>A line chart is shown on the timeline. The dark line indicates the FLOPs change, the light line indicates the memory change.</p>
<p><img alt="marey_memory_flops.png" src="_images/marey_memory_flops.png" /></p>
<p><em>Figure 24：line chart on the Marey timeline</em></p>
<p>As shown in Figure 24, the memory usage has a clear peak in the red box, which can be further analyzed with the operators on the marey timeline.</p>
</section>
<section id="time-overview-of-each-device">
<h3>Time overview of each device<a class="headerlink" href="#time-overview-of-each-device" title="Permalink to this headline"></a></h3>
<p>The time overview is displayed at the bottom of the page.</p>
<p><img alt="time_view.png" src="_images/time_view.png" /></p>
<p><em>Figure 25：time overview</em></p>
<p>The time overview is a double y-axis graph, showing the training time on the left and the communication time on the right. This view displays the training time of each device in each step, the average communication time and waiting time of each device. When user hovers on one step, the specific statistics can be seen in a pop-up card. This view serves as an entry for analysis. If user determines that the training process in a certain step is abnormal, he can click the certain step and the Marey view will show the execution details on the selected step for further analysis.</p>
</section>
</section>
<section id="specifications-1">
<h2>Specifications<a class="headerlink" href="#specifications-1" title="Permalink to this headline"></a></h2>
<ul>
<li><p>To limit the data size generated by the Profiler, MindSpore Insight suggests that for large neural networks, the profiled steps should be less than 10.</p>
<blockquote>
<div><p>The number of steps can be controlled by controlling the size of training dataset. For example, the <code class="docutils literal notranslate"><span class="pre">num_samples</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">mindspore.dataset.MindDataset</span></code> can control the size of the dataset. For details, please refer to:
<a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/dataset/mindspore.dataset.MindDataset.html">https://www.mindspore.cn/docs/en/master/api_python/dataset/mindspore.dataset.MindDataset.html</a></p>
</div></blockquote>
</li>
<li><p>The parse of Timeline data is time consuming, and usually the data of a few steps is enough to analyze the results. In order to speed up the data parse and UI display, Profiler will show at most 20M data (Contain 10+ step information for large networks).</p></li>
</ul>
</section>
<section id="notices">
<h2>Notices<a class="headerlink" href="#notices" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Currently running in PyNative mode is not supported.</p></li>
<li><p>Currently the training and inference process does not support performance debugging, only individual training or inference is supported.</p></li>
<li><p>Performance debugging does not support dynamic Shape scenarios, multi-subgraph scenarios, and control flow scenarios.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="performance_profiling_gpu.html" class="btn btn-neutral float-left" title="Performance Profiling (GPU)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="debugger.html" class="btn btn-neutral float-right" title="Debugger" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>