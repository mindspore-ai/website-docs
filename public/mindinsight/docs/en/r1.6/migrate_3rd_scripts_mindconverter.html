<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Migrating From Third Party Frameworks With MindConverter &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Performance Profiling" href="performance_profiling.html" />
    <link rel="prev" title="Use Mindoptimizer to Tune Hyperparameters" href="hyper_parameters_auto_tuning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_install.html">MindInsight Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="summary_record.html">Collecting Summary Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="dashboard.html">Viewing Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="lineage_and_scalars_comparison.html">Viewing Lineage and Scalars Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyper_parameters_auto_tuning.html">Use Mindoptimizer to Tune Hyperparameters</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Migrating From Third Party Frameworks With MindConverter</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-starts">Quick Starts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#start-with-mindconverter-cli">Start with MindConverter CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#start-with-mindconverter-api">Start with MindConverter API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#install-tools">Install Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#install-dependencies">Install Dependencies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Start with MindConverter CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Start with MindConverter API</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#install-mindconverter">Install MindConverter</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#installation-by-pip">Installation by pip</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installation-by-source-code">Installation by Source Code</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#migration-solution">Migration Solution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#practice-guidance">Practice Guidance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-0-export-the-model-file">Step 0：Export the model file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-1-migrate-the-model-definition">Step 1:Migrate the model definition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-migrate-the-data-processing">Step 2：Migrate the data processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-migrate-the-model-training">Step 3：Migrate the model training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-migrate-the-model-evaluation">Step 4：Migrate the model evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mindconverter-cli-usage">MindConverter CLI Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-list-supported-by-mindconverter">Model List Supported by MindConverter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mindconverter-error-code-definition">MindConverter Error Code Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#faq">FAQ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#install-dependencies-under-arm">Install dependencies under ARM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#determine-the-form-of-model-input-shape">Determine the form of model input shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="#export-the-model-file-of-tensorflow">Export the model file of Tensorflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rectify-parameters-of-forward-function-definition">Rectify parameters of forward function definition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mix-the-mindspore-model-with-the-original-training-scripts">Mix the MindSpore model with the original training scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#migration-reports-and-weights-mapping">Migration reports and weights mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ast-based-model-migration">AST-Based Model Migration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_profiling.html">Performance Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="debugger.html">Debugger</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_explanation.html">Explain Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="landscape.html">Visualization of Training Optimization Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_commands.html">MindInsight Commands</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Tuning Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning_guide.html">Performance Tuning Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindinsight.debugger.html">mindinsight.debugger</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindconverter.html">mindconverter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="training_visual_design.html">Overall Design of Training Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_visual_design.html">Computational Graph Visualization Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_visual_design.html">Tensor Visualization Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler_design.html">Performance Profiling Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Migrating From Third Party Frameworks With MindConverter</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/migrate_3rd_scripts_mindconverter.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="migrating-from-third-party-frameworks-with-mindconverter">
<h1>Migrating From Third Party Frameworks With MindConverter<a class="headerlink" href="#migrating-from-third-party-frameworks-with-mindconverter" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindinsight/docs/source_en/migrate_3rd_scripts_mindconverter.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindConverter is a migration tool to transform the model file of PyTorch(ONNX) or TensorFlow(PB) to MindSpore. The model file contains model structure definition(<code class="docutils literal notranslate"><span class="pre">network</span></code>) and weights information(<code class="docutils literal notranslate"><span class="pre">weights</span></code>), which will be transformed into model scripts(<code class="docutils literal notranslate"><span class="pre">model.py</span></code>) and weights file(<code class="docutils literal notranslate"><span class="pre">ckpt</span></code>) in MindSpore.</p>
<p><img alt="mindconverter-overview" src="_images/mindconverter-overview.png" /></p>
<p>Moreover, this tool is able to transform the model file of PyTorch to MindSpore by adding API(<code class="docutils literal notranslate"><span class="pre">pytorch2mindspore</span></code>) to original PyTorch scripts.</p>
</section>
<section id="quick-starts">
<h2>Quick Starts<a class="headerlink" href="#quick-starts" title="Permalink to this headline"></a></h2>
<p>MindConverter installation is described in <span class="xref myst">Install Tools</span>. After the installation, you will have the MindConverter CLI and MindConverter API with the basic usage as follows.</p>
<section id="start-with-mindconverter-cli">
<h3>Start with MindConverter CLI<a class="headerlink" href="#start-with-mindconverter-cli" title="Permalink to this headline"></a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mindconverter<span class="w"> </span>--model_file<span class="w"> </span>/path/to/model_file<span class="w"> </span>--shape<span class="w"> </span>SHAPE<span class="w"> </span>--input_nodes<span class="w"> </span>INPUTS<span class="w"> </span>--output_nodes<span class="w"> </span>OUTPUTS
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--model_file</span></code> specifies the path of model file, the model file supports <code class="docutils literal notranslate"><span class="pre">onnx</span></code> format or <code class="docutils literal notranslate"><span class="pre">pb</span></code> format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--shape</span></code> specifies the input shape of model. Multiple inputs are separated by space.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--input_nodes</span></code> specifies the input names of model. Multiple inputs are separated by space.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--output_nodes</span></code> specifies the output names of model. Multiple outputs are separated by space.</p></li>
<li><p>Output files are generated and saved under <code class="docutils literal notranslate"><span class="pre">$PWD/output</span></code> by default.</p></li>
</ul>
<p>Notes:</p>
<ol class="arabic simple">
<li><p>The model file is in <code class="docutils literal notranslate"><span class="pre">onnx</span></code> format. If the model input shape is a static value, just specify <code class="docutils literal notranslate"><span class="pre">--</span> <span class="pre">model_</span> <span class="pre">file</span></code> to complete the conversion. Otherwise, you need to specify <code class="docutils literal notranslate"><span class="pre">--shape</span></code> and <code class="docutils literal notranslate"><span class="pre">--input_nodes</span></code> to complete the conversion. <code class="docutils literal notranslate"><span class="pre">--output_nodes</span></code> can be omitted. For model input shape judgment, please refer to <span class="xref myst">Determine the form of model input shape</span>.</p></li>
<li><p>The model file is in <code class="docutils literal notranslate"><span class="pre">pb</span></code> format without special scenarios.</p></li>
</ol>
<p>For more CLI arguments, please refer to <span class="xref myst">MindConverter CLI Usage</span>.</p>
</section>
<section id="start-with-mindconverter-api">
<h3>Start with MindConverter API<a class="headerlink" href="#start-with-mindconverter-api" title="Permalink to this headline"></a></h3>
<p>Write the following code in PyTorch network scripts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindconverter</span> <span class="kn">import</span> <span class="n">pytorch2mindspore</span>
<span class="n">pytorch2mindspore</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>For the usage of API, please refer to <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/mindconverter.html">MindConverter API Description</a>.</p>
</section>
</section>
<section id="install-tools">
<h2>Install Tools<a class="headerlink" href="#install-tools" title="Permalink to this headline"></a></h2>
<section id="install-dependencies">
<h3>Install Dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline"></a></h3>
<p>The following dependencies are required for MindConverter usage and suggested to be installed under the x86 environment. Refer to <span class="xref myst">installation</span> under the ARM environment.</p>
<section id="id1">
<h4>Start with MindConverter CLI<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Corresponding version of MindSpore is required(e.g. r1.2).</span>
pip<span class="w"> </span>install<span class="w"> </span>mindspore~<span class="o">=</span><span class="m">1</span>.2.0

<span class="c1"># ONNX and relevant libraries are required.</span>
pip<span class="w"> </span>install<span class="w"> </span>onnx~<span class="o">=</span><span class="m">1</span>.8.0
pip<span class="w"> </span>install<span class="w"> </span>onnxoptimizer~<span class="o">=</span><span class="m">0</span>.1.2
pip<span class="w"> </span>install<span class="w"> </span>onnxruntime~<span class="o">=</span><span class="m">1</span>.5.2

<span class="c1"># tf2onnx is required if model file is in pb format.</span>
pip<span class="w"> </span>install<span class="w"> </span>tf2onnx~<span class="o">=</span><span class="m">1</span>.7.1
</pre></div>
</div>
</section>
<section id="id2">
<h4>Start with MindConverter API<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Corresponding version of MindSpore is required(e.g. r1.6).</span>
pip<span class="w"> </span>install<span class="w"> </span>mindspore~<span class="o">=</span><span class="m">1</span>.6.0

<span class="c1"># torch is required, and official LTS version 1.8.2 is recommended.</span>
pip<span class="w"> </span>install<span class="w"> </span>torch~<span class="o">=</span><span class="m">1</span>.8.2+cpu<span class="w"> </span>-f<span class="w"> </span>https://download.pytorch.org/whl/lts/1.8/torch_lts.html
</pre></div>
</div>
</section>
</section>
<section id="install-mindconverter">
<h3>Install MindConverter<a class="headerlink" href="#install-mindconverter" title="Permalink to this headline"></a></h3>
<p>You can install MindConverter either by pip or by source code.</p>
<section id="installation-by-pip">
<h4>Installation by pip<a class="headerlink" href="#installation-by-pip" title="Permalink to this headline"></a></h4>
<p>Install from PyPI:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mindconverter
</pre></div>
</div>
<p>Install with customized version:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>https://ms-release.obs.cn-north-4.myhuaweicloud.com/<span class="o">{</span>version<span class="o">}</span>/MindInsight/any/mindconverter-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl<span class="w"> </span>--trusted-host<span class="w"> </span>ms-release.obs.cn-north-4.myhuaweicloud.com<span class="w"> </span>-i<span class="w"> </span>https://pypi.tuna.tsinghua.edu.cn/simple
</pre></div>
</div>
<blockquote>
<div><ul class="simple">
<li><p>When the network is connected, dependency items are automatically downloaded during .whl package installation. (For details about other dependency items, see <a class="reference external" href="https://gitee.com/mindspore/mindinsight/blob/r1.6/ecosystem_tools/mindconverter/requirements.txt">requirements.txt</a>). In other cases, you need to manually install dependency items.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">{version}</span></code> denotes the version of MindConverter. For example, when you are downloading MindConverter 1.6.0, <code class="docutils literal notranslate"><span class="pre">{version}</span></code> should be 1.6.0.</p></li>
<li><p>MindConverter supports only Linux distro with x86 architecture 64-bit or ARM architecture 64-bit.</p></li>
</ul>
</div></blockquote>
</section>
<section id="installation-by-source-code">
<h4>Installation by Source Code<a class="headerlink" href="#installation-by-source-code" title="Permalink to this headline"></a></h4>
<p>Downloading Source Code from Gitee.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://gitee.com/mindspore/mindinsight.git<span class="w"> </span>-b<span class="w"> </span>r1.6
</pre></div>
</div>
<p>Compiling MindConverter by any of the following installation methods:</p>
<ol class="arabic">
<li><p>Run the following command in the root directory of the source code:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>mindinsight/ecosystem_tools/mindconverter
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt<span class="w"> </span>-i<span class="w"> </span>https://pypi.tuna.tsinghua.edu.cn/simple
python<span class="w"> </span>setup.py<span class="w"> </span>install
</pre></div>
</div>
</li>
<li><p>Build the <code class="docutils literal notranslate"><span class="pre">whl</span></code> package for installation.</p>
<p>Enter the root directory of the source code, first execute the MindConverter compilation script in the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory, and then execute the command to install the <code class="docutils literal notranslate"><span class="pre">whl</span></code> package generated in the <code class="docutils literal notranslate"><span class="pre">output</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>mindinsight/ecosystem_tools/mindconverter
bash<span class="w"> </span>build/build.sh<span class="w"> </span>mindconverter
pip<span class="w"> </span>install<span class="w"> </span>output/mindconverter-<span class="o">{</span>version<span class="o">}</span>-py3-none-any.whl<span class="w"> </span>-i<span class="w"> </span>https://pypi.tuna.tsinghua.edu.cn/simple
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
<section id="migration-solution">
<h2>Migration Solution<a class="headerlink" href="#migration-solution" title="Permalink to this headline"></a></h2>
<p>A typical model project contains 4 main components. Tips for migrating each component are as follows:</p>
<ul class="simple">
<li><p>Model definition（<code class="docutils literal notranslate"><span class="pre">model.py</span></code>）</p>
<ol class="arabic simple">
<li><p>Transform the model structure with MindConverter CLI.</p></li>
<li><p>Manually enhance the readability of the generated model scripts(Optional).</p></li>
<li><p>Mix the generated model with the original project to validate equivalence of the migration. Refer to <span class="xref myst">FAQ</span>.</p></li>
</ol>
</li>
<li><p>Data processing（<code class="docutils literal notranslate"><span class="pre">dataset.py</span></code>）</p>
<ol class="arabic simple">
<li><p>For a built-in dataset, please query <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> for migration.</p></li>
<li><p>For a customized dataset and data augmentation, please refer to <span class="xref myst">the migration template</span>.</p></li>
</ol>
</li>
<li><p>Model training（<code class="docutils literal notranslate"><span class="pre">train.py</span></code>）</p>
<ol class="arabic simple">
<li><p>The loss function(<code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>) can be migrated by querying <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> or user’s implementation.</p></li>
<li><p>The optimizer(<code class="docutils literal notranslate"><span class="pre">optimizer</span></code>) can be migrated by querying <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> or user’s implementation.</p></li>
<li><p>As the training codes could be flexible and significantly different from MindSpore, implementation by the users is recommended. Please refer to <span class="xref myst">FAQ</span>.</p></li>
</ol>
</li>
<li><p>Model evaluation（<code class="docutils literal notranslate"><span class="pre">eval.py</span></code>）</p>
<ol class="arabic simple">
<li><p>The metric(<code class="docutils literal notranslate"><span class="pre">metric</span></code>) can be migrated by querying <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> or user’s implementation.</p></li>
<li><p>As the evaluation codes could be flexible and significantly different from MindSpore, implementation by the users is recommended. Please refer to <span class="xref myst">FAQ</span>.</p></li>
</ol>
</li>
</ul>
</section>
<section id="practice-guidance">
<h2>Practice Guidance<a class="headerlink" href="#practice-guidance" title="Permalink to this headline"></a></h2>
<section id="step-0-export-the-model-file">
<h3>Step 0：Export the model file<a class="headerlink" href="#step-0-export-the-model-file" title="Permalink to this headline"></a></h3>
<p>Exporting ONNX model file from PyTorch model(refer to <span class="xref myst">FAQ</span> for Tensorflow guidance) requires operators mapping between <a class="reference external" href="https://pytorch.org/docs/stable/onnx.html#supported-operators">PyTorch</a> and <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#">ONNX</a>. Guidance is as follows:</p>
<ol class="arabic">
<li><p>Download source codes, weights file and relevant dataset files of the model project.</p></li>
<li><p>Dive into the model definition. Make sure that all parameters of the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function are Tensor type. Please refer to <span class="xref myst">FAQ</span>.</p></li>
<li><p>Locate the model object and the input shape information from the model evaluation. Export the model object into onnx format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.pytorch.model</span> <span class="kn">import</span> <span class="n">PyTorchNetwork</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">PyTorchNetwork</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/weights.pth&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">param_dict</span><span class="p">)</span>

<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="s1">&#39;/path/to/model.onnx&#39;</span><span class="p">,</span> <span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

</pre></div>
</div>
</li>
<li><p>Validate the equivalence onnx model file against the original scripts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnxruntime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s1">&#39;/path/to/model.onnx&#39;</span><span class="p">)</span>
<span class="n">input_node</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="n">input_node</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/input.npy&#39;</span><span class="p">)})</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/output.npy&#39;</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="step-1-migrate-the-model-definition">
<h3>Step 1:Migrate the model definition<a class="headerlink" href="#step-1-migrate-the-model-definition" title="Permalink to this headline"></a></h3>
<p>Run the following MindConverter CLI to generate the model scripts(<code class="docutils literal notranslate"><span class="pre">model.py</span></code>), weights information(<code class="docutils literal notranslate"><span class="pre">ckpt</span></code>), <span class="xref myst">migration reports and weights mapping</span>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mindconverter<span class="w"> </span>--model_file<span class="w"> </span>/path/to/model.onnx
</pre></div>
</div>
<p>If you need to specify the model input shape, input node names and output node names. <a class="reference external" href="https://github.com/lutzroeder/netron">Netron</a> is recommended to get the above information.</p>
<p>Model scripts(<code class="docutils literal notranslate"><span class="pre">model.py</span></code>) and weights information(<code class="docutils literal notranslate"><span class="pre">ckpt</span></code>) can be used not only to validate the equivalence of migration, but also to generate the <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/save_model.html#mindir">MindIR</a> file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.mindspore.model</span> <span class="kn">import</span> <span class="n">MindSporeNetwork</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">MindSporeNetwork</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s1">&#39;network.ckpt&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/input.npy&#39;</span><span class="p">)</span>
<span class="n">output_benchmark</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/output.npy&#39;</span><span class="p">)</span>

<span class="c1"># Validate the equivalence of migration.</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">output_data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">output_benchmark</span><span class="p">)</span>

<span class="c1"># Generate the MindIR file.</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_data</span><span class="p">)),</span> <span class="n">file_name</span><span class="o">=</span><span class="s1">&#39;your_network_name&#39;</span><span class="p">,</span> <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;MINDIR&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Notes:</p>
<ol class="arabic simple">
<li><p>The Dropout operator will be lost after conversion because the inference mode is used to load the ONNX or TensorFlow model. Manually re-implementation is necessary.</p></li>
<li><p>This script conversion tool relies on operators which supported by MindConverter and MindSpore. Unsupported operators may not be successfully mapped to MindSpore operators. You can manually edit, or implement the mapping based on MindConverter, and make <a class="reference external" href="https://gitee.com/mindspore/mindinsight/blob/r1.6/ecosystem_tools/mindconverter/tutorial/add_onnx2mindspore_operator_mapper_advanced_tutorial.ipynb">contribution</a> to our MindInsight repository. We appreciate your support for the MindSpore community.</p></li>
<li><p>MindConverter converts dynamic input shape to constant one based on <code class="docutils literal notranslate"><span class="pre">--shape</span></code> while using graph based scheme, as a result, it is required that inputs’ shape used to retrain or inference in MindSpore are the same as that used to convert using MindConverter. If the input shape has changed, please running MindConverter again with new <code class="docutils literal notranslate"><span class="pre">--shape</span></code> or fixing shape related parameters in the old script.</p></li>
<li><p>MindSpore script and MindSpore checkpoint file are saved in the one file folder path, while report file and weight map file are saved in the other one.</p></li>
<li><p>The security and consistency of the model file should be guaranteed by the user.</p></li>
</ol>
</section>
<section id="step-2-migrate-the-data-processing">
<h3>Step 2：Migrate the data processing<a class="headerlink" href="#step-2-migrate-the-data-processing" title="Permalink to this headline"></a></h3>
<p>For a built-in dataset, please query <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> for migration. For a customized dataset and data augmentation, self implementation is recommended. For more data processing migration, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/dataset_sample.html">the programming guidance</a>.</p>
<p>Source codes with PyTorch framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
        <span class="c1"># Define data augmentation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
        <span class="p">])</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># Execute data augmentation.</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>
</div>
<p>Corresponding generated codes with MindSpore framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.dataset</span> <span class="kn">import</span> <span class="n">GeneratorDataset</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset</span> <span class="kn">import</span> <span class="n">py_transforms</span> <span class="k">as</span> <span class="n">transforms</span>

<span class="k">class</span> <span class="nc">CustomGenerator</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
        <span class="c1"># Define data augmentation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
        <span class="p">])</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># Execute data augmentation.</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">CustomGenerator</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-3-migrate-the-model-training">
<h3>Step 3：Migrate the model training<a class="headerlink" href="#step-3-migrate-the-model-training" title="Permalink to this headline"></a></h3>
<p>The loss function(<code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>) can be migrated by querying <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> or user’s implementation. For more loss function migration, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/loss.html">the programming guidance</a>.</p>
<p>The optimizer(<code class="docutils literal notranslate"><span class="pre">optimizer</span></code>) can be migrated by querying <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> or user’s implementation. For more optimizer migration, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/optim.html">the programming guidance</a>.</p>
<p>As the training codes could be flexible and significantly different from MindSpore, implementation by the users is recommended.</p>
<p>Source codes with PyTorch framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.pytorch.model</span> <span class="kn">import</span> <span class="n">PyTorchNetwork</span>

<span class="c1"># Create a instance of network model.</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">PyTorchNetwork</span><span class="p">()</span>

<span class="c1"># Define optimizer and learning rate.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">DECAY_RATE</span><span class="p">)</span>

<span class="c1"># Launch the model training.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCH_SIZE</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Corresponding generated codes(Low-Level API) with MindSpore framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.mindspore.model</span> <span class="kn">import</span> <span class="n">MindSporeNetwork</span>

<span class="c1"># Create a instance of network model.</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">MindSporeNetwork</span><span class="p">()</span>

<span class="c1"># Define learning rate and optimizer.</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ExponentialDecayLR</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="n">DECAY_RATE</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>

<span class="c1"># Launch the model training.</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="n">train_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">train_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="n">data_iterator</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="n">EPOCH_SIZE</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCH_SIZE</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_iterator</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<p>Corresponding generated codes(High-Level API) with MindSpore framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.mindspore.model</span> <span class="kn">import</span> <span class="n">MindSporeNetwork</span>

<span class="c1"># Create a instance of network model.</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">MindSporeNetwork</span><span class="p">()</span>

<span class="c1"># Define learning rate and optimizer.</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ExponentialDecayLR</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="n">DECAY_RATE</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>

<span class="c1"># Launch the model training.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">EPOCH_SIZE</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-migrate-the-model-evaluation">
<h3>Step 4：Migrate the model evaluation<a class="headerlink" href="#step-4-migrate-the-model-evaluation" title="Permalink to this headline"></a></h3>
<p>The metric(<code class="docutils literal notranslate"><span class="pre">metric</span></code>) can be migrated by querying <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">API mapping</a> or user’s implementation.</p>
<p>As the evaluation codes could be flexible and significantly different from MindSpore, implementation by the users is recommended. For more model evaluation migration, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/multi_platform_inference_ascend_910.html">the programming guidance</a>.</p>
<p>Source codes with PyTorch framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.pytorch.model</span> <span class="kn">import</span> <span class="n">PyTorchNetwork</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">PyTorchNetwork</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/weights.path&#39;</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">param_dict</span><span class="p">)</span>


<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">metric_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
<p>Corresponding generated codes(Low-Level API) with MindSpore framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.mindspore.model</span> <span class="kn">import</span> <span class="n">MindSporeNetwork</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">MindSporeNetwork</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s1">&#39;/path/to/weights.ckpt&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="n">data_iterator</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">()</span>
<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_iterator</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">metric_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
<p>Corresponding generated codes(High-Level API) with MindSpore framework are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.mindspore.model</span> <span class="kn">import</span> <span class="n">MindSporeNetwork</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">MindSporeNetwork</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s1">&#39;/path/to/weights.ckpt&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;accuracy&#39;</span><span class="p">})</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="mindconverter-cli-usage">
<h2>MindConverter CLI Usage<a class="headerlink" href="#mindconverter-cli-usage" title="Permalink to this headline"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>usage:<span class="w"> </span>mindconverter<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span><span class="o">[</span>--version<span class="o">]</span>
<span class="w">                     </span><span class="o">[</span>--model_file<span class="w"> </span>MODEL_FILE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--shape<span class="w"> </span>SHAPE<span class="w"> </span><span class="o">[</span>SHAPE<span class="w"> </span>...<span class="o">]]</span>
<span class="w">                     </span><span class="o">[</span>--input_nodes<span class="w"> </span>INPUT_NODES<span class="w"> </span><span class="o">[</span>INPUT_NODES<span class="w"> </span>...<span class="o">]]</span>
<span class="w">                     </span><span class="o">[</span>--output_nodes<span class="w"> </span>OUTPUT_NODES<span class="w"> </span><span class="o">[</span>OUTPUT_NODES<span class="w"> </span>...<span class="o">]]</span>
<span class="w">                     </span><span class="o">[</span>--output<span class="w"> </span>OUTPUT<span class="o">]</span><span class="w"> </span><span class="o">[</span>--report<span class="w"> </span>REPORT<span class="o">]</span>
</pre></div>
</div>
<p>Arguments are as follows：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Argument</p></th>
<th class="text-center head"><p>Mandatory</p></th>
<th class="head"><p>Description</p></th>
<th class="text-center head"><p>Type</p></th>
<th class="text-center head"><p>Default</p></th>
<th class="text-center head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-h, –help</p></td>
<td class="text-center"><p>N</p></td>
<td><p>Show the help message.</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>–version</p></td>
<td class="text-center"><p>N</p></td>
<td><p>Show the version info.</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>-</p></td>
</tr>
<tr class="row-even"><td><p>–model_file</p></td>
<td class="text-center"><p>Y</p></td>
<td><p>Specify the path of model file.</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>/path/to/model.onnx</p></td>
</tr>
<tr class="row-odd"><td><p>–shape</p></td>
<td class="text-center"><p>Y</p></td>
<td><p>Specify the input shape of model. Multiple inputs are separated by space.</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>1,3,224,224</p></td>
</tr>
<tr class="row-even"><td><p>–input_nodes</p></td>
<td class="text-center"><p>Y</p></td>
<td><p>Specify the input names of model. Multiple inputs are separated by space.</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>input_1:0</p></td>
</tr>
<tr class="row-odd"><td><p>–output_nodes</p></td>
<td class="text-center"><p>Y</p></td>
<td><p>Specify the output names of model. Multiple outputs are separated by space.</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-center"><p>-</p></td>
<td class="text-center"><p>output_1:0 output_2:0</p></td>
</tr>
<tr class="row-even"><td><p>–output</p></td>
<td class="text-center"><p>N</p></td>
<td><p>Specify the directory path for generated files.</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-center"><p>$PWD</p></td>
<td class="text-center"><p>/path/to/output/dir</p></td>
</tr>
</tbody>
</table>
</section>
<section id="model-list-supported-by-mindconverter">
<h2>Model List Supported by MindConverter<a class="headerlink" href="#model-list-supported-by-mindconverter" title="Permalink to this headline"></a></h2>
<p>For supported models (tested based on PyTorch 1.5.0 and TensorFlow 1.15.0, x86 Ubuntu released version), please refer to <a class="reference external" href="https://gitee.com/mindspore/mindinsight/blob/r1.6/ecosystem_tools/mindconverter/docs/supported_model_list.md#">LINK</a>.</p>
</section>
<section id="mindconverter-error-code-definition">
<h2>MindConverter Error Code Definition<a class="headerlink" href="#mindconverter-error-code-definition" title="Permalink to this headline"></a></h2>
<p>For error code defined in MindConverter, please refer to <a class="reference external" href="https://gitee.com/mindspore/mindinsight/blob/r1.6/ecosystem_tools/mindconverter/docs/error_code_definition.md#">LINK</a>.</p>
</section>
<section id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline"></a></h2>
<section id="install-dependencies-under-arm">
<h3>Install dependencies under ARM<a class="headerlink" href="#install-dependencies-under-arm" title="Permalink to this headline"></a></h3>
<p>MindConverter under the ARM environment requires compiling <code class="docutils literal notranslate"><span class="pre">protobuf</span></code>/<code class="docutils literal notranslate"><span class="pre">onnx</span></code>/<code class="docutils literal notranslate"><span class="pre">onnxoptimizer</span></code> from scratch. Since the compiling is complicated and error prone, we strongly recommend to use MindConverter under the x86 environment.</p>
<ol class="arabic">
<li><p>Compile <code class="docutils literal notranslate"><span class="pre">protobuf</span></code>(refer to <a class="reference external" href="https://github.com/onnx/onnx">ONNX</a>) and install cpp backend.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># build, compile and install protobuf</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/protocolbuffers/protobuf.git
<span class="nb">cd</span><span class="w"> </span>protobuf
git<span class="w"> </span>checkout<span class="w"> </span>v3.16.0
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
mkdir<span class="w"> </span>build_source
<span class="nb">cd</span><span class="w"> </span>build_source
cmake<span class="w"> </span>../cmake<span class="w"> </span>-Dprotobuf_BUILD_SHARED_LIBS<span class="o">=</span>OFF<span class="w"> </span>-DCMAKE_INSTALL_PREFIX<span class="o">=</span>/usr/local/protobuf<span class="w"> </span>-DCMAKE_INSTALL_SYSCONFDIR<span class="o">=</span>/etc<span class="w"> </span>-DCMAKE_POSITION_INDEPENDENT_CODE<span class="o">=</span>ON<span class="w"> </span>-Dprotobuf_BUILD_TESTS<span class="o">=</span>OFF<span class="w"> </span>-DCMAKE_BUILD_TYPE<span class="o">=</span>Release
make<span class="w"> </span>-j<span class="k">$(</span>nproc<span class="k">)</span>
make<span class="w"> </span>install

<span class="c1"># Install cpp backend.</span>
<span class="nb">cd</span><span class="w"> </span>../python
python<span class="w"> </span>setup.py<span class="w"> </span>install<span class="w"> </span>--cpp_implementation
</pre></div>
</div>
</li>
<li><p>Configure environment variables for <code class="docutils literal notranslate"><span class="pre">protobuf</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">PROTOBUF_PATH</span><span class="o">=</span>/usr/local/protobuf
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PROTOBUF_PATH</span>/bin:<span class="nv">$PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PKG_CONFIG_PATH</span><span class="o">=</span><span class="nv">$PROTOBUF_PATH</span>/lib/pkgconfig
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$PROTOBUF_PATH</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIBRARY_PATH</span><span class="o">=</span><span class="nv">$PROTOBUF_PATH</span>/lib:<span class="nv">$LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION</span><span class="o">=</span>cpp
</pre></div>
</div>
</li>
<li><p>Validate cpp backend of <code class="docutils literal notranslate"><span class="pre">protobuf</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">google.protobuf.internal</span> <span class="kn">import</span> <span class="n">api_implementation</span>
<span class="nb">print</span><span class="p">(</span><span class="n">api_implementation</span><span class="o">.</span><span class="n">Type</span><span class="p">())</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">onnx</span></code> should be recompiled and installed to guarantee running <code class="docutils literal notranslate"><span class="pre">onnx</span></code> with <code class="docutils literal notranslate"><span class="pre">protobuf</span></code> built by static library. Please refer to <a class="reference external" href="https://github.com/onnx/onnx">the installation guidance</a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/onnx/onnx.git
<span class="nb">cd</span><span class="w"> </span>onnx
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
<span class="c1"># prefer lite proto</span>
<span class="nb">set</span><span class="w"> </span><span class="nv">CMAKE_ARGS</span><span class="o">=</span>-DONNX_USE_LITE_PROTO<span class="o">=</span>ON
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
</li>
<li><p>Compile and install <code class="docutils literal notranslate"><span class="pre">onnxoptimizer</span></code>. Please refer to <a class="reference external" href="https://github.com/onnx/optimizer">the installation guidance</a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>--recursive<span class="w"> </span>https://github.com/onnx/optimizer<span class="w"> </span>onnxoptimizer
<span class="nb">cd</span><span class="w"> </span>onnxoptimizer
pip3<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
</li>
<li><p>Install <code class="docutils literal notranslate"><span class="pre">onnxruntime</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>onnxruntime~<span class="o">=</span><span class="m">1</span>.5.2
</pre></div>
</div>
</li>
</ol>
</section>
<section id="determine-the-form-of-model-input-shape">
<h3>Determine the form of model input shape<a class="headerlink" href="#determine-the-form-of-model-input-shape" title="Permalink to this headline"></a></h3>
<p>Using <a class="reference external" href="https://github.com/lutzroeder/netron">Netron</a> load the onnx model file, click the top node, and observe the type of each input in <code class="docutils literal notranslate"><span class="pre">INPUTS</span></code> in the sidebar. If the type is a specific value, such as <code class="docutils literal notranslate"><span class="pre">int64[1,9]</span></code>, the current input is static. Otherwise, it is dynamic, such as <code class="docutils literal notranslate"><span class="pre">int64[batch,sequence]</span></code>.</p>
</section>
<section id="export-the-model-file-of-tensorflow">
<h3>Export the model file of Tensorflow<a class="headerlink" href="#export-the-model-file-of-tensorflow" title="Permalink to this headline"></a></h3>
<p>Exporting the PB model file from a Tensorflow model requires operators mapping between <a class="reference external" href="https://github.com/onnx/tensorflow-onnx/blob/master/support_status.md#">Tensorflow</a> and <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#">ONNX</a>. For models defined by Keras, guidance is as follows:</p>
<p>TensorFlow 1.x</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">graph_io</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.applications.inception_v3</span> <span class="kn">import</span> <span class="n">InceptionV3</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">InceptionV3</span><span class="p">()</span>
<span class="n">INPUT_NODES</span> <span class="o">=</span> <span class="p">[</span><span class="n">ipt</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">ipt</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">]</span>
<span class="n">OUTPUT_NODES</span> <span class="o">=</span> <span class="p">[</span><span class="n">opt</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">outputs</span><span class="p">]</span>

<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_learning_phase</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span>
<span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">graph_inf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">graph_util</span><span class="o">.</span><span class="n">remove_training_nodes</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">())</span>
    <span class="n">graph_frozen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">graph_util</span><span class="o">.</span><span class="n">convert_variables_to_constants</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">graph_inf</span><span class="p">,</span> <span class="n">OUTPUT_NODES</span><span class="p">)</span>
    <span class="n">graph_io</span><span class="o">.</span><span class="n">write_graph</span><span class="p">(</span><span class="n">graph_frozen</span><span class="p">,</span> <span class="n">logdir</span><span class="o">=</span><span class="s2">&quot;/path/to/output/dir&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;model.pb&quot;</span><span class="p">,</span> <span class="n">as_text</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input nodes name: </span><span class="si">{</span><span class="n">INPUT_NODES</span><span class="si">}</span><span class="s2">, output nodes name: </span><span class="si">{</span><span class="n">OUTPUT_NODES</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>TensorFlow 2.x</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework.convert_to_constants</span> <span class="kn">import</span> <span class="n">convert_variables_to_constants_v2</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.applications</span> <span class="kn">import</span> <span class="n">InceptionV3</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">InceptionV3</span><span class="p">()</span>
<span class="n">spec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">full_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
<span class="n">frozen_func</span> <span class="o">=</span> <span class="n">convert_variables_to_constants_v2</span><span class="p">(</span><span class="n">full_model</span><span class="p">)</span>
<span class="n">frozen_func</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">write_graph</span><span class="p">(</span><span class="n">frozen_func</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">logdir</span><span class="o">=</span><span class="s2">&quot;/path/to/output/dir&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;model.pb&quot;</span><span class="p">,</span> <span class="n">as_text</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>TensorFlow is required for exporting PB model file but it is not explicitly declared as mandatory dependency for MindConverter. If the user wants to use graph based MindConverter, please install TensorFlow(TensorFlow 1.15.x is recommended).</p>
</section>
<section id="rectify-parameters-of-forward-function-definition">
<h3>Rectify parameters of forward function definition<a class="headerlink" href="#rectify-parameters-of-forward-function-definition" title="Permalink to this headline"></a></h3>
<p>Some models define non-Tensor parameters within forward function are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">Operator</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">LossFunction</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
<p>The above <code class="docutils literal notranslate"><span class="pre">label</span></code> is a non-Tensor parameter which needs to be rectified.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">Operator</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</section>
<section id="mix-the-mindspore-model-with-the-original-training-scripts">
<h3>Mix the MindSpore model with the original training scripts<a class="headerlink" href="#mix-the-mindspore-model-with-the-original-training-scripts" title="Permalink to this headline"></a></h3>
<p>Validate the equivalence of migration by mixing the MindSpore model and weights with the PyTorch training scripts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Replace the following classpath according to the actual situation.</span>
<span class="kn">from</span> <span class="nn">customized.path.to.mindspore.model</span> <span class="kn">import</span> <span class="n">MindSporeNetwork</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">MindSporeNetwork</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s1">&#39;network.ckpt&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="n">ms_data</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">ms_output</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">ms_data</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">ms_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">metric_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

</pre></div>
</div>
</section>
<section id="migration-reports-and-weights-mapping">
<h3>Migration reports and weights mapping<a class="headerlink" href="#migration-reports-and-weights-mapping" title="Permalink to this headline"></a></h3>
<p>For operators that are not successfully converted, the conversion report records the unconverted code lines and operator information, and at the same time identifies the input/output shape of the node in the code (represented as <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">output_shape</span></code>), which is convenient for users to modify manually. An example of the <code class="docutils literal notranslate"><span class="pre">Reshape</span></code> operator is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Classifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Suppose input of `reshape` is x.</span>
        <span class="n">reshape_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># skip codes ...</span>
</pre></div>
</div>
<p>It is convenient to replace the operators according to the <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">output_shape</span></code> parameters. The replacement is like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Classifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Suppose input of `reshape` is x.</span>
        <span class="n">reshape_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">))</span>
        <span class="c1"># skip codes ...</span>
</pre></div>
</div>
<p>Weight information in MindSpore(<code class="docutils literal notranslate"><span class="pre">converted_weight</span></code>) and that in source framework(<code class="docutils literal notranslate"><span class="pre">source_weight</span></code>) are saved in weight mapping. An example is as follows:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;resnet50&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;converted_weight&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;conv2d_0.weight&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;shape&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;data_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Float32&quot;</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;source_weight&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;conv1.weight&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;shape&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;data_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;float32&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="ast-based-model-migration">
<h3>AST-Based Model Migration<a class="headerlink" href="#ast-based-model-migration" title="Permalink to this headline"></a></h3>
<p>MindConverter supports AST-based model migration for PyTorch scripts. It parses and analyzes original scripts, then replaces them with the MindSpore AST to generate codes.</p>
<blockquote>
<div><p>Since the result may differ due to the coding style of original scripts, AST-based model migration is now DEPRECATED and will be removed in r2.0.</p>
</div></blockquote>
<p>Assume the PyTorch script is located at <code class="docutils literal notranslate"><span class="pre">/path/to/model.py</span></code>, and outputs the transformed MindSpore script to <code class="docutils literal notranslate"><span class="pre">/path/to/output/dir</span></code>. Use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mindconverter<span class="w"> </span>--in_file<span class="w"> </span>/path/to/model.py<span class="w"> </span>--output<span class="w"> </span>/path/to/output/dir
</pre></div>
</div>
<p>In the conversion report, non-converted code is listed as follows. <code class="docutils literal notranslate"><span class="pre">x,</span> <span class="pre">y</span></code> indicates the line number and the column number of the original scripts. For non-converted operators, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/migration_guide/en/r1.6/api_mapping/pytorch_api_mapping.html">MindSpore API mapping</a>. For unsupported operators, the corresponding code lines will remain in the original way.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>line x:y: [UnConvert] &#39;operator&#39; didn&#39;t convert. ...
</pre></div>
</div>
<p>For non-converted operators, suggestions are provided in the report. For example, MindConverter suggests that replace <code class="docutils literal notranslate"><span class="pre">torch.nn.AdaptiveAvgPool2d</span></code> with <code class="docutils literal notranslate"><span class="pre">mindspore.ops.ReduceMean</span></code> in <code class="docutils literal notranslate"><span class="pre">line</span> <span class="pre">157:23</span></code>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> [Start Convert]
 [Insert] &#39;from mindspore import ops&#39; is inserted to the converted file.
 line 1:0: [Convert] &#39;import torch&#39; is converted to &#39;import mindspore&#39;.
 ...
 line 157:23: [UnConvert] &#39;nn.AdaptiveAvgPool2d&#39; didn&#39;t convert. Maybe could convert to mindspore.ops.ReduceMean.
 ...
 [Convert Over]
</pre></div>
</div>
<p>The following cases are not supported:</p>
<ol class="arabic">
<li><p>Specific classes and functions.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> members, including <code class="docutils literal notranslate"><span class="pre">shape</span></code>，<code class="docutils literal notranslate"><span class="pre">ndim</span></code> and <code class="docutils literal notranslate"><span class="pre">dtype</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.AdaptiveXXXPoolXd</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.adaptive_XXX_poolXd()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.Dropout</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.Tensor.unsqueeze()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.chunk()</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.Tensor.chunk()</span></code></p></li>
</ul>
</li>
<li><p>Subclasses of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Code snippets from torchvision.models.mobilenet</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">ConvBNReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
       <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
       <span class="nb">super</span><span class="p">(</span><span class="n">ConvBNReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_planes</span><span class="p">),</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
       <span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hyper_parameters_auto_tuning.html" class="btn btn-neutral float-left" title="Use Mindoptimizer to Tune Hyperparameters" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="performance_profiling.html" class="btn btn-neutral float-right" title="Performance Profiling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>