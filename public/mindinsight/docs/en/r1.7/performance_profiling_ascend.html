<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance Profiling (Ascend-Graph) &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Performance Profiling (Ascend-PyNative)" href="performance_profiling_ascend_pynative.html" />
    <link rel="prev" title="Performance Profiling" href="performance_profiling.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_install.html">MindInsight Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="summary_record.html">Collecting Summary Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="dashboard.html">Viewing Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="lineage_and_scalars_comparison.html">Viewing Lineage and Scalars Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyper_parameters_auto_tuning.html">Use Mindoptimizer to Tune Hyperparameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrate_3rd_scripts_mindconverter.html">Migrating From Third Party Frameworks With MindConverter</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="performance_profiling.html">Performance Profiling</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Performance Profiling (Ascend-Graph)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operation-process">Operation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-the-training-script">Preparing the Training Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launch-mindinsight">Launch MindInsight</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-performance">Training Performance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#step-trace-analysis">Step Trace Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operator-performance-analysis">Operator Performance Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calculation-quantity-analysis">Calculation quantity analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-preparation-performance-analysis">Data Preparation Performance Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#timeline-analysis">Timeline Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#resource-utilization">Resource Utilization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cpu-utilization-analysis">CPU Utilization Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-analysis">Memory Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#specifications">Specifications</a></li>
<li class="toctree-l3"><a class="reference internal" href="#notices">Notices</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_profiling_ascend_pynative.html">Performance Profiling (Ascend-PyNative)</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_profiling_gpu.html">Performance Profiling (GPU-Graph)</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_profiling_of_cluster.html">Cluster Performance Profiling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="debugger.html">Debugger</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_explanation.html">Explain Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="landscape.html">Training Optimization Process Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_commands.html">MindInsight Commands</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tuning Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="accuracy_problem_preliminary_location.html">Guide to Locating Accuracy Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="accuracy_optimization.html">Accuracy Problem Locating and Optimization Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning_guide.html">Performance Tuning Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindinsight.debugger.html">mindinsight.debugger</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindconverter.html">mindconverter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="training_visual_design.html">Overall Design of Training Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_visual_design.html">Computational Graph Visualization Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_visual_design.html">Tensor Visualization Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler_design.html">Performance Profiling Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="performance_profiling.html">Performance Profiling</a> &raquo;</li>
      <li>Performance Profiling (Ascend-Graph)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/performance_profiling_ascend.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="performance-profiling-ascend-graph">
<h1>Performance Profiling (Ascend-Graph)<a class="headerlink" href="#performance-profiling-ascend-graph" title="Permalink to this headline"></a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/docs/blob/r1.7/docs/mindinsight/docs/source_en/performance_profiling_ascend.rst"><img alt="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source_en.png" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source_en.png" /></a>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This article describes how to use MindSpore Profiler for performance
debugging on Ascend AI processors.</p>
</section>
<section id="operation-process">
<h2>Operation Process<a class="headerlink" href="#operation-process" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Prepare a training script, add profiler APIs in the training script
and run the training script.</p></li>
<li><p>Start MindInsight and specify the summary-base-dir using startup
parameters, note that summary-base-dir is the parent directory of the
directory created by Profiler. For example, the directory created by
Profiler is <code class="docutils literal notranslate"><span class="pre">/home/user/code/data/</span></code>, the summary-base-dir should be
<code class="docutils literal notranslate"><span class="pre">/home/user/code</span></code>. After MindInsight is started, access the
visualization page based on the IP address and port number. The
default access IP address is <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8080</span></code>.</p></li>
<li><p>Find the training in the list, click the performance profiling link
and view the data on the web page.</p></li>
</ul>
</section>
<section id="preparing-the-training-script">
<h2>Preparing the Training Script<a class="headerlink" href="#preparing-the-training-script" title="Permalink to this headline"></a></h2>
<p>To enable the performance profiling of neural networks, MindSpore
Profiler APIs should be added into the script.</p>
<ul>
<li><p>Before training starts, the MindSpore <code class="docutils literal notranslate"><span class="pre">Profiler</span></code> object needs to be
initialized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The parameters of Profiler are as follows:
<a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/mindspore.profiler.html">https://www.mindspore.cn/docs/en/r1.7/api_python/mindspore.profiler.html</a></p>
</div>
</li>
<li><p>At the end of the training, <code class="docutils literal notranslate"><span class="pre">Profiler.analyse()</span></code> should be called
to finish profiling and generate the perforamnce analyse results.</p></li>
</ul>
<p>Profiler can control whether performance data collection is turned on or
off based on step (epoch) with the start_profile parameter. For the data
sinking mode of graph mode, CANN can only be told to turn on and off
after each epoch, so for the data sinking mode, it needs to turn on and
off based on the epoch.</p>
<p>The code for a normal scenario is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.profiler</span> <span class="kn">import</span> <span class="n">Profiler</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">generator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
    <span class="c1"># Init Profiler</span>
    <span class="c1"># Note that the Profiler should be initialized after context.set_context and before model.train</span>
    <span class="n">profiler</span> <span class="o">=</span> <span class="n">Profiler</span><span class="p">(</span><span class="n">output_path</span> <span class="o">=</span> <span class="s1">&#39;./profiler_data&#39;</span><span class="p">)</span>
    <span class="c1"># Train Model</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="c1"># Profiler end</span>
    <span class="n">profiler</span><span class="o">.</span><span class="n">analyse</span><span class="p">()</span>
</pre></div>
</div>
<p>Graph mode:</p>
<ul>
<li><p>When dataset_sink_mode is set to False, it needs to be enabled based
on step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">Callback</span>
<span class="k">class</span> <span class="nc">StopAtStep</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_step</span><span class="p">,</span> <span class="n">stop_step</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StopAtStep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_step</span> <span class="o">=</span> <span class="n">start_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_step</span> <span class="o">=</span> <span class="n">stop_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span> <span class="o">=</span> <span class="n">Profiler</span><span class="p">(</span><span class="n">start_profile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">step_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">step_num</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_step_num</span>
        <span class="k">if</span> <span class="n">step_num</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_step</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">step_num</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_step_num</span>
        <span class="k">if</span> <span class="n">step_num</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_step</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">analyse</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>When dataset_sink_mode is set to True, It needs to be enabled based
on epoch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">StopAtEpoch</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_epoch</span><span class="p">,</span> <span class="n">stop_epoch</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StopAtEpoch</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="n">start_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_epoch</span> <span class="o">=</span> <span class="n">stop_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span> <span class="o">=</span> <span class="n">Profiler</span><span class="p">(</span><span class="n">start_profile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">epoch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">epoch_num</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_epoch_num</span>
        <span class="k">if</span> <span class="n">epoch_num</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">epoch_num</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_epoch_num</span>
        <span class="k">if</span> <span class="n">epoch_num</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_epoch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">analyse</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<p>Custom training：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">profiler</span> <span class="o">=</span> <span class="n">Profiler</span><span class="p">(</span><span class="n">start_profile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="n">train</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">100</span><span class="p">:</span>
        <span class="n">profiler</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">200</span><span class="p">:</span>
        <span class="n">profiler</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

<span class="n">profiler</span><span class="o">.</span><span class="n">analyse</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="launch-mindinsight">
<h2>Launch MindInsight<a class="headerlink" href="#launch-mindinsight" title="Permalink to this headline"></a></h2>
<p>The MindInsight launch command can refer to <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.7/mindinsight_commands.html">MindInsight
Commands</a>.</p>
</section>
<section id="training-performance">
<h2>Training Performance<a class="headerlink" href="#training-performance" title="Permalink to this headline"></a></h2>
<p>Users can access the Training Performance by selecting a specific
training from the training list, and click the performance profiling
link.</p>
<figure class="align-default">
<img alt="performance_overall.png" src="_images/performance_overall.png" />
</figure>
<p><em>Figure:Overall Performance</em></p>
<p>Figure above displays the overall performance of the training, including the
overall data of Step Trace, Operator Performance, Data Preparation
Performance and Timeline. The data shown in these components include:</p>
<ul class="simple">
<li><p>Step Trace: It will divide the training steps into several stages and
collect execution time for each stage. The overall performance page
will show the step trace graph.</p></li>
<li><p>Operator Performance: It will collect the execution time of operators
and operator types. The overall performance page will show the pie
graph for different operator types.</p></li>
<li><p>Data Preparation Performance: It will analyse the performance of the
data input stages. The overall performance page will show the number
of steps that may be the bottleneck for these stages.</p></li>
<li><p>Timeline: It will collect execution time for stream tasks on the
devices. The tasks will be shown on the time axis. The overall
performance page will show the statistics for streams and tasks.</p></li>
</ul>
<p>Users can click the detail link to see the details of each components.
Besides, MindInsight Profiler will try to analyse the performance data,
the assistant on the left will show performance tuning suggestions for
this training.</p>
<section id="step-trace-analysis">
<h3>Step Trace Analysis<a class="headerlink" href="#step-trace-analysis" title="Permalink to this headline"></a></h3>
<div class="line-block">
<div class="line">The Step Trace Component is used to show the general performance of
the stages in the training. Step Trace will divide the training into
several stages:</div>
<div class="line">Step Gap (The time between the end of one step and the computation of
next step), Forward/Backward Propagation, All Reduce and Parameter
Update. It will show the execution time for each stage, and help to
find the bottleneck stage quickly.</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Step Trace does not support heterogeneous training currently.</p>
</div>
<figure class="align-default">
<img alt="step_trace.png" src="_images/step_trace.png" />
</figure>
<p><em>Figure:Step Trace Analysis</em></p>
<p>Figure above displays the Step Trace page. The Step Trace detail will show
the start/finish time for each stage. By default, it shows the average
time for all the steps. Users can also choose a specific step to see its
step trace statistics.</p>
<p>The graphs at the bottom of the page show the execution time of Step
Interval, Forward/Backward Propagation and Step Tail (The time between
the end of Backward Propagation and the end of Parameter Update) changes
according to different steps, it will help to decide whether we can
optimize the performance of some stages. Here are more details:</p>
<ul class="simple">
<li><p><strong>Step Interval</strong> is the duration for reading data from data queues.
If this part takes long time, it is advised to check the data
preparation for further analysis.</p></li>
<li><p><strong>Forward and Backward Propagation</strong> is the duration for executing
the forward and backward operations on the network, which handle the
main calculation work of a step. If this part takes long time, it is
advised to check the statistics of operators or timeline for further
analysis.</p></li>
<li><p><strong>Step Tail</strong> is the duration for performing parameter aggregation
and update operations in parallel training. If the operation takes
long time, it is advised to check the statistics of communication
operators and the status of parallelism.</p></li>
</ul>
<p>In order to divide the stages, the Step Trace Component need to figure
out the forward propagation start operator and the backward propagation
end operator. MindSpore will automatically figure out the two operators
to reduce the profiler configuration work. The first operator after
<code class="docutils literal notranslate"><span class="pre">get_next</span></code> will be selected as the forward start operator and the
operator before the last all reduce will be selected as the backward end
operator. <strong>However, Profiler do not guarantee that the automatically
selected operators will meet the user’s expectation in all cases.</strong>
Users can set the two operators manually as follows:</p>
<ul class="simple">
<li><p>Set environment variable <code class="docutils literal notranslate"><span class="pre">PROFILING_FP_START</span></code> to configure the
forward start operator, for example,
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">PROFILING_FP_START=fp32_vars/conv2d/BatchNorm</span></code>.</p></li>
<li><p>Set environment variable <code class="docutils literal notranslate"><span class="pre">PROFILING_BP_END</span></code> to configure the
backward end operator, for example,
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">PROFILING_BP_END=loss_scale/gradients/AddN_70</span></code>.</p></li>
</ul>
</section>
<section id="operator-performance-analysis">
<h3>Operator Performance Analysis<a class="headerlink" href="#operator-performance-analysis" title="Permalink to this headline"></a></h3>
<p>The operator performance analysis component is used to display the
execution time of the operators(AICORE/AICPU/HOSTCPU) during MindSpore
run.</p>
<ul class="simple">
<li><p>AICORE：AI Core operator is the main component of the computing core
of Ascend AI processor, which is responsible for executing vector and
tensor related computation intensive operators. TBE (Tensor Boost
Engine) is an extended operator development tool based on TVM (Tensor
Virtual Machine) framework. Users can use TBE to register AI Core
operator information.</p></li>
<li><p>AICPU：AI CPU operator is a kind of CPU operator (including control
operator, scalar, vector and other general-purpose calculations) that
AI CPU is responsible for executing Hisilicon SOC in Ascend
processor. The same operator in MindSpore may have AI Core operator
and AI CPU operator at the same time. The framework will give
priority to AI Core operator. If there is no AI Core operator or the
selection is not satisfied, AI CPU operator will be called.</p></li>
<li><p>HOSTCPU：The host side CPU is mainly responsible for distributing the
graph or operator to Ascend chip, and the operator can also be
developed on the host side CPU according to the actual needs. The
host CPU operator refers to the operator running on the host side
CPU.</p></li>
</ul>
<figure class="align-default">
<img alt="op_type_statistics.png" src="_images/op_type_statistics.PNG" />
</figure>
<p><em>Figure:Statistics for Operator Types</em></p>
<p>Figure above displays the statistics for the operator types, including:</p>
<ul class="simple">
<li><p>Choose pie or bar graph to show the proportion time occupied by each
operator type. The time of one operator type is calculated by
accumulating the execution time of operators belonging to this type.</p></li>
<li><p>Display top 20 operator types with the longest execution time, show
the proportion and execution time (ms) of each operator type.</p></li>
</ul>
<figure class="align-default">
<img alt="op_statistics.png" src="_images/op_statistics.PNG" />
</figure>
<p><em>Figure:Statistics for Operators</em></p>
<p>Figure above displays the statistics table for the operators, including:</p>
<ul class="simple">
<li><p>Choose All: Display statistics for the operators, including operator
name, type, execution time, full scope time, information, etc. The
table will be sorted by execution time by default.</p></li>
<li><p>Choose Type: Display statistics for the operator types, including
operator type name, execution time, execution frequency and
proportion of total time. Users can click on each line, querying for
all the operators belonging to this type.</p></li>
<li><p>Search: There is a search box on the right, which can support fuzzy
search for operators/operator types.</p></li>
</ul>
<p>Statistics for the information related to calculation quantity of AICORE
operator, including operator level and model level information.</p>
</section>
<section id="calculation-quantity-analysis">
<h3>Calculation quantity analysis<a class="headerlink" href="#calculation-quantity-analysis" title="Permalink to this headline"></a></h3>
<p>The Calculation Quantity Analysis module shows the actual calculation
quantity data, including calculation quantity data for operator
granularity, scope level granularity, and model granularity. The actual
calculation quantity refers to the amount of calculation that is running
on the device, which is different from the theoretical calculation
quantity. For example, the matrix computing unit on the Ascend910 device
is dealing with a matrix of 16x16 size, so in the runtime, the original
matrix will be padded to 16x16. Only calculation quantity on AICORE
devices is supported currently. The information about calculation
quantity has three indicators:</p>
<ul class="simple">
<li><p>FLOPs: the number of floating point operations（the unit is
million）.</p></li>
<li><p>FLOPS: the number of floating point operations per second (the unit
is billion).</p></li>
<li><p>FLOPS utilization: obtained by dividing the FLOPS by the peak FLOPS
of the AICORE device.</p></li>
</ul>
<figure class="align-default">
<img alt="flops_statistics.png" src="_images/flops-single-card.png" />
</figure>
<p><em>Figure:Calculation Quantity Analysis</em></p>
<p>The red box in figure above includes calculation quantity data on operator
granularity, scope level granularity, and model granularity. Click the
“details” to see the scope level calculation quantity data.</p>
<figure class="align-default">
<img alt="flops_scope_statistics.png" src="_images/flops-scope.png" />
</figure>
<p><em>Figure:Scope Level FLOPs</em></p>
<p>Figure above is a sankey diagram that presents data in the structure of a
tree where the cursor selects a scope to see the specific FLOPs value.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This figure only draws the Scope hierarchy structure of the operator
(the specific name of the operator at the last layer is not shown).
Since the depth of each operator’s hierarchy is not equal in the
training process, it may occur that the sum of time of adjacent
levels is not equal.</p>
</div>
</section>
<section id="data-preparation-performance-analysis">
<h3>Data Preparation Performance Analysis<a class="headerlink" href="#data-preparation-performance-analysis" title="Permalink to this headline"></a></h3>
<div class="line-block">
<div class="line">The Data preparation performance analysis component is used to analyse
the execution of data input pipeline for the training. The data input
pipeline can be divided into three stages:</div>
<div class="line">the data process pipeline, data transfer from host to device and data
fetch on device. The component will analyse the performance of each
stage in detail and display the results.</div>
</div>
<figure class="align-default">
<img alt="minddata_profile.png" src="_images/minddata_profile.png" />
</figure>
<p><em>Figure:Data Preparation Performance Analysis</em></p>
<p>Figure above displays the page of data preparation performance analysis
component. It consists of two tabs: the step gap and the data process.</p>
<p>The step gap page is used to analyse whether there is performance
bottleneck in the three stages. We can get our conclusion from the data
queue graphs:</p>
<ul class="simple">
<li><p>The data queue size stands for the queue length when the training
fetches data from the queue on the device. If the data queue size is
0, the training will wait until there is data in the queue; If the
data queue size is greater than 0, the training can get data very
quickly, and it means data preparation stage is not the bottleneck
for this training step.</p></li>
<li><p>The host queue size can be used to infer the speed of data process
and data transfer. If the host queue size is 0, it means we need to
speed up the data process stage.</p></li>
<li><p>If the size of the host queue is always large and the size of the
data queue is continuously small, there may be a performance
bottleneck in data transfer.</p></li>
</ul>
<figure class="align-default">
<img alt="data_op_profile.png" src="_images/data_op_profile.png" />
</figure>
<p><em>Figure:Data Process Pipeline Analysis</em></p>
<p>Figure above displays the page of data process pipeline analysis. The data
queues are used to exchange data between the data processing operators.
The data size of the queues reflect the data consume speed of the
operators, and can be used to infer the bottleneck operator. The queue
usage percentage stands for the average value of data size in queue
divide data queue maximum size, the higher the usage percentage, the
more data that is accumulated in the queue. The graph at the bottom of
the page shows the data processing pipeline operators with the data
queues, the user can click one queue to see how the data size changes
according to the time, and the operators connected to the queue. The
data process pipeline can be analysed as follows:</p>
<ul class="simple">
<li><p>When the input queue usage percentage of one operator is high, and
the output queue usage percentage is low, the operator may be the
bottleneck.</p></li>
<li><p>For the leftmost operator, if the usage percentage of all the queues
on the right are low, the operator may be the bottleneck.</p></li>
<li><p>For the rightmost operator, if the usage percentage of all the queues
on the left are high, the operator may be the bottleneck.</p></li>
</ul>
<p>To optimize the performance of data processing operators, there are some
suggestions:</p>
<ul class="simple">
<li><p>If the Dataset Operator is the bottleneck, try to increase the
<code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code>.</p></li>
<li><p>If a GeneratorOp type operator is the bottleneck, try to increase the
<code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> and replace the operator to
<code class="docutils literal notranslate"><span class="pre">MindRecordDataset</span></code>.</p></li>
<li><p>If a MapOp type operator is the bottleneck, try to increase the
<code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code>. If it is a python operator, try to optimize
the training script.</p></li>
<li><p>If a BatchOp type operator is the bottleneck, try to adjust the size
of <code class="docutils literal notranslate"><span class="pre">prefetch_size</span></code>.</p></li>
</ul>
</section>
<section id="timeline-analysis">
<h3>Timeline Analysis<a class="headerlink" href="#timeline-analysis" title="Permalink to this headline"></a></h3>
<p>The Timeline component can display:</p>
<ul class="simple">
<li><p>The operators (AICORE/AICPU/HOSTCPU operators) are executed on which
device.</p></li>
<li><p>The MindSpore stream split strategy for this neural network.</p></li>
<li><p>The execution sequence and execution time of the operator on the
device.</p></li>
<li><p>The step number of training (Currently dynamic shape scene,
multi-graph scene and heterogeneous training scene are not supported,
steps data may be inaccurate in these scene.).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Scope</span> <span class="pre">Name</span></code> of the operator, the number of each operator’s
<code class="docutils literal notranslate"><span class="pre">Scope</span> <span class="pre">Name</span></code> could be selected and download corresponding timeline
file. For example, the full name of one operator is
<code class="docutils literal notranslate"><span class="pre">Default/network/lenet5/Conv2D-op11</span></code>, thus the first <code class="docutils literal notranslate"><span class="pre">Scope</span> <span class="pre">Name</span></code>
of this operator is <code class="docutils literal notranslate"><span class="pre">Default</span></code>, the second <code class="docutils literal notranslate"><span class="pre">Scope</span> <span class="pre">Name</span></code> is
<code class="docutils literal notranslate"><span class="pre">network</span></code>. If two <code class="docutils literal notranslate"><span class="pre">Scope</span> <span class="pre">Name</span></code> for each operator is selected,
then the <code class="docutils literal notranslate"><span class="pre">Default</span></code> and <code class="docutils literal notranslate"><span class="pre">network</span></code> will be displayed.</p></li>
</ul>
<p>Users can get the most detailed information from the Timeline:</p>
<ul class="simple">
<li><p>From the High level, users can analyse whether the stream split
strategy can be optimized and whether the step tail is too long.</p></li>
<li><p>From the Low level, users can analyse the execution time for all the
operators, etc.</p></li>
</ul>
<p>Users can click the download button on the overall performance page to
view Timeline details. The Timeline data file (json format) will be
stored on local machine, and can be displayed by tools. We suggest to
use <code class="docutils literal notranslate"><span class="pre">chrome://tracing</span></code> or
<a class="reference external" href="https://ui.perfetto.dev/#!viewer">Perfetto</a> to visualize the
Timeline.</p>
<ul class="simple">
<li><p>Chrome tracing: Click “load” on the upper left to load the file.</p></li>
<li><p>Perfetto: Click “Open trace file” on the left to load the file.</p></li>
</ul>
<figure class="align-default">
<img alt="timeline.png" src="_images/timeline.png" />
</figure>
<p><em>Figure:Timeline Analysis</em></p>
<p>The Timeline consists of the following parts:</p>
<ul>
<li><p>Device and Stream List: It will show the stream list on each device.
Each stream consists of a series of tasks. One rectangle stands for
one task, and the area stands for the execution time of the task.</p>
<p>Each color block represents the starting time and length of operator
execution. The detailed explanation of timeline is as follows:</p>
<ul class="simple">
<li><p>Process Device ID: contains the timeline of operators executed on
AI Core.</p>
<ul>
<li><p>Step: the number of training steps.</p></li>
<li><p>Scope Name: the Scope Name of operators.</p></li>
<li><p>Stream #ID: operators executed on the stream.</p></li>
</ul>
</li>
<li><p>Process AI CPU Op: the timeline of operators executed on the AI
CPU.</p></li>
<li><p>Process Communication Op: the timeline for the execution of
communication operators.</p></li>
<li><p>Process Host CPU Op: contains the timeline of operators executed
on the Host CPU.</p></li>
<li><p>Process Op Overlap Analyse: the timeline of all computation
operators and communication operators merged, it can be used to
analyse the proportion of communication time.</p>
<ul>
<li><p>Merged Computation Op: it is the timeline after all computation
operators are merged.</p></li>
<li><p>Merged Communication Op: it is the timeline after all
communication operators are merged.</p></li>
<li><p>Pure Communication Op: pure communication time (the timeline of
the communication operator after removing the overlap with the
computation operator time).</p></li>
<li><p>Free Time: there is no communication operator and calculation
operator in the execution timeline.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>The Operator Information: When we click one task, the corresponding
operator of this task will be shown at the bottom.</p></li>
</ul>
<p>W/A/S/D can be applied to zoom in and out of the Timeline graph.</p>
</section>
</section>
<section id="resource-utilization">
<h2>Resource Utilization<a class="headerlink" href="#resource-utilization" title="Permalink to this headline"></a></h2>
<p>Resource utilization includes cpu usage analysis and memory usage
analysis.</p>
<figure class="align-default">
<img alt="resource_visibility.png" src="_images/resource_visibility.png" />
</figure>
<p><em>Figure:Overview of resource utilization</em></p>
<p>Overview of resource utilization：Including CPU utilization analysis and
memory usage analysis. You can view the details by clicking the View
Details button in the upper right corner.</p>
<section id="cpu-utilization-analysis">
<h3>CPU Utilization Analysis<a class="headerlink" href="#cpu-utilization-analysis" title="Permalink to this headline"></a></h3>
<p>CPU utilization, which is mainly used to assist performance debugging.
After the performance bottleneck is determined according to the queue
size, the performance can be debugged according to the CPU utilization
(if the user utilization is too low, increase the number of threads; if
the system utilization is too high, decrease the number of threads). CPU
utilization includes CPU utilization of the whole machine, process and
Data pipeline operator.</p>
<figure class="align-default">
<img alt="device_utilization.png" src="_images/device_cpu_utilization.png" />
</figure>
<p><em>Figure:CPU utilization of the whole machine</em></p>
<p>CPU utilization of the whole machine: Show the overall CPU usage of the
device in the training process, including user utilization, system
utilization, idle utilization, IO utilization, current number of active
processes, and context switching times. If the user utilization is low,
you can try to increase the number of operator threads to increase the
CPU utilization; if the system utilization is high, and the number of
context switching and CPU waiting for processing is large, it indicates
that the number of threads needs to be reduced accordingly.</p>
<figure class="align-default">
<img alt="process_cpu_utilization.png" src="_images/process_cpu_utilizaton.png" />
</figure>
<p><em>Figure:Process utilization</em></p>
<p>Process utilization: Show the CPU usage of a single process. The
combination of whole machine utilization and process utilization can
determine whether other processes affect the training process.</p>
<figure class="align-default">
<img alt="data_op_utilization.png" src="_images/data_op_utilization.png" />
</figure>
<p><em>Figure:Operator utilization</em></p>
<p>Operator utilization: Show the CPU utilization of Data pipeline single
operator. We can adjust the number of threads of the corresponding
operator according to the actual situation. If the number of threads is
small and takes up a lot of CPU, you can consider whether you need to
optimize the code.</p>
<p>Common scenarios of CPU utilization:</p>
<ul class="simple">
<li><p>According to the queue size, the network debugging personnel can
judge that the performance of MindData has a bottleneck. They can
adjust the number of threads by combining the utilization rate of the
whole machine and the utilization rate of the operator.</p></li>
<li><p>Developers can check the utilization of operators. If an operator
consumes CPU utilization, they can confirm whether the code needs to
be optimized.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default sampling interval is 1000ms. You can change the sampling
interval through
<code class="docutils literal notranslate"><span class="pre">mindspore.dataset.config.get_monitor_sampling_interval()</span></code>. For
details：</p>
<p><a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/mindspore.dataset.config.html#mindspore.dataset.config.set_monitor_sampling_interval">https://www.mindspore.cn/docs/en/r1.7/api_python/mindspore.dataset.config.html#mindspore.dataset.config.set_monitor_sampling_interval</a></p>
</div>
</section>
<section id="memory-analysis">
<h3>Memory Analysis<a class="headerlink" href="#memory-analysis" title="Permalink to this headline"></a></h3>
<p>This page is used to show the memory usage of the neural network model
on the <strong>device</strong>, which is an <strong>ideal prediction</strong> based on the
theoretical calculation results. The content of the page includes:</p>
<ul class="simple">
<li><p>An overview of the memory usage of the model, including the total
available memory, peak memory and other information.</p></li>
<li><p>The memory occupied varies in the execution order while the model is
running.</p></li>
<li><p>The memory usage of each operator is decomposed and displayed in the
table of <code class="docutils literal notranslate"><span class="pre">Operator</span> <span class="pre">Memory</span> <span class="pre">Allocation</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Memory Analysis does not support heterogeneous training currently.</p>
</div>
<figure class="align-default">
<img alt="memory.png" src="_images/memory.png" />
</figure>
<p><em>Figure:Memory Analysis</em></p>
<p>Users can obtain the summary of memory usage via the
<code class="docutils literal notranslate"><span class="pre">Memory</span> <span class="pre">Allocation</span> <span class="pre">Overview</span></code>. In addition, they can obtain more
detailed information from <code class="docutils literal notranslate"><span class="pre">Memory</span> <span class="pre">Usage</span></code>, including:</p>
<ul class="simple">
<li><p><strong>Line Chart</strong>: Changes in model memory usage, including static
memory, total occupied memory and total available memory.</p></li>
<li><p><strong>Zooming</strong>: There is a zoom scroll bar under the line chart. Users
can zoom in or out the line chart by adjusting its size to observe
more details.</p></li>
<li><p><strong>FP/BP</strong>: The execution positions of the start of
<code class="docutils literal notranslate"><span class="pre">Forward</span> <span class="pre">Propagation</span></code> and the end of <code class="docutils literal notranslate"><span class="pre">Backward</span> <span class="pre">Propagation</span></code> of
the model on the line chart.</p></li>
<li><p><strong>Details of Nodes</strong>: Hovering over the line chart, the information
of the corresponding execution operator is shown, including the
execution order of the operator, the name of the operator, the memory
occupied by the operator, the total memory occupied by the model in
the current position, and the relative memory change compared with
the previous execution position.</p></li>
<li><p><strong>Memory Decomposition</strong>: Left clicking a position on the line chart,
the memory breakdowns of the execution position is shown in the table
below the line chart, called <code class="docutils literal notranslate"><span class="pre">Operator</span> <span class="pre">Memory</span> <span class="pre">Allocation</span></code>. The
table shows the memory decomposition of the corresponding execution
position, i.e., the output tensor of which operators are allocated
the occupied memory of the current execution position. The module
provides users with abundant information, including tensor name,
tensor size, tensor type, data type, shape, format, and the active
lifetime of tensor memory.</p></li>
</ul>
<figure class="align-default">
<img alt="memory_graphics.png" src="_images/memory_graphics.png" />
</figure>
<p><em>Figure:Memory Statistics</em></p>
</section>
</section>
<section id="specifications">
<h2>Specifications<a class="headerlink" href="#specifications" title="Permalink to this headline"></a></h2>
<ul>
<li><p>To limit the data size generated by the Profiler, MindInsight
suggests that for large neural network, the profiled steps should be
less than 10.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The number of steps can be controlled by controlling the size of
training dataset. For example, the <code class="docutils literal notranslate"><span class="pre">num_samples</span></code> parameter in
<code class="docutils literal notranslate"><span class="pre">mindspore.dataset.MindDataset</span></code> can control the size of the
dataset. For details, please refer to:
<a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/dataset/mindspore.dataset.MindDataset.html">https://www.mindspore.cn/docs/en/r1.7/api_python/dataset/mindspore.dataset.MindDataset.html</a></p>
</div>
</li>
<li><p>The parse of Timeline data is time consuming, and usually the data of
a few steps is enough to analyze the results. In order to speed up
the data parse and UI display, Profiler will show at most 20M data
(Contain 10+ step information for large networks).</p></li>
</ul>
</section>
<section id="notices">
<h2>Notices<a class="headerlink" href="#notices" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Currently the training and inference process does not support
performance debugging, only individual training or inference is
supported.</p></li>
<li><p>Ascend performance debugging does not support dynamic Shape
scenarios, multi-subgraph scenarios, and control flow scenarios.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="performance_profiling.html" class="btn btn-neutral float-left" title="Performance Profiling" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="performance_profiling_ascend_pynative.html" class="btn btn-neutral float-right" title="Performance Profiling (Ascend-PyNative)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>