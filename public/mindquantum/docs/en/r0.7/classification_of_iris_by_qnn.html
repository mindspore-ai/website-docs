<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Classification of iris by quantum neural network &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quantum Approximate Optimization Algorithm" href="quantum_approximate_optimization_algorithm.html" />
    <link rel="prev" title="Advanced gradient calculation of variational quantum circuits" href="get_gradient_of_PQC_with_mindquantum.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindquantum_install.html">MindQuantum Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="parameterized_quantum_circuit.html">Variational Quantum Circuit</a></li>
<li class="toctree-l1"><a class="reference internal" href="initial_experience_of_quantum_neural_network.html">Initial experience of quantum neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_gradient_of_PQC_with_mindquantum.html">Advanced gradient calculation of variational quantum circuits</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Variational Quantum Algorithm</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Classification of iris by quantum neural network</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#importing-the-iris-dataset">Importing the iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-visualization">Data visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-preprocessing">Data preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-encoder">Building Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-ansatz">Building Ansatz</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-the-hamiltonian">Building the Hamiltonian</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-a-quantum-neural-network">Building a quantum neural network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#accuracy-during-training">Accuracy during training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#predicting">Predicting</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quantum_approximate_optimization_algorithm.html">Quantum Approximate Optimization Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="qnn_for_nlp.html">The Application of Quantum Neural Network in NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="vqe_for_quantum_chemistry.html">VQE Application in Quantum Chemistry Computing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">General Quantum Algorithm</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quantum_phase_estimation.html">Quantum Phase Estimation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="grover_search_algorithm.html">Grover search algorithm based on MindQuantum</a></li>
<li class="toctree-l1"><a class="reference internal" href="shor_algorithm.html">Shor’s algorithm based on MindQuantum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.core.html">mindquantum.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.simulator.html">mindquantum.simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.framework.html">mindquantum.framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.algorithm.html">mindquantum.algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.io.html">mindquantum.io</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.engine.html">mindquantum.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindquantum.utils.html">mindquantum.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Classification of iris by quantum neural network</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/classification_of_iris_by_qnn.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="classification-of-iris-by-quantum-neural-network">
<h1>Classification of iris by quantum neural network<a class="headerlink" href="#classification-of-iris-by-quantum-neural-network" title="Permalink to this headline"></a></h1>
<p>Translator: <a class="reference external" href="https://gitee.com/unseenme">unseenme</a></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/mindquantum/docs/source_en/classification_of_iris_by_qnn.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In the previous tutorial, we introduced what a variational quantum circuit is, and experienced how to build a quantum neural network to solve a small problem by a simple example. In this tutorial, we will experience the upgrade, and will introduce how to solve problems in classical machine learning by building quantum neural networks. The problem we choose is: Iris classification problem in supervised learning.</p>
<p>Problem description: The iris dataset is a commonly used dataset in classical machine learning. The dataset contains a total of 150 samples (divided into 3 different subgenus: setosa, versicolor and virginica, 50 samples for each subgenus), each sample contains 4 features, sepal length, sepal width, petal length and petal width.</p>
<p>We selected the first 100 samples (setosa and versicolor), and randomly selected 80 samples as the training set, and trained the quantum classifier (Ansatz) by building a quantum neural network. After training, the classification test is performed on the remaining 20 samples, and the prediction accuracy is expected to be as high as possible.</p>
<p>Idea: We need to divide the 100 samples into 80 training samples and 20 test samples, calculate the parameters required to build the encoder according to the classical data of the training samples, and then build the encoder to encode the classical data of the training samples into quantum state, then, build Ansatz, train the parameters in Ansatz by the built quantum neural network layer and MindSpore operator, to obtain the final classifier, and finally, classify the remaining 20 test samples to obtain predictions ‘s accuracy.</p>
</section>
<section id="environment-preparation">
<h2>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h2>
<p>First, we need to import the iris dataset, and before importing the dataset, we need to use the datasets module in the sklearn library, so the reader needs to check whether the sklearn library is installed, and for checking, we can execute the following code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>show<span class="w"> </span>scikit-learn
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Name: scikit-learn
Version: 1.0.1
Summary: A set of python modules for machine learning and data mining
Home-page: http://scikit-learn.org
Author:
Author-email:
License: new BSD
Location: /usr/local/lib/python3.7/dist-packages
Requires: joblib, numpy, scipy, threadpoolctl
Required-by:
</pre></div>
</div>
<p>If no error is reported, it means it has been installed. To briefly explain, sklearn is the abbreviation of scikit-learn, which is a third-party module based on Python. The sklearn library integrates some common machine learning methods. When performing machine learning tasks, there is no need to implement algorithms. Most machine learning tasks can be completed by simply calling the modules provided in the sklearn library.</p>
<p>If the sklearn library is not installed, it can be installed by running the following code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>scikit-learn
</pre></div>
</div>
<p>Then, we set the number of threads required for this tutorial.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>                                                 <span class="c1"># import os library</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OMP_NUM_THREADS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;2&#39;</span>                       <span class="c1"># Set the number of threads of the quantum circuit simulator to 2 by os.environ</span>
</pre></div>
</div>
<p>Note:</p>
<p>(1) os is a standard library that contains many functions for manipulating files and directories;</p>
<p>(2) The os.environ() module can obtain and modify environment variables; in general, we need to set the number of threads at the beginning;</p>
</section>
<section id="importing-the-iris-dataset">
<h2>Importing the iris dataset<a class="headerlink" href="#importing-the-iris-dataset" title="Permalink to this headline"></a></h2>
<p>With the above preparation, now we can import the iris dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                        <span class="c1"># Import the numpy library and abbreviated as np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>                              <span class="c1"># Import datasets module for loading iris datasets</span>

<span class="n">iris_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>                       <span class="c1"># Load the iris dataset and save to iris_dataset</span>

<span class="nb">print</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                            <span class="c1"># Print the data dimensions of the samples of iris_dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>                         <span class="c1"># Print the feature names of the samples of iris_dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>                          <span class="c1"># Print the subgenus names contained in the samples of iris_dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>                                <span class="c1"># print an array of labels for the samples of iris_dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                          <span class="c1"># Print the data dimension of the labels of the samples of iris_dataset</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(150, 4)
[&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]
[&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
(150,)
</pre></div>
</div>
<p>As can be seen from the above print, the dataset has a total of 150 samples, and each sample has 4 features: sepal length, sepal width, petal length and petal width. And the sample contains 3 different subgenera: setosa, versicolor and virginica, each sample has a corresponding classification number, 0 means the sample belongs to setosa, 1 means the sample belongs to versicolor, 2 means the sample belongs to virginica, so there is an array of 150 numbers to represent the subgenus type of the sample.</p>
<p>Since we only select the first 100 samples, execute the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>         <span class="c1"># Select the first 100 data of the data of iris_dataset, convert its data type to float32, and store it in X</span>
<span class="n">X_feature_names</span> <span class="o">=</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">feature_names</span>              <span class="c1"># Store the feature names of iris_dataset in X_feature_names</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>                 <span class="c1"># Select the first 100 data of the target of iris_dataset, convert its data type to int, and store it in y</span>
<span class="n">y_target_names</span> <span class="o">=</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">target_names</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>            <span class="c1"># Select the first 2 data of target_names of iris_dataset and store them in y_target_names</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                                            <span class="c1"># Data dimensions for print samples</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_feature_names</span><span class="p">)</span>                                    <span class="c1"># feature name of the print sample</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_target_names</span><span class="p">)</span>                                     <span class="c1"># Subgenus names included in the print sample</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>                                                  <span class="c1"># an array of labels to print the samples</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                                            <span class="c1"># Data dimension of labels for printing samples</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(100, 4)
[&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]
[&#39;setosa&#39; &#39;versicolor&#39;]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
(100,)
</pre></div>
</div>
<p>As can be seen from the above print, there are only 100 samples in the dataset X at this time, and each sample still has 4 features, which are still sepal length, sepal width, petal length and petal width. At this time, there are only 2 different subgenera: setosa and versicolor, and each sample has a corresponding classification number, 0 means it belongs to setosa, 1 means it belongs to versicolor, so there is an array of 100 numbers to represent the subgenus type of the sample.</p>
</section>
<section id="data-visualization">
<h2>Data visualization<a class="headerlink" href="#data-visualization" title="Permalink to this headline"></a></h2>
<p>In order to understand the data set composed of these 100 samples more intuitively, we draw a scatter plot composed of different features of all samples, and execute the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                                                           <span class="c1"># Import the matplotlib.pyplot module and abbreviated to plt</span>

<span class="n">feature_name</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;sepal length&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;sepal width&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;petal length&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;petal width&#39;</span><span class="p">}</span> <span class="c1"># Label the different feature names as 0, 1, 2, 3</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="mi">23</span><span class="p">))</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>                                        <span class="c1"># Draw a graph of size 23*23, including 4*4=16 subgraphs</span>

<span class="n">colormap</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;g&#39;</span><span class="p">}</span>                                                               <span class="c1"># Make the sample with label 0 red and the sample with label 1 green</span>
<span class="n">cvalue</span> <span class="o">=</span> <span class="p">[</span><span class="n">colormap</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y</span><span class="p">]</span>                                                         <span class="c1"># Set the corresponding color of the label corresponding to 100 samples</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>                                                               <span class="c1"># Start drawing on the subgraph of [i][j]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">cvalue</span><span class="p">)</span>                                        <span class="c1"># Draw a scatterplot of the [i]th feature and the [j]th feature</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">feature_name</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>                                   <span class="c1"># Set the name of the X-axis to the [i]th feature name and the font size to 22</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">feature_name</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>                                   <span class="c1"># Set the name of the Y-axis to the [j]th feature name and font size to 22</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>                                                                                <span class="c1"># render the image</span>
</pre></div>
</div>
<p><img alt="Data Visualization" src="_images/data_classification_of_iris_by_qnn.png" /></p>
<p>As can be seen from the images presented above, the red dots represent the samples with the label “0”, and the green dots represent the samples with the label “1”. In addition, we found that the different characteristics of the two types of samples are relatively easy to distinguish.</p>
</section>
<section id="data-preprocessing">
<h2>Data preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline"></a></h2>
<p>Next, we need to calculate the parameters used to build the Encoder, and then divide the data set into training set and test set, and execute the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>           <span class="c1"># In each sample, a parameter is calculated using two adjacent feature values, that is, each sample will have 3 more parameters (because there are 4 feature values), and stored in alpha</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># In the dimension of axis=1, add the data values of alpha to the feature values of X</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                        <span class="c1"># Print the data dimension of the sample of X at this time</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(100, 7)
</pre></div>
</div>
<p>It can be seen from the above print that there are still 100 samples in the dataset X at this time, but each sample has 7 features at this time, the first 4 feature values are the original feature values, and the last 3 feature values are the ones calculated by the above preprocessing. The specific calculation formula is as follows:</p>
<div class="math notranslate nohighlight">
\[ X_{i+4}^{j} = X_{i}^{j} * X_{i+1}^{j}, i=0,1,2,j=1,2,...,100. \]</div>
<p>Finally, we divide the data set at this time into training set and test set, and execute the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>                                                   <span class="c1"># Import the train_test_split function for dividing the dataset</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Divide the dataset into training and test sets</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                                                                                   <span class="c1"># Print the data type of the samples in the training set</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                                                                                    <span class="c1"># Print the data type of the samples in the test set</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(80, 7)
(20, 7)
</pre></div>
</div>
<p>As can be seen from the above print, the training set at this time has 80 samples, the test set has 20 samples, and each sample has 7 features.</p>
<p>Note:</p>
<p>(1) append is mainly used to add some values ​​to the original array. The general format is as follows: np.append(arr, values, axis=None), <code class="docutils literal notranslate"><span class="pre">arr</span></code> is the array which the value needs to be added to, and <code class="docutils literal notranslate"><span class="pre">values</span></code> ​​is the value added to the array <code class="docutils literal notranslate"><span class="pre">arr</span></code>, <code class="docutils literal notranslate"><span class="pre">axis</span></code> indicates in which direction;</p>
<p>(2) <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> means that the data set is shuffled and returned in a different order each time. <code class="docutils literal notranslate"><span class="pre">Shuffle</span></code> is to avoid the impact of the order of data input on network training. Increase the randomness, improve the generalization performance of the network, avoid the gradient of weight update being too extreme due to the appearance of regular data, and avoid overfitting or underfitting of the final model.</p>
<p>(3) <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> is a commonly used function in cross-validation. It is mainly used to randomly select training data sets and test data sets in proportion from samples. The general format is as follows: <code class="docutils literal notranslate"><span class="pre">X_train,</span> <span class="pre">X_test,</span> <span class="pre">y_train,</span> <span class="pre">y_test</span> <span class="pre">=</span> <span class="pre">train_test_split(X,</span> <span class="pre">y,</span> <span class="pre">test_size</span> <span class="pre">,</span> <span class="pre">random_state,</span> <span class="pre">shuffle=True)</span></code>, <code class="docutils literal notranslate"><span class="pre">test_size</span></code> represents the proportion of test samples, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> represents the seed for generating random numbers, and <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> represents shuffling the dataset.</p>
</section>
<section id="building-encoder">
<h2>Building Encoder<a class="headerlink" href="#building-encoder" title="Permalink to this headline"></a></h2>
<p>According to the quantum circuit diagram shown, we can build an Encoder in MindQuantum to encode classical data into quantum states.</p>
<p><img alt="Encoder" src="_images/encoder_classification_of_iris_by_qnn.png" /></p>
<p>Here, the encoding method we use is IQP encoding (Instantaneous Quantum Polynomial encoding). Generally speaking, the encoding method of the Encoder is not fixed, and different encoding methods can be selected according to the needs of the problem, and sometimes the Encoder will be adjusted according to the final performance.</p>
<p>The values of the parameters α0, α1,…,α6 in the Encoder are substituted with the 7 feature values obtained in the above data preprocessing. ​</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pylint: disable=W0104</span>
<span class="kn">from</span> <span class="nn">mindquantum.core.circuit</span> <span class="kn">import</span> <span class="n">Circuit</span>         <span class="c1"># Import the Circuit module for building quantum circuits</span>
<span class="kn">from</span> <span class="nn">mindquantum.core.circuit</span> <span class="kn">import</span> <span class="n">UN</span>              <span class="c1"># Import UN module</span>
<span class="kn">from</span> <span class="nn">mindquantum.core.gates</span> <span class="kn">import</span> <span class="n">H</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">RZ</span>          <span class="c1"># Import quantum gates H, X, RZ</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">Circuit</span><span class="p">()</span>                                  <span class="c1"># Initialize the quantum circuit</span>

<span class="n">encoder</span> <span class="o">+=</span> <span class="n">UN</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>                                  <span class="c1"># H gates act on every 1 qubit</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>                                   <span class="c1"># i = 0, 1, 2, 3</span>
    <span class="n">encoder</span> <span class="o">+=</span> <span class="n">RZ</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;alpha</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>                 <span class="c1"># The RZ(alpha_i) gate acts on the ith qubit</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>                                   <span class="c1"># j = 0, 1, 2</span>
    <span class="n">encoder</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>                          <span class="c1"># The X gate acts on the j+1th qubit and is controlled by the jth qubit</span>
    <span class="n">encoder</span> <span class="o">+=</span> <span class="n">RZ</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;alpha</span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">4</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>             <span class="c1"># The RZ(alpha_{j+4}) gate acts on the 0th qubit</span>
    <span class="n">encoder</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>                          <span class="c1"># The X gate acts on the j+1th qubit and is controlled by the jth qubit</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>                          <span class="c1"># As the first layer of the entire quantum neural network, the Encoder does not need to take the derivative of the gradient in the encoding circuit, so no_grad() is added.</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>                                    <span class="c1"># Summary Encoder</span>
<span class="n">encoder</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>==================================Circuit Summary==================================
|Total number of gates  : 17.                                                     |
|Parameter gates        : 7.                                                      |
|with 7 parameters are  : alpha0, alpha1, alpha2, alpha3, alpha4, alpha5, alpha6. |
|Number qubit of circuit: 4                                                       |
===================================================================================

q0: ──H────RZ(alpha0)────●──────────────────●──────────────────────────────────────────────────
                         │                  │
q1: ──H────RZ(alpha1)────X────RZ(alpha4)────X────●──────────────────●──────────────────────────
                                                 │                  │
q2: ──H────RZ(alpha2)────────────────────────────X────RZ(alpha5)────X────●──────────────────●──
                                                                         │                  │
q3: ──H────RZ(alpha3)────────────────────────────────────────────────────X────RZ(alpha6)────X──
</pre></div>
</div>
<p>As can be seen from the summary of the Encoder, the quantum circuit consists of 17 quantum gates, of which 7 contain parameter quantum gates and the parameters are α0, α1, …, α6, and the number of qubits regulated by the quantum circuit is 4.</p>
<p>Note:</p>
<p>The UN module is used to map quantum gates to different target qubits and control qubits. The general format is as follows: <code class="docutils literal notranslate"><span class="pre">mindquantum.circuit.UN(gate,</span> <span class="pre">maps_obj,</span> <span class="pre">maps_ctrl=None)</span></code>, the gate in parentheses is the quantum gate we need to execute, <code class="docutils literal notranslate"><span class="pre">maps_obj</span></code> is the target qubit that needs to execute the quantum gate, <code class="docutils literal notranslate"><span class="pre">maps_ctrl</span></code> is the control qubit, if it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, there is no control qubit. If each qubit implements the same nonparametric quantum gate, <code class="docutils literal notranslate"><span class="pre">UN(gate,</span> <span class="pre">N)</span></code> can be written directly, where <code class="docutils literal notranslate"><span class="pre">N</span></code> represents the number of qubits.</p>
</section>
<section id="building-ansatz">
<h2>Building Ansatz<a class="headerlink" href="#building-ansatz" title="Permalink to this headline"></a></h2>
<p>According to the quantum circuit diagram shown, we can build Ansatz in MindQuantum.</p>
<p><img alt="Ansatz" src="_images/ansatz_classification_of_iris_by_qnn.png" /></p>
<p>Like Encoder, Ansatz’s encoding method is not fixed, and we can try different encoding methods to test the final result.</p>
<p>Here, we use HardwareEfficientAnsatz, the encoding method shown in the above quantum circuit diagram.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pylint: disable=W0104</span>
<span class="kn">from</span> <span class="nn">mindquantum.algorithm.nisq</span> <span class="kn">import</span> <span class="n">HardwareEfficientAnsatz</span>                                      <span class="c1"># Import HardwareEfficientAnsatz</span>
<span class="kn">from</span> <span class="nn">mindquantum.core.gates</span> <span class="kn">import</span> <span class="n">RY</span>                                                               <span class="c1"># Import quantum gate RY</span>

<span class="n">ansatz</span> <span class="o">=</span> <span class="n">HardwareEfficientAnsatz</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">single_rot_gate_seq</span><span class="o">=</span><span class="p">[</span><span class="n">RY</span><span class="p">],</span> <span class="n">entangle_gate</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">circuit</span>     <span class="c1"># Building Ansatz with HardwareEfficientAnsatz</span>
<span class="n">ansatz</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>                                                                                    <span class="c1"># Summary Ansatz</span>
<span class="n">ansatz</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>====================================================Circuit Summary====================================================
|Total number of gates  : 25.                                                                                         |
|Parameter gates        : 16.                                                                                         |
|with 16 parameters are : d0_n0_0, d0_n1_0, d0_n2_0, d0_n3_0, d1_n0_0, d1_n1_0, d1_n2_0, d1_n3_0, d2_n0_0, d2_n1_0... |
|Number qubit of circuit: 4                                                                                           |
=======================================================================================================================

q0: ──RY(d0_n0_0)────●────RY(d1_n0_0)────────────────────────●─────────RY(d2_n0_0)────────────────────────●─────────RY(d3_n0_0)────────────────────────────────
                     │                                       │                                            │
q1: ──RY(d0_n1_0)────X─────────●─────────RY(d1_n1_0)─────────X──────────────●─────────RY(d2_n1_0)─────────X──────────────●─────────RY(d3_n1_0)─────────────────
                               │                                            │                                            │
q2: ──RY(d0_n2_0)──────────────X──────────────●─────────RY(d1_n2_0)─────────X──────────────●─────────RY(d2_n2_0)─────────X──────────────●─────────RY(d3_n2_0)──
                                              │                                            │                                            │
q3: ──RY(d0_n3_0)─────────────────────────────X─────────RY(d1_n3_0)────────────────────────X─────────RY(d2_n3_0)────────────────────────X─────────RY(d3_n3_0)──
</pre></div>
</div>
<p>As can be seen from the summary of Ansatz, the quantum circuit consists of 25 quantum gates, of which 16 contain parameter quantum gates with parameters d2_n3_0, d1_n1_0, d0_n2_0, d1_n0_0, d3_n2_0, d2_n2_0, d0_n1_0, d3_n1_0, d2_n0_0, d3_n0_0 …, the number of qubits regulated by the quantum circuit is 4.</p>
<p>Note:</p>
<p>HardwareEfficientAnsatz is an Ansatz that is easy to implement on quantum chips. Its quantum circuit diagram consists of quantum gates in the red dashed box. The general format is as follows: <code class="docutils literal notranslate"><span class="pre">mindquantum.ansatz.HardwareEfficientAnsatz(n_qubits,</span> <span class="pre">single_rot_gate_seq,</span> <span class="pre">entangle_gate=X,</span> <span class="pre">entangle_mapping=&quot;linear&quot;</span> <span class="pre">,</span> <span class="pre">depth=1)</span></code>, <code class="docutils literal notranslate"><span class="pre">n_qubits</span></code> in parentheses represents the total number of qubits that Ansatz needs to act on, <code class="docutils literal notranslate"><span class="pre">single_rot_gate_seq</span></code> represents the parameter gate executed by each qubit at the beginning, and the parameter gate that needs to be executed later is also fixed, but the parameters are different, <code class="docutils literal notranslate"><span class="pre">entangle_gate=X</span></code> indicates that the executed entanglement gate is X, <code class="docutils literal notranslate"><span class="pre">entangle_mapping=&quot;linear&quot;</span></code> indicates that the entanglement gate will act on each pair of adjacent qubits, and <code class="docutils literal notranslate"><span class="pre">depth</span></code> indicates the number of times the quantum gate in the black dashed box needs to be repeated.</p>
<p>Then the complete quantum circuit is Encoder plus Ansatz.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pylint: disable=W0104</span>
<span class="n">circuit</span> <span class="o">=</span> <span class="n">encoder</span> <span class="o">+</span> <span class="n">ansatz</span>                   <span class="c1"># The complete quantum circuit consists of Encoder and Ansatz</span>
<span class="n">circuit</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">circuit</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>================================================Circuit Summary================================================
|Total number of gates  : 42.                                                                                 |
|Parameter gates        : 23.                                                                                 |
|with 23 parameters are : alpha0, alpha1, alpha2, alpha3, alpha4, alpha5, alpha6, d0_n0_0, d0_n1_0, d0_n2_0...|
|Number qubit of circuit: 4                                                                                   |
===============================================================================================================

q0: ──H────RZ(alpha0)────●──────────────────●────RY(d0_n0_0)──────────────────────────────────────────●─────────RY(d1_n0_0)────────────────────────────────────────────●─────────RY(d2_n0_0)────────────────────────●─────────RY(d3_n0_0)────────────────────────────────
                         │                  │                                                         │                                                                │                                            │
q1: ──H────RZ(alpha1)────X────RZ(alpha4)────X─────────●───────────────────────●────RY(d0_n1_0)────────X───────────────────────────────────────●────RY(d1_n1_0)─────────X──────────────●─────────RY(d2_n1_0)─────────X──────────────●─────────RY(d3_n1_0)─────────────────
                                                      │                       │                                                               │                                       │                                            │
q2: ──H────RZ(alpha2)─────────────────────────────────X─────────RZ(alpha5)────X─────────●────────────────────────────●─────────RY(d0_n2_0)────X─────────●─────────RY(d1_n2_0)─────────X──────────────●─────────RY(d2_n2_0)─────────X──────────────●─────────RY(d3_n2_0)──
                                                                                        │                            │                                  │                                            │                                            │
q3: ──H────RZ(alpha3)───────────────────────────────────────────────────────────────────X─────────RZ(alpha6)─────────X─────────RY(d0_n3_0)──────────────X─────────RY(d1_n3_0)────────────────────────X─────────RY(d2_n3_0)────────────────────────X─────────RY(d3_n3_0)──
</pre></div>
</div>
<p>From the summary of the complete quantum circuit, it can be seen that the quantum circuit consists of 42 quantum gates, of which 23 contain parameter quantum gates with parameters α0,α1,…,α6 and d2_n3_0, d1_n1_0, d0_n2_0, d1_n0_0, d3_n2_0, d2_n2_0, d0_n1_0, d3_n1_0, d2_n0_0, d3_n0_0…, the number of qubits controlled by this quantum circuit is 4.</p>
</section>
<section id="building-the-hamiltonian">
<h2>Building the Hamiltonian<a class="headerlink" href="#building-the-hamiltonian" title="Permalink to this headline"></a></h2>
<p>We perform Pauli Z operator measurements on the 2nd and 3rd qubits respectively, to construct the corresponding Hamiltonian.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindquantum.core.operators</span> <span class="kn">import</span> <span class="n">QubitOperator</span>                     <span class="c1"># Import the QubitOperator module for constructing the Pauli operator</span>
<span class="kn">from</span> <span class="nn">mindquantum.core.operators</span> <span class="kn">import</span> <span class="n">Hamiltonian</span>                       <span class="c1"># Import the Hamiltonian module for building the Hamiltonian</span>

<span class="n">bitOperator</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Z</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>   <span class="c1"># Perform the Pauli Z operator measurement on the 2nd and 3rd qubits respectively, and set the coefficients to 1 to construct the corresponding Hamiltonian</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.0 [Z2] , 1.0 [Z3] ]
</pre></div>
</div>
<p>It can be seen from the above print that there are 2 Hamiltonians constructed at this time, which are to perform the Pauli Z operator on the second and third qubits respectively, and set the coefficients to 1. We can obtain 2 Hamiltonian measurement values by the Pauli Z operator measurement. If the first measurement value is larger, the sample will be classified into the class labeled “0”. Similarly, if the second measurement value is larger, this sample will be classified into the class with the label “1”. By the training of the neural network, it is expected that the 1st measurement value of the sample labeled “0” in the training sample is larger, and the 2nd measurement value of the sample labeled “1” is larger, and finally this model is applied to predict the classification of new samples.</p>
</section>
<section id="building-a-quantum-neural-network">
<h2>Building a quantum neural network<a class="headerlink" href="#building-a-quantum-neural-network" title="Permalink to this headline"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pylint: disable=W0104</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>                                                                         <span class="c1"># Import the mindspore library and abbreviated as ms</span>
<span class="kn">from</span> <span class="nn">mindquantum.framework</span> <span class="kn">import</span> <span class="n">MQLayer</span>                                                      <span class="c1"># Import MQLayer</span>
<span class="kn">from</span> <span class="nn">mindquantum.simulator</span> <span class="kn">import</span> <span class="n">Simulator</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                                                                                 <span class="c1"># Set the seed for generating random numbers</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">Simulator</span><span class="p">(</span><span class="s1">&#39;projectq&#39;</span><span class="p">,</span> <span class="n">circuit</span><span class="o">.</span><span class="n">n_qubits</span><span class="p">)</span>
<span class="n">grad_ops</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">get_expectation_with_grad</span><span class="p">(</span><span class="n">hams</span><span class="p">,</span>
                                         <span class="n">circuit</span><span class="p">,</span>
                                         <span class="kc">None</span><span class="p">,</span>
                                         <span class="kc">None</span><span class="p">,</span>
                                         <span class="n">encoder</span><span class="o">.</span><span class="n">params_name</span><span class="p">,</span>
                                         <span class="n">ansatz</span><span class="o">.</span><span class="n">params_name</span><span class="p">,</span>
                                         <span class="n">parallel_worker</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">QuantumNet</span> <span class="o">=</span> <span class="n">MQLayer</span><span class="p">(</span><span class="n">grad_ops</span><span class="p">)</span>          <span class="c1"># Building a quantum neural network</span>
<span class="n">QuantumNet</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MQLayer&lt;
  (evolution): MQOps&lt;4 qubits projectq VQA Operator&gt;
  &gt;
</pre></div>
</div>
<p>As can be seen from the above printing, we have successfully built a quantum machine learning layer, which can seamlessly form a larger machine learning network with other operators in MindSpore.</p>
<p>Note:</p>
<p>MindSpore is an all-scenario deep learning framework that aims to achieve the three goals of easy development, efficient execution, and full-scenario coverage. It provides tensor differentiable programming capabilities that support heterogeneous acceleration, and also supports cloud, server, edge and end multiple hardware platforms.</p>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline"></a></h2>
<p>Next, we need to define the loss function, set the parameters to be optimized, and then combine the built quantum machine learning layer and MindSpore’s operators to form a larger machine learning network, and finally train the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">SoftmaxCrossEntropyWithLogits</span>                         <span class="c1"># Import the SoftmaxCrossEntropyWithLogits module to define the loss function</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">Accuracy</span>                                        <span class="c1"># Import the Adam module and the Accuracy module, which are used to define optimization parameters and evaluate the prediction accuracy respectively.</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset</span> <span class="kn">import</span> <span class="n">NumpySlicesDataset</span>                               <span class="c1"># Import the NumpySlicesDataset module for creating datasets that the model can recognize</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>            <span class="c1"># The loss function is defined by SoftmaxCrossEntropyWithLogits, sparse=True indicates that the specified label uses a sparse format, and reduction=&#39;mean&#39; indicates that the dimensionality reduction method of the loss function is averaging</span>
<span class="n">opti</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">QuantumNet</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>                  <span class="c1"># The parameters in Ansatz are optimized by the Adam optimizer. What needs to be optimized are the trainable parameters in Quantumnet, and the learning rate is set to 0.1</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">QuantumNet</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">opti</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Acc&#39;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>             <span class="c1"># Build a model: Combine the quantum machine learning layer built by MindQuantum and the operators of MindSpore to form a larger machine learning network</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">NumpySlicesDataset</span><span class="p">({</span><span class="s1">&#39;features&#39;</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">},</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># Create a dataset of training samples by NumpySlicesDataset, shuffle=False means not to shuffle the data, batch(5) means that the training set has 5 sample points per batch</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">NumpySlicesDataset</span><span class="p">({</span><span class="s1">&#39;features&#39;</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>                   <span class="c1"># Create a data set of test samples by NumpySlicesDataset, batch(5) means that there are 5 sample points in each batch of the test set</span>

<span class="k">class</span> <span class="nc">StepAcc</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>                                                        <span class="c1"># Define a callback function about the accuracy of each step</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">test_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acc</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="s1">&#39;Acc&#39;</span><span class="p">])</span>

<span class="n">monitor</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>                                                       <span class="c1"># Monitor the loss during training and print the loss value every 16 steps</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">StepAcc</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>                                               <span class="c1"># Calculate the accuracy of predictions using the established model and test samples</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">monitor</span><span class="p">,</span> <span class="n">acc</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="c1"># Train the model 20 times</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 16, loss is 0.6600145
epoch: 2 step: 16, loss is 0.4009103
epoch: 3 step: 16, loss is 0.39099234
epoch: 4 step: 16, loss is 0.3733629
epoch: 5 step: 16, loss is 0.3705962
epoch: 6 step: 16, loss is 0.37426245
epoch: 7 step: 16, loss is 0.37181872
epoch: 8 step: 16, loss is 0.37131247
epoch: 9 step: 16, loss is 0.37142643
epoch: 10 step: 16, loss is 0.37067422
epoch: 11 step: 16, loss is 0.3701976
epoch: 12 step: 16, loss is 0.36975253
epoch: 13 step: 16, loss is 0.36923727
epoch: 14 step: 16, loss is 0.3688001
epoch: 15 step: 16, loss is 0.3684062
epoch: 16 step: 16, loss is 0.36804128
epoch: 17 step: 16, loss is 0.36773998
epoch: 18 step: 16, loss is 0.36747772
epoch: 19 step: 16, loss is 0.36726192
epoch: 20 step: 16, loss is 0.36708587
</pre></div>
</div>
<p>As can be seen from the above print, after 20 iterations, the loss value keeps decreasing and tends to stabilize, finally converging to about 0.367.</p>
<p>Note:</p>
<p>(1) nn.SoftmaxCrossEntropyWithLogits can calculate the softmax cross entropy between data and labels. Use the cross-entropy loss to measure the distribution error between the probability of the input (computed using the softmax function) and the target, where the classes are mutually exclusive (only one class is positive), the general format is as follows: mindspore.nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction=”none”), sparse=False indicates whether the specified label uses sparse format, default value: False; reduction=”none” indicates the type of reduction applicable to the loss. The optional values ​​are mean, sum, and none. If none, no reduction is performed, default: “none”.</p>
<p>(2) The Adam module updates the gradient by the adaptive moment estimation algorithm, which can optimize the parameters in Ansazt, and the input is the trainable parameters in the neural network; the general format is as follows: nn.Adam(net.trainable_params(), learning_rate=0.1) , the learning rate can be adjusted by itself;</p>
<p>(3) mindspore.Model is a high-level API for training or testing. The model groups layers into objects with training and inference features. The general format is as follows: mindspore.Model(network, loss_fn=None, optimizer=None, metrics= None, eval_network=None, eval_indexes=None, amp_level=”O0”, acc_level=”O0”), where network is the network we want to train, namely Quantumnet; loss_fn is the objective function, here is the defined loss function; optimizer is the optimizer, used to update the weight, here is the defined opti; metrics is the dictionary or a set of metrics that the model needs to evaluate during training and testing, and here is the evaluation accuracy;</p>
<p>(4) Accuracy is used to calculate the accuracy of classification and multi-label data. The general format is as follows: mindspore.nn.Accuracy(eval_type=”classification”), used for classification (single label) and multi-label (multi-label classification)) The metric for calculating accuracy on the dataset, default: “classification”;</p>
<p>(5) NumpySlicesDataset uses a given data slice to create a dataset, which is mainly used to load Python data into the dataset. The general format is as follows: mindspore.dataset.NumpySlicesDataset(data, column_names=None, num_samples=None, num_parallel_workers=1, shuffle =None, sampler=None, num_shards=None, shard_id=None);</p>
<p>(6) Callback is an abstract base class for building callback classes, which are context managers that will input and output when passed to the model. You can use this mechanism to automatically initialize and release resources. The callback function will perform some operations in the current step or data loop;</p>
<p>(7) LossMonitor is mainly used to monitor the loss in training. If the loss is NAN or INF, it will terminate the training. The general format is as follows: mindspore.LossMonitor(per_print_times=1), per_print_times=1 means print the loss every second, default value: 1;</p>
<p>(8) The train module is used to train the model, where the iteration is controlled by the Python front-end; when the PyNative mode or CPU is set, the training process will be executed without the data set being received. The general format is as follows: train(epoch, train_dataset, callbacks= None, dataset_sink_mode=True, sink_size=-1), where epoch indicates the total number of iterations on the data; train_dataset is the train_loader we defined; callbacks is the loss value and accuracy we need to call back; dataset_sink_mode indicates whether to pass data by the dataset channel, in the tutorial it is no.</p>
</section>
<section id="accuracy-during-training">
<h2>Accuracy during training<a class="headerlink" href="#accuracy-during-training" title="Permalink to this headline"></a></h2>
<p>We have seen that the loss value tends to stabilize, then we can also present the prediction accuracy of the model during the training process. Execute the following code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">acc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Statistics of accuracy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Accuracy&#39;)
</pre></div>
</div>
<p><img alt="Accuracy" src="_images/accuracy_classification_of_iris_by_qnn.png" /></p>
<p>As can be seen from the above printed image, after about 50 steps, the prediction accuracy has converged to 1, which means that the prediction accuracy has reached 100%.</p>
</section>
<section id="predicting">
<h2>Predicting<a class="headerlink" href="#predicting" title="Permalink to this headline"></a></h2>
<p>Finally, we test the trained model and apply it on the test set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>                                            <span class="c1"># Import the ops module</span>

<span class="n">predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">))),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># Using the established model and test samples, get the classification predicted by the test samples</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>                   <span class="c1"># Calculate the prediction accuracy of the trained model applied to the test sample</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted classification result: &quot;</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>                           <span class="c1"># For test samples, print the predicted classification result</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Actual classification result: &quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>                               <span class="c1"># For test samples, print the actual classification result</span>

<span class="nb">print</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>                                                               <span class="c1"># Print the accuracy of model predictions</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Predicted classification result: [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]
Actual classification result: [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]
{&#39;Acc&#39;: 1.0}
</pre></div>
</div>
<p>It can be seen from the above print that the predicted classification results are completely consistent with the actual classification results, and the accuracy of the model prediction has reached 100%.</p>
<p>So far, we have experienced how to build a quantum neural network to solve a classic problem in classical machine learning - the iris classification problem. I believe everyone has a better understanding of using MindQuantum! Looking forward to digging more questions and giving full play to the powerful functions of MindQuantum!</p>
<p>To find out more about MindQuantum’s API, please click: <a class="reference external" href="https://mindspore.cn/mindquantum/">https://mindspore.cn/mindquantum/</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="get_gradient_of_PQC_with_mindquantum.html" class="btn btn-neutral float-left" title="Advanced gradient calculation of variational quantum circuits" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantum_approximate_optimization_algorithm.html" class="btn btn-neutral float-right" title="Quantum Approximate Optimization Algorithm" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>