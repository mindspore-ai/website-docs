<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep Probabilistic Programming Library &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Using the Uncertainty Evaluation Toolbox" href="using_the_uncertainty_toolbox.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="using_bnn.html">Using BNN to Implement an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_the_vae.html">Using the VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="one_click_conversion_from_dnn_to_bnn.html">One-click Conversion from DNN to BNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_the_uncertainty_toolbox.html">Using the Uncertainty Evaluation Toolbox</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep Probabilistic Programming Library</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#probability-distribution">Probability Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#probability-distribution-class">Probability Distribution Class</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#distribution-base-class">Distribution Base Class</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bernoulli-distribution">Bernoulli Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exponential-distribution">Exponential Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#geometric-distribution">Geometric Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#normal-distribution">Normal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#uniform-distribution">Uniform Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#categorical-distribution">Categorical Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cauchy-distribution">Cauchy Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lognormal-distribution">LogNormal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gumbel-distribution">Gumbel Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#logistic-distribution">Logistic Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#poisson-distribution">Poisson Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gamma-distribution">Gamma Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#beta-distribution">Beta Distribution</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#probability-distribution-class-application-in-pynative-mode">Probability Distribution Class Application in PyNative Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#probability-distribution-class-application-in-graph-mode">Probability Distribution Class Application in Graph Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transformeddistribution-class-api-design">TransformedDistribution Class API Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#invoking-a-transformeddistribution-instance-in-pynative-mode">Invoking a TransformedDistribution Instance in PyNative Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#invoking-a-transformeddistribution-instance-in-graph-mode">Invoking a TransformedDistribution Instance in Graph Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#probability-distribution-mapping">Probability Distribution Mapping</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#bijector-api-design">Bijector API Design</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#bijector-base-class">Bijector Base Class</a></li>
<li class="toctree-l4"><a class="reference internal" href="#powertransform">PowerTransform</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exp">Exp</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scalaraffine">ScalarAffine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#softplus">Softplus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gumbelcdf">GumbelCDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="#invert">Invert</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#invoking-the-bijector-instance-in-pynative-mode">Invoking the Bijector Instance in PyNative Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#invoking-a-bijector-instance-in-graph-mode">Invoking a Bijector Instance in Graph Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deep-probabilistic-network">Deep Probabilistic Network</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vae">VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conditionalvae">ConditionalVAE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#probability-inference-algorithm">Probability Inference Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bayesian-layer">Bayesian Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bayesian-conversion">Bayesian Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bayesian-toolbox">Bayesian Toolbox</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#uncertainty-estimation">Uncertainty Estimation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#anomaly-detection">Anomaly Detection</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Deep Probabilistic Programming Library</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/probability.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="deep-probabilistic-programming-library">
<h1>Deep Probabilistic Programming Library<a class="headerlink" href="#deep-probabilistic-programming-library" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/probability/docs/source_en/probability.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<p>MindSpore deep probabilistic programming is to combine Bayesian learning with deep learning, including probability distribution, probability distribution mapping, deep probability network, probability inference algorithm, Bayesian layer, Bayesian conversion, and Bayesian toolkit. For professional Bayesian learning users, it provides probability sampling, inference algorithms, and model build libraries. On the other hand, advanced APIs are provided for users who are unfamiliar with Bayesian deep learning, so that they can use Bayesian models without changing the deep learning programming logic.</p>
<section id="probability-distribution">
<h2>Probability Distribution<a class="headerlink" href="#probability-distribution" title="Permalink to this headline"></a></h2>
<p>Probability distribution (<code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.distribution</span></code>) is the basis of probabilistic programming. The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class provides various probability statistics APIs, such as <em>pdf</em> for probability density, <em>cdf</em> for cumulative density, <em>kl_loss</em> for divergence calculation, and <em>sample</em> for sampling. Existing probability distribution examples include Gaussian distribution, Bernoulli distribution, exponential distribution, geometric distribution, and uniform distribution.</p>
<section id="probability-distribution-class">
<h3>Probability Distribution Class<a class="headerlink" href="#probability-distribution-class" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Distribution</span></code>: base class of all probability distributions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Bernoulli</span></code>: Bernoulli distribution, with a parameter indicating the number of experiment successes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Exponential</span></code>: exponential distribution, with a rate parameter.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Geometric</span></code>: geometric distribution, with a parameter indicating the probability of initial experiment success.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Normal</span></code>: normal distribution (Gaussian distribution), with two parameters indicating the average value and standard deviation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Uniform</span></code>: uniform distribution, with two parameters indicating the minimum and maximum values on the axis.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Categorical</span></code>: categorical distribution, with one parameter indicating the probability of each category.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cauchy</span></code>: cauchy distribution, with two parameters indicating the location and scale.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LogNormal</span></code>: lognormal distribution, with two parameters indicating the location and scale.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Logistic</span></code>: logistic distribution, with two parameters indicating the location and scale.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Gumbel</span></code>: gumbel distribution, with two parameters indicating the location and scale.</p></li>
</ul>
<section id="distribution-base-class">
<h4>Distribution Base Class<a class="headerlink" href="#distribution-base-class" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Distribution</span></code> is the base class for all probability distributions.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class supports the following functions: <code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">sd</span></code>, <code class="docutils literal notranslate"><span class="pre">var</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code>, and <code class="docutils literal notranslate"><span class="pre">sample</span></code>. The input parameters vary according to the distribution. These functions can be used only in a derived class and their parameters are determined by the function implementation of the derived class.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>: probability density function (PDF) or probability quality function (PMF)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_prob</span></code>: log-like function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cdf</span></code>: cumulative distribution function (CDF)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>: log-cumulative distribution function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">survival_function</span></code>: survival function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: logarithmic survival function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>: average value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sd</span></code>: standard deviation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">var</span></code>: variance</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: entropy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: Kullback-Leibler divergence</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code>: cross entropy of two probability distributions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: random sampling of probability distribution</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: returns the parameters of the distribution used in the network</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_name</span></code>: returns the type of the distribution</p></li>
</ul>
</section>
<section id="bernoulli-distribution">
<h4>Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Permalink to this headline"></a></h4>
<p>Bernoulli distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Bernoulli.probs</span></code>: returns the probability of success in the Bernoulli experiment as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Bernoulli</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Bernoulli</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input parameter <em>probs1</em> that indicates the probability of experiment success is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: The input parameter <em>probs1</em> that indicates the probability of experiment success is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em> and <em>probs1_b</em> are mandatory. <em>dist</em> indicates another distribution type. Currently, only <em>‘Bernoulli’</em> is supported. <em>probs1_b</em> is the experiment success probability of distribution <em>b</em>. Parameter <em>probs1_a</em> of distribution <em>a</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. The input parameter <em>probs</em> that indicates the probability of experiment success is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and experiment success probability <em>probs1</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input parameter <em>probs1</em> that indicates the probability of experiment success is optional. Return <code class="docutils literal notranslate"><span class="pre">(probs1,)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: return <em>‘Bernoulli’</em>.</p></li>
</ul>
</section>
<section id="exponential-distribution">
<h4>Exponential Distribution<a class="headerlink" href="#exponential-distribution" title="Permalink to this headline"></a></h4>
<p>Exponential distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Exponential.rate</span></code>: returns the rate parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the <code class="docutils literal notranslate"><span class="pre">Exponential</span></code> private API to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Exponential</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input rate parameter <em>rate</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: The input rate parameter <em>rate</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em> and <em>rate_b</em> are mandatory.  <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Exponential’</em> is supported. <em>rate_b</em> is the rate parameter of distribution <em>b</em>. Parameter <em>rate_a</em> of distribution <em>a</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. The input rate parameter <em>rate</em>is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and rate parameter <em>rate</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input rate parameter <em>rate</em> is optional. Return <code class="docutils literal notranslate"><span class="pre">(rate,)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Exponential’</em>.</p></li>
</ul>
</section>
<section id="geometric-distribution">
<h4>Geometric Distribution<a class="headerlink" href="#geometric-distribution" title="Permalink to this headline"></a></h4>
<p>Geometric distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Geometric.probs</span></code>: returns the probability of success in the Bernoulli experiment as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Geometric</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Geometric</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input parameter <em>probs1</em> that indicates the probability of experiment success is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: The input parameter <em>probs1</em> that indicates the probability of experiment success is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em> and <em>probs1_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Geometric’</em> is supported. <em>probs1_b</em> is the experiment success probability of distribution <em>b</em>. Parameter <em>probs1_a</em> of distribution <em>a</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. The input parameter <em>probs1</em> that indicates the probability of experiment success is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and experiment success probability <em>probs1</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input parameter <em>probs1</em> that indicates the probability of experiment success is optional. Return <code class="docutils literal notranslate"><span class="pre">(probs1,)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Geometric’</em>.</p></li>
</ul>
</section>
<section id="normal-distribution">
<h4>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Permalink to this headline"></a></h4>
<p>Normal distribution (also known as Gaussian distribution), inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Normal</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Normal</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: Input parameters <em>mean</em> (for average value) and <em>sd</em> (for standard deviation) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: Input parameters <em>mean</em> (for average value) and <em>sd</em> (for standard deviation) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>mean_b</em>, and <em>sd_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Normal’</em> is supported. <em>mean_b</em> and <em>sd_b</em> indicate the mean value and standard deviation of distribution <em>b</em>, respectively. Input parameters mean value <em>mean_a</em> and standard deviation <em>sd_a</em> of distribution <em>a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. Input parameters mean value <em>mean_a</em> and standard deviation <em>sd_a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Input parameters sample shape <em>shape</em>, average value <em>mean_a</em>, and standard deviation <em>sd_a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: Input parameters mean value <em>mean</em> and standard deviation <em>sd</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(mean,</span> <span class="pre">sd)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Normal’</em>.</p></li>
</ul>
</section>
<section id="uniform-distribution">
<h4>Uniform Distribution<a class="headerlink" href="#uniform-distribution" title="Permalink to this headline"></a></h4>
<p>Uniform distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Uniform.low</span></code>: returns the minimum value as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Uniform.high</span></code>: returns the maximum value as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes <code class="docutils literal notranslate"><span class="pre">Uniform</span></code> to implement public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Uniform</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: Input parameters maximum value <em>high</em> and minimum value <em>low</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: Input parameters maximum value <em>high</em> and minimum value <em>low</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>high_b</em>, and <em>low_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Uniform’</em> is supported. <em>high_b</em> and <em>low_b</em> are parameters of distribution <em>b</em>. Input parameters maximum value <em>high</em> and minimum value <em>low</em> of distribution <em>a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. Input parameters maximum value <em>high</em> and minimum value <em>low</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Input parameters <em>shape</em>, maximum value <em>high</em>, and minimum value <em>low</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: Input parameters maximum value <em>high</em> and minimum value <em>low</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(low,</span> <span class="pre">high)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Uniform’</em>.</p></li>
</ul>
</section>
<section id="categorical-distribution">
<h4>Categorical Distribution<a class="headerlink" href="#categorical-distribution" title="Permalink to this headline"></a></h4>
<p>Categorical distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Categorical.probs</span></code>: returns the probability of each category as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input parameter <em>probs</em> that indicates the probability of each category is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: The input parameter <em>probs</em> that indicates the probability of each category is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em> and <em>probs_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Categorical’</em> is supported. <em>probs_b</em> is the categories’ probabilities of distribution <em>b</em>. Parameter <em>probs_a</em> of distribution <em>a</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. The input parameter <em>probs</em> that indicates the probability of each category is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and the categories’ probabilities <em>probs</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input parameter <em>probs</em> that indicates the probability of each category is optional. Return <code class="docutils literal notranslate"><span class="pre">(probs,)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Categorical’</em>.</p></li>
</ul>
</section>
<section id="cauchy-distribution">
<h4>Cauchy Distribution<a class="headerlink" href="#cauchy-distribution" title="Permalink to this headline"></a></h4>
<p>Cauchy distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Cauchy.loc</span></code>: returns the location parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cauchy.scale</span></code>: returns the scale parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Cauchy</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Cauchy</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: Input parameters <em>loc</em> (for location) and <em>scale</em> (for scale) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>loc_b</em>, and <em>scale_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Cauchy’</em> is supported. <em>loc_b</em> and <em>scale_b</em> indicate the location and scale of distribution <em>b</em>, respectively. Input parameters <em>loc</em> and <em>scale</em> of distribution <em>a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. Input parameters location <em>loc</em> and scale <em>scale</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Input parameters sample shape <em>shape</em>, location <em>loc</em> and scale <em>scale</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: Input parameters location <em>loc</em> and scale <em>scale</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(loc,</span> <span class="pre">scale)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Cauchy’</em>.</p></li>
</ul>
</section>
<section id="lognormal-distribution">
<h4>LogNormal Distribution<a class="headerlink" href="#lognormal-distribution" title="Permalink to this headline"></a></h4>
<p>LogNormal distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> class, constructed by <code class="docutils literal notranslate"><span class="pre">Exp</span></code> Bijector and <code class="docutils literal notranslate"><span class="pre">Normal</span></code> Distribution.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LogNormal.loc</span></code>: returns the location parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LogNormal.scale</span></code>: returns the scale parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">LogNormal</span></code> and <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">LogNormal</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>：Input parameters <em>loc</em> (for location) and <em>scale</em> (for scale) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: Input parameters <em>loc</em> (for location) and <em>scale</em> (for scale) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>loc_b</em>, and <em>scale_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘LogNormal’</em> is supported. <em>loc_b</em> and <em>scale_b</em> indicate the location and scale of distribution <em>b</em>, respectively. Input parameters <em>loc</em> and <em>scale</em> of distribution <em>a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. Input parameters location <em>loc</em> and scale <em>scale</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Input parameters sample shape <em>shape</em>, location <em>loc</em> and scale <em>scale</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: Input parameters location <em>loc</em> and scale <em>scale</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(loc,</span> <span class="pre">scale)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘LogNormal’</em>.</p></li>
</ul>
</section>
<section id="gumbel-distribution">
<h4>Gumbel Distribution<a class="headerlink" href="#gumbel-distribution" title="Permalink to this headline"></a></h4>
<p>Gumbel distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> class, constructed by <code class="docutils literal notranslate"><span class="pre">GumbelCDF</span></code> Bijector and <code class="docutils literal notranslate"><span class="pre">Uniform</span></code> Distribution.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Gumbel.loc</span></code>: returns the location parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Gumbel.scale</span></code>: returns the scale parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Gumbel</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>：No parameter.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: No parameter.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>loc_b</em>, and <em>scale_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Gumbel’</em> is supported. <em>loc_b</em> and <em>scale_b</em> indicate the location and scale of distribution <em>b</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Input parameters sample shape <em>shape</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: Input parameters location <em>loc</em> and scale <em>scale</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(loc,</span> <span class="pre">scale)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Gumbel’</em>.</p></li>
</ul>
</section>
<section id="logistic-distribution">
<h4>Logistic Distribution<a class="headerlink" href="#logistic-distribution" title="Permalink to this headline"></a></h4>
<p>Logistic distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Logistic.loc</span></code>: returns the location parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Logistic.scale</span></code>: returns the scale parameter as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Logistic</span></code> and <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Logistic</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>：Input parameters <em>loc</em> (for location) and <em>scale</em> (for scale) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: Input parameters <em>loc</em> (for location) and <em>scale</em> (for scale) are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. Input parameters location <em>loc</em> and scale <em>scale</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Input parameters sample shape <em>shape</em>, location <em>loc</em> and scale <em>scale</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: Input parameters location <em>loc</em> and scale <em>scale</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(loc,</span> <span class="pre">scale)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Logistic’</em>.</p></li>
</ul>
</section>
<section id="poisson-distribution">
<h4>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Permalink to this headline"></a></h4>
<p>Poisson distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Poisson.rate</span></code>: returns the rate as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Poisson</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Poisson</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input parameter <em>rate</em> is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. The input parameter rate* is optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and the parameter <em>rate</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input parameter <em>rate</em> is optional. Return <code class="docutils literal notranslate"><span class="pre">(rate,)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Poisson’</em>.</p></li>
</ul>
</section>
<section id="gamma-distribution">
<h4>Gamma Distribution<a class="headerlink" href="#gamma-distribution" title="Permalink to this headline"></a></h4>
<p>Gamma distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Gamma.concentration</span></code>: returns the concentration as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Gamma.rate</span></code>: returns the rate as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Gamma</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Gamma</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input parameters <em>concentration</em> and <em>rate</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: The input parameters <em>concentration</em> and <em>rate</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>concentration_b</em> and <em>rate_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Gamma’</em> is supported. <em>concentration_b</em> and <em>rate_b</em> are the parameters of distribution <em>b</em>. The input parameters <em>concentration_a</em> and <em>rate_a</em> for distribution <em>a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>, <code class="docutils literal notranslate"><span class="pre">survival_function</span></code>, and <code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: The input parameter <em>value</em> is mandatory. The input parameters <em>concentration</em> and <em>rate</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and parameters <em>concentration</em> and <em>rate</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input parameters <em>concentration</em> and <em>rate</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(concentration,</span> <span class="pre">rate)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Gamma’</em>.</p></li>
</ul>
</section>
<section id="beta-distribution">
<h4>Beta Distribution<a class="headerlink" href="#beta-distribution" title="Permalink to this headline"></a></h4>
<p>Beta distribution, inherited from the <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class.</p>
<p>Properties are described as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Beta.concentration1</span></code>: returns the rate as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Beta.concentration0</span></code>: returns the rate as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> base class invokes the private API in the <code class="docutils literal notranslate"><span class="pre">Beta</span></code> to implement the public APIs in the base class. <code class="docutils literal notranslate"><span class="pre">Beta</span></code> supports the following public APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>,<code class="docutils literal notranslate"><span class="pre">mode</span></code>,<code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">sd</span></code>: The input parameters <em>concentration1</em> and <em>concentration0</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entropy</span></code>: The input parameters <em>concentration1</em> and <em>concentration0</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> and <code class="docutils literal notranslate"><span class="pre">kl_loss</span></code>: The input parameters <em>dist</em>, <em>concentration1_b</em> and <em>rateconcentration0_b</em> are mandatory. <em>dist</em> indicates the name of another distribution type. Currently, only <em>‘Beta’</em> is supported. <em>concentration1_b</em> and <em>concentration0_b</em> are the parameters of distribution <em>b</em>. The input parameters <em>concentratio1n_a</em> and <em>concentration0_a</em> for distribution <em>a</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code> and <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>: The input parameter <em>value</em> is mandatory. The input parameters <em>concentration1</em> and <em>concentration0</em> are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Optional input parameters include sample shape <em>shape</em> and parameters <em>concentration1</em> and <em>concentration0</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_args</span></code>: The input parameters <em>concentration1</em> and <em>concentration0</em> are optional. Return <code class="docutils literal notranslate"><span class="pre">(concentration1,</span> <span class="pre">concentration0)</span></code> with type tuple.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_dist_type</span></code>: returns <em>‘Beta’</em>.</p></li>
</ul>
</section>
</section>
<section id="probability-distribution-class-application-in-pynative-mode">
<h3>Probability Distribution Class Application in PyNative Mode<a class="headerlink" href="#probability-distribution-class-application-in-pynative-mode" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Distribution</span></code> subclasses can be used in <strong>PyNative</strong> mode.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">Normal</span></code> as an example. Create a normal distribution whose average value is 0.0 and standard deviation is 1.0.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.distribution</span> <span class="k">as</span> <span class="nn">msd</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="n">my_normal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>

<span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
<span class="n">cdf</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="n">mean_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sd_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">kl</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">kl_loss</span><span class="p">(</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="n">mean_b</span><span class="p">,</span> <span class="n">sd_b</span><span class="p">)</span>

<span class="c1"># get the distribution args as a tuple</span>
<span class="n">dist_arg</span> <span class="o">=</span> <span class="n">my_normal</span><span class="o">.</span><span class="n">get_dist_args</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mean: &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;var: &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;entropy: &quot;</span><span class="p">,</span> <span class="n">entropy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;prob: &quot;</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cdf: &quot;</span><span class="p">,</span> <span class="n">cdf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;kl: &quot;</span><span class="p">,</span> <span class="n">kl</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dist_arg: &quot;</span><span class="p">,</span> <span class="n">dist_arg</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mean:  0.0
var:  1.0
entropy:  1.4189385
prob:  [0.35206532 0.3989423  0.35206532]
cdf:  [0.30853754 0.5        0.69146246]
kl:  0.44314718
dist_arg: (Tensor(shape=[], dtype=Float32, value= 0), Tensor(shape=[], dtype=Float32, value= 1))
</pre></div>
</div>
</section>
<section id="probability-distribution-class-application-in-graph-mode">
<h3>Probability Distribution Class Application in Graph Mode<a class="headerlink" href="#probability-distribution-class-application-in-graph-mode" title="Permalink to this headline"></a></h3>
<p>In graph mode, <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> subclasses can be used on the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.distribution</span> <span class="k">as</span> <span class="nn">msd</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="o">.</span><span class="n">kl_loss</span><span class="p">(</span><span class="s2">&quot;Normal&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">kl</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">pdf</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;pdf: &quot;</span><span class="p">,</span> <span class="n">pdf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;kl: &quot;</span><span class="p">,</span> <span class="n">kl</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>pdf:  [0.35206532 0.3989423  0.35206532]
kl:  0.5
</pre></div>
</div>
</section>
<section id="transformeddistribution-class-api-design">
<h3>TransformedDistribution Class API Design<a class="headerlink" href="#transformeddistribution-class-api-design" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code>, inherited from <code class="docutils literal notranslate"><span class="pre">Distribution</span></code>, is a base class for mathematical distribution that can be obtained by mapping f(x) changes. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">bijector</span></code>: returns the distribution transformation method.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">distribution</span></code>: returns the original distribution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_linear_transformation</span></code>: returns the linear transformation flag.</p></li>
</ul>
</li>
<li><p>API functions (The parameters of the following APIs are the same as those of the corresponding APIs of <code class="docutils literal notranslate"><span class="pre">distribution</span></code> in the constructor function.)</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cdf</span></code>: cumulative distribution function (CDF)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_cdf</span></code>: log-cumulative distribution function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">survival_function</span></code>: survival function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_survival</span></code>: logarithmic survival function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prob</span></code>: probability density function (PDF) or probability quality function (PMF)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_prob</span></code>: log-like function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: random sampling</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code>: a non-parametric function, which can be invoked only when <code class="docutils literal notranslate"><span class="pre">Bijector.is_constant_jacobian=true</span></code> is invoked.</p></li>
</ul>
</li>
</ol>
</section>
<section id="invoking-a-transformeddistribution-instance-in-pynative-mode">
<h3>Invoking a TransformedDistribution Instance in PyNative Mode<a class="headerlink" href="#invoking-a-transformeddistribution-instance-in-pynative-mode" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> subclass can be used in <strong>PyNative</strong> mode.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.bijector</span> <span class="k">as</span> <span class="nn">msb</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.distribution</span> <span class="k">as</span> <span class="nn">msd</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dtype</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="n">normal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exp</span> <span class="o">=</span> <span class="n">msb</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
<span class="n">LogNormal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">exp</span><span class="p">,</span> <span class="n">normal</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;LogNormal&quot;</span><span class="p">)</span>

<span class="c1"># compute cumulative distribution function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">cdf</span> <span class="o">=</span> <span class="n">LogNormal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>

<span class="c1"># generate samples from the distribution</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">LogNormal</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># get information of the distribution</span>
<span class="nb">print</span><span class="p">(</span><span class="n">LogNormal</span><span class="p">)</span>
<span class="c1"># get information of the underlying distribution and the bijector separately</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;underlying distribution:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">LogNormal</span><span class="o">.</span><span class="n">distribution</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bijector:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">LogNormal</span><span class="o">.</span><span class="n">bijector</span><span class="p">)</span>
<span class="c1"># get the computation results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cdf:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cdf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sample shape:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TransformedDistribution&lt;
  (_bijector): Exp&lt;exp&gt;
  (_distribution): Normal&lt;mean = 0.0, standard deviation = 1.0&gt;
  &gt;
underlying distribution:
 Normal&lt;mean = 0.0, standard deviation = 1.0&gt;
bijector:
 Exp&lt;exp&gt;
cdf:
 [0.7558914 0.9462397 0.9893489]
sample shape:
(3, 2)
</pre></div>
</div>
<p>When the <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> is constructed to map the transformed <code class="docutils literal notranslate"><span class="pre">is_constant_jacobian</span> <span class="pre">=</span> <span class="pre">true</span></code> (for example, <code class="docutils literal notranslate"><span class="pre">ScalarAffine</span></code>), the constructed <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> instance can use the <code class="docutils literal notranslate"><span class="pre">mean</span></code> API to calculate the average value. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">normal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">scalaraffine</span> <span class="o">=</span> <span class="n">msb</span><span class="o">.</span><span class="n">ScalarAffine</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="n">trans_dist</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">scalaraffine</span><span class="p">,</span> <span class="n">normal</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">trans_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>2.0
</pre></div>
</div>
</section>
<section id="invoking-a-transformeddistribution-instance-in-graph-mode">
<h3>Invoking a TransformedDistribution Instance in Graph Mode<a class="headerlink" href="#invoking-a-transformeddistribution-instance-in-graph-mode" title="Permalink to this headline"></a></h3>
<p>In graph mode, the <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> class can be used on the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dtype</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.bijector</span> <span class="k">as</span> <span class="nn">msb</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.distribution</span> <span class="k">as</span> <span class="nn">msd</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;transformed_distribution&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># create TransformedDistribution distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">msb</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lognormal</span> <span class="o">=</span> <span class="n">msd</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">cdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lognormal</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lognormal</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">sample</span>

<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;LogNormal&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">cdf</span><span class="p">,</span> <span class="n">sample</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cdf: &quot;</span><span class="p">,</span> <span class="n">cdf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sample shape: &quot;</span><span class="p">,</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cdf:  [0.7558914  0.86403143 0.9171715  0.9462397 ]
sample shape:  (2, 3)
</pre></div>
</div>
</section>
</section>
<section id="probability-distribution-mapping">
<h2>Probability Distribution Mapping<a class="headerlink" href="#probability-distribution-mapping" title="Permalink to this headline"></a></h2>
<p>Bijector (<code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.bijector</span></code>) is a basic component of probability programming. Bijector describes a random variable transformation method, and a new random variable <span class="math notranslate nohighlight">\(Y = f(x)\)</span> may be generated by using an existing random variable X and a mapping function f.
<code class="docutils literal notranslate"><span class="pre">Bijector</span></code> provides four mapping-related transformation methods. It can be directly used as an operator, or used to generate a <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class instance of a new random variable on an existing <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class instance.</p>
<section id="bijector-api-design">
<h3>Bijector API Design<a class="headerlink" href="#bijector-api-design" title="Permalink to this headline"></a></h3>
<section id="bijector-base-class">
<h4>Bijector Base Class<a class="headerlink" href="#bijector-base-class" title="Permalink to this headline"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">Bijector</span></code> class is the base class for all probability distribution mappings. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">name</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dtype</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">parameter</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_constant_jacobian</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">is_constant_jacobian</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_injective</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">is_injective</span></code>.</p></li>
</ul>
</li>
<li><p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, whose parameter is determined by <code class="docutils literal notranslate"><span class="pre">_forward</span></code> of the derived class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, whose parameter is determined by<code class="docutils literal notranslate"><span class="pre">_inverse</span></code> of the derived class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, whose parameter is determined by <code class="docutils literal notranslate"><span class="pre">_forward_log_jacobian</span></code> of the derived class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, whose parameter is determined by <code class="docutils literal notranslate"><span class="pre">_inverse_log_jacobian</span></code> of the derived class.</p></li>
</ul>
</li>
</ol>
<p>When <code class="docutils literal notranslate"><span class="pre">Bijector</span></code> is invoked as a function:
The input is a <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> class and a <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> is generated <strong>(cannot be invoked in a graph)</strong>.</p>
</section>
<section id="powertransform">
<h4>PowerTransform<a class="headerlink" href="#powertransform" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">PowerTransform</span></code> implements variable transformation with <span class="math notranslate nohighlight">\(Y = g(X) = {(1 + X * c)}^{1 / c}\)</span>. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">power</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">power</span></code> as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
<li><p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="exp">
<h4>Exp<a class="headerlink" href="#exp" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Exp</span></code> implements variable transformation with <span class="math notranslate nohighlight">\(Y = g(X)= exp(X)\)</span>. The APIs are as follows:</p>
<p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</section>
<section id="scalaraffine">
<h4>ScalarAffine<a class="headerlink" href="#scalaraffine" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">ScalarAffine</span></code> implements variable transformation with Y = g(X) = a * X + b. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scale</span></code>: returns the value of scale as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shift</span></code>: returns the value of shift as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
<li><p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="softplus">
<h4>Softplus<a class="headerlink" href="#softplus" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Softplus</span></code> implements variable transformation with <span class="math notranslate nohighlight">\(Y = g(X) = log(1 + e ^ {kX}) / k \)</span>. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sharpness</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">sharpness</span></code> as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
<li><p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="gumbelcdf">
<h4>GumbelCDF<a class="headerlink" href="#gumbelcdf" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">GumbelCDF</span></code> implements variable transformation with <span class="math notranslate nohighlight">\(Y = g(X) = \exp(-\exp(-\frac{X - loc}{scale}))\)</span>. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loc</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">loc</span></code> as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scale</span></code>: returns the value of <code class="docutils literal notranslate"><span class="pre">scale</span></code> as a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
<li><p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="invert">
<h4>Invert<a class="headerlink" href="#invert" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Invert</span></code> implements the inverse of another bijector. The APIs are as follows:</p>
<ol class="arabic simple">
<li><p>Properties</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">bijector</span></code>: returns the Bijector used during initialization with type <code class="docutils literal notranslate"><span class="pre">msb.Bijector</span></code>.</p></li>
</ul>
</li>
<li><p>Mapping functions</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward</span></code>: forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse</span></code>: backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_log_jacobian</span></code>: logarithm of the derivative of the forward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_log_jacobian</span></code>: logarithm of the derivative of the backward mapping, with an input parameter <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="invoking-the-bijector-instance-in-pynative-mode">
<h3>Invoking the Bijector Instance in PyNative Mode<a class="headerlink" href="#invoking-the-bijector-instance-in-pynative-mode" title="Permalink to this headline"></a></h3>
<p>Before the execution, import the required library file package. The main library of the Bijector class is <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.bijector</span></code>. After the library is imported, <code class="docutils literal notranslate"><span class="pre">msb</span></code> is used as the abbreviation of the library for invoking.</p>
<p>The following uses <code class="docutils literal notranslate"><span class="pre">PowerTransform</span></code> as an example. Create a <code class="docutils literal notranslate"><span class="pre">PowerTransform</span></code> object whose power is 2.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.bijector</span> <span class="k">as</span> <span class="nn">msb</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dtype</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="n">powertransform</span> <span class="o">=</span> <span class="n">msb</span><span class="o">.</span><span class="n">PowerTransform</span><span class="p">(</span><span class="n">power</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">forward</span> <span class="o">=</span> <span class="n">powertransform</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>
<span class="n">inverse</span> <span class="o">=</span> <span class="n">powertransform</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>
<span class="n">forward_log_jaco</span> <span class="o">=</span> <span class="n">powertransform</span><span class="o">.</span><span class="n">forward_log_jacobian</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>
<span class="n">inverse_log_jaco</span> <span class="o">=</span> <span class="n">powertransform</span><span class="o">.</span><span class="n">inverse_log_jacobian</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">powertransform</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward: &quot;</span><span class="p">,</span> <span class="n">forward</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inverse: &quot;</span><span class="p">,</span> <span class="n">inverse</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward_log_jacobian: &quot;</span><span class="p">,</span> <span class="n">forward_log_jaco</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inverse_log_jacobian: &quot;</span><span class="p">,</span> <span class="n">inverse_log_jaco</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PowerTransform&lt;power = 2.0&gt;
forward:  [2.236068  2.6457515 3.        3.3166249]
inverse:  [ 1.5       4.        7.5      12.000001]
forward_log_jacobian:  [-0.804719  -0.9729551 -1.0986123 -1.1989477]
inverse_log_jacobian:  [0.6931472 1.0986123 1.3862944 1.609438 ]
</pre></div>
</div>
</section>
<section id="invoking-a-bijector-instance-in-graph-mode">
<h3>Invoking a Bijector Instance in Graph Mode<a class="headerlink" href="#invoking-a-bijector-instance-in-graph-mode" title="Permalink to this headline"></a></h3>
<p>In graph mode, the <code class="docutils literal notranslate"><span class="pre">Bijector</span></code> subclass can be used on the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">import</span> <span class="nn">mindspore.nn.probability.bijector</span> <span class="k">as</span> <span class="nn">msb</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># create a PowerTransform bijector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">powertransform</span> <span class="o">=</span> <span class="n">msb</span><span class="o">.</span><span class="n">PowerTransform</span><span class="p">(</span><span class="n">power</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">powertransform</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">inverse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">powertransform</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">forward_log_jaco</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">powertransform</span><span class="o">.</span><span class="n">forward_log_jacobian</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">inverse_log_jaco</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">powertransform</span><span class="o">.</span><span class="n">inverse_log_jacobian</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">forward</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">forward_log_jaco</span><span class="p">,</span> <span class="n">inverse_log_jaco</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">forward</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">forward_log_jaco</span><span class="p">,</span> <span class="n">inverse_log_jaco</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">tx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward: &quot;</span><span class="p">,</span> <span class="n">forward</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inverse: &quot;</span><span class="p">,</span> <span class="n">inverse</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward_log_jaco: &quot;</span><span class="p">,</span> <span class="n">forward_log_jaco</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inverse_log_jaco: &quot;</span><span class="p">,</span> <span class="n">inverse_log_jaco</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>forward:  [2.236068  2.6457515 3.        3.3166249]
inverse:  [ 1.5       4.        7.5      12.000001]
forward_log_jacobian:  [-0.804719  -0.9729551 -1.0986123 -1.1989477]
inverse_log_jacobian:  [0.6931472 1.0986123 1.3862944 1.609438 ]
</pre></div>
</div>
</section>
</section>
<section id="deep-probabilistic-network">
<h2>Deep Probabilistic Network<a class="headerlink" href="#deep-probabilistic-network" title="Permalink to this headline"></a></h2>
<p>It is especially easy to use the MindSpore deep probabilistic programming library (<code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.dpn</span></code>) to construct a variational auto-encoder (VAE) for inference. You only need to define the encoder and decoder (a DNN model), invoke the VAE or conditional VAE (CVAE) API to form a derived network, invoke the ELBO API for optimization, and use the SVI API for variational inference. The advantage is that users who are not familiar with variational inference can build a probability model in the same way as they build a DNN model, and those who are familiar with variational inference can invoke these APIs to build a more complex probability model. VAE APIs are defined in <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.dpn</span></code>, where dpn represents the deep probabilistic network. <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.dpn</span></code> provides some basic APIs of the deep probabilistic network, for example, VAE.</p>
<section id="vae">
<h3>VAE<a class="headerlink" href="#vae" title="Permalink to this headline"></a></h3>
<p>First, we need to define the encoder and decoder and invoke the <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.dpn.VAE</span></code> API to construct the VAE network. In addition to the encoder and decoder, we need to input the hidden size of the encoder output variable and the latent size of the VAE network storage potential variable. Generally, the latent size is less than the hidden size.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.probability.dpn</span> <span class="kn">import</span> <span class="n">VAE</span>

<span class="n">IMAGE_SHAPE</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">IMAGE_SHAPE</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>


<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">()</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="conditionalvae">
<h3>ConditionalVAE<a class="headerlink" href="#conditionalvae" title="Permalink to this headline"></a></h3>
<p>Similarly, the usage of CVAE is similar to that of VAE. The difference is that CVAE uses the label information of datasets. It is a supervised learning algorithm, and has a better generation effect than VAE.</p>
<p>First, define the encoder and decoder and invoke the <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.dpn.ConditionalVAE</span></code> API to construct the CVAE network. The encoder here is different from that of the VAE because the label information of datasets needs to be input. The decoder is the same as that of the VAE. For the CVAE API, the number of dataset label categories also needs to be input. Other input parameters are the same as those of the VAE API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.probability.dpn</span> <span class="kn">import</span> <span class="n">ConditionalVAE</span>

<span class="n">IMAGE_SHAPE</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span> <span class="o">+</span> <span class="n">num_classes</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">OneHot</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>


<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">IMAGE_SHAPE</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>


<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>
<span class="n">cvae</span> <span class="o">=</span> <span class="n">ConditionalVAE</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Load a dataset, for example, Mnist. For details about the data loading and preprocessing process, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/quick_start/quick_start.html">Implementing an Image Classification Application</a>. The create_dataset function is used to create a data iterator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, use the infer API to perform variational inference on the VAE network.</p>
</section>
</section>
<section id="probability-inference-algorithm">
<h2>Probability Inference Algorithm<a class="headerlink" href="#probability-inference-algorithm" title="Permalink to this headline"></a></h2>
<p>Invoke the <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.infer.ELBO</span></code> API to define the loss function of the VAE network, invoke <code class="docutils literal notranslate"><span class="pre">WithLossCell</span></code> to encapsulate the VAE network and loss function, define the optimizer, and transfer them to the <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.infer.SVI</span></code> API. The <code class="docutils literal notranslate"><span class="pre">run</span></code> function of the SVI API can be understood to trigger training of the VAE network. You can specify the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> of the training, so that a trained network is returned. If you specify the <code class="docutils literal notranslate"><span class="pre">get_train_loss</span></code> function, the loss of the trained model will be returned.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.probability.infer</span> <span class="kn">import</span> <span class="n">ELBO</span><span class="p">,</span> <span class="n">SVI</span>

<span class="n">net_loss</span> <span class="o">=</span> <span class="n">ELBO</span><span class="p">(</span><span class="n">latent_prior</span><span class="o">=</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="n">output_prior</span><span class="o">=</span><span class="s1">&#39;Normal&#39;</span><span class="p">)</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">vae</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">vi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">=</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">vi</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">trained_loss</span> <span class="o">=</span> <span class="n">vi</span><span class="o">.</span><span class="n">get_train_loss</span><span class="p">()</span>
</pre></div>
</div>
<p>After obtaining the trained VAE network, use <code class="docutils literal notranslate"><span class="pre">vae.generate_sample</span></code> to generate a new sample. You need to specify the number of samples to be generated and the shape of the generated samples. The shape must be the same as that of the samples in the original dataset. You can also use <code class="docutils literal notranslate"><span class="pre">vae.reconstruct_sample</span></code> to reconstruct samples in the original dataset to test the reconstruction capability of the VAE network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">generated_sample</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">generate_sample</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">IMAGE_SHAPE</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="n">sample_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">reconstructed_sample</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">reconstruct_sample</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The shape of the generated sample is &#39;</span><span class="p">,</span> <span class="n">generated_sample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>The shape of the newly generated sample is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The shape of the generated sample is (64, 1, 32, 32)
</pre></div>
</div>
<p>The CVAE training process is similar to the VAE training process. However, when a trained CVAE network is used to generate a new sample and rebuild a new sample, label information needs to be input. For example, the generated new sample is 64 digits ranging from 0 to 7.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample_label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">generated_sample</span> <span class="o">=</span> <span class="n">cvae</span><span class="o">.</span><span class="n">generate_sample</span><span class="p">(</span><span class="n">sample_label</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">IMAGE_SHAPE</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="n">sample_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sample_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">reconstructed_sample</span> <span class="o">=</span> <span class="n">cvae</span><span class="o">.</span><span class="n">reconstruct_sample</span><span class="p">(</span><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The shape of the generated sample is &#39;</span><span class="p">,</span> <span class="n">generated_sample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>Check the shape of the newly generated sample:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The shape of the generated sample is  (64, 1, 32, 32)
</pre></div>
</div>
<p>If you want the generated sample to be better and clearer, you can define a more complex encoder and decoder. The example uses only two layers of full-connected layers.</p>
</section>
<section id="bayesian-layer">
<h2>Bayesian Layer<a class="headerlink" href="#bayesian-layer" title="Permalink to this headline"></a></h2>
<p>The following uses the APIs in <code class="docutils literal notranslate"><span class="pre">nn.probability.bnn_layers</span></code> of MindSpore to implement the BNN image classification model. The APIs in <code class="docutils literal notranslate"><span class="pre">nn.probability.bnn_layers</span></code> of MindSpore include <code class="docutils literal notranslate"><span class="pre">NormalPrior</span></code>, <code class="docutils literal notranslate"><span class="pre">NormalPosterior</span></code>, <code class="docutils literal notranslate"><span class="pre">ConvReparam</span></code>, <code class="docutils literal notranslate"><span class="pre">DenseReparam</span></code>, <code class="docutils literal notranslate"><span class="pre">DenseLocalReparam</span></code> and <code class="docutils literal notranslate"><span class="pre">WithBNNLossCell</span></code>. The biggest difference between BNN and DNN is that the weight and bias of the BNN layer are not fixed values, but follow a distribution. <code class="docutils literal notranslate"><span class="pre">NormalPrior</span></code> and <code class="docutils literal notranslate"><span class="pre">NormalPosterior</span></code> are respectively used to generate a prior distribution and a posterior distribution that follow a normal distribution. <code class="docutils literal notranslate"><span class="pre">ConvReparam</span></code> and <code class="docutils literal notranslate"><span class="pre">DenseReparam</span></code> are the Bayesian convolutional layer and fully connected layers implemented by using the reparameterization method, respectively. <code class="docutils literal notranslate"><span class="pre">DenseLocalReparam</span></code> is the Bayesian fully connected layers implemented by using the local reparameterization method. <code class="docutils literal notranslate"><span class="pre">WithBNNLossCell</span></code> is used to encapsulate the BNN and loss function.</p>
<p>For details about how to use the APIs in <code class="docutils literal notranslate"><span class="pre">nn.probability.bnn_layers</span></code> to build a Bayesian neural network and classify images, see <a class="reference external" href="https://www.mindspore.cn/probability/docs/zh-CN/r1.3/using_bnn.html">Applying the Bayesian Network</a>.</p>
</section>
<section id="bayesian-conversion">
<h2>Bayesian Conversion<a class="headerlink" href="#bayesian-conversion" title="Permalink to this headline"></a></h2>
<p>For researchers who are unfamiliar with the Bayesian model, the MDP provides the <code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.transform</span></code> API to convert the DNN model into the BNN model by one click.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function of the model conversion API <code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> is defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformToBNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainable_dnn</span><span class="p">,</span> <span class="n">dnn_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bnn_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">trainable_dnn</span><span class="o">.</span><span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">trainable_dnn</span><span class="o">.</span><span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">net_with_loss</span><span class="o">.</span><span class="n">backbone_network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="s2">&quot;_loss_fn&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dnn_factor</span> <span class="o">=</span> <span class="n">dnn_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bnn_factor</span> <span class="o">=</span> <span class="n">bnn_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bnn_loss_file</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">trainable_bnn</span></code> parameter is a trainable DNN model packaged by <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>, <code class="docutils literal notranslate"><span class="pre">dnn_factor</span></code> and <code class="docutils literal notranslate"><span class="pre">bnn_factor</span></code> are the coefficient of the overall network loss calculated by the loss function and the coefficient of the KL divergence of each Bayesian layer, respectively.
<code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> implements the following functions:</p>
<ul>
<li><p>Function 1: Convert the entire model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">transform_to_bnn_model</span></code> method can convert the entire DNN model into a BNN model. The definition is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="k">def</span> <span class="nf">transform_to_bnn_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">get_dense_args</span><span class="o">=</span><span class="k">lambda</span> <span class="n">dp</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;in_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                        <span class="s2">&quot;out_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">activation</span><span class="p">},</span>
                             <span class="n">get_conv_args</span><span class="o">=</span><span class="k">lambda</span> <span class="n">dp</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;in_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="s2">&quot;out_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                                                       <span class="s2">&quot;pad_mode&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                                                       <span class="s2">&quot;stride&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                       <span class="s2">&quot;padding&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="s2">&quot;dilation&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                       <span class="s2">&quot;group&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">group</span><span class="p">},</span>
                             <span class="n">add_dense_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">add_conv_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">      </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Transform the whole DNN model to BNN model, and wrap BNN model by TrainOneStepCell.</span>

<span class="sd">      Args:</span>
<span class="sd">          get_dense_args (function): The arguments gotten from the DNN full connection layer. Default: lambda dp:</span>
<span class="sd">              {&quot;in_channels&quot;: dp.in_channels, &quot;out_channels&quot;: dp.out_channels, &quot;has_bias&quot;: dp.has_bias}.</span>
<span class="sd">          get_conv_args (function): The arguments gotten from the DNN convolutional layer. Default: lambda dp:</span>
<span class="sd">              {&quot;in_channels&quot;: dp.in_channels, &quot;out_channels&quot;: dp.out_channels, &quot;pad_mode&quot;: dp.pad_mode,</span>
<span class="sd">              &quot;kernel_size&quot;: dp.kernel_size, &quot;stride&quot;: dp.stride, &quot;has_bias&quot;: dp.has_bias}.</span>
<span class="sd">          add_dense_args (dict): The new arguments added to BNN full connection layer. Default: {}.</span>
<span class="sd">          add_conv_args (dict): The new arguments added to BNN convolutional layer. Default: {}.</span>

<span class="sd">      Returns:</span>
<span class="sd">          Cell, a trainable BNN model wrapped by TrainOneStepCell.</span>
<span class="sd">     &quot;&quot;&quot;</span>

</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">get_dense_args</span></code> specifies the parameters to be obtained from the fully connected layer of the DNN model. The default value is the common parameters of the fully connected layers of the DNN and BNN models. For details about the parameters, see <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/nn/mindspore.nn.Dense.html">mindspore API</a>. <code class="docutils literal notranslate"><span class="pre">get_conv_args</span></code> specifies the parameters to be obtained from the convolutional layer of the DNN model. The default value is the common parameters of the convolutional layers of the DNN and BNN models. For details about the parameters, see <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/nn/mindspore.nn.Conv2d.html">MindSpore API</a>. <code class="docutils literal notranslate"><span class="pre">add_dense_args</span></code> and <code class="docutils literal notranslate"><span class="pre">add_conv_args</span></code> specify the new parameter values to be specified for the BNN layer. Note that the parameters in <code class="docutils literal notranslate"><span class="pre">add_dense_args</span></code> cannot be the same as those in <code class="docutils literal notranslate"><span class="pre">get_dense_args</span></code>. The same rule applies to <code class="docutils literal notranslate"><span class="pre">add_conv_args</span></code> and <code class="docutils literal notranslate"><span class="pre">get_conv_args</span></code>.</p>
</li>
<li><p>Function 2: Convert a specific layer.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">transform_to_bnn_layer</span></code> method can convert a specific layer (<code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code>) in the DNN model into a corresponding Bayesian layer. The definition is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="k">def</span> <span class="nf">transform_to_bnn_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dnn_layer</span><span class="p">,</span> <span class="n">bnn_layer</span><span class="p">,</span> <span class="n">get_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">      </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Transform a specific type of layers in DNN model to corresponding BNN layer.</span>

<span class="sd">      Args:</span>
<span class="sd">          dnn_layer_type (Cell): The type of DNN layer to be transformed to BNN layer. The optional values are</span>
<span class="sd">          nn.Dense, nn.Conv2d.</span>
<span class="sd">          bnn_layer_type (Cell): The type of BNN layer to be transformed to. The optional values are</span>
<span class="sd">              DenseReparameterization, ConvReparameterization.</span>
<span class="sd">          get_args (dict): The arguments gotten from the DNN layer. Default: None.</span>
<span class="sd">          add_args (dict): The new arguments added to BNN layer. Default: None.</span>

<span class="sd">      Returns:</span>
<span class="sd">          Cell, a trainable model wrapped by TrainOneStepCell, whose sprcific type of layer is transformed to the corresponding bayesian layer.</span>
<span class="sd">      &quot;&quot;&quot;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Dnn_layer</span></code> specifies a DNN layer to be converted into a BNN layer, and <code class="docutils literal notranslate"><span class="pre">bnn_layer</span></code> specifies a BNN layer to be converted into a DNN layer, and <code class="docutils literal notranslate"><span class="pre">get_args</span></code> and <code class="docutils literal notranslate"><span class="pre">add_args</span></code> specify the parameters obtained from the DNN layer and the parameters to be re-assigned to the BNN layer, respectively.</p>
</li>
</ul>
<p>For details about how to use <code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> in MindSpore, see <a class="reference external" href="https://www.mindspore.cn/probability/docs/zh-CN/r1.3/one_click_conversion_from_dnn_to_bnn.html">DNN-to-BNN Conversion with One Click</a>.</p>
</section>
<section id="bayesian-toolbox">
<h2>Bayesian Toolbox<a class="headerlink" href="#bayesian-toolbox" title="Permalink to this headline"></a></h2>
<section id="uncertainty-estimation">
<h3>Uncertainty Estimation<a class="headerlink" href="#uncertainty-estimation" title="Permalink to this headline"></a></h3>
<p>One of the advantages of the BNN is that uncertainty can be obtained. MDP provides a toolbox (<code class="docutils literal notranslate"><span class="pre">mindspore.nn.probability.toolbox</span></code>) for uncertainty estimation at the upper layer. You can easily use the toolbox to calculate uncertainty. Uncertainty means the uncertainty of the prediction result of the deep learning model. Currently, most deep learning algorithms can only provide high-confidence prediction results, but cannot determine the certainty of the prediction results. There are two types of uncertainty: aleatoric uncertainty and epistemic uncertainty.</p>
<ul class="simple">
<li><p>Aleatoric uncertainty: describes the internal noise of data, that is, the unavoidable error. This phenomenon cannot be weakened by adding sampling data.</p></li>
<li><p>Epistemic uncertainty: describes the estimation inaccuracy of input data incurred due to reasons such as poor training or insufficient training data. This may be alleviated by adding training data.</p></li>
</ul>
<p>The APIs of the uncertainty estimation toolbox are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: trained model whose uncertainty is to be estimated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_dataset</span></code>: dataset used for training, which is of the iterator type.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task_type</span></code>: model type. The value is a character string. Enter regression or classification.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_classes</span></code>: For a classification model, you need to specify the number of labels of the classification.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code>: number of epochs for training an uncertain model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epi_uncer_model_path</span></code>: path for storing or loading models that compute cognitive uncertainty.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ale_uncer_model_path</span></code>: path used to store or load models that calculate epistemic uncertainty.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">save_model</span></code>: whether to store the model, which is of the Boolean type.</p></li>
</ul>
<p>Before using the model, you need to train the model. The following uses LeNet5 as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.probability.toolbox.uncertainty_evaluation</span> <span class="kn">import</span> <span class="n">UncertaintyEvaluation</span>
<span class="kn">from</span> <span class="nn">mindspore.train.serialization</span> <span class="kn">import</span> <span class="n">load_checkpoint</span><span class="p">,</span> <span class="n">load_param_into_net</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># get trained model</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s1">&#39;checkpoint_lenet.ckpt&#39;</span><span class="p">)</span>
    <span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
    <span class="c1"># get train and eval dataset</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;workspace/mnist/train&#39;</span><span class="p">)</span>
    <span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;workspace/mnist/test&#39;</span><span class="p">)</span>
    <span class="n">evaluation</span> <span class="o">=</span> <span class="n">UncertaintyEvaluation</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">network</span><span class="p">,</span>
                                       <span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span>
                                       <span class="n">task_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span>
                                       <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                       <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                       <span class="n">epi_uncer_model_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                       <span class="n">ale_uncer_model_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                       <span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">eval_data</span> <span class="ow">in</span> <span class="n">ds_eval</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
        <span class="n">eval_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">eval_data</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">epistemic_uncertainty</span> <span class="o">=</span> <span class="n">evaluation</span><span class="o">.</span><span class="n">eval_epistemic_uncertainty</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>
        <span class="n">aleatoric_uncertainty</span> <span class="o">=</span> <span class="n">evaluation</span><span class="o">.</span><span class="n">eval_aleatoric_uncertainty</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The shape of epistemic uncertainty is &#39;</span><span class="p">,</span> <span class="n">epistemic_uncertainty</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The shape of epistemic uncertainty is &#39;</span><span class="p">,</span> <span class="n">aleatoric_uncertainty</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">eval_epistemic_uncertainty</span></code> calculates epistemic uncertainty, which is also called model uncertainty. Each estimation label of every sample has an uncertain value. <code class="docutils literal notranslate"><span class="pre">eval_aleatoric_uncertainty</span></code> calculates aleatoric uncertainty, which is also called data uncertainty. Each sample has an uncertain value.
The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The shape of epistemic uncertainty is (32, 10)
The shape of epistemic uncertainty is (32,)
</pre></div>
</div>
<p>The value of uncertainty is greater than or equal to zero. A larger value indicates higher uncertainty.</p>
</section>
<section id="anomaly-detection">
<h3>Anomaly Detection<a class="headerlink" href="#anomaly-detection" title="Permalink to this headline"></a></h3>
<p>Anomaly Detection can find outliers that are “different from the main data distribution”. For example, finding outliers in data preprocessing can help improve the model’s fitting ability.</p>
<p>MDP provides anomaly detection toolbox (<code class="docutils literal notranslate"><span class="pre">VAEAnomalyDetection</span></code>) based on the variational autoencoder (VAE) in the upper layer. Similar to the use of VAE, we only need to customize the encoder and decoder (DNN model), initialize the relevant parameters, then you can use the toolbox to detect abnormal points.</p>
<p>The interface of the VAE-based anomaly detection toolbox is as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">encoder</span></code>：Encoder(Cell)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">decoder</span></code>：Decoder(Cell)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code>：The size of encoder’s output tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">latent_size</span></code>：The size of the latent space</p></li>
</ul>
<p>Use Encoder and Decoder, set hidden_size and latent_size, initialize the class, and then pass the dataset to detect abnormal points.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.probability.toolbox</span> <span class="kn">import</span> <span class="n">VAEAnomalyDetection</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">()</span>
    <span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>
    <span class="n">ood</span> <span class="o">=</span> <span class="n">VAEAnomalyDetection</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span>
                              <span class="n">hidden_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;workspace/mnist/train&#39;</span><span class="p">)</span>
    <span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;workspace/mnist/test&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ood</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">ds_train</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">ds_eval</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">sample_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">ood</span><span class="o">.</span><span class="n">predict_outlier_score</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
        <span class="n">outlier</span> <span class="o">=</span> <span class="n">ood</span><span class="o">.</span><span class="n">predict_outlier</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">outlier</span><span class="p">)</span>
</pre></div>
</div>
<p>The output of <code class="docutils literal notranslate"><span class="pre">score</span></code> is the anomaly score of the sample. <code class="docutils literal notranslate"><span class="pre">outlier</span></code> is a Boolean type, <code class="docutils literal notranslate"><span class="pre">True</span></code> represents an abnormal point, and <code class="docutils literal notranslate"><span class="pre">False</span></code> represents a normal point.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="using_the_uncertainty_toolbox.html" class="btn btn-neutral float-left" title="Using the Uncertainty Evaluation Toolbox" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>