<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep Q Learning (DQN) with MindSpore Reinforcement &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ReplayBuffer Usage Introduction" href="replaybuffer.html" />
    <link rel="prev" title="MindSpore RL Configuration Instruction" href="custom_config_info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_install.html">MindSpore Reinforcement Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="custom_config_info.html">MindSpore RL Configuration Instruction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep Q Learning (DQN) with MindSpore Reinforcement</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#summary">summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#specifying-the-actor-learner-environment-abstraction-for-dqn">Specifying the Actor-Learner-Environment Abstraction for DQN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-dqntrainer-class">Defining the DQNTrainer class</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-dqnpolicy-class">Defining the DQNPolicy class</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-dqnactor-class">Defining the DQNActor class</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-dqnlearner-class">Defining the DQNLearner class</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#execute-and-view-results">Execute and view results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="replaybuffer.html">ReplayBuffer Usage Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment.html">Reinforcement Learning Environment Access</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement.html">mindspore_rl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Deep Q Learning (DQN) with MindSpore Reinforcement</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/dqn.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="deep-q-learning-dqn-with-mindspore-reinforcement">
<h1>Deep Q Learning (DQN) with MindSpore Reinforcement<a class="headerlink" href="#deep-q-learning-dqn-with-mindspore-reinforcement" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/reinforcement/docs/source_en/dqn.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="summary">
<h2>summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h2>
<p>To implement an reinforcement learning algorithm with MindSpore Reinforcement, a user needs to:</p>
<ul class="simple">
<li><p>provide an algorithm configuration, which separates the implementation of the algorithm from its deployment details;</p></li>
<li><p>implement the algorithm based on an actor-learner-environment abstraction;</p></li>
<li><p>create a session object that executes the implemented algorithm.</p></li>
</ul>
<p>This tutorial shows the use of the MindSpore Reinforcement API to implement the Deep Q Learning (DQN) algorithm. Note that, for clarity and readability, only API-related code sections are presented, and irrelevant code is omitted. The source code of the full DQN implementation for MindSpore Reinforcement can be found <a class="reference external" href="https://github.com/mindspore-lab/mindrl/tree/master/example/dqn">here</a>.</p>
</section>
<section id="specifying-the-actor-learner-environment-abstraction-for-dqn">
<h2>Specifying the Actor-Learner-Environment Abstraction for DQN<a class="headerlink" href="#specifying-the-actor-learner-environment-abstraction-for-dqn" title="Permalink to this headline"></a></h2>
<p>The DQN algorithm requires two deep neural networks, a <em>policy network</em> for approximating the action-value function (Q function) and a <em>target network</em> for stabilising the training. The policy network is the strategy on how to act on the environment, and the goal of the DQN algorithm is to train the policy network for maximum reward. In addition, the DQN algorithm uses an <em>experience replay</em> technique to maintain previous observations for off-policy learning, where an actor uses different behavioural policies to act on the environment.</p>
<p>MindSpore Reinforcement uses an <em>algorithm configuration</em> to specify the logical components (Actor, Learner, Policy and Network, Collect Environment, Eval Environment, Replayuffer) required by the DQN algorithm and the associated hyperparameters. It can execute the algorithm with different strategies based on the provided configuration, which allows the user to focus on the algorithm design.</p>
<p>The algorithm configuration is a Python dictionary that specifies how to construct different components of the DQN algorithm. The hyper-parameters of each component are configured in separate Python dictionaries. The DQN algorithm configuration can be defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">algorithm_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;actor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Number of Actor</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNActor</span><span class="p">,</span>                                                   <span class="c1"># The Actor class</span>
        <span class="s1">&#39;policies&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;init_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;collect_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluate_policy&#39;</span><span class="p">],</span>   <span class="c1"># The policy used to choose action</span>
    <span class="p">},</span>
    <span class="s1">&#39;learner&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Number of Leaarner</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNLearner</span><span class="p">,</span>                                                 <span class="c1"># The Learner class</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">learner_params</span><span class="p">,</span>                                           <span class="c1"># The parameters of Learner</span>
        <span class="s1">&#39;networks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;policy_network&#39;</span><span class="p">,</span> <span class="s1">&#39;target_network&#39;</span><span class="p">]</span>                    <span class="c1"># The networks which is used by Learner</span>
    <span class="p">},</span>
    <span class="s1">&#39;policy_and_network&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNPolicy</span><span class="p">,</span>                                                  <span class="c1"># The Policy class</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">policy_params</span>                                             <span class="c1"># The parameters of Policy</span>
    <span class="p">},</span>
    <span class="s1">&#39;collect_environment&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Number of Collect Environment</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">GymEnvironment</span><span class="p">,</span>                                             <span class="c1"># The Collect Environment class</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">collect_env_params</span>                                        <span class="c1"># The parameters of Collect Environment</span>
    <span class="p">},</span>
    <span class="s1">&#39;eval_environment&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Same as Collect Environment</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">GymEnvironment</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">eval_env_params</span>
    <span class="p">},</span>
    <span class="s1">&#39;replay_buffer&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                          <span class="c1"># Number of ReplayBuffer</span>
                      <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">ReplayBuffer</span><span class="p">,</span>                                 <span class="c1"># The ReplayBuffer class</span>
                      <span class="s1">&#39;capacity&#39;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>                                   <span class="c1"># The capacity of ReplayBuffer</span>
                      <span class="s1">&#39;data_shape&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">4</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)],</span>               <span class="c1"># Data shape of ReplayBuffer</span>
                      <span class="s1">&#39;data_type&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>  <span class="c1"># Data type off ReplayBuffer</span>
                      <span class="s1">&#39;sample_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">},</span>                                   <span class="c1"># Sample size of ReplayBuffer</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The configuration defines six top-level entries, each corresponding to an algorithmic component: <em>actor, learner, policy</em>, <em>replaybuffer</em> and two <em>environment</em>s. Each entry corresponds to a class, which must be defined by the user to implement the DQN algorithm’s logic.</p>
<p>A top-level entry has sub-entries that describe the component. The <em>number</em> entry defines the number of instances of the component used by the algorithm. The <em>type</em> entry refers to the name of the Python class that must be defined to implement the component. The <em>params</em> entry provides the necessary hyper-parameters for the component. The <em>policies</em> entry defines the policies used by the component. The <em>networks</em> in <em>earner</em> entry lists all neural networks used by this component. In the DQN example, only actors interact with the environment. The <em>reply_buffer</em> defines the <em>capacity, shape, sample size and data type</em> of the replay buffer.</p>
<p>For the DQN algorithm, we configure one actor <code class="docutils literal notranslate"><span class="pre">'number':</span> <span class="pre">1</span></code>, its Python class <code class="docutils literal notranslate"><span class="pre">'type':</span> <span class="pre">DQNActor</span></code>, and three behaviour policies <code class="docutils literal notranslate"><span class="pre">'policies':</span> <span class="pre">['init_policy',</span> <span class="pre">'collect_policy',</span> <span class="pre">'evaluate_policy']</span></code>.</p>
<p>Other components are defined in a similar way – please refer to the  <a class="reference external" href="https://github.com/mindspore-lab/mindrl/tree/master/example/dqn">complete DQN code example</a> and the <a class="reference external" href="https://www.mindspore.cn/reinforcement/docs/en/master/reinforcement.html">MindSpore Reinforcement API documentation</a> for more details.</p>
<p>Note that MindSpore Reinforcement uses a single <em>policy</em> class to define all policies and neural networks used by the algorithm. In this way, it hides the complexity of data sharing and communication between policies and neural networks.</p>
<p>In train.py, MindSpore Reinforcement executes the algorithm in the context of a <em>session</em>. A session allocates resources (on one or more cluster machines) and executes the compiled computational graph. A user passes the algorithm configuration to instantiate a Session class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_rl.core</span> <span class="kn">import</span> <span class="n">Session</span>
<span class="n">dqn_session</span> <span class="o">=</span> <span class="n">Session</span><span class="p">(</span><span class="n">dqn_algorithm_config</span><span class="p">)</span>
</pre></div>
</div>
<p>Invoke the <code class="docutils literal notranslate"><span class="pre">run</span></code> method and pass corresponding parameters  to execute the DQN algorithm. <em>class_type</em> is user-defined Trainer class, which will be described later, episode is the iteration times of the algorithm, params are the parameters that is used in the trainer class. It is written in configuration file. For more detail, please check config.py file in the code example. Callbacks define some metrics methods. It is described more detailly in Callbacks part of API documentation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.dqn_trainer</span> <span class="kn">import</span> <span class="n">DQNTrainer</span>
<span class="kn">from</span> <span class="nn">mindspore_rl.utils.callback</span> <span class="kn">import</span> <span class="n">CheckpointCallback</span><span class="p">,</span> <span class="n">LossCallback</span><span class="p">,</span> <span class="n">EvaluateCallback</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossCallback</span><span class="p">()</span>
<span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">CheckpointCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">trainer_params</span><span class="p">[</span><span class="s1">&#39;ckpt_path&#39;</span><span class="p">])</span>
<span class="n">eval_cb</span> <span class="o">=</span> <span class="n">EvaluateCallback</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">cbs</span> <span class="o">=</span> <span class="p">[</span><span class="n">loss_cb</span><span class="p">,</span> <span class="n">ckpt_cb</span><span class="p">,</span> <span class="n">eval_cb</span><span class="p">]</span>
<span class="n">dqn_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">class_type</span><span class="o">=</span><span class="n">DQNTrainer</span><span class="p">,</span> <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">trainer_params</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">cbs</span><span class="p">)</span>
</pre></div>
</div>
<p>To leverage MindSpore’s computational graph feature, users set the execution mode to <code class="docutils literal notranslate"><span class="pre">GRAPH_MODE</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
</pre></div>
</div>
<p>Methods that are annotated with <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code> will be compiled into the MindSpore computational graph for auto-parallelisation and acceleration. In this tutorial, we use this feature to implement an efficient <code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code> class.</p>
<section id="defining-the-dqntrainer-class">
<h3>Defining the DQNTrainer class<a class="headerlink" href="#defining-the-dqntrainer-class" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code> class expresses how the algorithm runs.  For example, iteratively collects experience through iteracting with environment and insert to replaybuffer, then obtain the data from the replay buffer to trains the targeted models. It must inherit from the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class, which is part of the MindSpore Reinforcement API.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> base class contains an <code class="docutils literal notranslate"><span class="pre">MSRL</span></code> (MindSpore Reinforcement) object, which allows the algorithm implementation to interact with MindSpore Reinforcement to implement the training logic. The <code class="docutils literal notranslate"><span class="pre">MSRL</span></code> class instantiates the RL algorithm components based on the previously defined algorithm configuration. It provides the function handlers that transparently bind to methods of actors, learners, or the replay buffer object, as defined by users. As a result, the <code class="docutils literal notranslate"><span class="pre">MSRL</span></code> class enables users to focus on the algorithm logic, while it transparently handles object creation, data sharing and communication between different algorithmic components on one or more workers. Users instantiate the <code class="docutils literal notranslate"><span class="pre">MSRL</span></code> object by creating the previously mentioned <code class="docutils literal notranslate"><span class="pre">Session</span></code> object with the algorithm configuration.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code> must overload the <code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code> for training, <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> for evaluation and <code class="docutils literal notranslate"><span class="pre">trainable_varaible</span></code> for saving checkpoint. In this tutorial, it is defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msrl</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQNTrainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">msrl</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Trainable variables for saving.&quot;&quot;&quot;</span>
        <span class="n">trainable_variables</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;policy_net&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">policy_network</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">trainable_variables</span>

    <span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">init_training</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize training&quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">collect_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
        <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_value</span><span class="p">):</span>
            <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">INIT</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_insert</span><span class="p">(</span>
                <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">collect_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
                <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">done</span>

    <span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Policy evaluate&quot;&quot;&quot;</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
        <span class="n">eval_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">eval_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_evaluate_episode</span><span class="p">):</span>
            <span class="n">episode_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">eval_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">done</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">EVAL</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
                <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
                <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">episode_reward</span>
            <span class="n">eval_iter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">total_reward</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_evaluate_episode</span>
        <span class="k">return</span> <span class="n">avg_reward</span>
</pre></div>
</div>
<p>User will call the <code class="docutils literal notranslate"><span class="pre">train</span></code> method in base class. It trains the models for the specified number of episodes (iterations), with each episode calling the user-defined <code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code> method. Finally, the train method evaluates the policy to obtain a reward value by calling the <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> method.</p>
<p>In each iteration of the training loop, the <code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code> method is invoked to train an episode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">train_one_episode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train one episode&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inited</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_training</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inited</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">collect_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">done</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">COLLECT</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_insert</span><span class="p">(</span>
            <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_learn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_sample</span><span class="p">())</span>
        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_period</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">total_reward</span><span class="p">,</span> <span class="n">steps</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code> annotation states that this method will be compiled into a MindSpore computational graph for acceleration. To support this, all scalar values must be defined as tensor types, e.g. <code class="docutils literal notranslate"><span class="pre">self.zero_value</span> <span class="pre">=</span> <span class="pre">Tensor(0,</span> <span class="pre">mindspore.float32)</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code> method first calls the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method of environment, <code class="docutils literal notranslate"><span class="pre">self.msrl.collect_environment.reset()</span></code> function to reset the environment. It then collects the experience from the environment with the <code class="docutils literal notranslate"><span class="pre">msrl.agent_act</span></code> function handler and inserts the experience data in the replay buffer using the <code class="docutils literal notranslate"><span class="pre">self.msrl.replay_buffer_insert</span></code> function. Afterwards, it invokes the <code class="docutils literal notranslate"><span class="pre">self.msrl.agent_learn</span></code>  function to train the target model. The input of <code class="docutils literal notranslate"><span class="pre">self.msrl.agent_learn</span></code> is a set of sampled results returned by  <code class="docutils literal notranslate"><span class="pre">self.msrl.replay_buffer_sample</span></code>.</p>
<p>The replay buffer class, <code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>, is provided by MindSpore Reinforcement. It defines <code class="docutils literal notranslate"><span class="pre">insert</span></code> and <code class="docutils literal notranslate"><span class="pre">sample</span></code> methods to store and sample the experience data in a replay buffer, respectively. Please refer to the <a class="reference external" href="https://github.com/mindspore-lab/mindrl/tree/master/example/dqn">complete DQN code example</a> for details.</p>
</section>
<section id="defining-the-dqnpolicy-class">
<h3>Defining the DQNPolicy class<a class="headerlink" href="#defining-the-dqnpolicy-class" title="Permalink to this headline"></a></h3>
<p>To implement the neural networks and define the policies, a user defines the <code class="docutils literal notranslate"><span class="pre">DQNPolicy</span></code> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;state_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;hidden_size&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;compute_type&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;state_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;hidden_size&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;compute_type&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>The constructor takes as input the previously-defined hyper-parameters of the Python dictionary type in config.py, <code class="docutils literal notranslate"><span class="pre">policy_params</span></code>.</p>
<p>Before defining the policy network and the target network, users must define the structure of the neural networks using MindSpore operators. For example, they may be objects of the <code class="docutils literal notranslate"><span class="pre">FullyConnectedNetwork</span></code> class, which is defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FullyConnectedNetwork</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FullyConnectedNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">input_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;XavierUniform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">output_size</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;XavierUniform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</pre></div>
</div>
<p>The DQN algorithm uses a loss function to optimize the weights of the neural networks. At this point, a user must define a neural network used to compute the loss function. This network is specified as a nested class of <code class="docutils literal notranslate"><span class="pre">DQNLearner</span></code>. In addition, an optimizer is required to train the network. The optimizer and the loss function are defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DQN Learner&quot;&quot;&quot;</span>

    <span class="k">class</span> <span class="nc">PolicyNetWithLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;DQN policy network with loss cell&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">DQNLearner</span><span class="o">.</span><span class="n">PolicyNetWithLossCell</span><span class="p">,</span>
                  <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;constructor for Loss Cell&quot;&quot;&quot;</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQNLearner</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="n">loss_q_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PolicyNetWithLossCell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">loss_q_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>The DQN algorithm is an <em>off-policy</em> algorithm that learns using a epsilon-greedy policy. It uses different behavioural policies for acting on the environment and collecting data. In this example, we use the <code class="docutils literal notranslate"><span class="pre">RandomPolicy</span></code> to initialize the training, the <code class="docutils literal notranslate"><span class="pre">EpsilonGreedyPolicy</span></code> to collect the experience during the training, and the <code class="docutils literal notranslate"><span class="pre">GreedyPolicy</span></code> to evaluate:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collect_policy</span> <span class="o">=</span> <span class="n">EpsilonGreedyPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;epsi_high&#39;</span><span class="p">],</span>
                                                  <span class="n">params</span><span class="p">[</span><span class="s1">&#39;epsi_low&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_policy</span> <span class="o">=</span> <span class="n">GreedyPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">)</span>
</pre></div>
</div>
<p>Since the above three behavioural policies are common for a range of RL algorithms, MindSpore Reinforcement provides them as reusable building blocks. Users may also define their own algorithm-specific behavioural policies.</p>
<p>Note that the names of the methods and the keys of the parameter dictionary must be consistent with the algorithm configuration defined earlier.</p>
</section>
<section id="defining-the-dqnactor-class">
<h3>Defining the DQNActor class<a class="headerlink" href="#defining-the-dqnactor-class" title="Permalink to this headline"></a></h3>
<p>To implement the <code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>, a user defines a new actor component that inherits from the <code class="docutils literal notranslate"><span class="pre">Actor</span></code> class provided by MindSpore Reinforcement. They must then overload the methods in <code class="docutils literal notranslate"><span class="pre">Actor</span></code> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
     <span class="o">...</span>
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Fill the replay buffer</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_policy</span><span class="p">()</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># Experience collection</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">ts0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">step_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span>

            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_policy</span><span class="p">(</span><span class="n">ts0</span><span class="p">,</span> <span class="n">step_tensor</span><span class="p">)</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># Evaluate the trained policy</span>
            <span class="n">ts0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_policy</span><span class="p">(</span><span class="n">ts0</span><span class="p">)</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;Phase is incorrect&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>
</div>
<p>The three methods act on the specified environment with different policies, which map states to actions. The methods take as input a tensor-typed value and return the trajectory from the environment.</p>
<p>A user should implement an environment class that defines a <code class="docutils literal notranslate"><span class="pre">step</span></code> method. To interact with an environment, the actor uses the <code class="docutils literal notranslate"><span class="pre">step</span></code> method, which collects a triplet as the return value. The triplet includes a new state after applying the <code class="docutils literal notranslate"><span class="pre">action</span></code>, an obtained reward as a float, and a boolean flag to reset the environment. For example, if using the OpenAI Gym library, MindSpore Reinforcement refactors it for computational-graph acceleration and provides the class <code class="docutils literal notranslate"><span class="pre">GymEnvironment</span></code>. Users specify the used environments in the algorithm configuration.</p>
<p>The constructor of the <code class="docutils literal notranslate"><span class="pre">DQNActor</span></code> class defines the environment, the reply buffer, the polices, and the networks. It takes as input the dictionary-typed parameters, which were defined in the algorithm configuration. Below, we only show the initialisation of the environment, other attributes are assigned in the similar way:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;collect_environment&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eval_env</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;eval_environment&#39;</span><span class="p">]</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="defining-the-dqnlearner-class">
<h3>Defining the DQNLearner class<a class="headerlink" href="#defining-the-dqnlearner-class" title="Permalink to this headline"></a></h3>
<p>To implement the <code class="docutils literal notranslate"><span class="pre">DQNLearner</span></code>, a class must inherit from the <code class="docutils literal notranslate"><span class="pre">Learner</span></code> class in the MindSpore Reinforcement API and overload the <code class="docutils literal notranslate"><span class="pre">learn</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
     <span class="o">...</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Model update&quot;&quot;&quot;</span>
        <span class="n">s0</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">s1</span> <span class="o">=</span> <span class="n">experience</span>
        <span class="n">next_state_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
        <span class="n">next_state_values</span> <span class="o">=</span> <span class="n">next_state_values</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">r1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>

        <span class="n">y_true</span> <span class="o">=</span> <span class="n">r1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_state_values</span>

        <span class="c1"># Modify last step reward</span>
        <span class="n">one</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">r1</span> <span class="o">==</span> <span class="o">-</span><span class="n">one</span><span class="p">,</span> <span class="n">one</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span><span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">success</span>
</pre></div>
</div>
<p>Here, the <code class="docutils literal notranslate"><span class="pre">learn</span></code> method takes as input the trajectory (sampled from a reply buffer) to train the policy network. The constructor assigns the network, the policy, and the discount rate to the DQNLearner, by receiving a dictionary-typed configuration from the algorithm configuration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQNLearner</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;policy_network&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;target_network&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="execute-and-view-results">
<h2>Execute and view results<a class="headerlink" href="#execute-and-view-results" title="Permalink to this headline"></a></h2>
<p>Execute script <code class="docutils literal notranslate"><span class="pre">train.py</span></code> to start DQN model training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">example</span><span class="o">/</span><span class="n">dqn</span><span class="o">/</span>
<span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>The execution results are shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>-----------------------------------------
Evaluation result in episode 0 is 95.300
-----------------------------------------
Episode 0, steps: 33.0, reward: 33.000
Episode 1, steps: 45.0, reward: 12.000
Episode 2, steps: 54.0, reward: 9.000
Episode 3, steps: 64.0, reward: 10.000
Episode 4, steps: 73.0, reward: 9.000
Episode 5, steps: 82.0, reward: 9.000
Episode 6, steps: 91.0, reward: 9.000
Episode 7, steps: 100.0, reward: 9.000
Episode 8, steps: 109.0, reward: 9.000
Episode 9, steps: 118.0, reward: 9.000
...
...
Episode 200, steps: 25540.0, reward: 200.000
Episode 201, steps: 25740.0, reward: 200.000
Episode 202, steps: 25940.0, reward: 200.000
Episode 203, steps: 26140.0, reward: 200.000
Episode 204, steps: 26340.0, reward: 200.000
Episode 205, steps: 26518.0, reward: 178.000
Episode 206, steps: 26718.0, reward: 200.000
Episode 207, steps: 26890.0, reward: 172.000
Episode 208, steps: 27090.0, reward: 200.000
Episode 209, steps: 27290.0, reward: 200.000
-----------------------------------------
Evaluation result in episode 210 is 200.000
-----------------------------------------
</pre></div>
</div>
<p><img alt="CartPole" src="_images/cartpole.gif" /></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_config_info.html" class="btn btn-neutral float-left" title="MindSpore RL Configuration Instruction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="replaybuffer.html" class="btn btn-neutral float-right" title="ReplayBuffer Usage Introduction" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>