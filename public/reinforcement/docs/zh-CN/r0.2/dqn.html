

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>使用MindSpore Reinforcement实现深度Q学习（DQN） &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="mindspore_rl" href="reinforcement.html" />
    <link rel="prev" title="强化学习配置说明" href="custom_config_info.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_install.html">安装MindSpore Reinforcement</a></li>
</ul>
<p class="caption"><span class="caption-text">使用指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="custom_config_info.html">强化学习配置说明</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">使用MindSpore Reinforcement实现深度Q学习（DQN）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">摘要</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dqnactor-learner-environment">指定DQN的Actor-Learner-Environment抽象</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dqntrainer">定义DQNTrainer类</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dqnpolicy">定义DQNPolicy类</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dqnactor">定义DQNActor类</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dqnlearner">定义DQNLearner类</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">执行并查看结果</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement.html">mindspore_rl</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>使用MindSpore Reinforcement实现深度Q学习（DQN）</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/dqn.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindspore-reinforcementq-dqn">
<h1>使用MindSpore Reinforcement实现深度Q学习（DQN）<a class="headerlink" href="#mindspore-reinforcementq-dqn" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/reinforcement/docs/source_zh_cn/dqn.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source.png"></a>
  </p>
<div class="section" id="id1">
<h2>摘要<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>为了使用MindSpore Reinforcement实现强化学习算法，用户需要：</p>
<ul class="simple">
<li><p>提供算法配置，将算法的实现与其部署细节分开；</p></li>
<li><p>基于Actor-Learner-Environment抽象实现算法；</p></li>
<li><p>创建一个执行已实现的算法的会话对象。</p></li>
</ul>
<p>本教程展示了使用MindSpore Reinforcement API实现深度Q学习（DQN）算法。注：为保证清晰性和可读性，仅显示与API相关的代码，不相关的代码已省略。点击<a class="reference external" href="https://gitee.com/mindspore/reinforcement/tree/r0.2/example/dqn">此处</a>获取MindSpore Reinforcement实现完整DQN的源代码。</p>
</div>
<div class="section" id="dqnactor-learner-environment">
<h2>指定DQN的Actor-Learner-Environment抽象<a class="headerlink" href="#dqnactor-learner-environment" title="Permalink to this headline">¶</a></h2>
<p>DQN算法需要两个深度神经网络，一个<em>策略网络</em>用于近似动作值函数(Q函数)，另一个<em>目标网络</em>用于稳定训练。策略网络指如何对环境采取行动的策略，DQN算法的目标是训练策略网络以获得最大的奖励。此外，DQN算法使用<em>经验回放</em>技术来维护先前的观察结果，进行off-policy学习。其中Actor使用不同的行为策略来对环境采取行动。</p>
<p>MindSpore Reinforcement使用<em>算法配置</em>指定DQN算法所需的逻辑组件（Agent、Actor、Learner、Environment）和关联的超参数。根据提供的配置，它使用不同的策略执行算法，以便用户可以专注于算法设计。</p>
<p>算法配置是一个Python字典，指定如何构造DQN算法的不同组件。每个组件的超参数在单独的Python字典中配置。DQN算法配置定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="s1">&#39;actor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNActor</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;policies&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;init_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;collect_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluate_policy&#39;</span><span class="p">],</span>
        <span class="s1">&#39;networks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;policy_network&#39;</span><span class="p">,</span> <span class="s1">&#39;target_network&#39;</span><span class="p">],</span>
        <span class="s1">&#39;environment&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;eval_environment&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;replay_buffer&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;capacity&#39;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">4</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)],</span>
                          <span class="s1">&#39;sample_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">]},</span>
    <span class="p">},</span>
    <span class="s1">&#39;learner&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNLearner</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">learner_params</span><span class="p">,</span>
        <span class="s1">&#39;networks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;target_network&#39;</span><span class="p">,</span> <span class="s1">&#39;policy_network_train&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;policy_and_network&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNPolicy</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">policy_params</span>
    <span class="p">},</span>
    <span class="s1">&#39;environment&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">GymEnvironment</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">env_params</span>
    <span class="p">},</span>
    <span class="s1">&#39;eval_environment&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">GymEnvironment</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">eval_env_params</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>以上配置定义了四个顶层项，每个配置对应一个算法组件：<em>actor、learner、policy</em>和<em>environment</em>。每个项对应一个类，该类必须由用户定义，以实现DQN算法的逻辑。</p>
<p>顶层项具有描述组件的子项。<em>number</em>定义算法使用的组件的实例数。<em>class</em>表示必须定义的Python类的名称，用于实现组件。<em>parameters</em>为组件提供必要的超参数。<em>policy</em>定义组件使用的策略。<em>networks</em>列出了此组件使用的所有神经网络。<em>environment</em>说明组件是否与环境交互。在DQN示例中，只有Actor与环境交互。<em>reply_buffer</em>定义回放缓冲区的<em>容量、形状、样本大小和数据类型</em>。</p>
<p>对于DQN算法，我们配置了一个Actor <code class="docutils literal notranslate"><span class="pre">'number':</span> <span class="pre">1</span></code>，三个行为策略<code class="docutils literal notranslate"><span class="pre">'policies':</span> <span class="pre">['init_policy',</span> <span class="pre">'collect_policy',</span> <span class="pre">'evaluation_policy']</span></code>，两个神经网络<code class="docutils literal notranslate"><span class="pre">'networks':</span> <span class="pre">['policy_network',</span> <span class="pre">'target_network']</span></code>，环境<code class="docutils literal notranslate"><span class="pre">'environment':</span> <span class="pre">True</span></code>，和回放缓冲区<code class="docutils literal notranslate"><span class="pre">'replay_buffer':{'capacity':100000,'shape':[...],'sample_size':64,'type':[..]}</span></code>。</p>
<p>回放缓冲区的容量设置为100,000，其样本大小为64。它存储shape为<code class="docutils literal notranslate"><span class="pre">[(4,),</span> <span class="pre">(1,),</span> <span class="pre">(1,),</span> <span class="pre">(4,)]</span></code>的张量数据。第二个维度的类型为int32，其他维度的类型为float32。这两种类型都由MindSpore提供：<code class="docutils literal notranslate"><span class="pre">'type':</span> <span class="pre">[mindspore.float32,</span> <span class="pre">mindspore.int32,</span> <span class="pre">mindspore.float32,</span> <span class="pre">mindspore.float32]}</span></code>。</p>
<p>其他组件也以类似的方式定义。有关更多详细信息，请参阅<a class="reference external" href="https://gitee.com/mindspore/reinforcement/tree/r0.2/example/dqn">完整代码示例</a>和<a class="reference external" href="https://www.mindspore.cn/reinforcement/docs/zh-CN/r0.2/reinforcement.html">API</a>。</p>
<p>请注意，MindSpore Reinforcement使用单个<em>policy</em>类来定义算法使用的所有策略和神经网络。通过这种方式，它隐藏了策略和神经网络之间数据共享和通信的复杂性。</p>
<p>MindSpore Reinforcement在<em>session</em>的上下文中执行算法。会话分配资源（在一台或多台群集计算机上）并执行编译后的计算图。用户传入算法配置以实例化Session类：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dqn_session</span> <span class="o">=</span> <span class="n">Session</span><span class="p">(</span><span class="n">dqn_algorithm_config</span><span class="p">)</span>
</pre></div>
</div>
<p>调用Session对象上的run方法执行DQN算法：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dqn_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">class_type</span><span class="o">=</span><span class="n">DQNTrainer</span><span class="p">,</span> <span class="n">episode</span><span class="o">=</span><span class="mi">650</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">trainer_parameters</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">run</span></code>方法将DQNTrainer类作为输入。下面描述了用于DQN算法的训练循环。</p>
<p>为使用MindSpore的计算图功能，将执行模式设置为<code class="docutils literal notranslate"><span class="pre">GRAPH_MODE</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">GRAPH_MODE</span></code>允许以<code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code>注释的函数和方法编译到<a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/api_structure.html">MindSopre计算图</a>用于自动并行和加速。在本教程中，我们使用此功能来实现一个高效的<code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code>类。</p>
<div class="section" id="dqntrainer">
<h3>定义DQNTrainer类<a class="headerlink" href="#dqntrainer" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">DQN训练器</span></code>类表示训练循环，该循环迭代地从回放缓冲区收集经验并训练目标模型。它必须继承自<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>类，该类是MindSpore Reinforcement API的一部分。</p>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code>基类包含<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>(MindSpore Reinforcement Learning)对象，该对象允许算法实现与MindSpore Reinforcement交互，以实现训练逻辑。<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>类根据先前定义的算法配置实例化RL算法组件。它提供了函数处理程序，这些处理程序透明地绑定到用户定义的Actor、Learner或回放缓冲区对象的方法。因此，<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>类让用户能够专注于算法逻辑，同时它透明地处理一个或多个worker上不同算法组件之间的对象创建、数据共享和通信。用户通过使用算法配置创建上文提到的<code class="docutils literal notranslate"><span class="pre">Session</span></code>对象来实例化<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>对象。</p>
<p><code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code>必须重载训练方法。在本教程中，它的定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_training</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episode</span><span class="p">):</span>
           <span class="n">reward</span><span class="p">,</span> <span class="n">episode_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_one_episode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_period</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code>方法首先调用<code class="docutils literal notranslate"><span class="pre">init_training</span></code>初始化训练。然后，它为指定数量的episode（iteration）训练模型，每个episode调用用户定义的<code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>方法。最后，train方法通过调用<code class="docutils literal notranslate"><span class="pre">evaluation</span></code>方法来评估策略以获得奖励值。</p>
<p>在训练循环的每次迭代中，调用<code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>方法来训练一个episode：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">train_one_episode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_period</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train one episode&quot;&quot;&quot;</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_reset_collect</span><span class="p">()</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">done</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_insert</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_learn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_sample</span><span class="p">())</span>
        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">update_period</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_update</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">total_reward</span><span class="p">,</span> <span class="n">steps</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code>注解表示此方法将被编译为MindSpore计算图用于加速。所有标量值都必须定义为张量类型，例如<code class="docutils literal notranslate"><span class="pre">self.zero_value</span> <span class="pre">=</span> <span class="pre">Tensor(0,</span> <span class="pre">mindspore.float32)</span></code>。</p>
<p><code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>方法首先调用<code class="docutils literal notranslate"><span class="pre">msrl.agent_reset_collect</span></code>函数（由MindSpore Reinforcement API提供）来重置环境。然后，它使用<code class="docutils literal notranslate"><span class="pre">msrl.agent_act</span></code>函数处理程序从环境中收集经验，并使用<code class="docutils literal notranslate"><span class="pre">msrl.agent_learn</span></code>函数训练目标模型。<code class="docutils literal notranslate"><span class="pre">msrl.agent_learn</span></code>的输入是<code class="docutils literal notranslate"><span class="pre">msrl.sample_replay_buffer</span></code>返回的采样结果。</p>
<p>回放缓存<code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>由MindSpore Reinfocement提供。它定义了<code class="docutils literal notranslate"><span class="pre">insert</span></code>和<code class="docutils literal notranslate"><span class="pre">sample</span></code>方法，分别用于对经验数据进行存储和采样。</p>
<p><code class="docutils literal notranslate"><span class="pre">init_training</span></code>和<code class="docutils literal notranslate"><span class="pre">evaluation</span></code>方法的实现类似。详细信息，请参阅<a class="reference external" href="https://gitee.com/mindspore/reinforcement/tree/r0.2/example/dqn">完整的DQN代码示例</a>。</p>
</div>
<div class="section" id="dqnpolicy">
<h3>定义DQNPolicy类<a class="headerlink" href="#dqnpolicy" title="Permalink to this headline">¶</a></h3>
<p>定义<code class="docutils literal notranslate"><span class="pre">DQNPolicy</span></code>类，用于实现神经网络并定义策略。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">():</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;state_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;hidden_size&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;state_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;hidden_size&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>构造函数将先前定义的Python字典类型的超参数<code class="docutils literal notranslate"><span class="pre">policy_parameters</span></code>作为输入。</p>
<p>在定义策略网络和目标网络之前，用户必须使用MindSpore算子定义神经网络的结构。例如，它们可能是<code class="docutils literal notranslate"><span class="pre">FullyConnectedNetwork</span></code>类的对象，该类定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FullyConnectedNetwork</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FullyConnectedNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">input_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;XavierUniform&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">output_size</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;XavierUniform&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</pre></div>
</div>
<p>DQN算法使用损失函数来优化神经网络的权重。此时，用户必须定义一个用于计算损失函数的神经网络。此网络被指定为<code class="docutils literal notranslate"><span class="pre">DQNPolicy</span></code>的嵌套类。此外，还需要优化器来训练网络。优化器和损失函数定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">():</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span>  <span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span>
                                       <span class="n">learning_rate</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">loss_Q_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PolicyNetWithLossCell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">loss_Q_net</span><span class="p">,</span> <span class="n">otimizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>DQN算法是一种<em>off-policy</em>算法，使用贪婪策略学习。它使用不同的行为策略来对环境采取行动和收集数据。在本示例中，我们用<code class="docutils literal notranslate"><span class="pre">RandomPolicy</span></code>初始化训练，用<code class="docutils literal notranslate"><span class="pre">EpsilonGreedyPolicy</span></code>收集训练期间的经验，用<code class="docutils literal notranslate"><span class="pre">GreedyPolicy</span></code>进行评估：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">():</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
         <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collect_policy</span> <span class="o">=</span> <span class="n">EpsilonGreedyPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;epsi_high&#39;</span><span class="p">],</span>
                                                  <span class="n">params</span><span class="p">[</span><span class="s1">&#39;epsi_low&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_policy</span> <span class="o">=</span> <span class="n">GreedyPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">)</span>
</pre></div>
</div>
<p>由于上述三种行为策略在一系列RL算法中非常常见，MindSpore Reinforcement将它们作为可重用的构建块提供。用户还可以自定义特定算法的行为策略。</p>
<p>请注意，参数字典的方法名称和键必须与前面定义的算法配置一致。</p>
</div>
<div class="section" id="dqnactor">
<h3>定义DQNActor类<a class="headerlink" href="#dqnactor" title="Permalink to this headline">¶</a></h3>
<p>定义一个新的Actor组件用于实现<code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>，该组件继承了MindSpore Reinforcement提供的<code class="docutils literal notranslate"><span class="pre">Actor</span></code>类。然后，必须重载trainer使用的方法：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
     <span class="o">...</span>
    <span class="k">def</span> <span class="nf">act_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fill the replay buffer&quot;&quot;&quot;</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_policy</span><span class="p">()</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Experience collection&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">ts0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">step_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span>

        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_policy</span><span class="p">(</span><span class="n">ts0</span><span class="p">,</span> <span class="n">step_tensor</span><span class="p">)</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate the trained policy&quot;&quot;&quot;</span>
        <span class="n">ts0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_policy</span><span class="p">(</span><span class="n">ts0</span><span class="p">)</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span>
</pre></div>
</div>
<p>这三种方法使用不同的策略作用于指定的环境，这些策略将状态映射到操作。这些方法将张量类型的值作为输入，并从环境返回轨迹。</p>
<p>为了与环境交互，Actor使用<code class="docutils literal notranslate"><span class="pre">Environment</span></code>类中定义的<code class="docutils literal notranslate"><span class="pre">step(action)</span></code>方法。对于应用到指定环境的操作，此方法会做出反应并返回三元组。三元组包括应用上一个操作后的新状态、作为浮点类型获得的奖励以及用于终止episode和重置环境的布尔标志。</p>
<p>回放缓冲区类<code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>定义了一个<code class="docutils literal notranslate"><span class="pre">insert</span></code>方法，<code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>对象调用该方法将经验数据存储在回放缓冲区中。</p>
<p><code class="docutils literal notranslate"><span class="pre">Environment</span></code>类和<code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>类由MindSpore Reinforcement API提供。</p>
<p><code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>类的构造函数定义了环境、回放缓冲区、策略和网络。它将字典类型的参数作为输入，这些参数在算法配置中定义。下面，我们只展示环境的初始化，其他属性以类似的方式分配：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;environment&#39;</span><span class="p">]</span>
         <span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="dqnlearner">
<h3>定义DQNLearner类<a class="headerlink" href="#dqnlearner" title="Permalink to this headline">¶</a></h3>
<p>为了实现<code class="docutils literal notranslate"><span class="pre">DQNLearner</span></code>，类必须继承MindSpore Reinforcement API中的<code class="docutils literal notranslate"><span class="pre">Learner</span></code>类，并重载<code class="docutils literal notranslate"><span class="pre">learn</span></code>方法：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
     <span class="o">...</span>
     <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
         <span class="n">state_0</span><span class="p">,</span> <span class="n">action_0</span><span class="p">,</span> <span class="n">reward_</span><span class="p">,</span> <span class="n">state_1</span> <span class="o">=</span> <span class="n">samples</span>
         <span class="n">next_state_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">state1</span><span class="p">)</span>
         <span class="n">y_true</span> <span class="o">=</span> <span class="n">reward_1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_state_values</span>
         <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span><span class="p">(</span><span class="n">state_0</span><span class="p">,</span> <span class="n">action_0</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">success</span>
</pre></div>
</div>
<p>在这里，<code class="docutils literal notranslate"><span class="pre">learn</span></code>方法将轨迹（从回放缓冲区采样）作为输入来训练策略网络。构造函数通过从算法配置接收字典类型的配置，将网络、策略和折扣率分配给DQNLearner：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;target_network&#39;</span><span class="p">]</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;policy_network_train&#39;</span><span class="p">]</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h2>执行并查看结果<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>执行脚本<code class="docutils literal notranslate"><span class="pre">train.py</span></code>以启动DQN模型训练。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">example</span><span class="o">/</span><span class="n">dqn</span><span class="o">/</span>
<span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>执行结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>-----------------------------------------
Evaluation result in episode 0 is 95.300
-----------------------------------------
Episode 0, steps: 33.0, reward: 33.000
Episode 1, steps: 45.0, reward: 12.000
Episode 2, steps: 54.0, reward: 9.000
Episode 3, steps: 64.0, reward: 10.000
Episode 4, steps: 73.0, reward: 9.000
Episode 5, steps: 82.0, reward: 9.000
Episode 6, steps: 91.0, reward: 9.000
Episode 7, steps: 100.0, reward: 9.000
Episode 8, steps: 109.0, reward: 9.000
Episode 9, steps: 118.0, reward: 9.000
...
...
Episode 200, steps: 25540.0, reward: 200.000
Episode 201, steps: 25740.0, reward: 200.000
Episode 202, steps: 25940.0, reward: 200.000
Episode 203, steps: 26140.0, reward: 200.000
Episode 204, steps: 26340.0, reward: 200.000
Episode 205, steps: 26518.0, reward: 178.000
Episode 206, steps: 26718.0, reward: 200.000
Episode 207, steps: 26890.0, reward: 172.000
Episode 208, steps: 27090.0, reward: 200.000
Episode 209, steps: 27290.0, reward: 200.000
-----------------------------------------
Evaluation result in episode 210 is 200.000
-----------------------------------------
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="reinforcement.html" class="btn btn-neutral float-right" title="mindspore_rl" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="custom_config_info.html" class="btn btn-neutral float-left" title="强化学习配置说明" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>