

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>使用MindSpore Reinforcement实现深度Q学习（DQN） &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ReplayBuffer 使用说明" href="replaybuffer.html" />
    <link rel="prev" title="强化学习配置说明" href="custom_config_info.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_install.html">安装MindSpore Reinforcement</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">使用指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="custom_config_info.html">强化学习配置说明</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">使用MindSpore Reinforcement实现深度Q学习（DQN）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#摘要">摘要</a></li>
<li class="toctree-l2"><a class="reference internal" href="#指定dqn的actor-learner-environment抽象">指定DQN的Actor-Learner-Environment抽象</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#定义dqntrainer类">定义DQNTrainer类</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义dqnpolicy类">定义DQNPolicy类</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义dqnactor类">定义DQNActor类</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义dqnlearner类">定义DQNLearner类</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#执行并查看结果">执行并查看结果</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="replaybuffer.html">ReplayBuffer 使用说明</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement.html">mindspore_rl</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">MindSpore Reinforcement Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>使用MindSpore Reinforcement实现深度Q学习（DQN）</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/dqn.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="使用mindspore-reinforcement实现深度q学习dqn">
<h1>使用MindSpore Reinforcement实现深度Q学习（DQN）<a class="headerlink" href="#使用mindspore-reinforcement实现深度q学习dqn" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/reinforcement/docs/source_zh_cn/dqn.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source.png"></a>
  </p>
<section id="摘要">
<h2>摘要<a class="headerlink" href="#摘要" title="Permalink to this headline"></a></h2>
<p>为了使用MindSpore Reinforcement实现强化学习算法，用户需要：</p>
<ul class="simple">
<li><p>提供算法配置，将算法的实现与其部署细节分开；</p></li>
<li><p>基于Actor-Learner-Environment抽象实现算法；</p></li>
<li><p>创建一个执行已实现的算法的会话对象。</p></li>
</ul>
<p>本教程展示了使用MindSpore Reinforcement API实现深度Q学习（DQN）算法。注：为保证清晰性和可读性，仅显示与API相关的代码，不相关的代码已省略。点击<a class="reference external" href="https://gitee.com/mindspore/reinforcement/tree/r0.5/example/dqn">此处</a>获取MindSpore Reinforcement实现完整DQN的源代码。</p>
</section>
<section id="指定dqn的actor-learner-environment抽象">
<h2>指定DQN的Actor-Learner-Environment抽象<a class="headerlink" href="#指定dqn的actor-learner-environment抽象" title="Permalink to this headline"></a></h2>
<p>DQN算法需要两个深度神经网络，一个<em>策略网络</em>用于近似动作值函数(Q函数)，另一个<em>目标网络</em>用于稳定训练。策略网络指如何对环境采取行动的策略，DQN算法的目标是训练策略网络以获得最大的奖励。此外，DQN算法使用<em>经验回放</em>技术来维护先前的观察结果，进行off-policy学习。其中Actor使用不同的行为策略来对环境采取行动。</p>
<p>MindSpore Reinforcement使用<em>算法配置</em>指定DQN算法所需的逻辑组件（Actor、Learner、Policy and Network、 Collect Environment、Eval Environment、Replayuffer）和关联的超参数。根据提供的配置，它使用不同的策略执行算法，以便用户可以专注于算法设计。</p>
<p>算法配置是一个Python字典，指定如何构造DQN算法的不同组件。每个组件的超参数在单独的Python字典中配置。DQN算法配置定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">algorithm_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;actor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Actor实例的数量</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNActor</span><span class="p">,</span>                                                   <span class="c1"># 需要创建的Actor类</span>
        <span class="s1">&#39;policies&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;init_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;collect_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluate_policy&#39;</span><span class="p">],</span>   <span class="c1"># Actor需要用到的选择动作的策略</span>
    <span class="p">},</span>
    <span class="s1">&#39;learner&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Learner实例的数量</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNLearner</span><span class="p">,</span>                                                 <span class="c1"># 需要创建的Learner类</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">learner_params</span><span class="p">,</span>                                           <span class="c1"># Learner需要用到的参数</span>
        <span class="s1">&#39;networks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;policy_network&#39;</span><span class="p">,</span> <span class="s1">&#39;target_network&#39;</span><span class="p">]</span>                    <span class="c1"># Learner中需要用到的网络</span>
    <span class="p">},</span>
    <span class="s1">&#39;policy_and_network&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">DQNPolicy</span><span class="p">,</span>                                                  <span class="c1"># 需要创建的Policy类</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">policy_params</span>                                             <span class="c1"># Policy中需要用到的参数</span>
    <span class="p">},</span>
    <span class="s1">&#39;collect_environment&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># Collect Environment实例的数量</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">GymEnvironment</span><span class="p">,</span>                                             <span class="c1"># 需要创建的Collect Environment类</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">collect_env_params</span>                                        <span class="c1"># Collect Environment中需要用到的参数</span>
    <span class="p">},</span>
    <span class="s1">&#39;eval_environment&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                                        <span class="c1"># 同Collect Environment</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">GymEnvironment</span><span class="p">,</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">eval_env_params</span>
    <span class="p">},</span>
    <span class="s1">&#39;replay_buffer&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                          <span class="c1"># ReplayBuffer实例的数量</span>
                      <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">ReplayBuffer</span><span class="p">,</span>                                 <span class="c1"># 需要创建的ReplayBuffer类</span>
                      <span class="s1">&#39;capacity&#39;</span><span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>                                   <span class="c1"># ReplayBuffer大小</span>
                      <span class="s1">&#39;data_shape&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">4</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)],</span>               <span class="c1"># ReplayBuffer中的数据Shape</span>
                      <span class="s1">&#39;data_type&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>  <span class="c1"># ReplayBuffer中的数据Type</span>
                      <span class="s1">&#39;sample_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">},</span>                                   <span class="c1"># ReplayBuffer单次采样的数据量</span>
<span class="p">}</span>
</pre></div>
</div>
<p>以上配置定义了六个顶层项，每个配置对应一个算法组件：<em>actor、learner、policy</em>、<em>replaybuffer</em>和两个<em>environment</em>。每个项对应一个类，该类必须由用户定义或者使用MIndSpore Reinforcement提供的组件，以实现DQN算法的逻辑。</p>
<p>顶层项具有描述组件的子项。<em>number</em>定义算法使用的组件的实例数。<em>type</em>表示必须定义的Python类的名称，用于实现组件。<em>params</em>为组件提供必要的超参数。<em>actor</em>中的<em>policies</em>定义组件使用的策略。<em>learner</em>中的<em>networks</em>列出了此组件使用的所有神经网络。在DQN示例中，只有Actor与环境交互。<em>replay_buffer</em>定义回放缓冲区的<em>容量、形状、样本大小和数据类型</em>。</p>
<p>对于DQN算法，我们配置了一个Actor <code class="docutils literal notranslate"><span class="pre">'number':</span> <span class="pre">1</span></code>，它的Python类<code class="docutils literal notranslate"><span class="pre">'type':</span> <span class="pre">DQNActor</span></code>，以及三个行为策略<code class="docutils literal notranslate"><span class="pre">'policies':</span> <span class="pre">['init_policy',</span> <span class="pre">'collect_policy',</span> <span class="pre">'evaluate_policy']</span></code>。</p>
<p>其他组件也以类似的方式定义。有关更多详细信息，请参阅<a class="reference external" href="https://gitee.com/mindspore/reinforcement/tree/r0.5/example/dqn">完整代码示例</a>和<a class="reference external" href="https://www.mindspore.cn/reinforcement/docs/zh-CN/r0.5/reinforcement.html">API</a>。</p>
<p>请注意，MindSpore Reinforcement使用单个<em>policy</em>类来定义算法使用的所有策略和神经网络。通过这种方式，它隐藏了策略和神经网络之间数据共享和通信的复杂性。</p>
<p>在train.py文件中，需要通过调用MindSpore Reinforcement的<em>session</em>来执行算法。<em>Session</em>在一台或多台群集计算机上分配资源并执行编译后的计算图。用户传入算法配置以实例化Session类：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_rl.core</span> <span class="kn">import</span> <span class="n">Session</span>
<span class="n">dqn_session</span> <span class="o">=</span> <span class="n">Session</span><span class="p">(</span><span class="n">dqn_algorithm_config</span><span class="p">)</span>
</pre></div>
</div>
<p>调用Session对象上的run方法，并传入对应的参数来执行DQN算法。其中<em>class_type</em>是我们定义的Trainer类在这里是DQNTrainer（后面会介绍如何实现Trainer类），episode为需要运行的循环次数，params为在config文件中定义的trainer所需要用到的参数具体可查看完整代码中<em>config.py</em>的内容，callbacks定义了需要用到的统计方法等具体请参考API中的Callback相关内容。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.dqn_trainer</span> <span class="kn">import</span> <span class="n">DQNTrainer</span>
<span class="kn">from</span> <span class="nn">mindspore_rl.utils.callback</span> <span class="kn">import</span> <span class="n">CheckpointCallback</span><span class="p">,</span> <span class="n">LossCallback</span><span class="p">,</span> <span class="n">EvaluateCallback</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossCallback</span><span class="p">()</span>
<span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">CheckpointCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">trainer_params</span><span class="p">[</span><span class="s1">&#39;ckpt_path&#39;</span><span class="p">])</span>
<span class="n">eval_cb</span> <span class="o">=</span> <span class="n">EvaluateCallback</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">cbs</span> <span class="o">=</span> <span class="p">[</span><span class="n">loss_cb</span><span class="p">,</span> <span class="n">ckpt_cb</span><span class="p">,</span> <span class="n">eval_cb</span><span class="p">]</span>
<span class="n">dqn_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">class_type</span><span class="o">=</span><span class="n">DQNTrainer</span><span class="p">,</span> <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">trainer_params</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">cbs</span><span class="p">)</span>
</pre></div>
</div>
<p>为使用MindSpore的计算图功能，将执行模式设置为<code class="docutils literal notranslate"><span class="pre">GRAPH_MODE</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code>注释的函数和方法将会编译到MindSpore计算图用于自动并行和加速。在本教程中，我们使用此功能来实现一个高效的<code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code>类。</p>
<section id="定义dqntrainer类">
<h3>定义DQNTrainer类<a class="headerlink" href="#定义dqntrainer类" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code>类表示算法的流程编排，主要流程为循环迭代地与环境交互将经验内存入<em>ReplayBuffer</em>中，然后从<em>ReplayBuffer</em>获取经验并训练目标模型。它必须继承自<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>类，该类是MindSpore Reinforcement API的一部分。</p>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code>基类包含<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>(MindSpore Reinforcement)对象，该对象允许算法实现与MindSpore Reinforcement交互，以实现训练逻辑。<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>类根据先前定义的算法配置实例化RL算法组件。它提供了函数处理程序，这些处理程序透明地绑定到用户定义的Actor、Learner或ReplayBuffer的方法。因此，<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>类让用户能够专注于算法逻辑，同时它透明地处理一个或多个worker上不同算法组件之间的对象创建、数据共享和通信。用户通过使用算法配置创建上文提到的<code class="docutils literal notranslate"><span class="pre">Session</span></code>对象来实例化<code class="docutils literal notranslate"><span class="pre">MSRL</span></code>对象。</p>
<p><code class="docutils literal notranslate"><span class="pre">DQNTrainer</span></code>必须重载<code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>用于训练，<code class="docutils literal notranslate"><span class="pre">evaluate</span></code>用于评估以及<code class="docutils literal notranslate"><span class="pre">trainable_variable</span></code>用于保存断点。在本教程中，它的定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msrl</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQNTrainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">msrl</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Trainable variables for saving.&quot;&quot;&quot;</span>
        <span class="n">trainable_variables</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;policy_net&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">policy_network</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">trainable_variables</span>

    <span class="nd">@ms_function</span>
    <span class="k">def</span> <span class="nf">init_training</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize training&quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">collect_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
        <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_value</span><span class="p">):</span>
            <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">INIT</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_insert</span><span class="p">(</span>
                <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">collect_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
                <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">done</span>

    <span class="nd">@ms_function</span>
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Policy evaluate&quot;&quot;&quot;</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
        <span class="n">eval_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">eval_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_evaluate_episode</span><span class="p">):</span>
            <span class="n">episode_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_value</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">eval_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">done</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">EVAL</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
                <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
                <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">episode_reward</span>
            <span class="n">eval_iter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">total_reward</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_evaluate_episode</span>
        <span class="k">return</span> <span class="n">avg_reward</span>
</pre></div>
</div>
<p>用户调用<code class="docutils literal notranslate"><span class="pre">train</span></code>方法会调用Trainer基类的<code class="docutils literal notranslate"><span class="pre">train</span></code>。然后，为它指定数量的episode（iteration）训练模型，每个episode调用用户定义的<code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>方法。最后，train方法通过调用<code class="docutils literal notranslate"><span class="pre">evaluate</span></code>方法来评估策略以获得奖励值。</p>
<p>在训练循环的每次迭代中，调用<code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>方法来训练一个episode：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">train_one_episode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train one episode&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inited</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_training</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inited</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">true</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">collect_environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">false</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">done</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_act</span><span class="p">(</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">COLLECT</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_insert</span><span class="p">(</span>
            <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">agent_learn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">replay_buffer_sample</span><span class="p">())</span>
        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_period</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">msrl</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">total_reward</span><span class="p">,</span> <span class="n">steps</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code>注解表示此方法将被编译为MindSpore计算图用于加速。所有标量值都必须定义为张量类型，例如<code class="docutils literal notranslate"><span class="pre">self.zero_value</span> <span class="pre">=</span> <span class="pre">Tensor(0,</span> <span class="pre">mindspore.float32)</span></code>。</p>
<p><code class="docutils literal notranslate"><span class="pre">train_one_episode</span></code>方法首先调用环境的<code class="docutils literal notranslate"><span class="pre">reset</span></code>方法，<code class="docutils literal notranslate"><span class="pre">self.msrl.collect_environment.reset()</span></code>函数来重置环境。然后，它使用<code class="docutils literal notranslate"><span class="pre">self.msrl.agent_act</span></code>函数处理程序从环境中收集经验，并通过<code class="docutils literal notranslate"><span class="pre">self.msrl.replay_buffer_insert</span></code>把经验存入到回放缓存中。在收集完经验后，使用<code class="docutils literal notranslate"><span class="pre">msrl.agent_learn</span></code>函数训练目标模型。<code class="docutils literal notranslate"><span class="pre">self.msrl.agent_learn</span></code>的输入是<code class="docutils literal notranslate"><span class="pre">self.msrl.replay_buffer_sample</span></code>返回的采样结果。</p>
<p>回放缓存<code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>由MindSpore Reinfocement提供。它定义了<code class="docutils literal notranslate"><span class="pre">insert</span></code>和<code class="docutils literal notranslate"><span class="pre">sample</span></code>方法，分别用于对经验数据进行存储和采样。详细信息，请参阅<a class="reference external" href="https://gitee.com/mindspore/reinforcement/tree/r0.5/example/dqn">完整的DQN代码示例</a>。</p>
</section>
<section id="定义dqnpolicy类">
<h3>定义DQNPolicy类<a class="headerlink" href="#定义dqnpolicy类" title="Permalink to this headline"></a></h3>
<p>定义<code class="docutils literal notranslate"><span class="pre">DQNPolicy</span></code>类，用于实现神经网络并定义策略。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;state_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;hidden_size&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;compute_type&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;state_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;hidden_size&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">],</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;compute_type&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>构造函数将先前在config.py中定义的Python字典类型的超参数<code class="docutils literal notranslate"><span class="pre">policy_params</span></code>作为输入。</p>
<p>在定义策略网络和目标网络之前，用户必须使用MindSpore算子定义神经网络的结构。例如，它们可能是<code class="docutils literal notranslate"><span class="pre">FullyConnectedNetwork</span></code>类的对象，该类定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FullyConnectedNetwork</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FullyConnectedNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">input_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;XavierUniform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">output_size</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;XavierUniform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</pre></div>
</div>
<p>DQN算法使用损失函数来优化神经网络的权重。此时，用户必须定义一个用于计算损失函数的神经网络。此网络被指定为<code class="docutils literal notranslate"><span class="pre">DQNLearner</span></code>的嵌套类。此外，还需要优化器来训练网络。优化器和损失函数定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DQN Learner&quot;&quot;&quot;</span>

    <span class="k">class</span> <span class="nc">PolicyNetWithLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;DQN policy network with loss cell&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">DQNLearner</span><span class="o">.</span><span class="n">PolicyNetWithLossCell</span><span class="p">,</span>
                  <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;constructor for Loss Cell&quot;&quot;&quot;</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQNLearner</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="n">loss_q_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PolicyNetWithLossCell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">loss_q_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>DQN算法是一种<em>off-policy</em>算法，使用epsilon-贪婪策略学习。它使用不同的行为策略来对环境采取行动和收集数据。在本示例中，我们用<code class="docutils literal notranslate"><span class="pre">RandomPolicy</span></code>初始化训练，用<code class="docutils literal notranslate"><span class="pre">EpsilonGreedyPolicy</span></code>收集训练期间的经验，用<code class="docutils literal notranslate"><span class="pre">GreedyPolicy</span></code>进行评估：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNPolicy</span><span class="p">():</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
         <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collect_policy</span> <span class="o">=</span> <span class="n">EpsilonGreedyPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;epsi_high&#39;</span><span class="p">],</span>
                                                  <span class="n">params</span><span class="p">[</span><span class="s1">&#39;epsi_low&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;action_space_dim&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_policy</span> <span class="o">=</span> <span class="n">GreedyPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span><span class="p">)</span>
</pre></div>
</div>
<p>由于上述三种行为策略在一系列RL算法中非常常见，MindSpore Reinforcement将它们作为可重用的构建块提供。用户还可以自定义特定算法的行为策略。</p>
<p>请注意，参数字典的方法名称和键必须与前面定义的算法配置一致。</p>
</section>
<section id="定义dqnactor类">
<h3>定义DQNActor类<a class="headerlink" href="#定义dqnactor类" title="Permalink to this headline"></a></h3>
<p>定义一个新的Actor组件用于实现<code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>，该组件继承了MindSpore Reinforcement提供的<code class="docutils literal notranslate"><span class="pre">Actor</span></code>类。然后，必须重载Actor中的方法：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
     <span class="o">...</span>
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Fill the replay buffer</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_policy</span><span class="p">()</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># Experience collection</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">ts0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">step_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span>

            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_policy</span><span class="p">(</span><span class="n">ts0</span><span class="p">,</span> <span class="n">step_tensor</span><span class="p">)</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">my_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">my_reward</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># Evaluate the trained policy</span>
            <span class="n">ts0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_policy</span><span class="p">(</span><span class="n">ts0</span><span class="p">)</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eval_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;Phase is incorrect&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>
</div>
<p>这三种方法使用不同的策略作用于指定的环境，这些策略将状态映射到操作。这些方法将张量类型的值作为输入，并从环境返回轨迹。</p>
<p>为了与环境交互，Actor使用<code class="docutils literal notranslate"><span class="pre">Environment</span></code>类中定义的<code class="docutils literal notranslate"><span class="pre">step(action)</span></code>方法。对于应用到指定环境的操作，此方法会做出反应并返回三元组。三元组包括应用上一个操作后的新状态、作为浮点类型获得的奖励以及用于终止episode和重置环境的布尔标志。</p>
<p>回放缓冲区类<code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>定义了一个<code class="docutils literal notranslate"><span class="pre">insert</span></code>方法，<code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>对象调用该方法将经验数据存储在回放缓冲区中。</p>
<p><code class="docutils literal notranslate"><span class="pre">Environment</span></code>类和<code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code>类由MindSpore Reinforcement API提供。</p>
<p><code class="docutils literal notranslate"><span class="pre">DQNActor</span></code>类的构造函数定义了环境、回放缓冲区、策略和网络。它将字典类型的参数作为输入，这些参数在算法配置中定义。下面，我们只展示环境的初始化，其他属性以类似的方式分配：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_environment</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;collect_environment&#39;</span><span class="p">]</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_eval_env</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;eval_environment&#39;</span><span class="p">]</span>
         <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="定义dqnlearner类">
<h3>定义DQNLearner类<a class="headerlink" href="#定义dqnlearner类" title="Permalink to this headline"></a></h3>
<p>为了实现<code class="docutils literal notranslate"><span class="pre">DQNLearner</span></code>，类必须继承MindSpore Reinforcement API中的<code class="docutils literal notranslate"><span class="pre">Learner</span></code>类，并重载<code class="docutils literal notranslate"><span class="pre">learn</span></code>方法：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
     <span class="o">...</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Model update&quot;&quot;&quot;</span>
        <span class="n">s0</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">s1</span> <span class="o">=</span> <span class="n">experience</span>
        <span class="n">next_state_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
        <span class="n">next_state_values</span> <span class="o">=</span> <span class="n">next_state_values</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">r1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>

        <span class="n">y_true</span> <span class="o">=</span> <span class="n">r1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_state_values</span>

        <span class="c1"># Modify last step reward</span>
        <span class="n">one</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">r1</span> <span class="o">==</span> <span class="o">-</span><span class="n">one</span><span class="p">,</span> <span class="n">one</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_network_train</span><span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">success</span>
</pre></div>
</div>
<p>在这里，<code class="docutils literal notranslate"><span class="pre">learn</span></code>方法将轨迹（从回放缓冲区采样）作为输入来训练策略网络。构造函数通过从算法配置接收字典类型的配置，将网络、策略和折扣率分配给DQNLearner：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQNLearner</span><span class="p">(</span><span class="n">Learner</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">DQNLearner</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_network</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;policy_network&#39;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;target_network&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="执行并查看结果">
<h2>执行并查看结果<a class="headerlink" href="#执行并查看结果" title="Permalink to this headline"></a></h2>
<p>执行脚本<code class="docutils literal notranslate"><span class="pre">train.py</span></code>以启动DQN模型训练。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">example</span><span class="o">/</span><span class="n">dqn</span><span class="o">/</span>
<span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>执行结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>-----------------------------------------
Evaluation result in episode 0 is 95.300
-----------------------------------------
Episode 0, steps: 33.0, reward: 33.000
Episode 1, steps: 45.0, reward: 12.000
Episode 2, steps: 54.0, reward: 9.000
Episode 3, steps: 64.0, reward: 10.000
Episode 4, steps: 73.0, reward: 9.000
Episode 5, steps: 82.0, reward: 9.000
Episode 6, steps: 91.0, reward: 9.000
Episode 7, steps: 100.0, reward: 9.000
Episode 8, steps: 109.0, reward: 9.000
Episode 9, steps: 118.0, reward: 9.000
...
...
Episode 200, steps: 25540.0, reward: 200.000
Episode 201, steps: 25740.0, reward: 200.000
Episode 202, steps: 25940.0, reward: 200.000
Episode 203, steps: 26140.0, reward: 200.000
Episode 204, steps: 26340.0, reward: 200.000
Episode 205, steps: 26518.0, reward: 178.000
Episode 206, steps: 26718.0, reward: 200.000
Episode 207, steps: 26890.0, reward: 172.000
Episode 208, steps: 27090.0, reward: 200.000
Episode 209, steps: 27290.0, reward: 200.000
-----------------------------------------
Evaluation result in episode 210 is 200.000
-----------------------------------------
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_config_info.html" class="btn btn-neutral float-left" title="强化学习配置说明" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="replaybuffer.html" class="btn btn-neutral float-right" title="ReplayBuffer 使用说明" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>