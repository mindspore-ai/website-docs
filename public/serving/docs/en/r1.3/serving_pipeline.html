<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Scheduling Multi-Graph Based on the Pipeline &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Servable Provided Through Model Configuration" href="serving_model.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="serving_install.html">MindSpore Serving Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="serving_example.html">MindSpore Serving-based Inference Service Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_distributed_example.html">MindSpore Serving-based Distributed Inference Service Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_grpc.html">gRPC-based MindSpore Serving Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_restful.html">RESTful-based MindSpore Serving Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_model.html">Servable Provided Through Model Configuration</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Scheduling Multi-Graph Based on the Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exporting-a-multi-graph-model">Exporting a Multi-Graph Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploying-the-distributed-inference-service">Deploying the Distributed Inference Service</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#starting-the-serving-server">Starting the Serving Server</a></li>
<li class="toctree-l4"><a class="reference internal" href="#starting-the-agent">Starting the Agent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#executing-inference">Executing Inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Scheduling Multi-Graph Based on the Pipeline</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/serving_pipeline.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="scheduling-multi-graph-based-on-the-pipeline">
<h1>Scheduling Multi-Graph Based on the Pipeline<a class="headerlink" href="#scheduling-multi-graph-based-on-the-pipeline" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Serving</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/serving/docs/source_en/serving_pipeline.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindSpore allows a model to generate multiple subgraphs and scheduling these subgraphs can improve the performance. For example, graphs corresponding to two phases are split in the GPT3 scenario. The initialization graph is at the first phase, which needs to be executed only once. The inference graph at the second phase needs to be executed multiple times based on the input sentence length N. Before the optimization, the two graphs were combined N times. Now, the performance of the inference service is improved by 5 to 6 times. MindSpore Serving provides the pineline function to schedule multiple graphs, improving the inference service performance in specific scenarios.</p>
<p>Currently, the pipeline has the following restrictions:</p>
<ul class="simple">
<li><p>Only the scenario where batchsize is set to 1 is supported.</p></li>
<li><p>If a pipeline exists, the service is presented as a pipeline. That is, the model method called by the client must be the registered pipeline method.</p></li>
</ul>
<p>The following uses a distributed scenario as an example to describe the pipeline deployment process.</p>
<section id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h3>
<p>Before running the sample network, ensure that MindSpore Serving has been properly installed and the environment variables are configured. To install and configure MindSpore Serving on your PC, go to the <a class="reference external" href="https://www.mindspore.cn/serving/docs/en/r1.3/serving_install.html">MindSpore Serving installation page</a>.</p>
</section>
<section id="exporting-a-multi-graph-model">
<h3>Exporting a Multi-Graph Model<a class="headerlink" href="#exporting-a-multi-graph-model" title="Permalink to this headline"></a></h3>
<p>For details about the files required for exporting a distributed model, see <a class="reference external" href="https://gitee.com/mindspore/serving/tree/r1.3/example/pipeline_distributed/export_model">export_model directory</a>. The following files are required:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export_model
├── distributed_inference.py
├── export_model.sh
├── net.py
└── rank_table_8pcs.json
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">net.py</span></code> is the definition of the MatMul network.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">distributed_inference.py</span></code> is used to configure distributed parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export_model.sh</span></code> creates the <code class="docutils literal notranslate"><span class="pre">device</span></code> directory on the current machine and exports the model file corresponding to each <code class="docutils literal notranslate"><span class="pre">device</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code> is the JSON file for configuring the networking information of the current multi-device environment. For details, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.3/model_zoo/utils/hccl_tools">rank_table</a>.</p></li>
</ul>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.3/example/matmul_distributed/export_model/net.py">net.py</a> to build a network that contains MatMul and Neg operators.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matmul_size</span><span class="p">,</span> <span class="n">init_val</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">matmul_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">matmul_size</span><span class="p">,</span> <span class="n">init_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">matmul_np</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">transpose_a</span><span class="o">=</span><span class="n">transpose_a</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neg</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul_weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.3/example/pipeline_distributed/export_model/distributed_inference.py">distributed_inference.py</a> to generate a multi-graph model. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_inference.html">Distributed Inference</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">net</span> <span class="kn">import</span> <span class="n">Net</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">export</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>


<span class="k">def</span> <span class="nf">test_inference</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;distributed inference after distributed training&quot;&quot;&quot;</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
    <span class="n">init</span><span class="p">(</span><span class="n">backend_name</span><span class="o">=</span><span class="s2">&quot;hccl&quot;</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;semi_auto_parallel&quot;</span><span class="p">,</span>
                                      <span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">group_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;./group_config.pb&quot;</span><span class="p">)</span>

    <span class="n">predict_data</span> <span class="o">=</span> <span class="n">create_predict_data</span><span class="p">()</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">matmul_size</span><span class="o">=</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">init_val</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">infer_predict_layout</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">predict_data</span><span class="p">))</span>
    <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_predict_network</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">predict_data</span><span class="p">),</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">&quot;matmul_0&quot;</span><span class="p">,</span> <span class="n">file_format</span><span class="o">=</span><span class="s2">&quot;MINDIR&quot;</span><span class="p">)</span>

    <span class="n">network_1</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">matmul_size</span><span class="o">=</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">init_val</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">)</span>
    <span class="n">model_1</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
    <span class="n">model_1</span><span class="o">.</span><span class="n">infer_predict_layout</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">predict_data</span><span class="p">))</span>
    <span class="n">export</span><span class="p">(</span><span class="n">model_1</span><span class="o">.</span><span class="n">_predict_network</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">predict_data</span><span class="p">),</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">&quot;matmul_1&quot;</span><span class="p">,</span> <span class="n">file_format</span><span class="o">=</span><span class="s2">&quot;MINDIR&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">create_predict_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;user-defined predict data&quot;&quot;&quot;</span>
    <span class="n">inputs_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">inputs_np</span><span class="p">)</span>
</pre></div>
</div>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.3/example/matmul_distributed/export_model/export_model.sh">export_model.sh</a> to export a multi-graph model. After the script is executed successfully, the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory is created in the upper-level directory. The structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>model
├── device0
│   ├── group_config.pb
│   └── matmul.mindir
├── device1
├── device2
├── device3
├── device4
├── device5
├── device6
└── device7
</pre></div>
</div>
<p>Each <code class="docutils literal notranslate"><span class="pre">device</span></code> directory contains two files <code class="docutils literal notranslate"><span class="pre">group_config.pb</span></code> (the model group configuration file) and <code class="docutils literal notranslate"><span class="pre">matmul_0.mindir</span></code> or <code class="docutils literal notranslate"><span class="pre">matmul_1.mindir</span></code> (the model files corresponding to the two graphs, respectively).</p>
</section>
<section id="deploying-the-distributed-inference-service">
<h3>Deploying the Distributed Inference Service<a class="headerlink" href="#deploying-the-distributed-inference-service" title="Permalink to this headline"></a></h3>
<p>Start the distributed inference service. For details, see <a class="reference external" href="https://gitee.com/mindspore/serving/tree/r1.3/example/pipeline_distributed">pipeline_distributed</a>. The following files are required:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>matmul_distributed
├── serving_agent.py
├── serving_server.py
├── matmul
│   └── servable_config.py
├── model
└── rank_table_8pcs.json
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> is the directory for storing model files.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">serving_server.py</span></code> is used to start service processes, including the <code class="docutils literal notranslate"><span class="pre">Main</span></code> and <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Worker</span></code> processes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">serving_agent.py</span></code> is used to start the <code class="docutils literal notranslate"><span class="pre">Agent</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">servable_config.py</span></code> is the <a class="reference external" href="https://www.mindspore.cn/serving/docs/en/r1.3/serving_model.html">model configuration file</a>. It uses <code class="docutils literal notranslate"><span class="pre">distributed.declare_servable</span></code> to declare a distributed model whose rank_size is 8 and stage_size is 1, and defines a pipeline method <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p></li>
</ul>
<p>Content of the configuration file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.server</span> <span class="kn">import</span> <span class="n">distributed</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.server</span> <span class="kn">import</span> <span class="n">register</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.server.register</span> <span class="kn">import</span> <span class="n">PipelineServable</span>

<span class="n">distributed</span><span class="o">.</span><span class="n">declare_servable</span><span class="p">(</span><span class="n">rank_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stage_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">with_batch_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;define preprocess, this example has one input and one output&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="nd">@register</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">fun1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">register</span><span class="o">.</span><span class="n">call_preprocess</span><span class="p">(</span><span class="n">add_preprocess</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">register</span><span class="o">.</span><span class="n">call_servable</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">subgraph</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="nd">@register</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">fun2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">register</span><span class="o">.</span><span class="n">call_servable</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">subgraph</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">servable1</span> <span class="o">=</span> <span class="n">PipelineServable</span><span class="p">(</span><span class="n">servable_name</span><span class="o">=</span><span class="s2">&quot;matmul&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;fun1&quot;</span><span class="p">,</span> <span class="n">version_number</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">servable2</span> <span class="o">=</span> <span class="n">PipelineServable</span><span class="p">(</span><span class="n">servable_name</span><span class="o">=</span><span class="s2">&quot;matmul&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;fun2&quot;</span><span class="p">,</span> <span class="n">version_number</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nd">@register</span><span class="o">.</span><span class="n">register_pipeline</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;z&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">servable1</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">servable2</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span>

</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">subgraph</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">call_servable</span></code> method specifies the graph number, which starts from 0. The number is the sequence number for loading graphs. In a standalone system, the number corresponds to the sequence number in the <code class="docutils literal notranslate"><span class="pre">servable_file</span></code> parameter list of the <code class="docutils literal notranslate"><span class="pre">declare_servable</span></code> interface. In a distributed system, this number corresponds to the sequence number in the <code class="docutils literal notranslate"><span class="pre">model_files</span></code> parameter list of the <code class="docutils literal notranslate"><span class="pre">startup_agents</span></code> interface.
The <code class="docutils literal notranslate"><span class="pre">PipelineServable</span></code> class declares the service function of the model, <code class="docutils literal notranslate"><span class="pre">servable_name</span></code> specifies the model name, <code class="docutils literal notranslate"><span class="pre">method</span></code> specifies the function method, <code class="docutils literal notranslate"><span class="pre">version_number</span></code> specifies the version number, <code class="docutils literal notranslate"><span class="pre">register_pipeline</span></code> registers the pipeline function, and the input parameter <code class="docutils literal notranslate"><span class="pre">output_names</span></code> specifies the output list.</p>
<section id="starting-the-serving-server">
<h4>Starting the Serving Server<a class="headerlink" href="#starting-the-serving-server" title="Permalink to this headline"></a></h4>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.3/example/pipeline_distributed/serving_server.py">serving_server.py</a> to call the <code class="docutils literal notranslate"><span class="pre">distributed.start_servable</span></code> method to deploy the distributed Serving server.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">mindspore_serving</span> <span class="kn">import</span> <span class="n">server</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.server</span> <span class="kn">import</span> <span class="n">distributed</span>


<span class="k">def</span> <span class="nf">start</span><span class="p">():</span>
    <span class="n">servable_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">distributed</span><span class="o">.</span><span class="n">start_servable</span><span class="p">(</span><span class="n">servable_dir</span><span class="p">,</span> <span class="s2">&quot;matmul&quot;</span><span class="p">,</span>
                               <span class="n">rank_table_json_file</span><span class="o">=</span><span class="s2">&quot;rank_table_8pcs.json&quot;</span><span class="p">,</span>
                               <span class="n">version_number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">distributed_address</span><span class="o">=</span><span class="s2">&quot;127.0.0.1:6200&quot;</span><span class="p">)</span>

    <span class="n">server</span><span class="o">.</span><span class="n">start_grpc_server</span><span class="p">(</span><span class="s2">&quot;127.0.0.1:5500&quot;</span><span class="p">)</span>
    <span class="n">server</span><span class="o">.</span><span class="n">start_restful_server</span><span class="p">(</span><span class="s2">&quot;127.0.0.1:1500&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">servable_dir</span></code> is the servable directory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">servable_name</span></code> indicates the servable name, which corresponds to a directory for storing the model configuration file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_table_json_file</span></code> is the JSON file for configuring the network information in the multi-device environment.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">distributed_address</span></code> is the <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Worker</span></code> address.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wait_agents_time_in_seconds</span></code> specifies the time limit for waiting for the completion of all <code class="docutils literal notranslate"><span class="pre">Agent</span></code> registrations. The default value is 0, indicating that the system keeps waiting for the completion of all <code class="docutils literal notranslate"><span class="pre">Agent</span></code> registrations.</p></li>
</ul>
</section>
<section id="starting-the-agent">
<h4>Starting the Agent<a class="headerlink" href="#starting-the-agent" title="Permalink to this headline"></a></h4>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.3/example/pipeline_distributed/serving_agent.py">serving_agent.py</a> to call the <code class="docutils literal notranslate"><span class="pre">startup_agents</span></code> method to start the eight <code class="docutils literal notranslate"><span class="pre">Agent</span></code> processes on the current machine. <code class="docutils literal notranslate"><span class="pre">Agent</span></code> obtains rank_table from <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Worker</span></code> so that <code class="docutils literal notranslate"><span class="pre">Agents</span></code> can communicate with each other using HCCL.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_serving.server</span> <span class="kn">import</span> <span class="n">distributed</span>


<span class="k">def</span> <span class="nf">start_agents</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Start all the agents in current machine&quot;&quot;&quot;</span>
    <span class="n">model_files</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">group_configs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">model_files</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;model/device</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">/matmul_0.mindir&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;model/device</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">/matmul_1.mindir&quot;</span><span class="p">])</span>
        <span class="n">group_configs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;model/device</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">/group_config.pb&quot;</span><span class="p">])</span>

    <span class="n">distributed</span><span class="o">.</span><span class="n">startup_agents</span><span class="p">(</span><span class="n">distributed_address</span><span class="o">=</span><span class="s2">&quot;127.0.0.1:6200&quot;</span><span class="p">,</span> <span class="n">model_files</span><span class="o">=</span><span class="n">model_files</span><span class="p">,</span>
                               <span class="n">group_config_files</span><span class="o">=</span><span class="n">group_configs</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">start_agents</span><span class="p">()</span>

</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">distributed_address</span></code> is the <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Worker</span></code> address.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_files</span></code> is a list of model file paths. Inputting multiple model files indicates that multiple graphs are supported. The file transfer sequence number determines the graph number corresponding to the <code class="docutils literal notranslate"><span class="pre">subgraph</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">call_servable</span></code> method.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">group_config_files</span></code> is the list of model group configuration file paths.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">agent_start_port</span></code> is the start port occupied by the <code class="docutils literal notranslate"><span class="pre">Agent</span></code>. The default value is 7000.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">agent_ip</span></code> is the IP address of the <code class="docutils literal notranslate"><span class="pre">Agent</span></code>. The default value is None. By default, the IP address used for the communication between the <code class="docutils literal notranslate"><span class="pre">Agent</span></code> and <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Worker</span></code> is obtained from the rank_table. If the IP address is unavailable, you need to set both <code class="docutils literal notranslate"><span class="pre">agent_ip</span></code> and <code class="docutils literal notranslate"><span class="pre">rank_start</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_start</span></code> is the start rank_id of the current machine. The default value is None.</p></li>
</ul>
</section>
</section>
<section id="executing-inference">
<h3>Executing Inference<a class="headerlink" href="#executing-inference" title="Permalink to this headline"></a></h3>
<p>To access the inference service through gRPC, you need to specify the IP address and port number of the gRPC server on the client. Execute <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.3/example/pipeline_distributed/serving_client.py">serving_client.py</a> to call the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method of the MatMul distributed model. This method corresponds to the registered pipeline method and is used to perform inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.client</span> <span class="kn">import</span> <span class="n">Client</span>


<span class="k">def</span> <span class="nf">run_matmul</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run client of distributed matmul&quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;localhost:5500&quot;</span><span class="p">,</span> <span class="s2">&quot;matmul&quot;</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span><span class="p">)</span>
    <span class="n">instance</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)}</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;result:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">run_matmul</span><span class="p">()</span>
</pre></div>
</div>
<p>If the following information is displayed, the Serving distributed inference service has correctly executed the multi-graph inference of the pipeline:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result:
[{&#39;x&#39;: array([[-96., -96., -96., ..., -96., -96., -96.],
      [-96., -96., -96., ..., -96., -96., -96.],
      [-96., -96., -96., ..., -96., -96., -96.],
      ...,
      [-96., -96., -96., ..., -96., -96., -96.],
      [-96., -96., -96., ..., -96., -96., -96.],
      [-96., -96., -96., ..., -96., -96., -96.]], dtype=float32), &#39;z&#39;: array([[-48., -48., -48., ..., -48., -48., -48.],
      [-48., -48., -48., ..., -48., -48., -48.],
      [-48., -48., -48., ..., -48., -48., -48.],
      ...,
      [-48., -48., -48., ..., -48., -48., -48.],
      [-48., -48., -48., ..., -48., -48., -48.],
      [-48., -48., -48., ..., -48., -48., -48.]], }]
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="serving_model.html" class="btn btn-neutral float-left" title="Servable Provided Through Model Configuration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>