

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Debugging in PyNative Mode &mdash; MindSpore r0.5 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Customized Debugging Information" href="customized_debugging_information.html" />
    <link rel="prev" title="Natural Language Processing (NLP) Application" href="nlp_application.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
</ul>
<p class="caption"><span class="caption-text">Use</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation/data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/saving_and_loading_model_parameters.html">Saving and Loading Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/multi_platform_inference.html">Multi-platform Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="computer_vision_application.html">Computer Vision Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_application.html">Natural Language Processing (NLP) Application</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Optimization</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Debugging in PyNative Mode</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#executing-a-single-operator">Executing a Single Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#executing-a-common-function">Executing a Common Function</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#improving-pynative-performance">Improving PyNative Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#debugging-network-train-model">Debugging Network Train Model</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="customized_debugging_information.html">Customized Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tutorials.html">Training Process Visualization</a></li>
</ul>
<p class="caption"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_precision.html">Mixed Precision</a></li>
</ul>
<p class="caption"><span class="caption-text">Usage on Device</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="on_device_inference.html">On-Device Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Network Migration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="network_migration.html">Network Migration</a></li>
</ul>
<p class="caption"><span class="caption-text">AI Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_security.html">Model Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="differential_privacy.html">Differential Privacy in Machine Learning</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Debugging in PyNative Mode</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced_use/debugging_in_pynative_mode.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="debugging-in-pynative-mode">
<h1>Debugging in PyNative Mode<a class="headerlink" href="#debugging-in-pynative-mode" title="Permalink to this headline">¶</a></h1>
<!-- TOC --><ul class="simple">
<li><p><a class="reference external" href="#debugging-in-pynative-mode">Debugging in PyNative Mode</a></p>
<ul>
<li><p><a class="reference external" href="#overview">Overview</a></p></li>
<li><p><a class="reference external" href="#executing-a-single-operator">Executing a Single Operator</a></p></li>
<li><p><a class="reference external" href="#executing-a-common-function">Executing a Common Function</a></p>
<ul>
<li><p><a class="reference external" href="#improving-pynative-performance">Improving PyNative Performance</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#debugging-network-train-model">Debugging Network Train Model</a></p></li>
</ul>
</li>
</ul>
<!-- /TOC --><p><a href="https://gitee.com/mindspore/docs/blob/r0.5/tutorials/source_en/advanced_use/debugging_in_pynative_mode.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MindSpore supports the following running modes which are optimized in terms of debugging or running:</p>
<ul class="simple">
<li><p>PyNative mode: dynamic graph mode. In this mode, operators in the neural network are delivered and executed one by one, facilitating the compilation and debugging of the neural network model.</p></li>
<li><p>Graph mode: static graph mode. In this mode, the neural network model is compiled into an entire graph and then delivered for execution. This mode uses technologies such as graph optimization to improve the running performance and facilitates large-scale deployment and cross-platform running.</p></li>
</ul>
<p>By default, MindSpore is in PyNative mode. You can switch it to the graph mode by calling <code class="docutils literal notranslate"><span class="pre">context.set_context(mode=context.GRAPH_MODE)</span></code>. Similarly, MindSpore in graph mode can be switched to the PyNative mode through <code class="docutils literal notranslate"><span class="pre">context.set_context(mode=context.PYNATIVE_MODE)</span></code>.</p>
<p>In PyNative mode, single operators, common functions, network inference, and separated gradient calculation can be executed. The following describes the usage and precautions.</p>
</div>
<div class="section" id="executing-a-single-operator">
<h2>Executing a Single Operator<a class="headerlink" href="#executing-a-single-operator" title="Permalink to this headline">¶</a></h2>
<p>Execute a single operator and output the result, as shown in the following example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[[[</span><span class="o">-</span><span class="mf">0.02190447</span> <span class="o">-</span><span class="mf">0.05208071</span> <span class="o">-</span><span class="mf">0.05208071</span> <span class="o">-</span><span class="mf">0.05208071</span> <span class="o">-</span><span class="mf">0.06265172</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.01529094</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.04228776</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.01529094</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.04228776</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.01529094</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.05286242</span> <span class="o">-</span><span class="mf">0.04228776</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.01430791</span> <span class="o">-</span><span class="mf">0.04892948</span> <span class="o">-</span><span class="mf">0.04892948</span> <span class="o">-</span><span class="mf">0.04892948</span> <span class="o">-</span><span class="mf">0.01096004</span><span class="p">]]</span>

<span class="p">[[</span> <span class="mf">0.00802889</span> <span class="o">-</span><span class="mf">0.00229866</span> <span class="o">-</span><span class="mf">0.00229866</span> <span class="o">-</span><span class="mf">0.00229866</span> <span class="o">-</span><span class="mf">0.00471579</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01172971</span> <span class="mf">0.02172665</span> <span class="mf">0.02172665</span> <span class="mf">0.02172665</span> <span class="mf">0.03261888</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01172971</span> <span class="mf">0.02172665</span> <span class="mf">0.02172665</span> <span class="mf">0.02172665</span> <span class="mf">0.03261888</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01172971</span> <span class="mf">0.02172665</span> <span class="mf">0.02172665</span> <span class="mf">0.02172665</span> <span class="mf">0.03261888</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01784375</span> <span class="mf">0.01185635</span> <span class="mf">0.01185635</span> <span class="mf">0.01185635</span> <span class="mf">0.01839031</span><span class="p">]]</span>

<span class="p">[[</span> <span class="mf">0.04841832</span> <span class="mf">0.03321705</span> <span class="mf">0.03321705</span> <span class="mf">0.03321705</span> <span class="mf">0.0342317</span> <span class="p">]</span>
<span class="p">[</span> <span class="mf">0.0651359</span> <span class="mf">0.04310361</span> <span class="mf">0.04310361</span> <span class="mf">0.04310361</span> <span class="mf">0.03355784</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.0651359</span> <span class="mf">0.04310361</span> <span class="mf">0.04310361</span> <span class="mf">0.04310361</span> <span class="mf">0.03355784</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.0651359</span> <span class="mf">0.04310361</span> <span class="mf">0.04310361</span> <span class="mf">0.04310361</span> <span class="mf">0.03355784</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.04680437</span> <span class="mf">0.03465693</span> <span class="mf">0.03465693</span> <span class="mf">0.03465693</span> <span class="mf">0.00171057</span><span class="p">]]</span>

<span class="p">[[</span><span class="o">-</span><span class="mf">0.01783456</span> <span class="o">-</span><span class="mf">0.00459451</span> <span class="o">-</span><span class="mf">0.00459451</span> <span class="o">-</span><span class="mf">0.00459451</span> <span class="mf">0.02316688</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01295831</span> <span class="mf">0.00879035</span> <span class="mf">0.00879035</span> <span class="mf">0.00879035</span> <span class="mf">0.01178642</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01295831</span> <span class="mf">0.00879035</span> <span class="mf">0.00879035</span> <span class="mf">0.00879035</span> <span class="mf">0.01178642</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.01295831</span> <span class="mf">0.00879035</span> <span class="mf">0.00879035</span> <span class="mf">0.00879035</span> <span class="mf">0.01178642</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.05016355</span> <span class="mf">0.03958241</span> <span class="mf">0.03958241</span> <span class="mf">0.03958241</span> <span class="mf">0.03443141</span><span class="p">]]]]</span>
</pre></div>
</div>
</div>
<div class="section" id="executing-a-common-function">
<h2>Executing a Common Function<a class="headerlink" href="#executing-a-common-function" title="Permalink to this headline">¶</a></h2>
<p>Combine multiple operators into a function, call the function to execute the operators, and output the result, as shown in the following example:</p>
<p><strong>Example Code</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tensor_add_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tensor_add_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]]</span>
</pre></div>
</div>
<blockquote>
<div><p>Parallel execution and summary is not supported in PyNative mode, so parallel and summary related operators can not be used.</p>
</div></blockquote>
<div class="section" id="improving-pynative-performance">
<h3>Improving PyNative Performance<a class="headerlink" href="#improving-pynative-performance" title="Permalink to this headline">¶</a></h3>
<p>MindSpore provides the staging function to improve the execution speed of inference tasks in PyNative mode. This function compiles Python functions or Python class methods into computational graphs in PyNative mode and improves the execution speed by using graph optimization technologies, as shown in the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="kn">from</span> <span class="nn">mindspore.common.api</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">TensorAddNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TensorAddNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>

    <span class="nd">@ms_function</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">TensorAddNet</span><span class="p">()</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># Staging mode</span>
<span class="n">tensor_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">tensor_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span> <span class="c1"># PyNative mode</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span> <span class="mf">3.</span><span class="p">]]</span>
</pre></div>
</div>
<p>In the preceding code, the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> decorator is added before <code class="docutils literal notranslate"><span class="pre">construct</span></code> of the <code class="docutils literal notranslate"><span class="pre">TensorAddNet</span></code> class. The decorator compiles the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method into a computational graph. After the input is given, the graph is delivered and executed, <code class="docutils literal notranslate"><span class="pre">tensor_add</span></code> in the preceding code is executed in the common PyNative mode.</p>
<p>It should be noted that, in a function to which the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> decorator is added, if an operator (such as <code class="docutils literal notranslate"><span class="pre">pooling</span></code> or <code class="docutils literal notranslate"><span class="pre">tensor_add</span></code>) that does not need parameter training is included, the operator can be directly called in the decorated function, as shown in the following example:</p>
<p><strong>Example Code</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="kn">from</span> <span class="nn">mindspore.common.api</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="n">tensor_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>

<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">tensor_add_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tensor_add_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">[[</span><span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>.<span class="o">]</span>
 <span class="o">[</span><span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>.<span class="o">]</span>
 <span class="o">[</span><span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>.<span class="o">]</span>
 <span class="o">[</span><span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>. <span class="m">2</span>.<span class="o">]]</span>
</pre></div>
</div>
<p>If the decorated function contains operators (such as <code class="docutils literal notranslate"><span class="pre">Convolution</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>) that require parameter training, these operators must be instantiated before the decorated function is called, as shown in the following example:</p>
<p><strong>Example Code</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.api</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="n">conv_obj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">conv_obj</span><span class="o">.</span><span class="n">init_parameters_data</span><span class="p">()</span>
<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">conv_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">conv_obj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">conv_fn</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">[[[[</span> <span class="m">0</span>.10377571 -0.0182163 -0.05221086<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.1428334 -0.01216263 <span class="m">0</span>.03171652<span class="o">]</span>
<span class="o">[</span>-0.00673915 -0.01216291 <span class="m">0</span>.02872104<span class="o">]]</span>

<span class="o">[[</span> <span class="m">0</span>.02906547 -0.02333629 -0.0358406 <span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.03805163 -0.00589525 <span class="m">0</span>.04790922<span class="o">]</span>
<span class="o">[</span>-0.01307234 -0.00916951 <span class="m">0</span>.02396654<span class="o">]]</span>

<span class="o">[[</span> <span class="m">0</span>.01477884 -0.06549098 -0.01571796<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.00526886 -0.09617482 <span class="m">0</span>.04676902<span class="o">]</span>
<span class="o">[</span>-0.02132788 -0.04203424 <span class="m">0</span>.04523344<span class="o">]]</span>

<span class="o">[[</span> <span class="m">0</span>.04590619 -0.00251453 -0.00782715<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.06099087 -0.03445276 <span class="m">0</span>.00022781<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.0563223 -0.04832596 -0.00948266<span class="o">]]]</span>

<span class="o">[[[</span> <span class="m">0</span>.08444098 -0.05898955 -0.039262 <span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.08322686 -0.0074796 <span class="m">0</span>.0411371 <span class="o">]</span>
<span class="o">[</span>-0.02319113 <span class="m">0</span>.02128408 -0.01493311<span class="o">]]</span>

<span class="o">[[</span> <span class="m">0</span>.02473745 -0.02558945 -0.0337843 <span class="o">]</span>
<span class="o">[</span>-0.03617039 -0.05027632 -0.04603915<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.03672804 <span class="m">0</span>.00507637 -0.08433761<span class="o">]]</span>

<span class="o">[[</span> <span class="m">0</span>.09628943 <span class="m">0</span>.01895323 -0.02196114<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.04779419 -0.0871575 <span class="m">0</span>.0055248 <span class="o">]</span>
<span class="o">[</span>-0.04382382 -0.00511185 -0.01168541<span class="o">]]</span>

<span class="o">[[</span> <span class="m">0</span>.0534859 <span class="m">0</span>.02526264 <span class="m">0</span>.04755395<span class="o">]</span>
<span class="o">[</span>-0.03438103 -0.05877855 <span class="m">0</span>.06530266<span class="o">]</span>
<span class="o">[</span> <span class="m">0</span>.0377498 -0.06117418 <span class="m">0</span>.00546303<span class="o">]]]]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="debugging-network-train-model">
<h2>Debugging Network Train Model<a class="headerlink" href="#debugging-network-train-model" title="Permalink to this headline">¶</a></h2>
<p>In PyNative mode, the gradient can be calculated separately. As shown in the following example, <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> is used to calculate all input gradients of the function or the network.</p>
<p><strong>Example Code</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">mainf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">C</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="s1">&#39;get_all&#39;</span><span class="p">,</span> <span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">mul</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mainf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>During network training, obtain the gradient, call the optimizer to optimize parameters (the breakpoint cannot be set during the reverse gradient calculation), and calculate the loss values. Then, network training is implemented in PyNative mode.</p>
<p><strong>Complete LeNet Sample Code</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ParameterTuple</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">TruncatedNormal</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">WithLossCell</span><span class="p">,</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">,</span> <span class="n">Momentum</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;weight initial for conv layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                     <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                     <span class="n">weight_init</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fc_with_initialize</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;weight initial for fc layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">weight_variable</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;weight initial&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LeNet5</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Lenet network</span>
<span class="sd">    Args:</span>
<span class="sd">        num_class (int): Num classes. Default: 10.</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output tensor</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; LeNet(num_class=10)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span> <span class="o">=</span> <span class="n">num_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
 
    
<span class="k">class</span> <span class="nc">GradWrap</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; GradWrap definition &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradWrap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">network</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="k">return</span> <span class="n">C</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="s1">&#39;get_by_list&#39;</span><span class="p">,</span> <span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">net_with_criterion</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="n">train_network</span> <span class="o">=</span> <span class="n">GradWrap</span><span class="p">(</span><span class="n">net_with_criterion</span><span class="p">)</span>
<span class="n">train_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">net</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">net</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
<span class="n">loss_output</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mf">2.3050091</span>
</pre></div>
</div>
<p>In the preceding execution, an intermediate result of network execution can be obtained at any required place in <code class="docutils literal notranslate"><span class="pre">construt</span></code> function, and the network can be debugged by using the Python Debugger (pdb).</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="customized_debugging_information.html" class="btn btn-neutral float-right" title="Customized Debugging Information" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="nlp_application.html" class="btn btn-neutral float-left" title="Natural Language Processing (NLP) Application" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, MindSpore

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>