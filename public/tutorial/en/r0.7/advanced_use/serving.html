<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MindSpore-based Inference Service Deployment &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Use</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation/data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/saving_and_loading_model_parameters.html">Saving and Loading Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/multi_platform_inference.html">Multi-Platform Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="computer_vision_application.html">Computer Vision Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_application.html">Natural Language Processing (NLP) Application</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debugging_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="customized_debugging_information.html">Customized Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tutorials.html">Training Process Visualization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization_aware.html">Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage on Device</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="on_device_inference.html">On-Device Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Network Migration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="network_migration.html">Network Migration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_security.html">Model Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="differential_privacy.html">Differential Privacy in Machine Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>MindSpore-based Inference Service Deployment</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_use/serving.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-based-inference-service-deployment">
<h1>MindSpore-based Inference Service Deployment<a class="headerlink" href="#mindspore-based-inference-service-deployment" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Environmental</span> <span class="pre">Setup</span></code> <code class="docutils literal notranslate"><span class="pre">Enterprise</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code>
<a href="https://gitee.com/mindspore/docs/blob/r0.7/tutorials/source_en/advanced_use/serving.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindSpore Serving is a lightweight and high-performance service module that helps MindSpore developers efficiently deploy online inference services in the production environment. After completing model training using MindSpore, you can export the MindSpore model and use MindSpore Serving to create an inference service for the model. Currently, only Ascend 910 is supported.</p>
</section>
<section id="starting-serving">
<h2>Starting Serving<a class="headerlink" href="#starting-serving" title="Permalink to this headline"></a></h2>
<p>After MindSpore is installed using <code class="docutils literal notranslate"><span class="pre">pip</span></code>, the Serving executable program is stored in <code class="docutils literal notranslate"><span class="pre">/{your</span> <span class="pre">python</span> <span class="pre">path}/lib/python3.7/site-packages/mindspore/ms_serving</span></code>.
Run the following command to start Serving:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ms_serving<span class="w"> </span><span class="o">[</span>--help<span class="o">]</span><span class="w"> </span><span class="o">[</span>--model_path<span class="o">=</span>&lt;MODEL_PATH&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>--model_name<span class="o">=</span>&lt;MODEL_NAME&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>--port<span class="o">=</span>&lt;PORT1&gt;<span class="o">]</span><span class="w"> </span>
<span class="w">                    </span><span class="o">[</span>--rest_api_port<span class="o">=</span>&lt;PORT2&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>--device_id<span class="o">=</span>&lt;DEVICE_ID&gt;<span class="o">]</span>
</pre></div>
</div>
<p>Parameters are described as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--help</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Displays the help information about the startup command.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--model_path=&lt;MODEL_PATH&gt;</span></code></p></td>
<td><p>Mandatory</p></td>
<td><p>Path for storing the model to be loaded.</p></td>
<td><p>String</p></td>
<td><p>Null</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--model_name=&lt;MODEL_NAME&gt;</span></code></p></td>
<td><p>Mandatory</p></td>
<td><p>Name of the model file to be loaded.</p></td>
<td><p>String</p></td>
<td><p>Null</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--port=&lt;PORT&gt;</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specifies the external Serving gRPC port number.</p></td>
<td><p>Integer</p></td>
<td><p>5500</p></td>
<td><p>1–65535</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--rest_api_port=&lt;PORT2&gt;</span></code></p></td>
<td><p>Specifies the external Serving REST API port number.</p></td>
<td><p>Integer</p></td>
<td><p>5500</p></td>
<td><p>1–65535</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--device_id=&lt;DEVICE_ID&gt;</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specifies device ID to be used.</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
<td><p>0 to 7</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>Before running the startup command, add the path <code class="docutils literal notranslate"><span class="pre">/{your</span> <span class="pre">python</span> <span class="pre">path}/lib:/{your</span> <span class="pre">python</span> <span class="pre">path}/lib/python3.7/site-packages/mindspore/lib</span></code> to the environment variable <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>.
port and rest_ api_port cannot be the same.</p>
</div></blockquote>
</section>
<section id="application-example">
<h2>Application Example<a class="headerlink" href="#application-example" title="Permalink to this headline"></a></h2>
<p>The following uses a simple network as an example to describe how to use MindSpore Serving.</p>
<section id="exporting-model">
<h3>Exporting Model<a class="headerlink" href="#exporting-model" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Before exporting the model, you need to configure MindSpore <a class="reference external" href="https://www.mindspore.cn/install/en">base environment</a>.</p>
</div></blockquote>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r0.7/serving/example/export_model/add_model.py">add_model.py</a> to build a network with only the Add operator and export the MindSpore inference deployment model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">add_model</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Execute the script to generate the <code class="docutils literal notranslate"><span class="pre">tensor_add.mindir</span></code> file. The input of the model is two one-dimensional tensors with shape [2,2], and the output is the sum of the two input tensors.</p>
</section>
<section id="starting-serving-inference">
<h3>Starting Serving Inference<a class="headerlink" href="#starting-serving-inference" title="Permalink to this headline"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ms_serving<span class="w"> </span>--model_path<span class="o">={</span>model<span class="w"> </span>directory<span class="o">}</span><span class="w"> </span>--model_name<span class="o">=</span>tensor_add.mindir
</pre></div>
</div>
<p>If the server prints the <code class="docutils literal notranslate"><span class="pre">MS</span> <span class="pre">Serving</span> <span class="pre">Listening</span> <span class="pre">on</span> <span class="pre">0.0.0.0:5500</span></code> log, the Serving has loaded the inference model.</p>
</section>
<section id="client-samples">
<h3>Client Samples<a class="headerlink" href="#client-samples" title="Permalink to this headline"></a></h3>
<section id="span-name-python-client-sample-python-client-sample-span">
<h4><span name="python-client-sample">Python Client Sample</span><a class="headerlink" href="#span-name-python-client-sample-python-client-sample-span" title="Permalink to this headline"></a></h4>
<blockquote>
<div><p>Before running the client sample, add the path <code class="docutils literal notranslate"><span class="pre">/{your</span> <span class="pre">python</span> <span class="pre">path}/lib/python3.7/site-packages/mindspore/</span></code> to the environment variable <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code>.</p>
</div></blockquote>
<p>Obtain <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r0.7/serving/example/python_client/ms_client.py">ms_client.py</a> and start the Python client.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>ms_client.py
</pre></div>
</div>
<p>If the following information is displayed, the Serving has correctly executed the inference of the Add network.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span> <span class="n">client</span> <span class="n">received</span><span class="p">:</span>
<span class="p">[[</span><span class="mf">2.</span> <span class="mf">2.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">2.</span> <span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="span-name-cpp-client-sample-c-client-sample-span">
<h4><span name="cpp-client-sample">C++ Client Sample</span><a class="headerlink" href="#span-name-cpp-client-sample-c-client-sample-span" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Obtain an executable client sample program.</p>
<p>Download the <a class="reference external" href="https://gitee.com/mindspore/mindspore">MindSpore source code</a>. You can use either of the following methods to compile and obtain the client sample program:</p>
<ul>
<li><p>When MindSpore is compiled using the source code, the Serving C++ client sample program is generated. You can find the <code class="docutils literal notranslate"><span class="pre">ms_client</span></code> executable program in the <code class="docutils literal notranslate"><span class="pre">build/mindspore/serving/example/cpp_client</span></code> directory.</p></li>
<li><p>Independent compilation</p>
<p>Preinstall <a class="reference external" href="https://gRPC.io">gRPC</a>.</p>
<p>Run the following command in the MindSpore source code path to compile a client sample program:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>mindspore/serving/example/cpp_client
mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>-D<span class="w"> </span><span class="nv">GRPC_PATH</span><span class="o">={</span>grpc_install_dir<span class="o">}</span><span class="w"> </span>..
make
</pre></div>
</div>
<p>In the preceding command, <code class="docutils literal notranslate"><span class="pre">{grpc_install_dir}</span></code> indicates the gRPC installation path. Replace it with the actual gRPC installation path.</p>
</li>
</ul>
</li>
<li><p>Start the client.</p>
<p>Execute <code class="docutils literal notranslate"><span class="pre">ms_client</span></code> to send an inference request to the Serving.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./ms_client<span class="w"> </span>--target<span class="o">=</span>localhost:5500
</pre></div>
</div>
<p>If the following information is displayed, the Serving has correctly executed the inference of the Add network.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Compute</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">Add</span> <span class="n">result</span> <span class="ow">is</span> <span class="mi">2</span> <span class="mi">4</span> <span class="mi">6</span> <span class="mi">8</span>
<span class="n">client</span> <span class="n">received</span><span class="p">:</span> <span class="n">RPC</span> <span class="n">OK</span>
</pre></div>
</div>
</li>
</ol>
<p>The client code consists of the following parts:</p>
<ol class="arabic">
<li><p>Implement the client based on MSService::Stub and create a client instance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MSClient</span> <span class="p">{</span>
 <span class="n">public</span><span class="p">:</span>
  <span class="n">explicit</span> <span class="n">MSClient</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Channel</span><span class="o">&gt;</span> <span class="n">channel</span><span class="p">)</span> <span class="p">:</span>  <span class="n">stub_</span><span class="p">(</span><span class="n">MSService</span><span class="p">::</span><span class="n">NewStub</span><span class="p">(</span><span class="n">channel</span><span class="p">))</span> <span class="p">{}</span>
 <span class="n">private</span><span class="p">:</span>
  <span class="n">std</span><span class="p">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">MSService</span><span class="p">::</span><span class="n">Stub</span><span class="o">&gt;</span> <span class="n">stub_</span><span class="p">;</span>
<span class="p">};</span>

<span class="n">MSClient</span> <span class="n">client</span><span class="p">(</span><span class="n">grpc</span><span class="p">::</span><span class="n">CreateChannel</span><span class="p">(</span><span class="n">target_str</span><span class="p">,</span> <span class="n">grpc</span><span class="p">::</span><span class="n">InsecureChannelCredentials</span><span class="p">()));</span>

</pre></div>
</div>
</li>
<li><p>Build the request input parameter <code class="docutils literal notranslate"><span class="pre">Request</span></code>, output parameter <code class="docutils literal notranslate"><span class="pre">Reply</span></code>, and gRPC client <code class="docutils literal notranslate"><span class="pre">Context</span></code> based on the actual network input.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PredictRequest</span> <span class="n">request</span><span class="p">;</span>
<span class="n">PredictReply</span> <span class="n">reply</span><span class="p">;</span>
<span class="n">ClientContext</span> <span class="n">context</span><span class="p">;</span>

<span class="o">//</span><span class="n">construct</span> <span class="n">tensor</span>
<span class="n">Tensor</span> <span class="n">data</span><span class="p">;</span>

<span class="o">//</span><span class="nb">set</span> <span class="n">shape</span>
<span class="n">TensorShape</span> <span class="n">shape</span><span class="p">;</span>
<span class="n">shape</span><span class="o">.</span><span class="n">add_dims</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>
<span class="o">*</span><span class="n">data</span><span class="o">.</span><span class="n">mutable_tensor_shape</span><span class="p">()</span> <span class="o">=</span> <span class="n">shape</span><span class="p">;</span>

<span class="o">//</span><span class="nb">set</span> <span class="nb">type</span>
<span class="n">data</span><span class="o">.</span><span class="n">set_tensor_type</span><span class="p">(</span><span class="n">ms_serving</span><span class="p">::</span><span class="n">MS_FLOAT32</span><span class="p">);</span>
<span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span> <span class="n">input_data</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">};</span>

<span class="o">//</span><span class="nb">set</span> <span class="n">datas</span>
<span class="n">data</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">input_data</span><span class="o">.</span><span class="n">size</span><span class="p">());</span>

<span class="o">//</span><span class="n">add</span> <span class="n">tensor</span> <span class="n">to</span> <span class="n">request</span>
<span class="o">*</span><span class="n">request</span><span class="o">.</span><span class="n">add_data</span><span class="p">()</span> <span class="o">=</span> <span class="n">data</span><span class="p">;</span>
<span class="o">*</span><span class="n">request</span><span class="o">.</span><span class="n">add_data</span><span class="p">()</span> <span class="o">=</span> <span class="n">data</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Call the gRPC API to communicate with the Serving that has been started, and obtain the return value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span> <span class="n">status</span> <span class="o">=</span> <span class="n">stub_</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">reply</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
<p>For details about the complete code, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r0.7/serving/example/cpp_client/ms_client.cc">ms_client</a>.</p>
</section>
</section>
<section id="rest-api-client-sample">
<h3>REST API Client Sample<a class="headerlink" href="#rest-api-client-sample" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Send data in the form of <code class="docutils literal notranslate"><span class="pre">data</span></code>:
<code class="docutils literal notranslate"><span class="pre">data</span></code> field: flatten each input data of network model into one-dimensional data. Suppose the network model has n inputs, and the final data structure is a two-dimensional list of 1 * n.
As in this example, flatten the model input data <code class="docutils literal notranslate"><span class="pre">[[1.0,</span> <span class="pre">2.0],</span> <span class="pre">[3.0,</span> <span class="pre">4.0]]</span></code> and <code class="docutils literal notranslate"><span class="pre">[[1.0,</span> <span class="pre">2.0],</span> <span class="pre">[3.0,</span> <span class="pre">4.0]]</span></code> to form <code class="docutils literal notranslate"><span class="pre">[[1.0,</span> <span class="pre">2.0,</span> <span class="pre">3.0,</span> <span class="pre">4.0],</span> <span class="pre">[1.0,</span> <span class="pre">2.0,</span> <span class="pre">3.0,</span> <span class="pre">4.0]]</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{&quot;data&quot;: [[1.0, 2.0, 3.0, 4.0], [1.0, 2.0, 3.0, 4.0]]}&#39;</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">5501</span>
</pre></div>
</div>
<p>The following return values are displayed, indicating that the serving service has correctly executed the reasoning of the add network, and the output data structure is similar to that of the input:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:[[</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">8.0</span><span class="p">]]}</span>
</pre></div>
</div>
</li>
<li><p>Send data in the form of <code class="docutils literal notranslate"><span class="pre">tensor</span></code>:
<code class="docutils literal notranslate"><span class="pre">tensor</span></code> field: composed of each input of the network model, keeping the original shape of input.
As in this example, the model input data <code class="docutils literal notranslate"><span class="pre">[[1.0,</span> <span class="pre">2.0],</span> <span class="pre">[3.0,</span> <span class="pre">4.0]]</span></code> and <code class="docutils literal notranslate"><span class="pre">[[1.0,</span> <span class="pre">2.0],</span> <span class="pre">[3.0,</span> <span class="pre">4.0]]</span></code> are combined into <code class="docutils literal notranslate"><span class="pre">[[[1.0,</span> <span class="pre">2.0],</span> <span class="pre">[3.0,</span> <span class="pre">4.0]],</span> <span class="pre">[[1.0,</span> <span class="pre">2.0],</span> <span class="pre">[3.0,</span> <span class="pre">4.0]]]</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{&quot;tensor&quot;: [[[1.0, 2.0], [3.0, 4.0]], [[1.0, 2.0], [3.0, 4.0]]]}&#39;</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">5501</span>
</pre></div>
</div>
<p>The following return values are displayed, indicating that the serving service has correctly executed the reasoning of the add network, and the output data structure is similar to that of the input:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;tensor&quot;</span><span class="p">:[[</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">8.0</span><span class="p">]]}</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>REST APICurrently only int32 and fp32 are supported as inputs.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>