<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-Platform Inference &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Computer Vision Applications" href="../advanced_use/computer_vision_application.html" />
    <link rel="prev" title="Saving and Loading Model Parameters" href="saving_and_loading_model_parameters.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Use</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data_preparation/data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="saving_and_loading_model_parameters.html">Saving and Loading Model Parameters</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multi-Platform Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference-on-the-ascend-910-ai-processor">Inference on the Ascend 910 AI processor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#inference-using-a-checkpoint-file">Inference Using a Checkpoint File</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inference-on-the-ascend-310-ai-processor">Inference on the Ascend 310 AI processor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#inference-using-an-onnx-or-air-file">Inference Using an ONNX or AIR File</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inference-on-a-gpu">Inference on a GPU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Inference Using a Checkpoint File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-using-an-onnx-file">Inference Using an ONNX File</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inference-on-a-cpu">Inference on a CPU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Inference Using a Checkpoint File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Inference Using an ONNX File</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#on-device-inference">On-Device Inference</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/computer_vision_application.html">Computer Vision Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/nlp_application.html">Natural Language Processing (NLP) Application</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/debugging_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/customized_debugging_information.html">Customized Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/visualization_tutorials.html">Training Process Visualization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/distributed_training_tutorials.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/quantization_aware.html">Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage on Device</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/on_device_inference.html">On-Device Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Network Migration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/network_migration.html">Network Migration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/model_security.html">Model Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/differential_privacy.html">Differential Privacy in Machine Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Multi-Platform Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/multi_platform_inference.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="multi-platform-inference">
<h1>Multi-Platform Inference<a class="headerlink" href="#multi-platform-inference" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span> <span class="pre">Application</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r0.7/tutorials/source_en/use/multi_platform_inference.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Models trained by MindSpore support the inference on different hardware platforms. This document describes the inference process on each platform.</p>
<p>The inference can be performed in either of the following methods based on different principles:</p>
<ul class="simple">
<li><p>Use a checkpoint file for inference. That is, use the inference API to load data and the checkpoint file for inference in the MindSpore training environment.</p></li>
<li><p>Convert the checkpiont file into a common model format, such as ONNX or AIR, for inference. The inference environment does not depend on MindSpore. In this way, inference can be performed across hardware platforms as long as the platform supports ONNX or AIR inference. For example, models trained on the Ascend 910 AI processor can be inferred on the GPU or CPU.</p></li>
</ul>
<p>MindSpore supports the following inference scenarios based on the hardware platform:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware Platform</p></th>
<th class="head"><p>Model File Format</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Ascend 910 AI processor</p></td>
<td><p>Checkpoint</p></td>
<td><p>The training environment dependency is the same as that of MindSpore.</p></td>
</tr>
<tr class="row-odd"><td><p>Ascend 310 AI processor</p></td>
<td><p>ONNX or AIR</p></td>
<td><p>Equipped with the ACL framework and supports the model in OM format. You need to use a tool to convert a model into the OM format.</p></td>
</tr>
<tr class="row-even"><td><p>GPU</p></td>
<td><p>Checkpoint</p></td>
<td><p>The training environment dependency is the same as that of MindSpore.</p></td>
</tr>
<tr class="row-odd"><td><p>GPU</p></td>
<td><p>ONNX</p></td>
<td><p>Supports ONNX Runtime or SDK, for example, TensorRT.</p></td>
</tr>
<tr class="row-even"><td><p>CPU</p></td>
<td><p>Checkpoint</p></td>
<td><p>The training environment dependency is the same as that of MindSpore.</p></td>
</tr>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>ONNX</p></td>
<td><p>Supports ONNX Runtime or SDK, for example, TensorRT.</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>Open Neural Network Exchange (ONNX) is an open file format designed for machine learning. It is used to store trained models. It enables different AI frameworks (such as PyTorch and MXNet) to store model data in the same format and interact with each other. For details, visit the ONNX official website <a class="reference external" href="https://onnx.ai/">https://onnx.ai/</a>.</p>
</div></blockquote>
<blockquote>
<div><p>Ascend Intermediate Representation (AIR) is an open file format defined by Huawei for machine learning and can better adapt to the Ascend AI processor. It is similar to ONNX.</p>
</div></blockquote>
<blockquote>
<div><p>Ascend Computer Language (ACL) provides C++ API libraries for users to develop deep neural network applications, including device management, context management, stream management, memory management, model loading and execution, operator loading and execution, and media data processing. It matches the Ascend AI processor and enables hardware running management and resource management.</p>
</div></blockquote>
<blockquote>
<div><p>Offline Model (OM) is supported by the Huawei Ascend AI processor. It implements preprocessing functions that can be completed without devices, such as operator scheduling optimization, weight data rearrangement and compression, and memory usage optimization.</p>
</div></blockquote>
<blockquote>
<div><p>NVIDIA TensorRT is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime to improve the inference speed of the deep learning model on edge devices. For details, see <a class="reference external" href="https://developer.nvidia.com/tensorrt">https://developer.nvidia.com/tensorrt</a>.</p>
</div></blockquote>
</section>
<section id="inference-on-the-ascend-910-ai-processor">
<h2>Inference on the Ascend 910 AI processor<a class="headerlink" href="#inference-on-the-ascend-910-ai-processor" title="Permalink to this headline"></a></h2>
<section id="inference-using-a-checkpoint-file">
<h3>Inference Using a Checkpoint File<a class="headerlink" href="#inference-using-a-checkpoint-file" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Input a validation dataset to validate a model using the <code class="docutils literal notranslate"><span class="pre">model.eval</span></code> API.</p>
<p>1.1 Local Storage</p>
<p>When the pre-trained models are saved locally, the steps of performing inference on validation dataset are as follows: firstly creating a model, then loading model and parameters using <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> and <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> in <code class="docutils literal notranslate"><span class="pre">mindspore.train.serialization</span></code> module, and finally performing inference on validation dataset once created. The processing method of the validation dataset is the same as that of the training dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">cfg</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Testing ==============&quot;</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ckpt_path</span><span class="p">)</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">),</span>
                         <span class="n">cfg</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_sink_mode</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== </span><span class="si">{}</span><span class="s2"> ==============&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
<p>In the preceding information:<br />
<code class="docutils literal notranslate"><span class="pre">model.eval</span></code> is an API for model validation. For details about the API, see <a class="reference external" href="https://www.mindspore.cn/api/en/r0.7/api/python/mindspore/mindspore.html#mindspore.Model.eval">https://www.mindspore.cn/api/en/r0.7/api/python/mindspore/mindspore.html#mindspore.Model.eval</a>.</p>
<blockquote>
<div><p>Inference sample code: <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r0.7/model_zoo/official/cv/lenet/eval.py">https://gitee.com/mindspore/mindspore/blob/r0.7/model_zoo/official/cv/lenet/eval.py</a>.</p>
</div></blockquote>
<p>1.2 Remote Storage</p>
<p>When the pre-trained models are saved remotely, the steps of performing inference on validation dataset are as follows: firstly creating a model, then loading model and parameters using <code class="docutils literal notranslate"><span class="pre">hub.load_weights</span></code>, and finally performing inference on validation dataset once created. The processing method of the validation dataset is the same as that of the training dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">cfg</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Testing ==============&quot;</span><span class="p">)</span>
<span class="n">hub</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">network_name</span><span class="o">=</span><span class="s2">&quot;lenet&quot;</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s2">&quot;device_target&quot;</span><span class="p">:</span>
                 <span class="s2">&quot;ascend&quot;</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">:</span><span class="s2">&quot;mnist&quot;</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;0.5.0&quot;</span><span class="p">})</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">),</span>
                         <span class="n">cfg</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dataset_sink_mode</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== </span><span class="si">{}</span><span class="s2"> ==============&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
<p>In the preceding information:</p>
<p><code class="docutils literal notranslate"><span class="pre">hub.load_weights</span></code> is an API for loading model parameters. PLease check the details in <a class="reference external" href="https://www.mindspore.cn/api/en/r0.7/api/python/mindspore/mindspore.hub.html#mindspore.hub.load_weights">https://www.mindspore.cn/api/en/r0.7/api/python/mindspore/mindspore.hub.html#mindspore.hub.load_weights</a>.</p>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">model.predict</span></code> API to perform inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
</pre></div>
</div>
<p>In the preceding information:<br />
<code class="docutils literal notranslate"><span class="pre">model.predict</span></code> is an API for inference. For details about the API, see <a class="reference external" href="https://www.mindspore.cn/api/en/r0.7/api/python/mindspore/mindspore.html#mindspore.Model.predict">https://www.mindspore.cn/api/en/r0.7/api/python/mindspore/mindspore.html#mindspore.Model.predict</a>.</p>
</li>
</ol>
</section>
</section>
<section id="inference-on-the-ascend-310-ai-processor">
<h2>Inference on the Ascend 310 AI processor<a class="headerlink" href="#inference-on-the-ascend-310-ai-processor" title="Permalink to this headline"></a></h2>
<section id="inference-using-an-onnx-or-air-file">
<h3>Inference Using an ONNX or AIR File<a class="headerlink" href="#inference-using-an-onnx-or-air-file" title="Permalink to this headline"></a></h3>
<p>The Ascend 310 AI processor is equipped with the ACL framework and supports the OM format which needs to be converted from the model in ONNX or AIR format. For inference on the Ascend 310 AI processor, perform the following steps:</p>
<ol class="arabic simple">
<li><p>Generate a model in ONNX or AIR format on the training platform. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/en/r0.7/use/saving_and_loading_model_parameters.html#export-air-model">Export AIR Model</a> and <a class="reference external" href="https://www.mindspore.cn/tutorial/en/r0.7/use/saving_and_loading_model_parameters.html#export-onnx-model">Export ONNX Model</a>.</p></li>
<li><p>Convert the ONNX or AIR model file into an OM model file and perform inference.</p>
<ul class="simple">
<li><p>For performing inference in the cloud environment (ModelArt), see the <a class="reference external" href="https://support.huaweicloud.com/bestpractice-modelarts/modelarts_10_0026.html">Ascend 910 training and Ascend 310 inference samples</a>.</p></li>
<li><p>For details about the local bare-metal environment where the Ascend 310 AI processor is deployed locally (compared with the cloud environment), see the document of the Ascend 310 AI processor software package.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="inference-on-a-gpu">
<h2>Inference on a GPU<a class="headerlink" href="#inference-on-a-gpu" title="Permalink to this headline"></a></h2>
<section id="id1">
<h3>Inference Using a Checkpoint File<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>The inference is the same as that on the Ascend 910 AI processor.</p>
</section>
<section id="inference-using-an-onnx-file">
<h3>Inference Using an ONNX File<a class="headerlink" href="#inference-using-an-onnx-file" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Generate a model in ONNX format on the training platform. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/en/r0.7/use/saving_and_loading_model_parameters.html#export-onnx-model">Export ONNX Model</a>.</p></li>
<li><p>Perform inference on a GPU by referring to the runtime or SDK document. For example, use TensorRT to perform inference on the NVIDIA GPU. For details, see <a class="reference external" href="https://github.com/onnx/onnx-tensorrt">TensorRT backend for ONNX</a>.</p></li>
</ol>
</section>
</section>
<section id="inference-on-a-cpu">
<h2>Inference on a CPU<a class="headerlink" href="#inference-on-a-cpu" title="Permalink to this headline"></a></h2>
<section id="id2">
<h3>Inference Using a Checkpoint File<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>The inference is the same as that on the Ascend 910 AI processor.</p>
</section>
<section id="id3">
<h3>Inference Using an ONNX File<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>Similar to the inference on a GPU, the following steps are required:</p>
<ol class="arabic simple">
<li><p>Generate a model in ONNX format on the training platform. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/en/r0.7/use/saving_and_loading_model_parameters.html#export-onnx-model">Export ONNX Model</a>.</p></li>
<li><p>Perform inference on a CPU by referring to the runtime or SDK document. For details about how to use the ONNX Runtime, see the <a class="reference external" href="https://github.com/microsoft/onnxruntime">ONNX Runtime document</a>.</p></li>
</ol>
</section>
</section>
<section id="on-device-inference">
<h2>On-Device Inference<a class="headerlink" href="#on-device-inference" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite is an inference engine for on-device inference. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/en/r0.7/use/saving_and_loading_model_parameters.html#export-mindir-model">Export MINDIR Model</a> and <a class="reference external" href="https://www.mindspore.cn/tutorial/en/r0.7/advanced_use/on_device_inference.html">On-Device Inference</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="saving_and_loading_model_parameters.html" class="btn btn-neutral float-left" title="Saving and Loading Model Parameters" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../advanced_use/computer_vision_application.html" class="btn btn-neutral float-right" title="Computer Vision Applications" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>