<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Saving and Loading Model Parameters &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multi-Platform Inference" href="multi_platform_inference.html" />
    <link rel="prev" title="Custom Operators" href="custom_operator.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Use</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data_preparation/data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Saving and Loading Model Parameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#saving-model-parameters">Saving Model Parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#checkpoint-configuration-policies">CheckPoint Configuration Policies</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loading-model-parameters">Loading Model Parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#for-inference-validation">For Inference Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#for-retraining">For Retraining</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#export-model">Export Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#export-air-model">Export AIR Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#export-onnx-model">Export ONNX Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#export-mindir-model">Export MINDIR Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Multi-Platform Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/computer_vision_application.html">Computer Vision Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/nlp_application.html">Natural Language Processing (NLP) Application</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/debugging_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/customized_debugging_information.html">Customized Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/visualization_tutorials.html">Training Process Visualization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/distributed_training_tutorials.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/quantization_aware.html">Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage on Device</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/on_device_inference.html">On-Device Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Network Migration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/network_migration.html">Network Migration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/model_security.html">Model Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_use/differential_privacy.html">Differential Privacy in Machine Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Saving and Loading Model Parameters</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/saving_and_loading_model_parameters.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="saving-and-loading-model-parameters">
<h1>Saving and Loading Model Parameters<a class="headerlink" href="#saving-and-loading-model-parameters" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Export</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r0.7/tutorials/source_en/use/saving_and_loading_model_parameters.md"><img alt="View Source On Gitee" src="../_images/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>During model training, you can add CheckPoints to save model parameters for inference and retraining after interruption.</p>
<p>The application scenarios are as follows:</p>
<ul class="simple">
<li><p>Inference after training.</p>
<ul>
<li><p>After model training, save model parameters for inference or prediction.</p></li>
<li><p>During training, through real-time accuracy validation, save the model parameters with the highest accuracy for prediction.</p></li>
</ul>
</li>
<li><p>Retraining.</p>
<ul>
<li><p>During a long-time training, save the generated CheckPoint files to prevent the training from starting from the beginning after it exits abnormally.</p></li>
<li><p>Train a model, save parameters, and perform fine-tuning for different tasks.</p></li>
</ul>
</li>
</ul>
<p>A CheckPoint file of MindSpore is a binary file that stores the values of all training parameters. The Google Protocol Buffers mechanism with good scalability is adopted, which is independent of the development language and platform.
The protocol format of CheckPoints is defined in <code class="docutils literal notranslate"><span class="pre">mindspore/ccsrc/utils/checkpoint.proto</span></code>.</p>
<p>The following uses an example to describe the saving and loading functions of MindSpore. The ResNet-50 network and the MNIST dataset are selected.</p>
</section>
<section id="saving-model-parameters">
<h2>Saving Model Parameters<a class="headerlink" href="#saving-model-parameters" title="Permalink to this headline"></a></h2>
<p>During model training, use the callback mechanism to transfer the object of the callback function <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> to save model parameters and generate CheckPoint files.
You can use the <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> object to set the CheckPoint saving policies.
The saved parameters are classified into network parameters and optimizer parameters.</p>
<p><code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> provides default configuration policies for users to quickly get started.
The following describes the usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_num</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ckpoint_cb</span><span class="p">)</span>
</pre></div>
</div>
<p>You can configure the CheckPoint policies as required.
The following describes the usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;resnet50&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_num</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ckpoint_cb</span><span class="p">)</span>
</pre></div>
</div>
<p>In the preceding code, initialize a <code class="docutils literal notranslate"><span class="pre">TrainConfig</span></code> class object to set the saving policies.
<code class="docutils literal notranslate"><span class="pre">save_checkpoint_steps</span></code> indicates the saving frequency. That is, parameters are saved every specified number of steps. <code class="docutils literal notranslate"><span class="pre">keep_checkpoint_max</span></code> indicates the maximum number of CheckPoint files that can be saved.
<code class="docutils literal notranslate"><span class="pre">prefix</span></code> indicates the prefix name of the generated CheckPoint file. <code class="docutils literal notranslate"><span class="pre">directory</span></code> indicates the directory for storing the file.
Create a <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> object and transfer it to the model.train method. Then you can use the CheckPoint function during training.
If you want to delete the <code class="docutils literal notranslate"><span class="pre">.ckpt</span></code> file, please delete the <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file simultaneously.</p>
<p>Generated CheckPoint files are as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p>resnet50-graph.meta # Generate compiled computation graph.</p></li>
<li><p>resnet50-1_32.ckpt  # The file name extension is .ckpt.</p></li>
<li><p>resnet50-2_32.ckpt  # The file name format contains the epoch and step correspond to the saved parameters.</p></li>
<li><p>resnet50-3_32.ckpt  # The file name indicates that the model parameters generated during the 32th step of the third epoch are saved.</p></li>
<li><p>…</p></li>
</ul>
</div></blockquote>
<p>If you use the same prefix and run the training script for multiple times, CheckPoint files with the same name may be generated.
MindSpore adds underscores (_) and digits at the end of the user-defined prefix to distinguish CheckPoints with the same name.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">resnet50_3-2_32.ckpt</span></code> indicates the CheckPoint file generated during the 32th step of the second epoch after the script is executed for the third time.</p>
<blockquote>
<div><ul class="simple">
<li><p>When the saved single model parameter is large (more than 64M), it will fail to save due to the limitation of Protobuf’s own data size. At this time, the restriction can be lifted by setting the environment variable <code class="docutils literal notranslate"><span class="pre">PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python</span></code>.</p></li>
<li><p>When performing distributed parallel training tasks, each process needs to set different <code class="docutils literal notranslate"><span class="pre">directory</span></code> parameters to save the CheckPoint file to a different directory to prevent file read and write confusion.</p></li>
</ul>
</div></blockquote>
<section id="checkpoint-configuration-policies">
<h3>CheckPoint Configuration Policies<a class="headerlink" href="#checkpoint-configuration-policies" title="Permalink to this headline"></a></h3>
<p>MindSpore provides two types of CheckPoint saving policies: iteration policy and time policy. You can create the <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> object to set the corresponding policies.
CheckpointConfig contains the following four parameters:</p>
<ul class="simple">
<li><p>save_checkpoint_steps: indicates the step interval for saving a CheckPoint file. That is, parameters are saved every specified number of steps. The default value is 1.</p></li>
<li><p>save_checkpoint_seconds: indicates the interval for saving a CheckPoint file. That is, parameters are saved every specified number of seconds. The default value is 0.</p></li>
<li><p>keep_checkpoint_max: indicates the maximum number of CheckPoint files that can be saved. The default value is 5.</p></li>
<li><p>keep_checkpoint_per_n_minutes: indicates the interval for saving a CheckPoint file. That is, parameters are saved every specified number of minutes. The default value is 0.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">save_checkpoint_steps</span></code> and <code class="docutils literal notranslate"><span class="pre">keep_checkpoint_max</span></code> are iteration policies, which can be configured based on the number of training iterations.
<code class="docutils literal notranslate"><span class="pre">save_checkpoint_seconds</span></code> and <code class="docutils literal notranslate"><span class="pre">keep_checkpoint_per_n_minutes</span></code> are time policies, which can be configured during training.</p>
<p>The two types of policies cannot be used together. Iteration policies have a higher priority than time policies. When the two types of policies are configured at the same time, only iteration policies take effect.
If a parameter is set to None, the related policy is canceled.
After the training script is normally executed, the CheckPoint file generated during the last step is saved by default.</p>
</section>
</section>
<section id="loading-model-parameters">
<h2>Loading Model Parameters<a class="headerlink" href="#loading-model-parameters" title="Permalink to this headline"></a></h2>
<p>After saving CheckPoint files, you can load parameters.</p>
<section id="for-inference-validation">
<h3>For Inference Validation<a class="headerlink" href="#for-inference-validation" title="Permalink to this headline"></a></h3>
<p>In inference-only scenarios, use <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> to directly load parameters to the network for subsequent inference validation.</p>
<p>The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">()</span>
<span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;resnet50-2_32.ckpt&quot;</span><span class="p">,</span> <span class="n">net</span><span class="o">=</span><span class="n">resnet</span><span class="p">)</span>
<span class="n">dateset_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mnist_path</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># define the test dataset</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset_eval</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> method loads network parameters in the parameter file to the model. After the loading, parameters in the network are those saved in CheckPoints.
The <code class="docutils literal notranslate"><span class="pre">eval</span></code> method validates the accuracy of the trained model.</p>
</section>
<section id="for-retraining">
<h3>For Retraining<a class="headerlink" href="#for-retraining" title="Permalink to this headline"></a></h3>
<p>In the retraining after task interruption and fine-tuning scenarios, you can load network parameters and optimizer parameters to the model.</p>
<p>The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># return a parameter dict for model</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;resnet50-2_32.ckpt&quot;</span><span class="p">)</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">()</span>
<span class="c1"># load the parameter into net</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="c1"># load the parameter into operator</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> method returns a parameter dictionary and then the <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> method loads parameters in the parameter dictionary to the network or optimizer.</p>
</section>
</section>
<section id="export-model">
<h2>Export Model<a class="headerlink" href="#export-model" title="Permalink to this headline"></a></h2>
<p>When you have a CheckPoint file, if you want to do inference, you need to generate corresponding models based on the network and CheckPoint. The <code class="docutils literal notranslate"><span class="pre">export</span></code> interface supports exporting multiple types of model file formats for inference on different hardware platforms.</p>
<section id="export-air-model">
<h3>Export AIR Model<a class="headerlink" href="#export-air-model" title="Permalink to this headline"></a></h3>
<p>AIR format file only supports Ascend AI processor. The code example of exporting this format file is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.serialization</span> <span class="kn">import</span> <span class="n">export</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">()</span>
<span class="c1"># return a parameter dict for model</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;resnet50-2_32.ckpt&quot;</span><span class="p">)</span>
<span class="c1"># load the parameter into net</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">export</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">file_name</span> <span class="o">=</span> <span class="s1">&#39;resnet50-2_32.air&#39;</span><span class="p">,</span> <span class="n">file_format</span> <span class="o">=</span> <span class="s1">&#39;AIR&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Before using the <code class="docutils literal notranslate"><span class="pre">export</span></code> interface, you need to import<code class="docutils literal notranslate"> <span class="pre">mindspore.train.serialization</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">input</span></code> parameter is used to specify the input shape and data type of the exported model.</p>
<p>It is recommended to use ‘.air’ as the suffix of AIR format files.</p>
</section>
<section id="export-onnx-model">
<h3>Export ONNX Model<a class="headerlink" href="#export-onnx-model" title="Permalink to this headline"></a></h3>
<p>ONNX format file is a general model file, which can be applied to many kinds of hardware, such as Ascend AI processor, GPU, CPU, etc. The code example of exporting this format file is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.serialization</span> <span class="kn">import</span> <span class="n">export</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">()</span>
<span class="c1"># return a parameter dict for model</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;resnet50-2_32.ckpt&quot;</span><span class="p">)</span>
<span class="c1"># load the parameter into net</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">export</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">file_name</span> <span class="o">=</span> <span class="s1">&#39;resnet50-2_32.onnx&#39;</span><span class="p">,</span> <span class="n">file_format</span> <span class="o">=</span> <span class="s1">&#39;ONNX&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>It is recommended to use ‘.onnx’ as the suffix of ONNX format files.</p>
</section>
<section id="export-mindir-model">
<h3>Export MINDIR Model<a class="headerlink" href="#export-mindir-model" title="Permalink to this headline"></a></h3>
<p>MINDIR format file can be applied to MindSpore Lite and MindSpore Serving. Currently, it supports inference network based on static graph without control flow semantics.</p>
<p>If you want to do inference on the device, then you need to generate corresponding MINDIR models based on the network and CheckPoint.
Currently we support the export of MINDIR models for inference based on graph mode, which don’t contain control flow. Taking the export of MINDIR model as an example to illustrate the implementation of model export,
the code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.serialization</span> <span class="kn">import</span> <span class="n">export</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">()</span>
<span class="c1"># return a parameter dict for model</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;resnet50-2_32.ckpt&quot;</span><span class="p">)</span>
<span class="c1"># load the parameter into net</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">export</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">file_name</span> <span class="o">=</span> <span class="s1">&#39;resnet50-2_32.mindir&#39;</span><span class="p">,</span> <span class="n">file_format</span> <span class="o">=</span> <span class="s1">&#39;MINDIR&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>It is recommended to use ‘.mindir’ as the suffix of MINDIR format files.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_operator.html" class="btn btn-neutral float-left" title="Custom Operators" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="multi_platform_inference.html" class="btn btn-neutral float-right" title="Multi-Platform Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>