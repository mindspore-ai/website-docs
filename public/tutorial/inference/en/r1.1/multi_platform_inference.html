<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-Platform Inference Overview &mdash; MindSpore r1.1 documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inference on the Ascend 910 AI processor" href="multi_platform_inference_ascend_910.html" />
    <link rel="prev" title="Inference Using MindSpore" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Inference Model</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multi-Platform Inference Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/docs/en?master">On-Device Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference Service</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="serving_example.html">MindSpore-based Inference Service Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_grpc.html">Access MindSpore Serving service based on gRPC interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_restful.html">Access MindSpore Serving service based on RESTful interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_model.html">Servable provided by configuration model</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Multi-Platform Inference Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/multi_platform_inference.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="multi-platform-inference-overview">
<h1>Multi-Platform Inference Overview<a class="headerlink" href="#multi-platform-inference-overview" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span> <span class="pre">Application</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/inference/source_en/multi_platform_inference.md"><img alt="View Source On Gitee" src="_images/logo_source.png" /></a></p>
<p>Models trained by MindSpore support the inference on different hardware platforms. This document describes the inference process on each platform.</p>
<p>The inference can be performed in either of the following methods based on different principles:</p>
<ul class="simple">
<li><p>Use a checkpoint file for inference, that is, use the inference API to load data and the checkpoint file for inference in the MindSpore training environment.</p></li>
<li><p>Convert the checkpoint file into a common model format, such as ONNX or AIR, for inference. The inference environment does not depend on MindSpore. In this way, inference can be performed across hardware platforms as long as the platform supports ONNX or AIR inference. For example, models trained on the Ascend 910 AI processor can be inferred on the GPU or CPU.</p></li>
</ul>
<p>MindSpore supports the following inference scenarios based on the hardware platform:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Hardware Platform</p></th>
<th class="head"><p>Model File Format</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Ascend 910 AI processor</p></td>
<td><p>Checkpoint</p></td>
<td><p>The training environment dependency is the same as that of MindSpore.</p></td>
</tr>
<tr class="row-odd"><td><p>Ascend 310 AI processor</p></td>
<td><p>ONNX or AIR</p></td>
<td><p>Equipped with the ACL framework and supports the model in OM format. You need to use a tool to convert a model into the OM format.</p></td>
</tr>
<tr class="row-even"><td><p>GPU</p></td>
<td><p>Checkpoint</p></td>
<td><p>The training environment dependency is the same as that of MindSpore.</p></td>
</tr>
<tr class="row-odd"><td><p>GPU</p></td>
<td><p>ONNX</p></td>
<td><p>Supports ONNX Runtime or SDK, for example, TensorRT.</p></td>
</tr>
<tr class="row-even"><td><p>CPU</p></td>
<td><p>Checkpoint</p></td>
<td><p>The training environment dependency is the same as that of MindSpore.</p></td>
</tr>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>ONNX</p></td>
<td><p>Supports ONNX Runtime or SDK, for example, TensorRT.</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>Open Neural Network Exchange (ONNX) is an open file format designed for machine learning. It is used to store trained models. It enables different AI frameworks (such as PyTorch and MXNet) to store model data in the same format and interact with each other. For details, visit the ONNX official website <a class="reference external" href="https://onnx.ai/">https://onnx.ai/</a>.</p></li>
<li><p>Ascend Intermediate Representation (AIR) is an open file format defined by Huawei for machine learning and can better adapt to the Ascend AI processor. It is similar to ONNX.</p></li>
<li><p>Ascend Computer Language (ACL) provides C++ API libraries for users to develop deep neural network applications, including device management, context management, stream management, memory management, model loading and execution, operator loading and execution, and media data processing. It matches the Ascend AI processor and enables hardware running management and resource management.</p></li>
<li><p>Offline Model (OM) is supported by the Huawei Ascend AI processor. It implements preprocessing functions that can be completed without devices, such as operator scheduling optimization, weight data rearrangement and compression, and memory usage optimization.</p></li>
<li><p>NVIDIA TensorRT is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime to improve the inference speed of the deep learning model on edge devices. For details, see <a class="reference external" href="https://developer.nvidia.com/tensorrt">https://developer.nvidia.com/tensorrt</a>.</p></li>
</ul>
</div></blockquote>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Inference Using MindSpore" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="multi_platform_inference_ascend_910.html" class="btn btn-neutral float-right" title="Inference on the Ascend 910 AI processor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>