<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Inference Using the MindIR Model on Ascend 310 AI Processors &mdash; MindSpore r1.1 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inference on a GPU" href="multi_platform_inference_gpu.html" />
    <link rel="prev" title="Inference on the Ascend 310 AI Processor" href="multi_platform_inference_ascend_310_air.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Inference Model</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Multi-Platform Inference Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Inference Using the MindIR Model on Ascend 310 AI Processors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-the-development-environment">Preparing the Development Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exporting-the-mindir-model">Exporting the MindIR Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-directory-structure">Inference Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-code">Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#introduce-to-building-script">Introduce to Building Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-inference-code">Building Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-inference-and-viewing-the-result">Performing Inference and Viewing the Result</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/docs/en?master">On-Device Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference Service</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="serving_example.html">MindSpore-based Inference Service Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_grpc.html">Access MindSpore Serving service based on gRPC interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_restful.html">Access MindSpore Serving service based on RESTful interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_model.html">Servable provided by configuration model</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a> &raquo;</li>
      <li>Inference Using the MindIR Model on Ascend 310 AI Processors</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/multi_platform_inference_ascend_310_mindir.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="inference-using-the-mindir-model-on-ascend-310-ai-processors">
<h1>Inference Using the MindIR Model on Ascend 310 AI Processors<a class="headerlink" href="#inference-using-the-mindir-model-on-ascend-310-ai-processors" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span> <span class="pre">Application</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/inference/source_en/multi_platform_inference_ascend_310_mindir.md" target="_blank"><img src="./_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Ascend 310 is a highly efficient and integrated AI processor oriented to edge scenarios. The Atlas 200 Developer Kit (Atlas 200 DK) is a developer board that uses the Atlas 200 AI accelerator module. Integrated with the HiSilicon Ascend 310 AI processor, the Atlas 200 allows data analysis, inference, and computing for various data such as images and videos, and can be widely used in scenarios such as intelligent surveillance, robots, drones, and video servers.</p>
<p>This tutorial describes how to use MindSpore to perform inference on the Atlas 200 DK. The process is as follows:</p>
<ol class="arabic simple">
<li><p>Prepare the development environment, including creating an SD card for the Atlas 200 DK, configuring the Python environment, and updating the development software package.</p></li>
<li><p>Export the MindIR model file. The ResNet-50 model is used as an example.</p></li>
<li><p>Build the inference code to generate an executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file.</p></li>
<li><p>Load the saved MindIR model, perform inference, and view the result.</p></li>
</ol>
<blockquote>
<div><p>You can obtain the complete executable sample code at <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample">https://gitee.com/mindspore/docs/tree/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample</a>.</p>
</div></blockquote>
</section>
<section id="preparing-the-development-environment">
<h2>Preparing the Development Environment<a class="headerlink" href="#preparing-the-development-environment" title="Permalink to this headline"></a></h2>
<p>For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/inference/en/r1.1/multi_platform_inference_ascend_310_air.html#preparing-the-development-environment">Inference on the Ascend 310 AI Processor</a>.</p>
</section>
<section id="exporting-the-mindir-model">
<h2>Exporting the MindIR Model<a class="headerlink" href="#exporting-the-mindir-model" title="Permalink to this headline"></a></h2>
<p>Train the target network on the Ascend 910 AI Processor, save it as a checkpoint file, and export the model file in MindIR format through the network and checkpoint file. For details about the export process, see <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.1/use/save_model.html#export-mindir-model">Export MindIR Model</a>.</p>
<blockquote>
<div><p>The <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/sample_resources/ascend310_resnet50_preprocess_sample/resnet50_imagenet.mindir">resnet50_imagenet.mindir</a> is a sample MindIR file exported using the ResNet-50 model.</p>
</div></blockquote>
</section>
<section id="inference-directory-structure">
<h2>Inference Directory Structure<a class="headerlink" href="#inference-directory-structure" title="Permalink to this headline"></a></h2>
<p>Create a directory to store the inference code project, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/ascend310_resnet50_preprocess_sample</span></code>. The directory code can be obtained from the <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample">official website</a>. The <code class="docutils literal notranslate"><span class="pre">model</span></code> directory stores the exported <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model files and the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory stores the images to be classified. The directory structure of the inference code project is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ascend310_resnet50_preprocess_sample
    ├── CMakeLists.txt                    // Build script
    ├── README.md                         // Usage description
    ├── main.cc                           // Main function
    ├── model
    │   └── resnet50_imagenet.mindir      // MindIR model file
    └── test_data
        ├── ILSVRC2012_val_00002138.JPEG  // Input sample image 1
        ├── ILSVRC2012_val_00003014.JPEG  // Input sample image 2
        ├── ...                           // Input sample image n
</pre></div>
</div>
</section>
<section id="inference-code">
<h2>Inference Code<a class="headerlink" href="#inference-code" title="Permalink to this headline"></a></h2>
<p>Inference sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample/main.cc">https://gitee.com/mindspore/docs/blob/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample/main.cc</a> .</p>
<p>Set global context, device target is <code class="docutils literal notranslate"><span class="pre">Ascend310</span></code> and evice id is <code class="docutils literal notranslate"><span class="pre">0</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">::</span><span class="n">GlobalContext</span><span class="o">::</span><span class="n">SetGlobalDeviceTarget</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">kDeviceTypeAscend310</span><span class="p">);</span>
<span class="n">ms</span><span class="o">::</span><span class="n">GlobalContext</span><span class="o">::</span><span class="n">SetGlobalDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
<p>Load mindir file:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load MindIR model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="o">=</span><span class="n">ms</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">LoadModel</span><span class="p">(</span><span class="n">resnet_file</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">);</span>
<span class="c1">// Build model with graph object</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="nf">resnet50</span><span class="p">((</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">)));</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Status</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Build</span><span class="p">({});</span>
</pre></div>
</div>
<p>Get informance of this model:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>Load image file:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Readfile is a function to read images</span>
<span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="nf">ReadFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">file</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">image_file</span><span class="p">);</span>
</pre></div>
</div>
<p>Image preprocess:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create the CPU operator provided by MindData to get the function object</span>
<span class="n">ms</span><span class="o">::</span><span class="n">dataset</span><span class="o">::</span><span class="n">Execute</span><span class="w"> </span><span class="n">preprocessor</span><span class="p">({</span><span class="n">ms</span><span class="o">::</span><span class="n">dataset</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Decode</span><span class="p">(),</span><span class="w">  </span><span class="c1">// Decode the input to RGB format</span>
<span class="w">                                   </span><span class="n">ms</span><span class="o">::</span><span class="n">dataset</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Resize</span><span class="p">({</span><span class="mi">256</span><span class="p">}),</span><span class="w">  </span><span class="c1">// Resize the image to the given size</span>
<span class="w">                                   </span><span class="n">ms</span><span class="o">::</span><span class="n">dataset</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Normalize</span><span class="p">({</span><span class="mf">0.485</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.456</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.406</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">},</span>
<span class="w">                                                                  </span><span class="p">{</span><span class="mf">0.229</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.224</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.225</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">}),</span><span class="w">  </span><span class="c1">// Normalize the input</span>
<span class="w">                                   </span><span class="n">ms</span><span class="o">::</span><span class="n">dataset</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">CenterCrop</span><span class="p">({</span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">}),</span><span class="w">  </span><span class="c1">// Crop the input image at the center</span>
<span class="w">                                   </span><span class="n">ms</span><span class="o">::</span><span class="n">dataset</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">HWC2CHW</span><span class="p">(),</span><span class="w">  </span><span class="c1">// shape (H, W, C) to shape(C, H, W)</span>
<span class="w">                                  </span><span class="p">});</span>
<span class="c1">// Call the function object to get the processed image</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">preprocessor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">image</span><span class="p">);</span>
</pre></div>
</div>
<p>Execute the model:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create outputs vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="c1">// Create inputs vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span>
<span class="w">                    </span><span class="n">image</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">image</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span>
<span class="c1">// Call the Predict function of Model for inference</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
<p>Print the result:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Output the maximum probability to the screen</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Image: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">image_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; infer result: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">GetMax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="introduce-to-building-script">
<h2>Introduce to Building Script<a class="headerlink" href="#introduce-to-building-script" title="Permalink to this headline"></a></h2>
<p>The building script is used to building applications: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample/CMakeLists.txt">https://gitee.com/mindspore/docs/blob/r1.1/tutorials/tutorial_code/ascend310_resnet50_preprocess_sample/CMakeLists.txt</a>.</p>
<p>Since MindSpore uses the <a class="reference external" href="https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html">old C++ ABI</a>, applications must be the same with MindSpore, otherwise the building will fail.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_compile_definitions</span><span class="p">(</span><span class="s">_GLIBCXX_USE_CXX11_ABI=0</span><span class="p">)</span>
<span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_CXX_STANDARD</span><span class="w"> </span><span class="s">17</span><span class="p">)</span>
</pre></div>
</div>
<p>Add head files to gcc search path:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">option</span><span class="p">(</span><span class="s">MINDSPORE_PATH</span><span class="w"> </span><span class="s2">&quot;mindspore install path&quot;</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/include</span><span class="p">)</span>
</pre></div>
</div>
<p>Find the shared libraries in MindSpore:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">find_library</span><span class="p">(</span><span class="s">MS_LIB</span><span class="w"> </span><span class="s">libmindspore.so</span><span class="w"> </span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>
<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span><span class="w"> </span><span class="s">MD_LIB</span><span class="w"> </span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/_c_dataengine*</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the source files to generate the target executable file, and link the MindSpore libraries for the executable file:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">resnet50_sample</span><span class="w"> </span><span class="s">main.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">resnet50_sample</span><span class="w"> </span><span class="o">${</span><span class="nv">MS_LIB</span><span class="o">}</span><span class="w"> </span><span class="o">${</span><span class="nv">MD_LIB</span><span class="o">}</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="building-inference-code">
<h2>Building Inference Code<a class="headerlink" href="#building-inference-code" title="Permalink to this headline"></a></h2>
<p>Go to the project directory <code class="docutils literal notranslate"><span class="pre">ascend310_resnet50_preprocess_sample</span></code> and set the following environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># control log level. 0-DEBUG, 1-INFO, 2-WARNING, 3-ERROR, default level is WARNING.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>

<span class="c1"># Conda environmental options</span>
<span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend<span class="w"> </span><span class="c1"># the root directory of run package</span>

<span class="c1"># lib libraries that the run package depends on</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/acllib/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/atc/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/opp/op_impl/built-in/ai_core/tbe/op_tiling:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># lib libraries that the mindspore depends on, modify &quot;pip3&quot; according to the actual situation</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="sb">`</span>pip3<span class="w"> </span>show<span class="w"> </span>mindspore-ascend<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Location<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $2&quot;/mindspore/lib&quot;}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>realpath<span class="sb">`</span>:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># Environment variables that must be configured</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TBE_IMPL_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe<span class="w">            </span><span class="c1"># TBE operator implementation tool path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp<span class="w">                                       </span><span class="c1"># OPP path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/atc/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span><span class="w">                       </span><span class="c1"># TBE operator compilation tool path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">TBE_IMPL_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span><span class="w">                                                       </span><span class="c1"># Python library that TBE implementation depends on</span>
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">cmake</span></code> command, modify <code class="docutils literal notranslate"><span class="pre">pip3</span></code> according to the actual situation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>.<span class="w"> </span>-DMINDSPORE_PATH<span class="o">=</span><span class="sb">`</span>pip3<span class="w"> </span>show<span class="w"> </span>mindspore-ascend<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Location<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $2&quot;/mindspore&quot;}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>realpath<span class="sb">`</span>
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">make</span></code> command for building.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
<p>After building, the executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file is generated in <code class="docutils literal notranslate"><span class="pre">ascend310_resnet50_preprocess_sample</span></code>.</p>
</section>
<section id="performing-inference-and-viewing-the-result">
<h2>Performing Inference and Viewing the Result<a class="headerlink" href="#performing-inference-and-viewing-the-result" title="Permalink to this headline"></a></h2>
<p>Log in to the Atlas 200 DK developer board, and create the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory for storing the MindIR file <code class="docutils literal notranslate"><span class="pre">resnet50_imagenet.mindir</span></code>, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/ascend310_resnet50_preprocess_sample/model</span></code>.
Create the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory to store images, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/ascend310_resnet50_preprocess_sample/test_data</span></code>.
Then, perform the inference.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./resnet50_sample
</pre></div>
</div>
<p>Inference is performed on all images stored in the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory. For example, if there are 9 images whose label is 0 in the <a class="reference external" href="http://image-net.org/download-images">ImageNet2012</a> validation set, the inference result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Image: ./test_data/ILSVRC2012_val_00002138.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00003014.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00006697.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00007197.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009111.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009191.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009346.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009379.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009396.JPEG infer result: 0
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="multi_platform_inference_ascend_310_air.html" class="btn btn-neutral float-left" title="Inference on the Ascend 310 AI Processor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="multi_platform_inference_gpu.html" class="btn btn-neutral float-right" title="Inference on a GPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>