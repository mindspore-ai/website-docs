

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MindSpore Serving-based Inference Service Deployment &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MindSpore Serving-based Distributed Inference Service Deployment" href="serving_distributed_example.html" />
    <link rel="prev" title="Inference on a CPU" href="multi_platform_inference_cpu.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Inference Model</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/docs/en?master">On-Device Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference Service</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">MindSpore Serving-based Inference Service Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#preparing-the-environment">Preparing the Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exporting-the-model">Exporting the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploying-the-serving-inference-service">Deploying the Serving Inference Service</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lightweight-deployment">Lightweight Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-deployment">Cluster Deployment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#inference-execution">Inference Execution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="serving_distributed_example.html">MindSpore Serving-based Distributed Inference Service Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_grpc.html">gRPC-based MindSpore Serving Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_restful.html">RESTful-based MindSpore Serving Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="serving_model.html">Servable Provided Through Model Configuration</a></li>
</ul>
<p class="caption"><span class="caption-text">Application Practice</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="nlp_tprr.html">Multi-hop Knowledge Reasoning Question-answering Model TPRR</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>MindSpore Serving-based Inference Service Deployment</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/serving_example.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-serving-based-inference-service-deployment">
<h1>MindSpore Serving-based Inference Service Deployment<a class="headerlink" href="#mindspore-serving-based-inference-service-deployment" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Serving</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<!-- TOC -->
<ul class="simple">
<li><p><a class="reference external" href="#mindspore-serving-based-inference-service-deployment">MindSpore Serving-based Inference Service Deployment</a></p>
<ul>
<li><p><a class="reference external" href="#overview">Overview</a></p>
<ul>
<li><p><a class="reference external" href="#preparing-the-environment">Preparing the Environment</a></p></li>
<li><p><a class="reference external" href="#exporting-the-model">Exporting the Model</a></p></li>
<li><p><a class="reference external" href="#deploying-the-serving-inference-service">Deploying the Serving Inference Service</a></p>
<ul>
<li><p><a class="reference external" href="#lightweight-deployment">Lightweight Deployment</a></p></li>
<li><p><a class="reference external" href="#cluster-deployment">Cluster Deployment</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#inference-execution">Inference Execution</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://gitee.com/mindspore/docs/blob/r1.2/tutorials/inference/source_en/serving_example.md" target="_blank"><img src="_static/logo_source.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Serving is a lightweight and high-performance service module that helps MindSpore developers efficiently deploy online inference services in the production environment. After completing model training on MindSpore, you can export the MindSpore model and use MindSpore Serving to create an inference service for the model.</p>
<p>The following uses a simple <code class="docutils literal notranslate"><span class="pre">Add</span></code> network as an example to describe how to use MindSpore Serving.</p>
<div class="section" id="preparing-the-environment">
<h3>Preparing the Environment<a class="headerlink" href="#preparing-the-environment" title="Permalink to this headline">¶</a></h3>
<p>Before running the sample network, ensure that MindSpore Serving has been properly installed. To install MindSpore Serving on your PC, go to the <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.2/README.md#installing-serving">MindSpore Serving installation page</a> and configure environment variables on the <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.2/README.md#configuring-environment-variables">MindSpore Serving environment configuration page</a>.</p>
</div>
<div class="section" id="exporting-the-model">
<h3>Exporting the Model<a class="headerlink" href="#exporting-the-model" title="Permalink to this headline">¶</a></h3>
<p>Use <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.2/example/add/export_model/add_model.py">add_model.py</a> to build a network with only the Add operator and export the MindSpore inference deployment model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">shutil</span> <span class="kn">import</span> <span class="n">copyfile</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Define Net of add&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;construct add net&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">export_net</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Export add net of 2x2 + 2x2, and copy output model `tensor_add.mindir` to directory ../add/1&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">add</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">file_name</span><span class="o">=</span><span class="s1">&#39;tensor_add&#39;</span><span class="p">,</span> <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;MINDIR&#39;</span><span class="p">)</span>
    <span class="n">dst_dir</span> <span class="o">=</span> <span class="s1">&#39;../add/1&#39;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">dst_dir</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="n">dst_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dst_dir</span><span class="p">,</span> <span class="s1">&#39;tensor_add.mindir&#39;</span><span class="p">)</span>
    <span class="n">copyfile</span><span class="p">(</span><span class="s1">&#39;tensor_add.mindir&#39;</span><span class="p">,</span> <span class="n">dst_file</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;copy tensor_add.mindir to &quot;</span> <span class="o">+</span> <span class="n">dst_dir</span> <span class="o">+</span> <span class="s2">&quot; success&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">export_net</span><span class="p">()</span>
</pre></div>
</div>
<p>To use MindSpore for neural network definition, inherit <code class="docutils literal notranslate"><span class="pre">mindspore.nn.Cell</span></code>. (A <code class="docutils literal notranslate"><span class="pre">Cell</span></code> is a base class of all neural networks.) Define each layer of a neural network in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method in advance, and then define the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method to complete the forward construction of the neural network. Use <code class="docutils literal notranslate"><span class="pre">export</span></code> of the <code class="docutils literal notranslate"><span class="pre">mindspore</span></code> module to export the model file.
For more detailed examples, see <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.2/quick_start/quick_start.html">Implementing an Image Classification Application</a>.</p>
<p>Execute the <code class="docutils literal notranslate"><span class="pre">add_model.py</span></code> script to generate the <code class="docutils literal notranslate"><span class="pre">tensor_add.mindir</span></code> file. The input of the model is two 2D tensors with shape [2,2], and the output is the sum of the two input tensors.</p>
</div>
<div class="section" id="deploying-the-serving-inference-service">
<h3>Deploying the Serving Inference Service<a class="headerlink" href="#deploying-the-serving-inference-service" title="Permalink to this headline">¶</a></h3>
<p>Start Serving with the following files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>test_dir
├── add/
│    └── servable_config.py
│    └── <span class="m">1</span>/
│        └── tensor_add.mindir
└── master_with_worker.py
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">master_with_worker.py</span></code>: Script file for starting the service.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add</span></code>: Model folder, which is named after the model name.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_add.mindir</span></code>: Model file generated by the network in the previous step, which is stored in folder 1 (the number indicates the version number). Different versions are stored in different folders. The version number must be a string of digits. By default, the latest model file is started.</p></li>
<li><p><a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.2/example/add/add/servable_config.py">servable_config.py</a>: <a class="reference external" href="https://www.mindspore.cn/tutorial/inference/en/r1.2/serving_model.html">Model configuration file</a>, which defines the model processing functions, including the <code class="docutils literal notranslate"><span class="pre">add_common</span></code> and <code class="docutils literal notranslate"><span class="pre">add_cast</span></code> methods. <code class="docutils literal notranslate"><span class="pre">add_common</span></code> defines an addition operation whose input is two pieces of float32 data, and <code class="docutils literal notranslate"><span class="pre">add_cast</span></code> defines an addition operation whose input is data with its type converted to float32.</p></li>
</ul>
<p>Content of the configuration file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.worker</span> <span class="kn">import</span> <span class="n">register</span>


<span class="k">def</span> <span class="nf">add_trans_datatype</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;define preprocess, this example has one input and one output&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x1</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">x2</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="c1"># when with_batch_dim is set to False, only 2x2 add is supported</span>
<span class="c1"># when with_batch_dim is set to True(default), Nx2 add is supported, while N is viewed as batch</span>
<span class="c1"># float32 inputs/outputs</span>
<span class="n">register</span><span class="o">.</span><span class="n">declare_servable</span><span class="p">(</span><span class="n">servable_file</span><span class="o">=</span><span class="s2">&quot;tensor_add.mindir&quot;</span><span class="p">,</span> <span class="n">model_format</span><span class="o">=</span><span class="s2">&quot;MindIR&quot;</span><span class="p">,</span> <span class="n">with_batch_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="c1"># register add_common method in add</span>
<span class="nd">@register</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">add_common</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>  <span class="c1"># only support float32 inputs</span>
    <span class="sd">&quot;&quot;&quot;method add_common data flow definition, only call model servable&quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">register</span><span class="o">.</span><span class="n">call_servable</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>


<span class="c1"># register add_cast method in add</span>
<span class="nd">@register</span><span class="o">.</span><span class="n">register_method</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">add_cast</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;method add_cast data flow definition, only call preprocess and model servable&quot;&quot;&quot;</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">register</span><span class="o">.</span><span class="n">call_preprocess</span><span class="p">(</span><span class="n">add_trans_datatype</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>  <span class="c1"># cast input to float32</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">register</span><span class="o">.</span><span class="n">call_servable</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<p>MindSpore Serving provides both lightweight deployment and cluster deployment. In lightweight deployment mode, the master and worker nodes are deployed in the same process. In cluster deployment mode, the master and worker nodes are deployed in different processes. If there is only one worker node, you can consider lightweight deployment, that is, deploy the master node in the process where the worker node is located. If there are multiple worker nodes, you can deploy them in a cluster and use one of them as the master node to manage all worker nodes. You can select the deployment mode based on the actual requirements.</p>
<div class="section" id="lightweight-deployment">
<h4>Lightweight Deployment<a class="headerlink" href="#lightweight-deployment" title="Permalink to this headline">¶</a></h4>
<p>The server calls a Python API to start the inference process shared by both master and worker nodes. The client directly connects to the inference service and delivers an inference task.
Run the <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.2/example/add/master_with_worker.py">master_with_worker.py</a> script to deploy lightweight service:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore_serving</span> <span class="kn">import</span> <span class="n">master</span>
<span class="kn">from</span> <span class="nn">mindspore_serving</span> <span class="kn">import</span> <span class="n">worker</span>

<span class="k">def</span> <span class="nf">start</span><span class="p">():</span>
    <span class="n">servable_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">worker</span><span class="o">.</span><span class="n">start_servable_in_master</span><span class="p">(</span><span class="n">servable_dir</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">master</span><span class="o">.</span><span class="n">start_grpc_server</span><span class="p">(</span><span class="s2">&quot;127.0.0.1&quot;</span><span class="p">,</span> <span class="mi">5500</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>If the server prints the <code class="docutils literal notranslate"><span class="pre">Serving</span> <span class="pre">gRPC</span> <span class="pre">start</span> <span class="pre">success,</span> <span class="pre">listening</span> <span class="pre">on</span> <span class="pre">0.0.0.0:5500</span></code> log, the Serving has loaded the inference model.</p>
</div>
<div class="section" id="cluster-deployment">
<h4>Cluster Deployment<a class="headerlink" href="#cluster-deployment" title="Permalink to this headline">¶</a></h4>
<p>The server consists of the master and worker processes. The master process manages all worker nodes in the cluster and distributes inference tasks. The cluster deployment is as follows:</p>
<p>Master deployment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore_serving</span> <span class="kn">import</span> <span class="n">master</span>

<span class="k">def</span> <span class="nf">start</span><span class="p">():</span>
    <span class="n">servable_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">master</span><span class="o">.</span><span class="n">start_grpc_server</span><span class="p">(</span><span class="s2">&quot;127.0.0.1&quot;</span><span class="p">,</span> <span class="mi">5500</span><span class="p">)</span>
    <span class="n">master</span><span class="o">.</span><span class="n">start_master_server</span><span class="p">(</span><span class="s2">&quot;127.0.0.1&quot;</span><span class="p">,</span> <span class="mi">6500</span><span class="p">)</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>Worker deployment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore_serving</span> <span class="kn">import</span> <span class="n">worker</span>

<span class="k">def</span> <span class="nf">start</span><span class="p">():</span>
    <span class="n">servable_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">worker</span><span class="o">.</span><span class="n">start_servable</span><span class="p">(</span><span class="n">servable_dir</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                          <span class="n">master_ip</span><span class="o">=</span><span class="s2">&quot;127.0.0.1&quot;</span><span class="p">,</span> <span class="n">master_port</span><span class="o">=</span><span class="mi">6500</span><span class="p">,</span>
                          <span class="n">worker_ip</span><span class="o">=</span><span class="s2">&quot;127.0.0.1&quot;</span><span class="p">,</span> <span class="n">worker_port</span><span class="o">=</span><span class="mi">6600</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>The lightweight and the cluster deployment modes use different APIs to start a worker. The <code class="docutils literal notranslate"><span class="pre">start_servable_in_master</span></code> API is used in lightweight deployment mode, while the <code class="docutils literal notranslate"><span class="pre">start_servable</span></code> API is used in cluster deployment mode.</p>
</div>
</div>
<div class="section" id="inference-execution">
<h3>Inference Execution<a class="headerlink" href="#inference-execution" title="Permalink to this headline">¶</a></h3>
<p>The client can access the inference service through either <a class="reference external" href="https://www.mindspore.cn/tutorial/inference/en/r1.2/serving_grpc.html">gRPC</a> or <a class="reference external" href="https://www.mindspore.cn/tutorial/inference/en/r1.2/serving_restful.html">RESTful</a>. The following uses gRPC as an example.
Execute <a class="reference external" href="https://gitee.com/mindspore/serving/blob/r1.2/example/add/client.py">client.py</a> to start the Python client.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore_serving.client</span> <span class="kn">import</span> <span class="n">Client</span>


<span class="k">def</span> <span class="nf">run_add_common</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;invoke servable add method add_common&quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span> <span class="mi">5500</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;add_common&quot;</span><span class="p">)</span>
    <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># instance 1</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;x1&quot;</span><span class="p">:</span> <span class="n">x1</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">:</span> <span class="n">x2</span><span class="p">})</span>

    <span class="c1"># instance 2</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;x1&quot;</span><span class="p">:</span> <span class="n">x1</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">:</span> <span class="n">x2</span><span class="p">})</span>

    <span class="c1"># instance 3</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;x1&quot;</span><span class="p">:</span> <span class="n">x1</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">:</span> <span class="n">x2</span><span class="p">})</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">run_add_cast</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;invoke servable add method add_cast&quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span> <span class="mi">5500</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;add_cast&quot;</span><span class="p">)</span>
    <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;x1&quot;</span><span class="p">:</span> <span class="n">x1</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">:</span> <span class="n">x2</span><span class="p">})</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">run_add_common</span><span class="p">()</span>
    <span class="n">run_add_cast</span><span class="p">()</span>
</pre></div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">Client</span></code> class defined by <code class="docutils literal notranslate"><span class="pre">mindspore_serving.client</span></code>. The client defines two cases to call two model methods. In the <code class="docutils literal notranslate"><span class="pre">run_add_common</span></code> case with three pairs of float32 arrays, each pair of arrays are added up. In the <code class="docutils literal notranslate"><span class="pre">run_add_cast</span></code> case, two int32 arrays are added up. If the results of the two cases are displayed as follows, the Serving has properly executed the <code class="docutils literal notranslate"><span class="pre">Add</span></code> network inference.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">[{</span><span class="s1">&#39;y&#39;</span>: array<span class="o">([[</span><span class="m">2</span>. , <span class="m">2</span>.<span class="o">]</span>,
        <span class="o">[</span><span class="m">2</span>.,  <span class="m">2</span>.<span class="o">]]</span>, <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)}</span>,<span class="o">{</span><span class="s1">&#39;y&#39;</span>: array<span class="o">([[</span><span class="m">4</span>. , <span class="m">4</span>.<span class="o">]</span>,
        <span class="o">[</span><span class="m">4</span>.,  <span class="m">4</span>.<span class="o">]]</span>, <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)}</span>,<span class="o">{</span><span class="s1">&#39;y&#39;</span>: array<span class="o">([[</span><span class="m">6</span>. , <span class="m">6</span>.<span class="o">]</span>,
        <span class="o">[</span><span class="m">6</span>.,  <span class="m">6</span>.<span class="o">]]</span>, <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)}]</span>
<span class="o">[{</span><span class="s1">&#39;y&#39;</span>: array<span class="o">([[</span><span class="m">2</span>. , <span class="m">2</span>.<span class="o">]</span>,
        <span class="o">[</span><span class="m">2</span>.,  <span class="m">2</span>.<span class="o">]]</span>, <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)}]</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="serving_distributed_example.html" class="btn btn-neutral float-right" title="MindSpore Serving-based Distributed Inference Service Deployment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="multi_platform_inference_cpu.html" class="btn btn-neutral float-left" title="Inference on a CPU" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>