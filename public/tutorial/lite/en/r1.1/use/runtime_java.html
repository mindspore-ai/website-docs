<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Runtime for Model Inference (Java) &mdash; MindSpore Lite r1.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Other Tools" href="tools.html" />
    <link rel="prev" title="Using Runtime for Model Inference (C++)" href="runtime_cpp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Training a LeNet Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Optimizing the Model (Quantization After Training)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Executing Model Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_cpp.html">Using Runtime for Model Inference (C++)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Runtime for Model Inference (Java)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#android-project-references-aar-package">Android project references AAR package</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-mindspore-lite-inference-framework">Running MindSpore Lite inference framework</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#loading-model">Loading Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-configuration-context">Creating Configuration Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-session">Creating Session</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compiling-graphs">Compiling Graphs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-data">Setting Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#graph-execution">Graph Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-output">Getting Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="#releasing-memory">Releasing Memory</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#example-of-android-project-using-mindspore-lite-inference-framework">Example of Android project using MindSpore Lite inference framework</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Other Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools_train.html">Other Tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">Executing Model Inference</a> &raquo;</li>
      <li>Using Runtime for Model Inference (Java)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_java.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-runtime-for-model-inference-java">
<h1>Using Runtime for Model Inference (Java)<a class="headerlink" href="#using-runtime-for-model-inference-java" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Android</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Loading</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/lite/source_en/use/runtime_java.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>After model conversion using MindSpore Lite, the model inference process needs to be completed in Runtime. This tutorial introduces how to use Java API to write inference code.</p>
<blockquote>
<div><p>For more details for Java API, please refer to <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/index.html">API Docs</a>.</p>
</div></blockquote>
</section>
<section id="android-project-references-aar-package">
<h2>Android project references AAR package<a class="headerlink" href="#android-project-references-aar-package" title="Permalink to this headline"></a></h2>
<p>First copy the <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}.aar</span></code> file to the <strong>libs</strong> directory of the target module, then add the local reference directory to the <code class="docutils literal notranslate"><span class="pre">repositories</span></code> of the target module’s <code class="docutils literal notranslate"><span class="pre">build.gradle</span></code>, and finally, add the AAR package in the <code class="docutils literal notranslate"><span class="pre">dependencies</span></code>. The details are as follows.</p>
<div class="highlight-groovy notranslate"><div class="highlight"><pre><span></span><span class="n">repositories</span><span class="w"> </span><span class="o">{</span>
<span class="w">    </span><span class="n">flatDir</span><span class="w"> </span><span class="o">{</span>
<span class="w">        </span><span class="n">dirs</span><span class="w"> </span><span class="s1">&#39;libs&#39;</span>
<span class="w">    </span><span class="o">}</span>
<span class="o">}</span>

<span class="n">dependencies</span><span class="w"> </span><span class="o">{</span>
<span class="w">    </span><span class="n">implementation</span><span class="o">(</span><span class="nl">name:</span><span class="s1">&#39;mindspore-lite-{version}&#39;</span><span class="o">,</span><span class="w"> </span><span class="nl">ext:</span><span class="s1">&#39;aar&#39;</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>
</div>
<blockquote>
<div><p>mindspore-lite-{version} is the name of the AAR file, you need to replace {version} with the corresponding version information.</p>
</div></blockquote>
</section>
<section id="running-mindspore-lite-inference-framework">
<h2>Running MindSpore Lite inference framework<a class="headerlink" href="#running-mindspore-lite-inference-framework" title="Permalink to this headline"></a></h2>
<p>Using MindSpore Lite in the Android project, you can choose to use C++ APIs or Java APIs to run the inference framework. Compared with C++ APIs, Java APIs can be called directly in Java Class without the need to implement the relevant code of the JNI layer, which is more convenient. Running the MindSpore Lite inference framework mainly includes the following steps:</p>
<ol class="arabic simple">
<li><p>Loading model: Read the MindSpore Lite model from the file system and parse the model.</p></li>
<li><p>Creating configuration context:  <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> saves some basic configuration parameters required by the session, which is used to guide graph compilation and graph execution. <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> mainly include <code class="docutils literal notranslate"><span class="pre">deviceType</span></code>: device type, <code class="docutils literal notranslate"><span class="pre">threadNum</span></code>: number of threads, <code class="docutils literal notranslate"><span class="pre">cpuBindMode</span></code>: CPU binding mode, and <code class="docutils literal notranslate"><span class="pre">enable_float16</span></code>: whether to use float16 operator as priority.</p></li>
<li><p>Creating session: Create the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> and call the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#init">init</a> method to configure the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> obtained in the previous step into the session.</p></li>
<li><p>Compiling graphs: Before graph execution, call the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#compilegraph">compileGraph</a> API of the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> to compile graphs, mainly for subgraph split and operator selection and scheduling. This process takes a long time. Therefore, it is recommended that <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> achieves multiple executions with one creation and one compilation.</p></li>
<li><p>Setting data: Before the graph is executed, data needs to be set in the input Tensor.</p></li>
<li><p>Graph execution: Run model inference using <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#rungraph">runGraph</a> of <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a>.</p></li>
<li><p>Getting output: After the execution of the graph is finished, the inference result can be obtained by output Tensor.</p></li>
<li><p>Releasing memory: When you finishing using the MindSpore Lite inference framework, you need to release the created <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> and <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/model.html#model">model</a>.</p></li>
</ol>
<section id="loading-model">
<h3>Loading Model<a class="headerlink" href="#loading-model" title="Permalink to this headline"></a></h3>
<p>When MindSpore Lite runs model inference, it is necessary to load the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model converted by the model conversion tool from the file system and perform model analysis.  <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/model.html#model">model</a> class provides <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/model.html#loadmodel">loadModel</a> so that it can load models from <code class="docutils literal notranslate"><span class="pre">Assets</span></code> or other file paths.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load the .ms model.</span>
<span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model</span><span class="p">.</span><span class="na">loadModel</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;model.ms&quot;</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Log</span><span class="p">.</span><span class="na">e</span><span class="p">(</span><span class="s">&quot;MS_LITE&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Load Model failed&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="kc">false</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="creating-configuration-context">
<h3>Creating Configuration Context<a class="headerlink" href="#creating-configuration-context" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> saves some basic configuration parameters required by the session, which is used to guide graph compilation and graph execution</p>
<p>MindSpore Lite supports heterogeneous inference. The preferred backend for inference is specified by <code class="docutils literal notranslate"><span class="pre">deviceType</span></code> in <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> and CPU and GPU is supported. During graph compilation, operator selection and scheduling are performed based on the preferred backend.</p>
<p>MindSpore Lite has a built-in thread pool shared by processes. During inference, <code class="docutils literal notranslate"><span class="pre">threadNum</span></code> is used to specify the maximum number of threads in the thread pool. The default maximum number is 2. It is recommended that the maximum number does not exceed 4. Otherwise, the performance may be affected.</p>
<p>MindSpore Lite supports the float16 operator mode for reasoning. If <code class="docutils literal notranslate"><span class="pre">enable</span> <span class="pre">float16</span></code> is set as <code class="docutils literal notranslate"><span class="pre">true</span></code>, the float16 operator will be used first.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create and init config.</span>
<span class="n">MSConfig</span><span class="w"> </span><span class="n">msConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSConfig</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">msConfig</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_CPU</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">CpuBindMode</span><span class="p">.</span><span class="na">MID_CPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Log</span><span class="p">.</span><span class="na">e</span><span class="p">(</span><span class="s">&quot;MS_LITE&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Init context failed&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="kc">false</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="creating-session">
<h3>Creating Session<a class="headerlink" href="#creating-session" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> is the main entrance of inference, graph compilation and graph execution can be done through <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a>. Create a <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> and call the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#init">init</a> method to configure the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> obtained in the previous step into the session. After <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> is initialized, <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/msconfig.html#msconfig">MSConfig</a> can be released.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create the MindSpore lite session.</span>
<span class="n">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">LiteSession</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">session</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="n">msConfig</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Log</span><span class="p">.</span><span class="na">e</span><span class="p">(</span><span class="s">&quot;MS_LITE&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Create session failed&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">msConfig</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="kc">false</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">msConfig</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="compiling-graphs">
<h3>Compiling Graphs<a class="headerlink" href="#compiling-graphs" title="Permalink to this headline"></a></h3>
<p>Before graph execution, call the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#compilegraph">compileGraph</a> API of the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> to compile graphs and further parse the Model instance loaded from the file, mainly for subgraph split and operator selection and scheduling. This process takes a long time. Therefore, it is recommended that <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a> achieves multiple executions with one creation and one compilation. After the graph is compiled, you can call the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/model.html#freebuffer">freeBuffer</a> function of the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/model.html#model">model</a> to release the MetaGraph in the MindSpore Lite Model, which is used to reduce the runtime memory, but the model cannot be compiled again after being released.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Compile graph.</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">session</span><span class="p">.</span><span class="na">compileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Log</span><span class="p">.</span><span class="na">e</span><span class="p">(</span><span class="s">&quot;MS_LITE&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Compile graph failed&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="na">freeBuffer</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="kc">false</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Note: when use model.freeBuffer(), the model can not be compiled.</span>
<span class="n">model</span><span class="p">.</span><span class="na">freeBuffer</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="setting-data">
<h3>Setting Data<a class="headerlink" href="#setting-data" title="Permalink to this headline"></a></h3>
<p>Java currently supports two types of data: <code class="docutils literal notranslate"><span class="pre">byte[]</span></code> or <code class="docutils literal notranslate"><span class="pre">ByteBuffer</span></code>, and set the input Tensor data.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Set input tensor values.</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="p">.</span><span class="na">getInputs</span><span class="p">();</span>
<span class="n">MSTensor</span><span class="w"> </span><span class="n">inTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="na">get</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="kt">byte</span><span class="o">[]</span><span class="w"> </span><span class="n">inData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">readFileFromAssets</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;model_inputs.bin&quot;</span><span class="p">);</span>
<span class="n">inTensor</span><span class="p">.</span><span class="na">setData</span><span class="p">(</span><span class="n">inData</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="graph-execution">
<h3>Graph Execution<a class="headerlink" href="#graph-execution" title="Permalink to this headline"></a></h3>
<p>Run model inference using <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#rungraph">runGraph</a> of <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">LiteSession</a>.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Run graph to infer results.</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">session</span><span class="p">.</span><span class="na">runGraph</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Log</span><span class="p">.</span><span class="na">e</span><span class="p">(</span><span class="s">&quot;MS_LITE&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Run graph failed&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="getting-output">
<h3>Getting Output<a class="headerlink" href="#getting-output" title="Permalink to this headline"></a></h3>
<p>After the inference is finished, the inference result can be obtained by output Tensor. The data types currently supported by the output tensor include <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">long</span></code>, and <code class="docutils literal notranslate"><span class="pre">byte</span></code>.</p>
<ul class="simple">
<li><p>There are three ways to obtain the output Tensor:</p>
<ul>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#getoutputmapbytensor">getOutputMapByTensor</a> method to directly obtain the mapping between the names of all model output tensors and the model output <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/mstensor.html#mstensor">MSTensor</a>.</p></li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#getoutputsbynodename">getOutputsByNodeName</a> method to obtain vectors of the model output <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/mstensor.html#mstensor">MSTensor</a> that is connected to the model output node based on the node name.</p></li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#getoutputbytensorname">getOutputByTensorName</a> method to obtain the model output <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/mstensor.html#mstensor">MSTensor</a> based on the tensor name.</p></li>
</ul>
</li>
</ul>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span>// Get output tensor values.
List&lt;String&gt; tensorNames = session.getOutputTensorNames();
Map&lt;String, MSTensor&gt; outputs = session.getOutputMapByTensor();
Set&lt;Map.Entry&lt;String, MSTensor&gt;&gt; entries = outputs.entrySet();
for (String tensorName : tensorNames) {
    MSTensor output = outputs.get(tensorName);
    if (output == null) {
        Log.e(&quot;MS_LITE&quot;, &quot;Can not find output &quot; + tensorName);
        return;
    }
    float[] results = output.getFloatData();

    // Apply infer results.
    ……
}
</pre></div>
</div>
</section>
<section id="releasing-memory">
<h3>Releasing Memory<a class="headerlink" href="#releasing-memory" title="Permalink to this headline"></a></h3>
<p>When you finish using the MindSpore Lite inference framework, you need to release the created <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/lite_session.html#litesession">session</a> and <a class="reference external" href="https://www.mindspore.cn/doc/api_java/en/r1.1/model.html#model">model</a>.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="kd">private</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">free</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">session</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="example-of-android-project-using-mindspore-lite-inference-framework">
<h2>Example of Android project using MindSpore Lite inference framework<a class="headerlink" href="#example-of-android-project-using-mindspore-lite-inference-framework" title="Permalink to this headline"></a></h2>
<p>Reasoning using the MindSpore Lite Java API mainly includes the steps of <code class="docutils literal notranslate"><span class="pre">Loading</span> <span class="pre">Model</span></code>, <code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">configuration</span> <span class="pre">context</span></code>, <code class="docutils literal notranslate"><span class="pre">Creating</span> <span class="pre">sessions</span></code>, <code class="docutils literal notranslate"><span class="pre">Compiling</span> <span class="pre">Graphs</span></code>, <code class="docutils literal notranslate"><span class="pre">Setting</span> <span class="pre">Data</span></code>, <code class="docutils literal notranslate"><span class="pre">Graph</span> <span class="pre">Execution</span></code>, <code class="docutils literal notranslate"><span class="pre">Getting</span> <span class="pre">Output</span></code>, and <code class="docutils literal notranslate"><span class="pre">Releasing</span> <span class="pre">Memory</span></code>.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span>private boolean init(Context context) {
    // Load the .ms model.
    model = new Model();
    if (!model.loadModel(context, &quot;model.ms&quot;)) {
        Log.e(&quot;MS_LITE&quot;, &quot;Load Model failed&quot;);
        return false;
    }

    // Create and init config.
    MSConfig msConfig = new MSConfig();
    if (!msConfig.init(DeviceType.DT_CPU, 2, CpuBindMode.MID_CPU)) {
        Log.e(&quot;MS_LITE&quot;, &quot;Init context failed&quot;);
        return false;
    }

    // Create the MindSpore lite session.
    session = new LiteSession();
    if (!session.init(msConfig)) {
        Log.e(&quot;MS_LITE&quot;, &quot;Create session failed&quot;);
        msConfig.free();
        return false;
    }
    msConfig.free();

    // Compile graph.
    if (!session.compileGraph(model)) {
        Log.e(&quot;MS_LITE&quot;, &quot;Compile graph failed&quot;);
        model.freeBuffer();
        return false;
    }

    // Note: when use model.freeBuffer(), the model can not be compiled.
    model.freeBuffer();

    return true;
}

private void DoInference(Context context) {
    // Set input tensor values.
    List&lt;MSTensor&gt; inputs = session.getInputs();
    byte[] inData = readFileFromAssets(context, &quot;model_inputs.bin&quot;);
    MSTensor inTensor = inputs.get(0);
    inTensor.setData(inData);

    // Run graph to infer results.
    if (!session.runGraph()) {
        Log.e(&quot;MS_LITE&quot;, &quot;Run graph failed&quot;);
        return;
    }

    // Get output tensor values.
    List&lt;String&gt; tensorNames = session.getOutputTensorNames();
    Map&lt;String, MSTensor&gt; outputs = session.getOutputMapByTensor();
    Set&lt;Map.Entry&lt;String, MSTensor&gt;&gt; entries = outputs.entrySet();
    for (String tensorName : tensorNames) {
        MSTensor output = outputs.get(tensorName);
        if (output == null) {
            Log.e(&quot;MS_LITE&quot;, &quot;Can not find output &quot; + tensorName);
            return;
        }
        float[] results = output.getFloatData();

        // Apply infer results.
        ……
    }
}

// Note: we must release the memory at the end, otherwise it will cause the memory leak.
private void free() {
    session.free();
    model.free();
}
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_cpp.html" class="btn btn-neutral float-left" title="Using Runtime for Model Inference (C++)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tools.html" class="btn btn-neutral float-right" title="Other Tools" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>