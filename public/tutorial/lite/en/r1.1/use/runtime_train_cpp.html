<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Runtime for Model Training (C++) &mdash; MindSpore Lite r1.1 documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Other Tools" href="tools_train.html" />
    <link rel="prev" title="Executing Model Training" href="runtime_train.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Training a LeNet Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Optimizing the Model (Quantization After Training)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Other Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime_train.html">Executing Model Training</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Runtime for Model Training (C++)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#session-creation">Session Creation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reading-models">Reading Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-contexts">Creating Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-sessions">Creating Sessions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training-mode">Training Mode</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#switching-between-train-mode-and-eval-mode">Switching between Train Mode and Eval Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#data-input">Data Input</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-input-tensors">Obtaining Input Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#copying-data">Copying Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#graph-execution">Graph Execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#executing-sessions">Executing Sessions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#callback-running">Callback Running</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-outputs">Obtaining Outputs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-output-tensors">Obtaining Output Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-version-string">Obtaining Version String</a></li>
<li class="toctree-l3"><a class="reference internal" href="#saving-the-trained-model">Saving the Trained Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tools_train.html">Other Tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime_train.html">Executing Model Training</a> &raquo;</li>
      <li>Using Runtime for Model Training (C++)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_train_cpp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-runtime-for-model-training-c">
<h1>Using Runtime for Model Training (C++)<a class="headerlink" href="#using-runtime-for-model-training-c" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Android</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Training</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Loading</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/lite/source_en/use/runtime_train_cpp.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The exact training scheme is encapsulated within the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model. The software that we will discuss below is not aware of it but rather perform training and inference in a generic manner.
Following the conversion of the model on the server to an <code class="docutils literal notranslate"><span class="pre">.ms</span></code> format, the file should be downloaded to the embedded device for the ToD process.</p>
<p>A sequence diagram explaining the train sequence is shown in the image below:</p>
<p><img alt="img" src="../_images/side_train_sequence.png" /></p>
<p>In this diagram the drawn objects represents:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OS</span></code>: A software element that is responsible to access storage data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">User</span></code>: The application/object that performs the training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>: An object that is responsible to load data from the storage and perform pre-processing prior to using it in the training (e.g., reading an image, rescaling it to a given size and converting it to bitmap).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainSession</span></code>: A software module provided by MindSpore Lite, that provides flatbuffer DeSerialization into a network of nodes and interconnecting tensors. It performs graph compilation and calls the graph executor for train and inference.</p></li>
</ul>
</section>
<section id="session-creation">
<h2>Session Creation<a class="headerlink" href="#session-creation" title="Permalink to this headline"></a></h2>
<p>In MindSpore Lite framework, <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> class provides the main API to the system. Here we will see how to interact with a <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> object.</p>
<section id="reading-models">
<h3>Reading Models<a class="headerlink" href="#reading-models" title="Permalink to this headline"></a></h3>
<p>A Model file is flatbuffer-serialized file which was converted using the <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/en/r1.1/use/converter_tool.html">MindSpore Model Converter Tool</a>. These files have a <code class="docutils literal notranslate"><span class="pre">.ms</span></code> extension. Before model training and/or inference, the model needs to be loaded from the file system and parsed. Related operations are mainly implemented in the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.1/lite.html#model"><code class="docutils literal notranslate"><span class="pre">Model</span></code></a> class which holds the model data such as the network structure, tensors sizes, weights data and operators attributes.</p>
<blockquote>
<div><p>Unlike in MindSpore Lite framework, in MindSpore Lite the user is not allowed to access the <code class="docutils literal notranslate"><span class="pre">Model</span></code> object, since it is being used by <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> during training. All interaction with <code class="docutils literal notranslate"><span class="pre">Model</span></code> including instantiation, Compiling and deletion are handled within <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code>.</p>
</div></blockquote>
</section>
<section id="creating-contexts">
<h3>Creating Contexts<a class="headerlink" href="#creating-contexts" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.1/lite.html#context"><code class="docutils literal notranslate"><span class="pre">Context</span></code></a> is a Mindspore Lite Object that contains basic configuration parameters required by the sessions to guide graph compilation and execution. It allows to define the device on which to run the model, e.g., CPU or GPU, the number of threads used for training and inference and the memory allocation scheme.
Currently, only single threaded CPU device is supported by TrainSession.</p>
<p>Once the TrainSession is created with the <code class="docutils literal notranslate"><span class="pre">Context</span></code> object, it is no longer needed and can be deleted.</p>
</section>
<section id="creating-sessions">
<h3>Creating Sessions<a class="headerlink" href="#creating-sessions" title="Permalink to this headline"></a></h3>
<p>There are two methods to create a session:</p>
<ul class="simple">
<li><p>The first API allows MindSpore Lite to access the filesystem and read the model from a file, parse it, compile it and produce a valid TrainSession object. The <code class="docutils literal notranslate"><span class="pre">Context</span></code> described above is passed to the TrainSession as a basic configuration. The static function has the following signature <code class="docutils literal notranslate"><span class="pre">TrainSession</span> <span class="pre">*TrainSession::CreateSession(const</span> <span class="pre">string&amp;</span> <span class="pre">filename,</span> <span class="pre">const</span> <span class="pre">Context</span> <span class="pre">*context,</span> <span class="pre">bool</span> <span class="pre">mode)</span></code>, where <code class="docutils literal notranslate"><span class="pre">filename</span></code> is the model’s file name, context is the <code class="docutils literal notranslate"><span class="pre">Context</span></code> and mode is the initial training mode of the session (Train/Eval). On Success, a fully compiled and ready to use <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> instance is returned by the function, this instance must be freed using <code class="docutils literal notranslate"><span class="pre">delete</span></code> on the termination of the process.</p></li>
<li><p>The second API is similar to the first but uses an in-memory copy of the flatbuffer in order to create the <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code>. The static function has the following signature <code class="docutils literal notranslate"><span class="pre">TrainSession</span> <span class="pre">*TrainSession::CreateSession(const</span> <span class="pre">char*</span> <span class="pre">buf,</span> <span class="pre">size_t</span> <span class="pre">size,</span> <span class="pre">const</span> <span class="pre">Context</span> <span class="pre">*context,</span> <span class="pre">bool</span> <span class="pre">mode)</span></code>, where <code class="docutils literal notranslate"><span class="pre">buf</span></code> is a pointer to the in-memory buffer and <code class="docutils literal notranslate"><span class="pre">size</span></code> is its length. On Success, a fully compiled and ready-to-use <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> instance is returned by the function. If needed, the buf pointer can be freed immediately. The returned <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> instance must be freed using <code class="docutils literal notranslate"><span class="pre">delete</span></code> when no longer needed.</p></li>
</ul>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline"></a></h3>
<p>The following sample code demonstrates how to create a <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> that will run on a single thread in CPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/train_session.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/context.h&quot;</span>

<span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="n">context</span><span class="p">.</span><span class="n">device_list_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">device_info_</span><span class="p">.</span><span class="n">cpu_device_info_</span><span class="p">.</span><span class="n">cpu_bind_mode_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">NO_BIND</span><span class="p">;</span>
<span class="n">context</span><span class="p">.</span><span class="n">thread_num_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="k">auto</span><span class="w"> </span><span class="n">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">TrainSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&quot;model_tod.ms&quot;</span><span class="p">),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="training-mode">
<h2>Training Mode<a class="headerlink" href="#training-mode" title="Permalink to this headline"></a></h2>
<p>With TrainSessions, a network can be used for both inference and training.
These two modes differ in several aspects:</p>
<ol class="arabic simple">
<li><p>The input of the network: Running inference requires only the data, while running training requires both data and labels</p></li>
<li><p>The output of the network: Running inference returns the predicted values in the output, while running in training mode returns the loss</p></li>
<li><p>In training mode, the weights of the layers are updated in each Run, while in inference mode they are static</p></li>
<li><p>Some layers behave differently in inference vs. training mode, e.g., updating the accumulated batch mean and variance in Batch Normalization layers</p></li>
</ol>
<section id="switching-between-train-mode-and-eval-mode">
<h3>Switching between Train Mode and Eval Mode<a class="headerlink" href="#switching-between-train-mode-and-eval-mode" title="Permalink to this headline"></a></h3>
<p>To switch between these two modes <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> class exposes the following public methods:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Set model to train mode</span>
<span class="c1">/// \return STATUS as an error code of compiling graph, STATUS is defined in errorcode.h</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">Train</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief Set model to eval mode</span>
<span class="c1">/// \return STATUS as an error code of compiling graph, STATUS is defined in errorcode.h</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">Eval</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>Example<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>The following sample code shows how to set a <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> object to train mode.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assuming session is a valid instance of TrainSession</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">Train</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Could not set session to train mode&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="data-input">
<h2>Data Input<a class="headerlink" href="#data-input" title="Permalink to this headline"></a></h2>
<section id="obtaining-input-tensors">
<h3>Obtaining Input Tensors<a class="headerlink" href="#obtaining-input-tensors" title="Permalink to this headline"></a></h3>
<p>Before graph execution, whether it is during training or inference, the input data must be filled-in into the model input tensors.
MindSpore Lite provides the following methods to obtain model input tensors:</p>
<ol class="arabic">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetInputsByTensorName</span></code> method to obtain model input tensors that are connected to the model input node based on the tensor name.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get input MindSpore Lite MSTensors of model by tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] tensor_name  Define tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \return  MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="nf">GetInputsByTensorName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor_name</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> method to directly obtain the vectors of all model input tensors.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get input MindSpore Lite MSTensors of model.</span>
<span class="c1">///</span>
<span class="c1">/// \return  The vector of MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">GetInputs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
<p>If the model requires more than one input tensor (this is certainly the case during training, where both data and labels serve as inputs of the network) it is the user’s responsibility to know the inputs order or their tensorName. This can be obtained from the Python model.
Alternatively, one can deduce this information from the sizes of the input tensors.</p>
</section>
<section id="copying-data">
<h3>Copying Data<a class="headerlink" href="#copying-data" title="Permalink to this headline"></a></h3>
<p>After model input tensors are obtained, the data must be copied into the tensors. The following methods allows to access the size of the data, it’s shape, the number of elements, the data type and the writable pointer. See also detailed description in the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.1/tensor.html#mstensor">MSTensor</a> API documentation.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get byte size of data in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return  Byte size of data in MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="nf">Size</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief Get shape of the MindSpore Lite MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return A vector of int as the shape of the MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">shape</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief Get number of element in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return Number of element in MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">ElementsNum</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief Get data type of the MindSpore Lite MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \note TypeId is defined in mindspore/mindspore/core/ir/dtype/type_id.h. Only number types in TypeId enum are</span>
<span class="c1">/// suitable for MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return MindSpore Lite TypeId of the MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">TypeId</span><span class="w"> </span><span class="nf">data_type</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief  Get the pointer of data in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \note  The data pointer can be used to both write and read data in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return  The pointer points to data in MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="nf">MutableData</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Example<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>The following sample code shows how to obtain the entire graph input <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> and enter the model input data to <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assuming session is a valid instance of TrainSession</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>

<span class="c1">// Assuming the model has two input tensors, the first is for data and the second for labels</span>
<span class="kt">int</span><span class="w"> </span><span class="n">data_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kt">int</span><span class="w"> </span><span class="n">label_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Unexpected amount of input tensors. Expected 2, model requires &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Assuming batch_size and data_size variables holds the Batch size and the size of a single data tensor, respectively:</span>
<span class="c1">// And assuming sparse labels are used</span>
<span class="k">if</span><span class="w"> </span><span class="p">((</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">data_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">batch_size</span><span class="o">*</span><span class="n">data_size</span><span class="p">)</span><span class="w"> </span><span class="o">||</span>
<span class="w">    </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">batch_size</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input data size does not match model input&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Assuming data_ptr is the pointer to a batch of data tensors</span>
<span class="c1">// and iassuming label_ptr is a pointer to a batch of label indices (obtained by the DataLoder)</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">data_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">((</span><span class="n">in_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="o">||</span><span class="w"> </span><span class="p">(</span><span class="n">in_labels</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model&#39;s input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">data_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">data_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">());</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_labels</span><span class="p">,</span><span class="w"> </span><span class="n">label_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">());</span>
<span class="c1">// After filling the input tensors the data_ptr and label_ptr may be freed</span>
<span class="c1">// The input tensors themselves are managed by MindSpore Lite and users are not allowed to access them or delete them</span>
</pre></div>
</div>
<p>Note:</p>
<ul class="simple">
<li><p>The data layout in the model input tensors of MindSpore Lite must be NHWC.</p></li>
<li><p>The Tensors returned by <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> and <code class="docutils literal notranslate"><span class="pre">GetInputsByTensorName</span></code> methods shuold not be released by users.</p></li>
</ul>
</section>
</section>
<section id="graph-execution">
<h2>Graph Execution<a class="headerlink" href="#graph-execution" title="Permalink to this headline"></a></h2>
<section id="executing-sessions">
<h3>Executing Sessions<a class="headerlink" href="#executing-sessions" title="Permalink to this headline"></a></h3>
<p>Whether a <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> object is in train mode or in eval mode, the way to make it execute, i.e., to run the data through the graph, is by calling <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Run session with callbacks.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] before Define a call_back_function to be called before running each node.</span>
<span class="c1">/// \param[in] after Define a call_back_function called after running each node.</span>
<span class="c1">///</span>
<span class="c1">/// \note RunGraph should be called after CompileGraph.</span>
<span class="c1">///</span>
<span class="c1">/// \return STATUS as an error code of running graph, STATUS is defined in errorcode.h.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">RunGraph</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">KernelCallBack</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">KernelCallBack</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
<p>Prior to each run of the graph, the user must make sure that the data is properly loaded to the input tensors.</p>
</section>
<section id="callback-running">
<h3>Callback Running<a class="headerlink" href="#callback-running" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite framework allows the user to set two callback functions that will be called before and after each node runs. Such functions can assist the developer to trace his network, to debug it, and to measure how long it took each node to run. The callback parameters are as follows:</p>
<ul class="simple">
<li><p>The current input tensors of the running node</p></li>
<li><p>The current output tensors of the running node</p></li>
<li><p>Name and type of the running node</p></li>
</ul>
<p>While the node name and type will be the same before and after the node runs, the output tensors will differ between the two callbacks invocations.
For some operators, also the input tesnors will vary.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  CallBackParam defines input arguments for callback function.</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">CallBackParam</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">node_name</span><span class="p">;</span><span class="w"> </span><span class="cm">/**&lt; node name argument */</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">node_type</span><span class="p">;</span><span class="w"> </span><span class="cm">/**&lt; node type argument */</span>
<span class="p">};</span>

<span class="c1">/// \brief KernelCallBack defined the function pointer for callBack.</span>
<span class="k">using</span><span class="w"> </span><span class="n">KernelCallBack</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="kt">bool</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                          </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                          </span><span class="k">const</span><span class="w"> </span><span class="n">CallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">opInfo</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>Example<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>The following sample code demonstrates how to define two callback functions, the first will be called before each layer runs, and the second after it ran</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assuming session is a valid instance of TrainSession and that data was assigned to the input tensors</span>

<span class="c1">// Definition of a callback function that will be called before forwarding operator</span>
<span class="kt">bool</span><span class="w"> </span><span class="nf">before_callback</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                    </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">CallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Before forwarding: input size is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Definition of callback function that will be called after forwarding operator</span>
<span class="kt">bool</span><span class="w"> </span><span class="nf">after_callback</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                    </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">CallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;After forwarding: output size is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="c1">// Hand over the callback functions to RunGraph when performing the training or inference</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session_</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">(</span><span class="n">before_callback</span><span class="p">,</span><span class="w"> </span><span class="n">after_callback</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Run graph failed.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="obtaining-outputs">
<h2>Obtaining Outputs<a class="headerlink" href="#obtaining-outputs" title="Permalink to this headline"></a></h2>
<section id="obtaining-output-tensors">
<h3>Obtaining Output Tensors<a class="headerlink" href="#obtaining-output-tensors" title="Permalink to this headline"></a></h3>
<p>After each execution of the graph, the user might want to read the model’s outputs, whether it is the loss in the case of train mode, or the predicted output in the case of evaluation mode.</p>
<p>MindSpore Lite provides the following methods to obtain the model’s output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<ol class="arabic">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> method to obtain the output tensors that belong to a certain node:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get output MindSpore Lite MSTensors of model by node name.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] node_name Define node name.</span>
<span class="c1">///</span>
<span class="c1">/// \return  The vector of MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">node_name</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> method to obtain ian output tensor, based on the tensor name.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get output MindSpore Lite MSTensors of model by tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] tensor_name  Define tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \return  Pointer of MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="nf">GetOutputByTensorName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor_name</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> method to obtain all the output tensors, ordered by their tensor name:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get output MindSpore Lite MSTensors of model mapped by tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \return  The map of output tensor name and MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">GetOutputs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
<p>After model output tensors are obtained, you need to enter data into the tensors. Use the <code class="docutils literal notranslate"><span class="pre">Size</span></code> method of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> to obtain the size of the data to be entered into tensors, use the <code class="docutils literal notranslate"><span class="pre">data_type</span></code> method to obtain the data type of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>, and use the <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> method of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> to obtain the writable pointer.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get byte size of data in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return  Byte size of data in MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="nf">Size</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief  Get data type of the MindSpore Lite MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \note  TypeId is defined in mindspore/mindspore/core/ir/dtype/type_id.h. Only number types in TypeId enum are</span>
<span class="c1">/// suitable for MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return  MindSpore Lite TypeId of the MindSpore Lite MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="n">TypeId</span><span class="w"> </span><span class="nf">data_type</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">/// \brief  Get the pointer of data in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \note The data pointer can be used to both write and read data in MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return  The pointer points to data in MSTensor.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="nf">MutableData</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3>Example<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> method and print the first ten data or all data records of each output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume that session is a vlaid TrainSession object</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one output node.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_node_iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_map</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out_node_iter</span><span class="o">-&gt;</span><span class="n">first</span><span class="p">;</span>
<span class="c1">// Assume that the unique output node has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out_node_iter</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">;</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Assume that the data format of output data is float 32.</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">data_type</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">TypeId</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output of lenet should in float32&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">());</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of out_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Print the first 10 float data or all output data of the output tensor.</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output data: &quot;</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="c1">// The elements in outputs do not need to be free by users, because outputs are managed by the MindSpore Lite.</span>
</pre></div>
</div>
<p>Note that the vectors or map returned by the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code>, <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> and <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> methods do not need to be released by users.</p>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume that session is a vlaid TrainSession instance</span>
<span class="c1">// Assume that model has a output node named output_node_name_0.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_vec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;output_node_name_0&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named output_node_name_0 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume that session is a vlaid TrainSession instance</span>
<span class="c1">// We can use GetOutputTensorNames method to get the names of all the output tensors of the model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Use output tensor name returned by GetOutputTensorNames as key</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor_name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tensor_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="obtaining-version-string">
<h2>Obtaining Version String<a class="headerlink" href="#obtaining-version-string" title="Permalink to this headline"></a></h2>
<p>The following sample code shows how to obtain version string using <code class="docutils literal notranslate"><span class="pre">Version</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/version.h&quot;</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Version</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="saving-the-trained-model">
<h2>Saving the Trained Model<a class="headerlink" href="#saving-the-trained-model" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides the following API to save the trained model:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">/// \brief Save the trained model into a flatbuffer file</span>
<span class="w">  </span><span class="c1">///</span>
<span class="w">  </span><span class="c1">/// \param[in] filename Filename to save flatbuffer to</span>
<span class="w">  </span><span class="c1">///</span>
<span class="w">  </span><span class="c1">/// \return 0 on success or -1 in case of error</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">SaveToFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">filename</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
<p>You can load the saved model to do re-training or inference.</p>
<blockquote>
<div><ul class="simple">
<li><p>The trained model by MindSpore Lite can only be inferenced by MindSpore Lite training framework, that is, to create <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code> first and call the <code class="docutils literal notranslate"><span class="pre">Eval()</span></code> api setting to the inference mode.</p></li>
<li><p>You can not use benchmark tool to run the trained model, please use <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/zh-CN/r1.1/use/benchmark_train_tool.html">benchmark_train</a> instead.</p></li>
</ul>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_train.html" class="btn btn-neutral float-left" title="Executing Model Training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tools_train.html" class="btn btn-neutral float-right" title="Other Tools" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>