

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using Runtime to Perform Inference (C++) &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using Runtime to Perform Inference (Java)" href="runtime_java.html" />
    <link rel="prev" title="Executing Model Inference" href="runtime.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Simplified MindSpore Lite C++ Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Simplified MindSpore Lite Java Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_codegen.html">Compile a MNIST Classification Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Training a LeNet Model</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="code_generator.html">Code Generator</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Optimizing the Model (Quantization After Training)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Executing Model Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Runtime to Perform Inference (C++)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-a-model">Loading a Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-a-configuration-context">Creating a Configuration Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-number-of-threads">Configuring the Number of Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-cpu-backend">Configuring the CPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-gpu-backend">Configuring the GPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-npu-backend">Configuring the NPU Backend</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#creating-a-session">Creating a Session</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-a-graph">Building a Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputting-data">Inputting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-inference">Executing Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-output">Obtaining Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="#releasing-memory">Releasing Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#optimizing-the-memory-size">Optimizing the Memory Size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#core-binding-operations">Core Binding Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resizing-the-input-dimension">Resizing the Input Dimension</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parallel-sessions">Parallel Sessions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-memory-pool">Sharing a Memory Pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calling-back-a-model-during-the-running-process">Calling Back a Model During the Running Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="#simplified-createsession-api-invocation-process">Simplified CreateSession API Invocation Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="#viewing-logs">Viewing Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-version-number">Obtaining the Version Number</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">Using Runtime to Perform Inference (Java)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Other Tools</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools_train.html">Other Tools</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="runtime.html">Executing Model Inference</a> &raquo;</li>
        
      <li>Using Runtime to Perform Inference (C++)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/runtime_cpp.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="using-runtime-to-perform-inference-c">
<h1>Using Runtime to Perform Inference (C++)<a class="headerlink" href="#using-runtime-to-perform-inference-c" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Android</span></code> <code class="docutils literal notranslate"><span class="pre">C++</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span> <span class="pre">Application</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Loading</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<!-- TOC -->
<ul class="simple">
<li><p><a class="reference external" href="#using-runtime-to-perform-inference-c">Using Runtime to Perform Inference (C++)</a></p>
<ul>
<li><p><a class="reference external" href="#overview">Overview</a></p></li>
<li><p><a class="reference external" href="#loading-a-model">Loading a Model</a></p></li>
<li><p><a class="reference external" href="#creating-a-configuration-context">Creating a Configuration Context</a></p>
<ul>
<li><p><a class="reference external" href="#configuring-the-number-of-threads">Configuring the Number of Threads</a></p></li>
<li><p><a class="reference external" href="#configuring-the-cpu-backend">Configuring the CPU Backend</a></p></li>
<li><p><a class="reference external" href="#configuring-the-gpu-backend">Configuring the GPU Backend</a></p></li>
<li><p><a class="reference external" href="#configuring-the-npu-backend">Configuring the NPU Backend</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#creating-a-session">Creating a Session</a></p></li>
<li><p><a class="reference external" href="#building-a-graph">Building a Graph</a></p></li>
<li><p><a class="reference external" href="#inputting-data">Inputting Data</a></p></li>
<li><p><a class="reference external" href="#executing-inference">Executing Inference</a></p></li>
<li><p><a class="reference external" href="#obtaining-output">Obtaining Output</a></p></li>
<li><p><a class="reference external" href="#releasing-memory">Releasing Memory</a></p></li>
<li><p><a class="reference external" href="#advanced-usage">Advanced Usage</a></p>
<ul>
<li><p><a class="reference external" href="#optimizing-the-memory-size">Optimizing the Memory Size</a></p></li>
<li><p><a class="reference external" href="#core-binding-operations">Core Binding Operations</a></p></li>
<li><p><a class="reference external" href="#resizing-the-input-dimension">Resizing the Input Dimension</a></p></li>
<li><p><a class="reference external" href="#parallel-sessions">Parallel Sessions</a></p></li>
<li><p><a class="reference external" href="#sharing-a-memory-pool">Sharing a Memory Pool</a></p></li>
<li><p><a class="reference external" href="#calling-back-a-model-during-the-running-process">Calling Back a Model During the Running Process</a></p></li>
<li><p><a class="reference external" href="#simplified-createsession-api-invocation-process">Simplified CreateSession API Invocation Process</a></p></li>
<li><p><a class="reference external" href="#viewing-logs">Viewing Logs</a></p></li>
<li><p><a class="reference external" href="#obtaining-the-version-number">Obtaining the Version Number</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://gitee.com/mindspore/docs/blob/r1.2/tutorials/lite/source_en/use/runtime_cpp.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.2/resource/_static/logo_source.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>After the model is converted into a <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model by using the MindSpore Lite model conversion tool, the inference process can be performed in Runtime. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/en/r1.2/use/converter_tool.html">Converting Models for Inference</a>. This tutorial describes how to use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/index.html">C++ API</a> to perform inference.</p>
<p>To use the MindSpore Lite inference framework, perform the following steps:</p>
<ol class="simple">
<li><p>Load the model: Read the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model converted by using the model conversion tool from the file system, import the model by using <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#import">mindspore::lite::Model::Import</a>, parse the model, and create the <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">*</span></code>. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/en/r1.2/use/converter_tool.html">Converting Models for Inference</a>.</p></li>
<li><p>Create a configuration context: Create a configuration <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#context">Context</a> to save some basic configuration parameters required by a session to guide graph build and execution.</p></li>
<li><p>Create a session: Create <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> and configure the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#context">Context</a> obtained in the previous step to the session.</p></li>
<li><p>Build the graph: Before performing inference, call the <code class="docutils literal notranslate"><span class="pre">CompileGraph</span></code> API of <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> to build the graph. In the graph build phase, graph partition and operator selection and scheduling are performed, which takes a long time. Therefore, it is recommended that with the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> created each time, one graph be built. In this case, the inference will be performed for multiple times.</p></li>
<li><p>Input data: Before the graph is exed, data needs to be filled in to the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p>Perform inference: Use <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code> of the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> to perform model inference.</p></li>
<li><p>Obtain the output: After the graph execution is complete, you can obtain the inference result by <code class="docutils literal notranslate"><span class="pre">outputting</span> <span class="pre">the</span> <span class="pre">tensor</span></code>.</p></li>
<li><p>Release the memory: If the MindSpore Lite inference framework is not required, release the created <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> and <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#model">Model</a>.</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime1.png" /></p>
<blockquote>
<div><p>For details about the calling process of MindSpore Lite inference, see <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/en/r1.2/quick_start/quick_start_cpp.html">Simplified MindSpore Lite C++ Demo</a>.</p>
</div></blockquote>
</div>
<div class="section" id="loading-a-model">
<h2>Loading a Model<a class="headerlink" href="#loading-a-model" title="Permalink to this headline">¶</a></h2>
<p>When MindSpore Lite is used for model inference, the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model file converted by using the model conversion tool needs to be read from the file system and created from the memory data by using the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#import">mindspore::lite::Model::Import</a> static function. <code class="docutils literal notranslate"><span class="pre">Model</span></code> holds model data such as weight data and operator attributes. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/en/r1.2/use/converter_tool.html">Converting Models for Inference</a>.</p>
<p>The <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#model">Model</a> instance returned by the <code class="docutils literal notranslate"><span class="pre">mindspore::lite::Model::Import</span></code> function is a pointer created through <code class="docutils literal notranslate"><span class="pre">new</span></code>. If the instance is not required, release it through <code class="docutils literal notranslate"><span class="pre">delete</span></code>.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L325">main.cc</a> demonstrates how to read a MindSpore Lite model from the file system and parse the model by using <code class="docutils literal notranslate"><span class="pre">mindspore::lite::Model::Import</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Read model file.</span>
<span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span> <span class="o">*</span><span class="n">model_buf</span> <span class="o">=</span> <span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">model_buf</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Read model file failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Load the .ms model.</span>
<span class="k">auto</span> <span class="n">model</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Model</span><span class="o">::</span><span class="n">Import</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Import model file failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-a-configuration-context">
<h2>Creating a Configuration Context<a class="headerlink" href="#creating-a-configuration-context" title="Permalink to this headline">¶</a></h2>
<p>The context saves some basic configuration parameters required by the session to guide graph build and execution. If you use <code class="docutils literal notranslate"><span class="pre">new</span></code> to create a <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a> and do not need it any more, use <code class="docutils literal notranslate"><span class="pre">delete</span></code> to release it. Generally, the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a> is released after the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> is created. The parameters contained in <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a> are defined as follows:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#thread-num">thread_num_</a>: MindSpore Lite has a built-in thread pool shared by processes. During inference, <code class="docutils literal notranslate"><span class="pre">thread_num_</span></code> is used to specify the maximum number of threads in the thread pool. The default value is 2.</p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#allocator">allocator</a>: MindSpore Lite supports dynamic memory allocation and release. If <code class="docutils literal notranslate"><span class="pre">allocator</span></code> is not specified, a default <code class="docutils literal notranslate"><span class="pre">allocator</span></code> is generated during inference. You can also use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#context">Context</a> method to share the memory allocator in multiple <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a>. For details about the calling method, see the usage of <a class="reference external" href="#sharing-a-memory-pool">Sharing a Memory Pool</a>.</p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#device-list">device_list_</a>: MindSpore Lite supports heterogeneous inference. The backend configuration information for inference is specified by <code class="docutils literal notranslate"><span class="pre">device_list_</span></code> in <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a>. By default, the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#devicecontext">DeviceContext</a> of the CPU is stored. During graph build, operator selection and scheduling are performed based on the backend configuration information in <code class="docutils literal notranslate"><span class="pre">device_list_</span></code>. Currently, only CPU and GPU heterogeneity or CPU and NPU heterogeneity is supported. When the GPU’s <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#devicecontext">DeviceContext</a> is configured, GPU-based inference is preferentially used. When the NPU’s <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#devicecontext">DeviceContext</a> is configured, NPU-based inference is preferentially used.</p></li>
</ul>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">device_list_[0]</span></code> must be <code class="docutils literal notranslate"><span class="pre">DeviceContext</span></code> of the CPU, and <code class="docutils literal notranslate"><span class="pre">device_list_[1]</span></code> must be <code class="docutils literal notranslate"><span class="pre">DeviceContext</span></code> of the GPU or <code class="docutils literal notranslate"><span class="pre">DeviceContext</span></code> of the NPU. Currently, the CPU, GPU, and NPU cannot be set at a time.</p>
</div></blockquote>
<div class="section" id="configuring-the-number-of-threads">
<h3>Configuring the Number of Threads<a class="headerlink" href="#configuring-the-number-of-threads" title="Permalink to this headline">¶</a></h3>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L109">main.cc</a> demonstrates how to configure the number of threads:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New context failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">thread_num_</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="configuring-the-cpu-backend">
<h3>Configuring the CPU Backend<a class="headerlink" href="#configuring-the-cpu-backend" title="Permalink to this headline">¶</a></h3>
<p>When the backend to be executed is the CPU, <code class="docutils literal notranslate"><span class="pre">device_list_[0]</span></code> is the <code class="docutils literal notranslate"><span class="pre">DeviceContext</span></code> of the CPU by default after <code class="docutils literal notranslate"><span class="pre">Context</span></code> is created. You can directly configure the <code class="docutils literal notranslate"><span class="pre">enable_float16_</span></code> and <code class="docutils literal notranslate"><span class="pre">cpu_bind_mode_</span></code> attributes in <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#cpudeviceinfo">CpuDeviceInfo</a>.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L102">main.cc</a> demonstrates how to create a CPU backend, set the CPU core binding mode to large-core priority, and enable float16 inference:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New context failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU device context has default values.</span>
<span class="k">auto</span> <span class="o">&amp;</span><span class="n">cpu_device_info</span> <span class="o">=</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">device_list_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">device_info_</span><span class="p">.</span><span class="n">cpu_device_info_</span><span class="p">;</span>
<span class="c1">// The large core takes priority in thread and core binding methods. This parameter will work in the BindThread interface. For specific binding effect, see the &quot;Run Graph&quot; section.</span>
<span class="n">cpu_device_info</span><span class="p">.</span><span class="n">cpu_bind_mode_</span> <span class="o">=</span> <span class="n">HIGHER_CPU</span><span class="p">;</span>
<span class="c1">// Use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="p">.</span><span class="n">enable_float16_</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</pre></div>
</div>
<blockquote>
<div><p>Float16 takes effect only when the CPU is of the ARM v8.2 architecture. Other models and x86 platforms that are not supported are automatically rolled back to Float32.</p>
</div></blockquote>
</div>
<div class="section" id="configuring-the-gpu-backend">
<h3>Configuring the GPU Backend<a class="headerlink" href="#configuring-the-gpu-backend" title="Permalink to this headline">¶</a></h3>
<p>If the backend to be exed is heterogeneous inference based on CPUs and GPUs, you need to set <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#devicecontext">DeviceContext</a> for both CPUs and GPUs. After the configuration, GPU-based inference is preferentially used. <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#gpudeviceinfo">GpuDeviceInfo</a> contains the <code class="docutils literal notranslate"><span class="pre">enable_float16_</span></code> public attribute, which is used to enable Float16 inference.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L120">main.cc</a> demonstrates how to create the CPU and GPU heterogeneous inference backend and how to enable Float16 inference for the GPU.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// If GPU device context is set. The preferred backend is GPU, which means, if there is a GPU operator, it will run on the GPU first, otherwise it will run on the CPU.</span>
<span class="n">DeviceContext</span> <span class="n">gpu_device_ctx</span><span class="p">{</span><span class="n">DT_GPU</span><span class="p">,</span> <span class="p">{</span><span class="nb">false</span><span class="p">}};</span>
<span class="c1">// GPU use float16 operator as priority.</span>
<span class="n">gpu_device_ctx</span><span class="p">.</span><span class="n">device_info_</span><span class="p">.</span><span class="n">gpu_device_info_</span><span class="p">.</span><span class="n">enable_float16_</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
<span class="c1">// The GPU device context needs to be push_back into device_list to work.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">device_list_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_ctx</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>Currently, the backend of GPU is based on OpenCL. GPUs of Mali and Adreno are supported. The OpenCL version is 2.0.</p>
<p>The configuration is as follows:</p>
<p>CL_TARGET_OPENCL_VERSION=200</p>
<p>CL_HPP_TARGET_OPENCL_VERSION=120</p>
<p>CL_HPP_MINIMUM_OPENCL_VERSION=120</p>
</div></blockquote>
</div>
<div class="section" id="configuring-the-npu-backend">
<h3>Configuring the NPU Backend<a class="headerlink" href="#configuring-the-npu-backend" title="Permalink to this headline">¶</a></h3>
<p>When the backend to be exed is heterogeneous inference based on CPUs and GPUs, you need to set the CPU’s and NPU’s <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#devicecontext">DeviceContext</a>. After the configuration, the NPU’s inference is preferentially used. The <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#npudeviceinfo">NpuDeviceInfo</a> contains the public attribute <code class="docutils literal notranslate"><span class="pre">frequency_</span></code>, which is used to set the NPU’s frequency.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L137">main.cc</a> shows how to create the CPU and NPU heterogeneous inference backend and set the NPU frequency to 3.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">DeviceContext</span> <span class="n">npu_device_ctx</span><span class="p">{</span><span class="n">DT_NPU</span><span class="p">};</span>
<span class="n">npu_device_ctx</span><span class="p">.</span><span class="n">device_info_</span><span class="p">.</span><span class="n">npu_device_info_</span><span class="p">.</span><span class="n">frequency_</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="c1">// The NPU device context needs to be push_back into device_list to work.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">device_list_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">npu_device_ctx</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="creating-a-session">
<h2>Creating a Session<a class="headerlink" href="#creating-a-session" title="Permalink to this headline">¶</a></h2>
<p>When MindSpore Lite is used for inference, <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> is the main entry for inference. You can use <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> to build and execute graphs. Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a> created in the previous step to call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#createsession">CreateSession</a> method of the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> to create the LiteSession.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L275">main.cc</a> demonstrates how to create a <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Use Context to create Session.</span>
<span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
<span class="c1">// After the LiteSession is created, the Context can be released.</span>
<span class="p">...</span>
<span class="k">if</span> <span class="p">(</span><span class="n">session</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>The <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> instance returned by the function is a pointer that is created using <code class="docutils literal notranslate"><span class="pre">new</span></code>. If the instance is not required, you need to release it using <code class="docutils literal notranslate"><span class="pre">delete</span></code>.</p>
<p>After the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> is created, the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#context">Context</a> created in the previous step can be released.</p>
</div></blockquote>
</div>
<div class="section" id="building-a-graph">
<h2>Building a Graph<a class="headerlink" href="#building-a-graph" title="Permalink to this headline">¶</a></h2>
<p>Before executing a graph, call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#compilegraph">CompileGraph</a> API of <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> to build the graph and parse the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#model">Model</a> instance loaded from the file for graph partition and operator selection and scheduling. This takes a long time. Therefore, it is recommended that with the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> created each time, one graph be built. In this case, the inference will be performed for multiple times.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L282">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">CompileGraph</span></code> to build a graph.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session and a Model instance named model before.</span>
<span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Compile failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="c1">// session and model need to be released by users manually.</span>
    <span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="inputting-data">
<h2>Inputting Data<a class="headerlink" href="#inputting-data" title="Permalink to this headline">¶</a></h2>
<p>Before executing a graph, obtain the input <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> of the model and copy the input data to the input <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> of the model using <code class="docutils literal notranslate"><span class="pre">memcpy</span></code>. In addition, you can use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#size">Size</a> method of <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> to obtain the size of the data to be filled in to the tensor, use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#data-type">data_type</a> method to obtain the data type of the tensor, and use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mutabledata">MutableData</a> method of <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> to obtain the writable pointer.</p>
<p>MindSpore Lite provides two methods to obtain the input tensor of a model.</p>
<ol>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getinputsbytensorname">GetInputsByTensorName</a> method to obtain the tensor connected to the input node from the model input tensor based on the name of the model input tensor. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L169">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetInputsByTensorName</span></code> to obtain the input tensor and fill in data.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume that the model has only one input tensor named 2031_2030_1_construct_wrapper:x.</span>
<span class="k">auto</span> <span class="n">in_tensor</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputsByTensorName</span><span class="p">(</span><span class="s">&quot;2031_2030_1_construct_wrapper:x&quot;</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">in_tensor</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Input tensor is nullptr&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span> <span class="n">input_data</span> <span class="o">=</span> <span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">input_data</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;MallocData for inTensor failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span> <span class="n">input_buf</span><span class="p">,</span> <span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getinputs">GetInputs</a> method to directly obtain the vectors of all model input tensors. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L150">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> to obtain the input tensor and fill in data.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="k">auto</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one input tensor.</span>
<span class="k">auto</span> <span class="n">in_tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">in_tensor</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Input tensor is nullptr&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span> <span class="o">*</span><span class="n">in_data</span> <span class="o">=</span> <span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">in_data</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Data of in_tensor is nullptr&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span> <span class="n">input_buf</span><span class="p">,</span> <span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>The data layout in the input tensor of the MindSpore Lite model must be <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>. For more information about data pre-processing, see <a class="reference external" href="https://www.mindspore.cn/tutorial/lite/en/r1.2/quick_start/quick_start.html#id11">Implementing an Image Classification Application (C++)</a>.</p>
<p><a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getinputs">GetInputs</a> and <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getinputsbyname">GetInputsByTensorName</a> methods return vectors that do not need to be released by users.</p>
</div></blockquote>
</div>
<div class="section" id="executing-inference">
<h2>Executing Inference<a class="headerlink" href="#executing-inference" title="Permalink to this headline">¶</a></h2>
<p>After a MindSpore Lite session builds a graph, you can call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#rungraph">RunGraph</a> function of <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> for model inference.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L347">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code> to perform inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;RunGraph failed&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="obtaining-output">
<h2>Obtaining Output<a class="headerlink" href="#obtaining-output" title="Permalink to this headline">¶</a></h2>
<p>After performing inference, MindSpore Lite can obtain the inference result of the model. MindSpore Lite provides three methods to obtain the output <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> of a model.</p>
<ol>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getoutputsbynodename">GetOutputsByNodeName</a> method to obtain the vector of the tensor connected to the model output <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> based on the name of the model output node. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L184">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> to obtain the output tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session before.</span>
<span class="c1">// Assume that model has an output node named Default/head-MobileNetV2Head/Softmax-op204.</span>
<span class="k">auto</span> <span class="n">output_vec</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;Default/head-MobileNetV2Head/Softmax-op204&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named Default/Sigmoid-op204 has only one output tensor.</span>
<span class="k">auto</span> <span class="n">out_tensor</span> <span class="o">=</span> <span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">out_tensor</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Output tensor is nullptr&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Post-processing your result data.</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getoutputbytensorname">GetOutputByTensorName</a> method to obtain the corresponding model output <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> based on the name of the model output tensor. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L212">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetOutputsByTensorName</span></code> to obtain the output tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="c1">// We can use GetOutputTensorNames method to get all name of output tensor of model which is in order.</span>
<span class="k">auto</span> <span class="n">tensor_names</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Assume we have created a LiteSession instance named session before.</span>
<span class="c1">// Use output tensor name returned by GetOutputTensorNames as key</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">tensor_name</span> <span class="p">:</span> <span class="n">tensor_names</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">out_tensor</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">out_tensor</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Output tensor is nullptr&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getoutputs">GetOutputs</a> method to directly obtain the names of all model output <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> and a map of the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/tensor.html#mstensor">MSTensor</a> pointer. The following <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L242">sample code</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> to obtain the output tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="k">auto</span> <span class="n">out_tensors</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">out_tensor</span> <span class="p">:</span> <span class="n">out_tensors</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>The vector or map returned by the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getoutputsbynodename">GetOutputsByNodeName</a>, <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getoutputbytensorname">GetOutputByTensorName</a>, and <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#getoutputs">GetOutputs</a> methods does not need to be released by the user.</p>
</div></blockquote>
</div>
<div class="section" id="releasing-memory">
<h2>Releasing Memory<a class="headerlink" href="#releasing-memory" title="Permalink to this headline">¶</a></h2>
<p>If the MindSpore Lite inference framework is not required, you need to release the created LiteSession and model. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L361">main.cc</a> demonstrates how to release the memory before the program ends.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model buffer.</span>
<span class="c1">// Assume that the variable of Model * is named model.</span>
<span class="k">delete</span> <span class="n">model</span><span class="p">;</span>
<span class="c1">// Delete session buffer.</span>
<span class="c1">// Assume that the variable of Session * is named session.</span>
<span class="k">delete</span> <span class="n">session</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="optimizing-the-memory-size">
<h3>Optimizing the Memory Size<a class="headerlink" href="#optimizing-the-memory-size" title="Permalink to this headline">¶</a></h3>
<p>If the memory is greatly limited, you can call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#free">Free</a> API to reduce the memory usage after the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#model">Model</a> is compiled into the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#compilegraph">CompileGraph</a> by the graph. Once the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#free">Free</a> API of a <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#model">Model</a> is called, the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#model">Model</a> cannot build graphs.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L340">main.cc</a> demonstrates how to call the <code class="docutils literal notranslate"><span class="pre">Free</span></code> API of <code class="docutils literal notranslate"><span class="pre">Model</span></code> to release <code class="docutils literal notranslate"><span class="pre">MetaGraph</span></code> to reduce the memory size.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Compile graph.</span>
<span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Compile failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Note: when use model-&gt;Free(), the model can not be compiled again.</span>
<span class="n">model</span><span class="o">-&gt;</span><span class="n">Free</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="section" id="core-binding-operations">
<h3>Core Binding Operations<a class="headerlink" href="#core-binding-operations" title="Permalink to this headline">¶</a></h3>
<p>The built-in thread pool of MindSpore Lite supports core binding and unbinding. By calling the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#bindthread">BindThread</a> API, you can bind working threads in the thread pool to specified CPU cores for performance analysis. The core binding operation is related to the context specified by the user when the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> is created. The core binding operation sets the affinity between the thread and the CPU based on the core binding policy in the context.</p>
<p>Note that core binding is an affinity operation and may not be bound to a specified CPU core. It may be affected by system scheduling. In addition, after the core binding, you need to perform the unbinding operation after the code is performed.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L346">main.cc</a> demonstrates how to bind the large core first when performing inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New context failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU device context has default values.</span>
<span class="k">auto</span> <span class="o">&amp;</span><span class="n">cpu_device_info</span> <span class="o">=</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">device_list_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">device_info_</span><span class="p">.</span><span class="n">cpu_device_info_</span><span class="p">;</span>
<span class="c1">// The large core takes priority in thread and core binding methods. This parameter will work in the BindThread</span>
<span class="c1">// interface. For specific binding effect, see the &quot;Run Graph&quot; section.</span>
<span class="n">cpu_device_info</span><span class="p">.</span><span class="n">cpu_bind_mode_</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">HIGHER_CPU</span><span class="p">;</span>

<span class="p">...</span>

<span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="n">session</span><span class="o">-&gt;</span><span class="n">BindThread</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;RunGraph failed&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">session</span><span class="o">-&gt;</span><span class="n">BindThread</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>There are three options for core binding: HIGHER_CPU, MID_CPU, and NO_BIND.</p>
<p>The rule for determining the core binding mode is based on the frequency of CPU cores instead of the CPU architecture.</p>
<p>HIGHER_CPU: indicates that threads in the thread pool are preferentially bound to the core with the highest frequency. The first thread is bound to the core with the highest frequency, the second thread is bound to the core with the second highest frequency, and so on.</p>
<p>MID_CPU: indicates that threads are bound to cores with the third or fourth highest frequency preferentially, which is determined based on experience. When there are no such cores, threads are bound to cores with the highest frequency.</p>
</div></blockquote>
</div>
<div class="section" id="resizing-the-input-dimension">
<h3>Resizing the Input Dimension<a class="headerlink" href="#resizing-the-input-dimension" title="Permalink to this headline">¶</a></h3>
<p>When MindSpore Lite is used for inference, if the input shape needs to be resized, you can call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#resize">Resize</a> API of <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> to reset the shape of the input tensor after a session is created and a graph is built.</p>
<blockquote>
<div><p>Some networks do not support variable dimensions. As a result, an error message is displayed and the model exits unexpectedly. For example, the model contains the MatMul operator, one input tensor of the MatMul operator is the weight, and the other input tensor is the input. If a variable dimension API is called, the input tensor does not match the shape of the weight tensor. As a result, the inference fails.</p>
</div></blockquote>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L368">main.cc</a> demonstrates how to perform <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#resize">Resize</a> on the input tensor of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a LiteSession instance named session.</span>
<span class="c1">// Compile graph.</span>
<span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Compile failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">...</span>
<span class="k">auto</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">resize_shape</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">};</span>
<span class="c1">// Assume the model has only one input,resize input shape to [1, 128, 128, 3]</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">new_shapes</span><span class="p">;</span>
<span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">resize_shape</span><span class="p">);</span>
<span class="n">session</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">new_shapes</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="parallel-sessions">
<h3>Parallel Sessions<a class="headerlink" href="#parallel-sessions" title="Permalink to this headline">¶</a></h3>
<p>MindSpore Lite supports parallel inference for multiple <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a>. The thread pool and memory pool of each <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> are independent. However, multiple threads cannot call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#rungraph">RunGraph</a> API of a single <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> at the same time.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L463">main.cc</a> demonstrates how to infer multiple <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> in parallel:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span> <span class="nf">RunSessionParallel</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">model_path</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="kt">char</span> <span class="o">*</span><span class="n">model_buf</span> <span class="o">=</span> <span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">model_buf</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Read model file failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="mi">-1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">// Load the .ms model.</span>
  <span class="k">auto</span> <span class="n">model</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Model</span><span class="o">::</span><span class="n">Import</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
  <span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Import model file failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="mi">-1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">// Compile MindSpore Lite model.</span>
  <span class="k">auto</span> <span class="n">session1</span> <span class="o">=</span> <span class="n">CreateSessionAndCompileByModel</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">session1</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Create session failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="mi">-1</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Compile MindSpore Lite model.</span>
  <span class="k">auto</span> <span class="n">session2</span> <span class="o">=</span> <span class="n">CreateSessionAndCompileByModel</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">session2</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Create session failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="mi">-1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">// Note: when use model-&gt;Free(), the model can not be compiled again.</span>
  <span class="n">model</span><span class="o">-&gt;</span><span class="n">Free</span><span class="p">();</span>

  <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">thread1</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span> <span class="p">{</span>
    <span class="n">GetInputsByTensorNameAndSetData</span><span class="p">(</span><span class="n">session1</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">status</span> <span class="o">=</span> <span class="n">session1</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Inference error &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">status</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
      <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Session1 inference success&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="p">});</span>

  <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">thread2</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span> <span class="p">{</span>
    <span class="n">GetInputsByTensorNameAndSetData</span><span class="p">(</span><span class="n">session2</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">status</span> <span class="o">=</span> <span class="n">session2</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Inference error &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">status</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
      <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Session2 inference success&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="p">});</span>

  <span class="n">thread1</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
  <span class="n">thread2</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>

  <span class="c1">// Get outputs data.</span>
  <span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="n">session1</span><span class="p">);</span>
  <span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="n">session2</span><span class="p">);</span>

  <span class="c1">// Delete model buffer.</span>
  <span class="k">delete</span> <span class="n">model</span><span class="p">;</span>
  <span class="c1">// Delete session buffer.</span>
  <span class="k">delete</span> <span class="n">session1</span><span class="p">;</span>
  <span class="k">delete</span> <span class="n">session2</span><span class="p">;</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>MindSpore Lite does not support multi-thread parallel execution of inference for a single <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a>. Otherwise, the following error information is displayed:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ERROR</span> <span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_session</span><span class="p">.</span><span class="nl">cc</span><span class="p">:</span><span class="mi">297</span><span class="p">]</span> <span class="n">RunGraph</span><span class="p">]</span> <span class="mi">10</span> <span class="n">Not</span> <span class="n">support</span> <span class="n">multi</span><span class="o">-</span><span class="n">threading</span>
</pre></div>
</div>
</div>
<div class="section" id="sharing-a-memory-pool">
<h3>Sharing a Memory Pool<a class="headerlink" href="#sharing-a-memory-pool" title="Permalink to this headline">¶</a></h3>
<p>If there are multiple sessions, you can configure the same <code class="docutils literal notranslate"><span class="pre">allocator</span></code> in <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a> to share the memory pool and reduce the memory size during running. The maximum memory size of the memory pool is <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">GB</span></code>, and the maximum memory size allocated each time is <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">GB</span></code>.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L535">main.cc</a> demonstrates how to share the memory pool between two <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context1</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context1</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New context failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span> <span class="n">session1</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context1</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
<span class="k">if</span> <span class="p">(</span><span class="n">session1</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session1</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Compile failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span> <span class="n">context2</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context2</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New  context failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Use the same allocator to share the memory pool.</span>
<span class="n">context2</span><span class="o">-&gt;</span><span class="n">allocator</span> <span class="o">=</span> <span class="n">context1</span><span class="o">-&gt;</span><span class="n">allocator</span><span class="p">;</span>

<span class="k">auto</span> <span class="n">session2</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context2</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
<span class="k">if</span> <span class="p">(</span><span class="n">session2</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">ret</span> <span class="o">=</span> <span class="n">session2</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Compile failed while running &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="calling-back-a-model-during-the-running-process">
<h3>Calling Back a Model During the Running Process<a class="headerlink" href="#calling-back-a-model-during-the-running-process" title="Permalink to this headline">¶</a></h3>
<p>MindSpore Lite can pass two <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/mindspore.html#kernelcallback">KernelCallBack</a> function pointers to <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#rungraph">RunGraph</a> to call back a model for inference. Compared with common graph execution, callback execution can obtain additional information during the running process to help developers analyze performance and debug bugs. Additional information includes:</p>
<ul class="simple">
<li><p>Name of the running node</p></li>
<li><p>Input and output tensors before the current node is inferred</p></li>
<li><p>Input and output tensors after the current node is inferred</p></li>
</ul>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L633">main.cc</a> demonstrates how to define two callback functions as the pre-callback pointer and post-callback pointer and pass them to the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#rungraph">RunGraph</a> API for callback inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Definition of callback function before forwarding operator.</span>
<span class="k">auto</span> <span class="n">before_call_back</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">before_inputs</span><span class="p">,</span>
                            <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">before_outputs</span><span class="p">,</span>
                            <span class="k">const</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">CallBackParam</span> <span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Before forwarding &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">call_param</span><span class="p">.</span><span class="n">node_name</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">call_param</span><span class="p">.</span><span class="n">node_type</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Definition of callback function after forwarding operator.</span>
<span class="k">auto</span> <span class="n">after_call_back</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">after_inputs</span><span class="p">,</span>
                           <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">after_outputs</span><span class="p">,</span>
                           <span class="k">const</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">CallBackParam</span> <span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;After forwarding &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">call_param</span><span class="p">.</span><span class="n">node_name</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">call_param</span><span class="p">.</span><span class="n">node_type</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">(</span><span class="n">before_call_back</span><span class="p">,</span> <span class="n">after_call_back</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Inference error &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">ret</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="simplified-createsession-api-invocation-process">
<h3>Simplified CreateSession API Invocation Process<a class="headerlink" href="#simplified-createsession-api-invocation-process" title="Permalink to this headline">¶</a></h3>
<p>Create a <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> by invoking the static method <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#createsession">CreateSession</a> of the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a> based on the created <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#id2">Context</a> and the read model buffer and buffer size. When this API is used to create a session, the model is loaded and the graph is built internally. You do not need to call the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#import">Import</a> and <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#compilegraph">CompileGraph</a> APIs again.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L425">main.cc</a> demonstrates how to call the simplified CreateSession API to create a <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/session.html#litesession">LiteSession</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New context failed while running&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Use model buffer and context to create Session.</span>
<span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">context</span><span class="p">);</span>

<span class="k">if</span> <span class="p">(</span><span class="n">session</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="viewing-logs">
<h3>Viewing Logs<a class="headerlink" href="#viewing-logs" title="Permalink to this headline">¶</a></h3>
<p>If an exception occurs during inference, you can view logs to locate the fault. For the Android platform, use the <code class="docutils literal notranslate"><span class="pre">Logcat</span></code> command line to view the MindSpore Lite inference log information and use <code class="docutils literal notranslate"><span class="pre">MS_LITE</span></code> to filter the log information.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>logcat -s <span class="s2">&quot;MS_LITE&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="obtaining-the-version-number">
<h3>Obtaining the Version Number<a class="headerlink" href="#obtaining-the-version-number" title="Permalink to this headline">¶</a></h3>
<p>MindSpore Lite provides the <a class="reference external" href="https://www.mindspore.cn/doc/api_cpp/en/r1.2/lite.html#version">Version</a> method to obtain the version number, which is included in the <code class="docutils literal notranslate"><span class="pre">include/version.h</span></code> header file. You can call this method to obtain the version number of MindSpore Lite.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/examples/runtime_cpp/main.cc#L712">main.cc</a> demonstrates how to obtain the version number of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&quot;include/version.h&quot;</span><span class="cp"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">version</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Version</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="runtime_java.html" class="btn btn-neutral float-right" title="Using Runtime to Perform Inference (Java)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="runtime.html" class="btn btn-neutral float-left" title="Executing Model Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>