<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>优化模型(训练后量化) &mdash; MindSpore Lite r1.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="预处理数据" href="data_preprocessing.html" />
    <link rel="prev" title="推理模型转换" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">实现一个图像分类应用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">训练一个LeNet模型</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">优化模型(训练后量化)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">权重量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">参数说明</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">使用步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">部分模型精度结果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id7">全量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">参数说明</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">使用步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">部分模型精度结果</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">其他工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">执行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools_train.html">其他工具</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>优化模型(训练后量化)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>优化模型(训练后量化)<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">模型转换</span></code> <code class="docutils literal notranslate"><span class="pre">模型调优</span></code> <code class="docutils literal notranslate"><span class="pre">中级</span></code> <code class="docutils literal notranslate"><span class="pre">高级</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/lite/source_zh_cn/use/post_training_quantization.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>对于已经训练好的<code class="docutils literal notranslate"><span class="pre">float32</span></code>模型，通过训练后量化将其转为<code class="docutils literal notranslate"><span class="pre">int8</span></code>，不仅能减小模型大小，而且能显著提高推理性能。在MindSpore Lite中，这部分功能集成在模型转换工具<code class="docutils literal notranslate"><span class="pre">conveter_lite</span></code>内，通过增加命令行参数，便能够转换得到量化后模型。</p>
<p>MindSpore Lite训练后量化分为两类：</p>
<ol class="arabic simple">
<li><p>权重量化：对模型的权值进行量化，仅压缩模型大小，推理时仍然执行<code class="docutils literal notranslate"><span class="pre">float32</span></code>推理；</p></li>
<li><p>全量化：对模型的权值、激活值等统一进行量化，推理时执行<code class="docutils literal notranslate"><span class="pre">int</span></code>运算，能提升模型推理速度、降低功耗。</p></li>
</ol>
<p>训练后量化在两种情况下所需的数据类型和参数设定不同，但均可通过转换工具设定。有关转换工具<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>的使用方法可参考<a class="reference external" href="https://www.mindspore.cn/tutorial/lite/zh-CN/r1.1/use/converter_tool.html">转换为MindSpore Lite模型</a>。在此基础之上进行配置，启用训练后量化。</p>
</section>
<section id="id3">
<h2>权重量化<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>支持1~16之间的任意比特量化，量化比特数越低，模型压缩率越大，但是精度损失通常也比较大。可以结合使用<a class="reference external" href="https://www.mindspore.cn/tutorial/lite/zh-CN/r1.1/use/benchmark_tool.html">Benchmark工具</a>进行精度评估，确定合适的量化比特数；通常平均相对误差(accuracyThreshold)满足4%以内，精度误差是比较小的。下面对权重量化的使用方式和效果进行阐述。</p>
<section id="id4">
<h3>参数说明<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>权重量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--bitNum<span class="o">=</span>BitNumValue<span class="w"> </span>--quantWeightSize<span class="o">=</span>ConvWeightQuantSizeThresholdValue<span class="w"> </span>--quantWeightChannel<span class="o">=</span>ConvWeightQuantChannelThresholdValue
</pre></div>
</div>
<p>下面对此命令的量化相关参数进行说明：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantType=&lt;QUANTTYPE&gt;</span></code></p></td>
<td><p>必选</p></td>
<td><p>设置为WeightQuant，启用权重量化</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>必须设置为WeightQuant</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--bitNum=&lt;BITNUM&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定权重量化的比特数，目前支持1bit～16bit量化</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>[1，16]</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantWeightSize=&lt;QUANTWEIGHTSIZE&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定参与权重量化的卷积核尺寸阈值，若卷积核尺寸大于该值，则对此权重进行量化；建议设置为500</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
<td><p>[0，+∞）</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--quantWeightChannel=&lt;QUANTWEIGHTCHANNEL&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定参与权重量化的卷积通道数阈值，若卷积通道数大于该值，则对此权重进行量化；建议设置为16</p></td>
<td><p>Integer</p></td>
<td><p>16</p></td>
<td><p>[0，+∞）</p></td>
</tr>
</tbody>
</table>
<p>用户可根据模型及自身需要对权重量化的参数作出调整。</p>
<blockquote>
<div><p>为保证权重量化的精度，建议<code class="docutils literal notranslate"><span class="pre">--bitNum</span></code>参数设定范围为8bit～16bit。</p>
</div></blockquote>
</section>
<section id="id5">
<h3>使用步骤<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>正确编译出<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>可执行文件。该部分可参考构建文档<a class="reference external" href="https://www.mindspore.cn/tutorial/lite/zh-CN/r1.1/use/build.html">编译MindSpore Lite</a>，获得<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>工具，并配置环境变量。</p></li>
<li><p>以TensorFlow Lite模型为例，执行权重量化模型转换命令:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>Inception_v3.tflite<span class="w"> </span>--outputFile<span class="o">=</span>Inception_v3.tflite<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--bitNum<span class="o">=</span><span class="m">8</span><span class="w"> </span>--quantWeightSize<span class="o">=</span><span class="m">0</span><span class="w"> </span>--quantWeightChannel<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
</li>
<li><p>上述命令执行成功后，便可得到量化后的模型<code class="docutils literal notranslate"><span class="pre">Inception_v3.tflite.ms</span></code>，量化后的模型大小通常会下降到FP32模型的1/4。</p></li>
</ol>
</section>
<section id="id6">
<h3>部分模型精度结果<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>测试数据集</p></th>
<th class="head"><p>FP32模型精度</p></th>
<th class="head"><p>权重量化精度（8bit）</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>77.60%</p></td>
<td><p>77.53%</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>70.96%</p></td>
<td><p>70.56%</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>71.56%</p></td>
<td><p>71.53%</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得。</p>
</div></blockquote>
</section>
</section>
<section id="id7">
<h2>全量化<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<p>针对需要提升模型运行速度、降低模型运行功耗的场景，可以使用训练后全量化功能。下面对全量化的使用方式和效果进行阐述。</p>
<section id="id8">
<h3>参数说明<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<p>全量化转换命令的一般形式为：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>PostTraining<span class="w"> </span>--bitNum<span class="o">=</span><span class="m">8</span><span class="w"> </span>--configFile<span class="o">=</span>config.cfg
</pre></div>
</div>
<p>下面对此命令的量化相关参数进行说明：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantType=&lt;QUANTTYPE&gt;</span></code></p></td>
<td><p>必选</p></td>
<td><p>设置为PostTraining，启用全量化</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>必须设置为PostTraining</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--configFile=&lt;CONFIGFILE&gt;</span></code></p></td>
<td><p>必选</p></td>
<td><p>校准数据集配置文件路径</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--bitNum=&lt;BITNUM&gt;</span></code></p></td>
<td><p>可选</p></td>
<td><p>设定全量化的比特数，目前支持1bit～8bit量化</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>[1，8]</p></td>
</tr>
</tbody>
</table>
<p>为了计算激活值的量化参数，用户需要提供校准数据集。校准数据集最好来自真实推理场景，能表征模型的实际输入情况，数量在100个左右。
校准数据集配置文件采用<code class="docutils literal notranslate"><span class="pre">key=value</span></code>的方式定义相关参数，需要配置的<code class="docutils literal notranslate"><span class="pre">key</span></code>如下:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数名</p></th>
<th class="head"><p>属性</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>参数类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>取值范围</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>image_path</p></td>
<td><p>必选</p></td>
<td><p>存放校准数据集的目录；如果模型有多个输入，请依次填写对应的数据所在目录，目录路径间请用<code class="docutils literal notranslate"><span class="pre">,</span></code>隔开</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>该目录存放可直接用于执行推理的输入数据。由于目前框架还不支持数据预处理，所有数据必须事先完成所需的转换，使得它们满足推理的输入要求</p></td>
</tr>
<tr class="row-odd"><td><p>batch_count</p></td>
<td><p>可选</p></td>
<td><p>使用的输入数目</p></td>
<td><p>Integer</p></td>
<td><p>100</p></td>
<td><p>（0，+∞）</p></td>
</tr>
<tr class="row-even"><td><p>method_x</p></td>
<td><p>可选</p></td>
<td><p>网络层输入输出数据量化算法</p></td>
<td><p>String</p></td>
<td><p>KL</p></td>
<td><p>KL、MAX_MIN、RemovalOutlier。 <br> KL：基于<a class="reference external" href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">KL散度</a>对数据范围作量化校准。 <br> MAX_MIN：基于最大值、最小值计算数据的量化参数。 <br> RemovalOutlier：按照一定比例剔除数据的极大极小值，再计算量化参数。 <br> 在校准数据集与实际推理时的输入数据相吻合的情况下，推荐使用MAX_MIN；而在校准数据集噪声比较大的情况下，推荐使用KL或者RemovalOutlier</p></td>
</tr>
<tr class="row-odd"><td><p>thread_num</p></td>
<td><p>可选</p></td>
<td><p>使用校准数据集执行推理流程时的线程数</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
<td><p>（0，+∞）</p></td>
</tr>
<tr class="row-even"><td><p>bias_correction</p></td>
<td><p>可选</p></td>
<td><p>是否对量化误差进行校正</p></td>
<td><p>Boolean</p></td>
<td><p>false</p></td>
<td><p>true、flase。使能后，能提升转换后的模型精度，建议设置为true</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>对于多输入模型，要求不同输入数据分别存放在各自不同的目录，同时各自目录中的所有文件的文件名按照字典序递增排序后，能够一一对应。例如：模型有两个输入input0、input1，校准数据集共2组（batch_count=2）；input0的对应数据存放在/dir/input0/目录，输入数据文件名为：data_1.bin、data_2.bin；input1的对应数据存放在/dir/input1/目录，输入数据文件名为：data_a.bin、data_b.bin，则认为(data_1.bin, data_a.bin)构成一组输入，（data_2.bin, data_b.bin）构成另一组输入。</p>
</div></blockquote>
</section>
<section id="id9">
<h3>使用步骤<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>正确编译出<code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>可执行文件。</p></li>
<li><p>准备校准数据集，假设存放在<code class="docutils literal notranslate"><span class="pre">/dir/images</span></code>目录，编写配置文件<code class="docutils literal notranslate"><span class="pre">config.cfg</span></code>，内容如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image_path</span><span class="o">=/</span><span class="nb">dir</span><span class="o">/</span><span class="n">images</span>
<span class="n">batch_count</span><span class="o">=</span><span class="mi">100</span>
<span class="n">method_x</span><span class="o">=</span><span class="n">MAX_MIN</span>
<span class="n">thread_num</span><span class="o">=</span><span class="mi">1</span>
<span class="n">bias_correction</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>校准数据集可以选择测试数据集的子集，要求<code class="docutils literal notranslate"><span class="pre">/dir/images</span></code>目录下存放的每个文件均是预处理好的输入数据，每个文件都可以直接用于推理的输入。</p>
</li>
<li><p>以MindSpore模型为例，执行全量化的模型转换命令:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>lenet.mindir<span class="w"> </span>--outputFile<span class="o">=</span>lenet_quant<span class="w"> </span>--quantType<span class="o">=</span>PostTraining<span class="w"> </span>--configFile<span class="o">=</span>config.cfg
</pre></div>
</div>
</li>
<li><p>上述命令执行成功后，便可得到量化后的模型<code class="docutils literal notranslate"><span class="pre">lenet_quant.ms</span></code>，通常量化后的模型大小会下降到FP32模型的1/4。</p></li>
</ol>
</section>
<section id="id10">
<h3>部分模型精度结果<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>测试数据集</p></th>
<th class="head"><p>method_x</p></th>
<th class="head"><p>FP32模型精度</p></th>
<th class="head"><p>全量化精度（8bit）</p></th>
<th class="head"><p>说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>77.60%</p></td>
<td><p>77.40%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>70.96%</p></td>
<td><p>70.31%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>MAX_MIN</p></td>
<td><p>71.56%</p></td>
<td><p>71.16%</p></td>
<td><p>校准数据集随机选择ImageNet Validation数据集中的100张</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>以上所有结果均在x86环境上测得，均设置<code class="docutils literal notranslate"><span class="pre">bias_correction=true</span></code>。</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="推理模型转换" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="预处理数据" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>