<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>使用CodeGen编译一个MNIST分类模型 &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="训练一个LeNet模型" href="train_lenet.html" />
    <link rel="prev" title="实现一个图像分割应用（Java）" href="image_segmentation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">获取MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">下载MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">编译MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick_start_cpp.html">体验MindSpore Lite C++极简Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_java.html">体验MindSpore Lite Java极简Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">实现一个图像分类应用（C++）</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_segmentation.html">实现一个图像分割应用（Java）</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">使用CodeGen编译一个MNIST分类模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">模型编译体验</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">详细步骤</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">生成代码</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">部署应用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id6">编译依赖</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">构建与运行</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">编写推理代码示例</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id9">更多详情</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#android">Android平台编译部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="#arm-cortex-m">Arm Cortex-M平台编译部署</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet.html">训练一个LeNet模型</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter_tool.html">推理模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/code_generator.html">代码生成工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">优化模型(训练后量化)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">预处理数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">执行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">专用芯片集成说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/tools.html">其他工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">端侧训练</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter_train.html">训练模型转换</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">执行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/tools_train.html">其他工具</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">模型支持</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>使用CodeGen编译一个MNIST分类模型</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/quick_start_codegen.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="codegenmnist">
<h1>使用CodeGen编译一个MNIST分类模型<a class="headerlink" href="#codegenmnist" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">IoT</span></code> <code class="docutils literal notranslate"><span class="pre">C++</span></code> <code class="docutils literal notranslate"><span class="pre">全流程</span></code> <code class="docutils literal notranslate"><span class="pre">模型编译</span></code> <code class="docutils literal notranslate"><span class="pre">模型代码生成</span></code> <code class="docutils literal notranslate"><span class="pre">模型部署</span></code> <code class="docutils literal notranslate"><span class="pre">推理应用</span></code> <code class="docutils literal notranslate"><span class="pre">初级</span></code> <code class="docutils literal notranslate"><span class="pre">中级</span></code> <code class="docutils literal notranslate"><span class="pre">高级</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.2/tutorials/lite/source_zh_cn/quick_start/quick_start_codegen.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>本教程介绍如何使用MindSpore Lite代码生成工具codegen，快速生成以及部署轻量化推理代码。</p>
<p>其主要流程如下图流程图所示：</p>
<p><img alt="img" src="../_images/lite_codegen.png" /></p>
<ol class="arabic simple">
<li><p>使用训练框架，如MindSpore等，得到训练好的模型；</p></li>
<li><p>使用MindSpore Lite转换工具converter，将预训练模型转换为<code class="docutils literal notranslate"><span class="pre">*.ms</span></code>格式文件；</p></li>
<li><p>使用codegen代码生成工具，输入<code class="docutils literal notranslate"><span class="pre">*.ms</span></code>文件自动生成推理代码；</p></li>
<li><p>通过交叉编译，生成支持不同平台的推理库文件。</p></li>
</ol>
<p>我们推荐从MNIST分类模型推理代码入手，了解codegen生成代码、编译构建、部署等流程，从而达到快速入门的效果。</p>
</section>
<section id="id2">
<h2>模型编译体验<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>用户可以使用脚本一键式编译生成MNIST分类模型的推理代码并执行推理，得到单次推理输出。下载<a class="reference external" href="https://gitee.com/mindspore/mindspore">MindSpore源码</a>，进入<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/mindspore/lite/micro/example/mnist_x86"><code class="docutils literal notranslate"><span class="pre">mindspore/mindspore/lite/micro/examples/mnist_x86</span></code></a>目录，执行脚本<code class="docutils literal notranslate"><span class="pre">mnist.sh</span></code>自动生成模型推理代码并编译工程目录，即可得到单次推理输出。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>mnist.sh
</pre></div>
</div>
<p>推理结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>======run benchmark======
input 0: mnist_input.bin

outputs:
name: Softmax-7, DataType: 43, Size: 40, Shape: [1 10], Data:
0.000000, 0.000000, 0.000000, 1.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,
======run success=======
</pre></div>
</div>
<p>也可以按照<strong>详细步骤</strong>从生成代码开始逐步完成使用codegen编译一个MNIST分类模型的全流程。</p>
</section>
<section id="id3">
<h2>详细步骤<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>在编译此工程之前需要预先获取Ubuntu-x64 CPU平台的<a class="reference external" href="https://www.mindspore.cn/tutorial/lite/zh-CN/r1.2/use/downloads.html">Release包</a>，解压后得到<code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-linux-x64</span></code>，将其拷贝到当前目录下。</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">{version}</span></code>为版本号字符串，如<code class="docutils literal notranslate"><span class="pre">1.2.0</span></code>。</p>
</div></blockquote>
<p>以本教程为例，预置x86平台的Release包目录如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  mindspore-lite-{version}-linux-x64
 └── tools
     └── codegen # 代码生成工具
         ├── codegen                # 可执行程序
         ├── include                # 推理框架头文件
         │   ├── nnacl              # nnacl 算子头文件
         │   └── wrapper
         ├── lib
         │   └── libwrapper.a       # MindSpore Lite CodeGen生成代码依赖的部分算子静态库
         └── third_party
             ├── include
             │   └── CMSIS          # ARM CMSIS NN 算子头文件
             └── lib
                 └── libcmsis_nn.a  # ARM CMSIS NN 算子静态库
</pre></div>
</div>
<section id="id4">
<h3>生成代码<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>下载<a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mnist_lite/mnist.ms">MNIST分类网络</a>。使用Release包中的codegen编译MNIST分类模型，生成对应的x86平台推理代码。生成代码的具体命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./codegen<span class="w"> </span>--codePath<span class="o">=</span>.<span class="w"> </span>--modelPath<span class="o">=</span>mnist.ms<span class="w"> </span>--target<span class="o">=</span>x86
</pre></div>
</div>
<p>codegen在当前目录下将生成mnist目录，其中包含了可编译构建的mnist分类模型的代码。</p>
<blockquote>
<div><p>关于codegen的更多使用命令说明，可参见<a class="reference external" href="https://www.mindspore.cn/tutorial/lite/zh-CN/r1.2/use/code_generator.html">codegen工具的详细介绍</a>。</p>
</div></blockquote>
</section>
<section id="id5">
<h3>部署应用<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<p>接下来介绍如何构建MindSpore Lite CodeGen生成的模型推理代码工程，并在x86平台完成部署。上文中codegen生成的代码与<code class="docutils literal notranslate"><span class="pre">mindspore/mindspore/lite/micro/example/mnist_x86</span></code>相同，本章节编译、构建步骤将对该目录展开，用户也可参照相同操作，编译上文codegen生成mnist目录代码。</p>
<section id="id6">
<h4>编译依赖<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</section>
<section id="id7">
<h4>构建与运行<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p><strong>生成代码工程说明</strong></p>
<p>进入<code class="docutils literal notranslate"><span class="pre">mindspore/mindspore/lite/micro/example/mnist_x86</span></code>目录中。</p>
<p>生成代码工程目录说明：</p>
<p>当前目录下预置了MNIST分类网络生成的代码。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mnist_x86/                         # 生成代码的根目录
├── benchmark                      # 生成代码的benchmark目录
└── src                            # 模型推理代码目录
</pre></div>
</div>
</li>
<li><p><strong>代码编译</strong></p>
<p>组织模型生成的推理代码以及算子静态库，编译生成模型推理静态库并编译生成benchmark可执行文件,</p>
<p>进入代码工程目录下，新建并进入build目录：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
</pre></div>
</div>
<p>开始编译：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>-DPKG_PATH<span class="o">={</span>path<span class="w"> </span>to<span class="o">}</span>/mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-x64<span class="w"> </span>..
make
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">{path</span> <span class="pre">to}</span></code>和<code class="docutils literal notranslate"><span class="pre">{version}</span></code>需要用户根据实际情况填写。</p>
</div></blockquote>
<p>代码工程编译成功结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Scanning dependencies of target net
[ 12%] Building C object src/CMakeFiles/net.dir/net.c.o
[ 25%] Building CXX object src/CMakeFiles/net.dir/session.cc.o
[ 37%] Building CXX object src/CMakeFiles/net.dir/tensor.cc.o
[ 50%] Building C object src/CMakeFiles/net.dir/weight.c.o
[ 62%] Linking CXX static library libnet.a
unzip raw static library libnet.a
raw static library libnet.a size:
-rw-r--r-- 1 user user 58K Mar 22 10:09 libnet.a
generate specified static library libnet.a
new static library libnet.a size:
-rw-r--r-- 1 user user 162K Mar 22 10:09 libnet.a
[ 62%] Built target net
Scanning dependencies of target benchmark
[ 75%] Building CXX object CMakeFiles/benchmark.dir/benchmark/benchmark.cc.o
[ 87%] Building C object CMakeFiles/benchmark.dir/benchmark/load_input.c.o
[100%] Linking CXX executable benchmark
[100%] Built target benchmark
</pre></div>
</div>
<p>此时在<code class="docutils literal notranslate"><span class="pre">mnist_x86/build/src/</span></code>目录下生成了<code class="docutils literal notranslate"><span class="pre">libnet.a</span></code>，推理执行库，在<code class="docutils literal notranslate"><span class="pre">mnist_x86/build</span></code>目录下生成了<code class="docutils literal notranslate"><span class="pre">benchmark</span></code>可执行文件。</p>
</li>
<li><p><strong>代码部署</strong></p>
<p>本示例部署于x86平台。由代码工程编译成功以后的产物为<code class="docutils literal notranslate"><span class="pre">benchmark</span></code>可执行文件，将其拷贝到用户的目标Linux服务器中即可执行。</p>
<p>在目标Linux服务上执行编译成功的二进制文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>mnist_input.bin<span class="w"> </span>net.bin
</pre></div>
</div>
<blockquote>
<div><p>mnist_input.bin在<code class="docutils literal notranslate"><span class="pre">example/mnist_x86</span></code>目录下，<code class="docutils literal notranslate"><span class="pre">net.bin</span></code>为模型参数文件，在<code class="docutils literal notranslate"><span class="pre">example/mnist_x86/src</span></code>目录下。</p>
</div></blockquote>
<p>生成结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>start run benchmark
input 0: mnist_input.bin
output size: 1
uint8:
Name: Softmax-7, DataType: 43, Size: 40, Shape: 1 10, Data:
0.000000, 0.000000, 0.000000, 1.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,
run benchmark success
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id8">
<h4>编写推理代码示例<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h4>
<p>本教程中的<code class="docutils literal notranslate"><span class="pre">benchmark</span></code>内部实现主要用于指导用户如何编写以及调用codegen编译的模型推理代码接口。以下为接口调用的详细介绍，详情代码可以参见<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/mindspore/lite/micro/example/mnist_x86">examples/mnist_x86</a>下的示例代码示例：</p>
<ol class="arabic">
<li><p><strong>构建推理的上下文以及会话</strong></p>
<p>本教程生成的代码为非并行代码，无需上下文context，可直接设为空。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">model_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="w"> </span><span class="o">*</span><span class="n">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">model_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">model_size</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;create lite session failed&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p><strong>输入数据准备</strong></p>
<p>用户所需要准备的输入数据内存空间，若输入是持久化文件，可通过读文件方式获取。若输入数据已经存在内存中，则此处无需读取，可直接传入数据指针。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Assume we have got input data in memory.</span>
<span class="w">  </span><span class="n">memcpy</span><span class="p">(</span><span class="n">input</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">(),</span><span class="w"> </span><span class="n">input_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">());</span>
</pre></div>
</div>
</li>
<li><p><strong>执行推理</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
</pre></div>
</div>
</li>
<li><p><strong>推理结束获取输出</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">Vector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs_name</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
<span class="w">      </span><span class="c1">// deal with output</span>
<span class="w">      </span><span class="p">......</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p><strong>释放内存session</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p><strong>推理代码整体调用流程</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Assume we have got model_buffer data in memory.</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">model_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="w"> </span><span class="o">*</span><span class="n">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">model_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">model_size</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;create lite session failed&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Assume we have got input data in memory.</span>
<span class="w">  </span><span class="n">memcpy</span><span class="p">(</span><span class="n">input</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">(),</span><span class="w"> </span><span class="n">input_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">());</span>

<span class="w">  </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>

<span class="w">  </span><span class="n">Vector</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs_name</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
<span class="w">      </span><span class="c1">// deal with output</span>
<span class="w">      </span><span class="p">......</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
<section id="id9">
<h2>更多详情<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<section id="android">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/micro/example/mobilenetv2/README.md#">Android平台编译部署</a><a class="headerlink" href="#android" title="Permalink to this headline"></a></h3>
</section>
<section id="arm-cortex-m">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/lite/micro/example/mnist_stm32f746/README.md#">Arm Cortex-M平台编译部署</a><a class="headerlink" href="#arm-cortex-m" title="Permalink to this headline"></a></h3>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="image_segmentation.html" class="btn btn-neutral float-left" title="实现一个图像分割应用（Java）" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train_lenet.html" class="btn btn-neutral float-right" title="训练一个LeNet模型" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>