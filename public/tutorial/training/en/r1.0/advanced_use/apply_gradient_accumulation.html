<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Applying Gradient Accumulation Algorithm &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Applying Quantization Aware Training" href="apply_quantization_aware_training.html" />
    <link rel="prev" title="Enabling Graph Kernel Fusion" href="enable_graph_kernel_fusion.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Use</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/publish_model.html">Publishing Models using MindSpore Hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Process Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="convert_dataset.html">Converting Dataset to MindRecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_operator.html">Custom Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrate_script.html">Migrating Training Scripts from Third Party Frameworks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tutorials.html">Training Process Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Applying Gradient Accumulation Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#creating-a-gradient-accumulation-model">Creating a Gradient Accumulation Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#importing-library-files">Importing Library Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset">Loading the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-training-model">Defining the Training Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-training-process">Defining the Training Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-and-saving-the-model">Training and Saving the Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#experiment-result">Experiment Result</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Compression</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="improve_model_security_nad.html">Improving Model Security with NAD Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_differential_privacy.html">Protecting User Privacy with Differential Privacy Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_fuzzing.html">Testing Model Security Using Fuzz Testing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Applying Gradient Accumulation Algorithm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_use/apply_gradient_accumulation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="applying-gradient-accumulation-algorithm">
<h1>Applying Gradient Accumulation Algorithm<a class="headerlink" href="#applying-gradient-accumulation-algorithm" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Optimization</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.0/tutorials/training/source_en/advanced_use/apply_gradient_accumulation.md"><img alt="View Source On Gitee" src="../_images/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes the gradient accumulation training method to solve the problem that some large-scale networks cannot train large batch_size due to insufficient memory.</p>
<p>In a traditional training method, after a loss and a gradient are calculated, a parameter is directly updated by using the obtained gradient.</p>
<p>Different from the traditional training method, the concept of mini-batch is introduced to the gradient accumulation. The loss and gradient are computed for each mini-batch data, but the model parameters are not updated immediately. Instead, the obtained gradients are accumulated first, and then after the number (N) of mini-batches is specified, the accumulated gradient is used to update the network parameters. Before the next training, the accumulated gradients are cleared and re-accumulated.</p>
<p>The ultimate objective is to achieve the same effect as training with N x mini-batch data.</p>
<blockquote>
<div><p>This tutorial is applicable to GPUs. You can download the main training sample code from <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.0/tutorials/tutorial_code/gradient_accumulation">https://gitee.com/mindspore/docs/tree/r1.0/tutorials/tutorial_code/gradient_accumulation</a>.</p>
</div></blockquote>
</section>
<section id="creating-a-gradient-accumulation-model">
<h2>Creating a Gradient Accumulation Model<a class="headerlink" href="#creating-a-gradient-accumulation-model" title="Permalink to this headline"></a></h2>
<p>The MNIST dataset is used as an example to describe how to customize a simple model to implement gradient accumulation.</p>
<section id="importing-library-files">
<h3>Importing Library Files<a class="headerlink" href="#importing-library-files" title="Permalink to this headline"></a></h3>
<p>The following are the required public modules and MindSpore modules and library files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterable</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ParameterTuple</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.train.dataset_helper</span> <span class="kn">import</span> <span class="n">DatasetHelper</span>
<span class="kn">from</span> <span class="nn">mindspore.train.serialization</span> <span class="kn">import</span> <span class="n">save_checkpoint</span>
<span class="kn">from</span> <span class="nn">model_zoo.official.cv.lenet.src.dataset</span> <span class="kn">import</span> <span class="n">create_dataset</span>
<span class="kn">from</span> <span class="nn">model_zoo.official.cv.lenet.src.lenet</span> <span class="kn">import</span> <span class="n">LeNet5</span>
</pre></div>
</div>
</section>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">MnistDataset</span></code> API provided by the dataset of MindSpore to load the MNIST dataset. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.0/model_zoo/official/cv/lenet/src/dataset.py">dataset.py</a> in the lenet directory of model_zoo.</p>
</section>
<section id="defining-the-network">
<h3>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h3>
<p>The following uses the LeNet network as an example. You can also use other networks, such as ResNet-50 and BERT. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.0/model_zoo/official/cv/lenet/src/lenet.py">lenet.py</a> in the lenet directory of model_zoo.</p>
</section>
<section id="defining-the-training-model">
<h3>Defining the Training Model<a class="headerlink" href="#defining-the-training-model" title="Permalink to this headline"></a></h3>
<p>The training process is divided into three parts: forward and backward training, parameter update, and accumulated gradient clearance.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TrainForwardBackward</span></code> calculates the loss and gradient, and uses grad_sum to implement gradient accumulation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainOptim</span></code> updates parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainClear</span></code> clears the gradient accumulation variable grad_sum.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_sum_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;grad_sum_op&quot;</span><span class="p">)</span>
<span class="n">_clear_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;clear_op&quot;</span><span class="p">)</span>


<span class="nd">@_sum_op</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_cumulative_gard</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply gard sum to cumulative gradient.&quot;&quot;&quot;</span>
    <span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AssignAdd</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="nd">@_clear_op</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_clear_grad_sum</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">zero</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply zero to clear grad_sum.&quot;&quot;&quot;</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">success</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">success</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">zero</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">success</span>


<span class="k">class</span> <span class="nc">TrainForwardBackward</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainForwardBackward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">add_flags</span><span class="p">(</span><span class="n">defer_inline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="n">sens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">ops</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_sum_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">TrainOptim</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOptim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TrainClear</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">,</span> <span class="n">zeros</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainClear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span> <span class="o">=</span> <span class="n">zeros</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_clear_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">success</span>
</pre></div>
</div>
</section>
<section id="defining-the-training-process">
<h3>Defining the Training Process<a class="headerlink" href="#defining-the-training-process" title="Permalink to this headline"></a></h3>
<p>Each mini-batch calculates the loss and gradient through forward and backward training, and uses mini_steps to control the accumulated times before each parameter update. After the number of accumulation times is reached, the parameter is updated and the accumulated gradient variable is cleared.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientAccumulation</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;grad_sum&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_zeros</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_forward_backward_network</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_optim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_optim</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_clear</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_clear</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_transform_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform callback to a list.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">callbacks</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_train_forward_backward_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build forward and backward network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">)</span>
        <span class="n">loss_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainForwardBackward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">,</span> <span class="n">loss_scale</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_build_train_optim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build optimizer network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainOptim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_build_train_clear</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build clear network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainClear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zeros</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">train_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">mini_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Training process. The data would be passed to network directly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataset_helper</span> <span class="o">=</span> <span class="n">DatasetHelper</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epoch_num</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
            <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">next_element</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_helper</span><span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span><span class="p">(</span><span class="o">*</span><span class="n">next_element</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">mini_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch:&quot;</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;step:&quot;</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;loss is &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_optim</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_clear</span><span class="p">()</span>

            <span class="n">train_dataset</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span><span class="p">,</span> <span class="s2">&quot;gradient_accumulation.ckpt&quot;</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-and-saving-the-model">
<h3>Training and Saving the Model<a class="headerlink" href="#training-and-saving-the-model" title="Permalink to this headline"></a></h3>
<p>Call the network, optimizer, and loss function, and then customize the <code class="docutils literal notranslate"><span class="pre">train_process</span></code> API of <code class="docutils literal notranslate"><span class="pre">GradientAccumulation</span></code> to train the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;MindSpore Gard Cumulative Example&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device_target&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;device where the code will be implemented (default: GPU)&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_path&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;./Data&quot;</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;path where the dataset is saved&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">)</span>

    <span class="n">network</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
    <span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GradientAccumulation</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Training ==============&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train_process</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">mini_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="experiment-result">
<h2>Experiment Result<a class="headerlink" href="#experiment-result" title="Permalink to this headline"></a></h2>
<p>After 10 epochs, the accuracy on the test set is about 96.31%.</p>
<p><strong>Training Execution:</strong></p>
<ol class="arabic">
<li><p>Run the training code and view the running result.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>train.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data
</pre></div>
</div>
<p>The output is as follows. The loss value decreases during training.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">27</span><span class="w"> </span>loss<span class="w"> </span>is<span class="w">  </span><span class="m">0</span>.3660637
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">28</span><span class="w"> </span>loss<span class="w"> </span>is<span class="w">  </span><span class="m">0</span>.25238192
...
epoch:<span class="w"> </span><span class="m">3</span><span class="w"> </span>step:<span class="w"> </span><span class="m">2</span><span class="w"> </span>loss<span class="w"> </span>is<span class="w">  </span><span class="m">0</span>.12296932
epoch:<span class="w"> </span><span class="m">3</span><span class="w"> </span>step:<span class="w"> </span><span class="m">3</span><span class="w"> </span>loss<span class="w"> </span>is<span class="w">  </span><span class="m">0</span>.15799297
...
epoch:<span class="w"> </span><span class="m">10</span><span class="w"> </span>step:<span class="w"> </span><span class="m">448</span><span class="w"> </span>loss<span class="w"> </span>is<span class="w">  </span><span class="m">0</span>.06443884
epoch:<span class="w"> </span><span class="m">10</span><span class="w"> </span>step:<span class="w"> </span><span class="m">449</span><span class="w"> </span>loss<span class="w"> </span>is<span class="w">  </span><span class="m">0</span>.0067842817
</pre></div>
</div>
</li>
<li><p>Check the saved checkpoint files.</p>
<p>The model file <code class="docutils literal notranslate"><span class="pre">gradient_accumulation.ckpt</span></code> is saved during training.</p>
</li>
</ol>
<p><strong>Model Validation:</strong></p>
<p>Use the saved checkpoint file to load the validation dataset through <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.0/model_zoo/official/cv/lenet/train.py">eval.py</a> in the lenet directory of model_zoo.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>eval.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data<span class="w"> </span>--ckpt_path<span class="o">=</span>./gradient_accumulation.ckpt<span class="w"> </span>--device_target<span class="o">=</span>GPU
</pre></div>
</div>
<p>The output is as follows. The accuracy of the validation dataset is about 96.31%, which is the same as the result when the value of batch_size is 32.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">==============</span><span class="w"> </span>Starting<span class="w"> </span><span class="nv">Testing</span><span class="w"> </span><span class="o">==============</span>
<span class="o">==============</span><span class="w"> </span><span class="o">{</span><span class="s1">&#39;Accuracy&#39;</span>:<span class="w"> </span><span class="m">0</span>.9631730769230769<span class="o">}</span><span class="w"> </span><span class="o">==============</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="enable_graph_kernel_fusion.html" class="btn btn-neutral float-left" title="Enabling Graph Kernel Fusion" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="apply_quantization_aware_training.html" class="btn btn-neutral float-right" title="Applying Quantization Aware Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>