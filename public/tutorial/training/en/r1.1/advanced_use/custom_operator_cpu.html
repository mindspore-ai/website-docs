<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Operators (CPU) &mdash; MindSpore r1.1 documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/training.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Migrating Training Scripts from Third Party Frameworks" href="migrate_script.html" />
    <link rel="prev" title="Custom Operators (Ascend)" href="custom_operator_ascend.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Use</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/publish_model.html">Publishing Models using MindSpore Hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Process Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="convert_dataset.html">Converting Dataset to MindRecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build Networks</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="custom_operator.html">Custom Operator</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="custom_operator_ascend.html">Custom Operators (Ascend)</a></li>
<li class="toctree-l2"><a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.1/quick_start/quick_video/gpu_operator_development.html">Custom Operators (GPU)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Custom Operators (CPU)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#registration-operator-s-primitives">Registration Operator’s Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implementing-cpu-operators-and-registration-operators-information">Implementing CPU Operators and Registration Operators Information</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#implementing-cpu-operators">Implementing CPU Operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#registration-operators-information">Registration Operators Information</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#editing-mindspore">Editing MindSpore</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-custom-cpu-operators">Using Custom CPU Operators</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-operators-bprop-functions">Defining Operators’ BProp Functions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="migrate_script.html">Migrating Training Scripts from Third Party Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_deep_probability_programming.html">Deep Probabilistic Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="achieve_high_order_differentiation.html">Achieve High Order Differentiation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tutorials.html">Training Process Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_cache.html">Application of Single-Node Tensor Cache</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Compression</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="improve_model_security_nad.html">Improving Model Security with NAD Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_differential_privacy.html">Protecting User Privacy with Differential Privacy Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_fuzzing.html">Testing Model Security Using Fuzz Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_membership_inference.html">Using Membership Inference to Test Model Security</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Use MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="custom_operator.html">Custom Operator</a> &raquo;</li>
      <li>Custom Operators (CPU)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_use/custom_operator_cpu.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="custom-operators-cpu">
<h1>Custom Operators (CPU)<a class="headerlink" href="#custom-operators-cpu" title="Permalink to this headline"></a></h1>
<p>Translator: <a class="reference external" href="https://gitee.com/julyai">JuLyAi</a></p>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">developing</span></code> <code class="docutils literal notranslate"><span class="pre">advanced_use</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/training/source_en/advanced_use/custom_operator_cpu.md"><img alt="View Source On Gitee" src="../_images/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When the built-in operators are not enough for developing the network, you can extend your custom CPU operators fast and conveniently using MindSpore’s Python API and C++ API.</p>
<p>To add a custom operator, you need to complete 3 parts of the work, including operator primitives registration, operators implementation and operators information registration.</p>
<p>Among them:</p>
<ul class="simple">
<li><p>Operator primitives: Defining the front-end interface prototype of operators in the network; The basic unit of a network model, mainly including operator’s name, attributes (optional), input / output name, output shape reasoning method, output dtype reasoning method, etc.</p></li>
<li><p>Operators implementation: Using the C++ API provided by the framework and combining with the specific characteristics of the operators, the internal calculation logic of the operator can be realized.</p></li>
</ul>
<p>This paper will take the custom <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> operator as an example to introduce the steps of customizing operators.</p>
</section>
<section id="registration-operator-s-primitives">
<h2>Registration Operator’s Primitives<a class="headerlink" href="#registration-operator-s-primitives" title="Permalink to this headline"></a></h2>
<p>Each operator’s primitive is a subclass inherited from the class <code class="docutils literal notranslate"><span class="pre">PrimitiveWithCheck</span></code>, whose type name is the operator’s name.</p>
<p>The CPU operator primitives are defined under the path <code class="docutils literal notranslate"><span class="pre">mindspore/ops/operations</span></code>, and the appropriate file is selected according to the operator type. Definition of CPU operators’ primitives’ interface is as follows:</p>
<ul class="simple">
<li><p>Attributes are defined by the input parameters of construction function <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. Operators in this use case have no init attributes, thus <code class="docutils literal notranslate"><span class="pre">__init__</span></code> has no additional input parameters.</p></li>
<li><p>The input and output names are defined by the function <code class="docutils literal notranslate"><span class="pre">init_prim_io_names</span></code>.</p></li>
<li><p>Checking shape of the output tensor is defined in <code class="docutils literal notranslate"><span class="pre">check_shape</span></code> function. Checking dtype of the output tensor is defined in <code class="docutils literal notranslate"><span class="pre">check_dtype</span></code> function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_checkparam</span></code> file defines a series of operations for validity checking, such as value checking, type checking, etc.</p></li>
</ul>
<p>Taking <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> operator’s primitive as an example, the following example codes are given.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">PrimitiveWithInfer</span>

<span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The definition of the Transpose primitive.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Transpose&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;perm&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">p_value</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_value</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The dimension of x and perm must be equal.&#39;</span><span class="p">)</span>
        <span class="n">out_shapes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">p_value</span><span class="p">:</span>
            <span class="n">out_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">out_shapes</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">perm_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_dtype</span>
</pre></div>
</div>
</section>
<section id="implementing-cpu-operators-and-registration-operators-information">
<h2>Implementing CPU Operators and Registration Operators Information<a class="headerlink" href="#implementing-cpu-operators-and-registration-operators-information" title="Permalink to this headline"></a></h2>
<section id="implementing-cpu-operators">
<h3>Implementing CPU Operators<a class="headerlink" href="#implementing-cpu-operators" title="Permalink to this headline"></a></h3>
<p>Usually, to implement a CPU operator needs to write a head file and a source file. The file path is <code class="docutils literal notranslate"><span class="pre">mindspore/ccsrc/backend/kernel_compiler/cpu</span></code>. If the logical realization of the operator is by calling the third-party library <code class="docutils literal notranslate"><span class="pre">MKL-DNN</span></code>, it will be placed in the subdirectory <code class="docutils literal notranslate"><span class="pre">mkldnn</span></code>. Please refer to <a class="reference external" href="https://github.com/oneapi-src/oneMKL">oneMkl</a> and <a class="reference external" href="https://github.com/oneapi-src/oneDNN">oneDNN</a> for details.</p>
<p>The head file of the operator contains the registration information of the operator and the declaration of the class. The operator class inherits from the parent class of <code class="docutils literal notranslate"><span class="pre">CPUKernel</span></code> and overloads <code class="docutils literal notranslate"><span class="pre">InitKernel</span></code> and <code class="docutils literal notranslate"><span class="pre">Launch</span></code>.</p>
<p>The source file of the operator is the implementation of the class. It mainly overloads the InitKernel and Launch functions. The head file example codes of the <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> operator are as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransposeCPUFwdKernel</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">CPUKernel</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">TransposeCPUFwdKernel</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">TransposeCPUFwdKernel</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">InitKernel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">CNodePtr</span><span class="w"> </span><span class="o">&amp;</span><span class="n">kernel_node</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">Launch</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">AddressPtr</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">AddressPtr</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">workspace</span><span class="p">,</span>
<span class="w">              </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">AddressPtr</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">shape_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">axis_</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The input parameters of the function <code class="docutils literal notranslate"><span class="pre">InitKernel</span></code> contain a constant reference to the node pointer. Through the member function of the class <code class="docutils literal notranslate"><span class="pre">AnfRuntimeAlgorithm</span></code>, the input and output shape of the operator node and the attribute information of the operator can be obtained.</p></li>
<li><p>The input parameters of the function <code class="docutils literal notranslate"><span class="pre">Launch</span></code> are 3 vectors, including all the input addresses, workspace addresses and all the output addresses, respectively. The concrete implementation logic of the operator is described in the function body.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shape_</span></code> and <code class="docutils literal notranslate"><span class="pre">axis_</span></code> are 2 member variables defined.</p></li>
</ul>
<p>The definition of the function <code class="docutils literal notranslate"><span class="pre">InitKernel</span></code> in the source file is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">TransposeCPUFwdKernel::InitKernel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">CNodePtr</span><span class="w"> </span><span class="o">&amp;</span><span class="n">kernel_node</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_EXCEPTION_IF_NULL</span><span class="p">(</span><span class="n">kernel_node</span><span class="p">);</span>
<span class="w">  </span><span class="n">shape_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AnfAlgo</span><span class="o">::</span><span class="n">GetInputDeviceShape</span><span class="p">(</span><span class="n">kernel_node</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="n">axis_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AnfAlgo</span><span class="o">::</span><span class="n">GetNodeAttr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">kernel_node</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;perm&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">shape_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">axis_</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">EXCEPTION</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The size of input shape and transpose axis shape must be equal.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The functions in the class <code class="docutils literal notranslate"><span class="pre">AnfRuntimeAlgorithm</span></code> implement various operations on operator nodes. <code class="docutils literal notranslate"><span class="pre">shape_</span></code> represents the shape of the first input of the operator. <code class="docutils literal notranslate"><span class="pre">axis_</span></code> represents the attribute “perm” of the operator.</p></li>
<li><p>The parameter “perm” of the<code class="docutils literal notranslate"><span class="pre">Transpose</span></code> operator’s primitive is as an input, but “perm” is actually considered as the attribute of the operation when parsing.</p></li>
</ul>
<blockquote>
<div><p>For details of the class <code class="docutils literal notranslate"><span class="pre">AnfRuntimeAlgorithm</span></code>, please refer to the declaration in MindSpore source codes under <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.1/mindspore/ccsrc/backend/session/anf_runtime_algorithm.h">mindspore/ccsrc/backend/session/anf_runtime_algorithm.h</a>.</p>
</div></blockquote>
<p>The definition of the function <code class="docutils literal notranslate"><span class="pre">Launch</span></code> in the source file is as follows: First, get the address of each input and output in turn, and then transform the dimension according to <code class="docutils literal notranslate"><span class="pre">axis_</span></code>, and assign the value to the space pointed to by the output address.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">bool</span><span class="w"> </span><span class="nf">TransposeCPUFwdKernel::Launch</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">AddressPtr</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                   </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">AddressPtr</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="cm">/*workspace*/</span><span class="p">,</span>
<span class="w">                                   </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">AddressPtr</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">addr</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">addr</span><span class="p">);</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IntToSize</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">shape_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IntToSize</span><span class="p">(</span><span class="n">shape_</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">shape_size</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">kMaxDim</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">EXCEPTION</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">shape_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;-D, but transpose supports max &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">kMaxDim</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;-D inputs.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">pos_array</span><span class="p">[</span><span class="n">kMaxDim</span><span class="p">];</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size_offset</span><span class="p">[</span><span class="n">kMaxDim</span><span class="p">];</span>
<span class="w">  </span><span class="n">size_offset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">shape_</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">shape_size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size_offset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size_offset</span><span class="p">[</span><span class="n">SizeToInt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">shape_</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">temp_position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">position</span><span class="p">;</span>
<span class="w">    </span><span class="n">pos_array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp_position</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">size_offset</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">shape_size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">temp_position</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="n">pos_array</span><span class="p">[</span><span class="n">SizeToInt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">size_offset</span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">];</span>
<span class="w">      </span><span class="n">pos_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp_position</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">size_offset</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">new_position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pos_array</span><span class="p">[</span><span class="n">axis_</span><span class="p">[</span><span class="n">SizeToInt</span><span class="p">(</span><span class="n">shape_size</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">]];</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">new_position_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shape_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">--</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">new_position_size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shape_</span><span class="p">[</span><span class="n">axis_</span><span class="p">[</span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">]];</span>
<span class="w">      </span><span class="n">new_position</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">pos_array</span><span class="p">[</span><span class="n">axis_</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">new_position_size</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">new_position</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">position</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="registration-operators-information">
<h3>Registration Operators Information<a class="headerlink" href="#registration-operators-information" title="Permalink to this headline"></a></h3>
<p>Operators information is the key information to guide the back-end selection of implementing operators. The first parameter of <code class="docutils literal notranslate"><span class="pre">MS_REG_CPU_KERNEL</span></code> is the name of the registration operator, which is consistent with the operator name in the primitives. The second parameter indicates the type of each input and output in turn. The last parameter is the name of the class which the operators implement. <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> operator registration codes are as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">MS_REG_CPU_KERNEL</span><span class="p">(</span><span class="n">Transpose</span><span class="p">,</span><span class="w"> </span><span class="n">KernelAttr</span><span class="p">().</span><span class="n">AddInputAttr</span><span class="p">(</span><span class="n">kNumberTypeFloat32</span><span class="p">).</span><span class="n">AddOutputAttr</span><span class="p">(</span><span class="n">kNumberTypeFloat32</span><span class="p">),</span>
<span class="w">                  </span><span class="n">TransposeCPUFwdKernel</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>The number and order of the input and output information defined in operator information, the number and order of input and output information in operator implementation, and the number and order of input and output name list in operator primitives should be consistent.</p>
</div></blockquote>
</section>
</section>
<section id="editing-mindspore">
<h2>Editing MindSpore<a class="headerlink" href="#editing-mindspore" title="Permalink to this headline"></a></h2>
<p>After writing the custom CPU operators, you need to recompile and reinstall MindSpore. For details, please refer to <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.1/install/mindspore_cpu_install_source.md#">Installation Document</a>.</p>
</section>
<section id="using-custom-cpu-operators">
<h2>Using Custom CPU Operators<a class="headerlink" href="#using-custom-cpu-operators" title="Permalink to this headline"></a></h2>
<p>After compiling and installing, the custom CPU operators can be used directly through the import primitives. Take the single operator network test of <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> as an example.</p>
<p>Define the network in document <code class="docutils literal notranslate"><span class="pre">test_transpose.py</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">test_net</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">transpose</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output: &quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>-s<span class="w"> </span>test_transpose.py::test_net
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>output: [[0, 3]
        [1, 4]
        [2, 5]]
</pre></div>
</div>
</section>
<section id="defining-operators-bprop-functions">
<h2>Defining Operators’ BProp Functions<a class="headerlink" href="#defining-operators-bprop-functions" title="Permalink to this headline"></a></h2>
<p>If an operator needs to support automatic differentiation, its back-propagation function (bprop) needs to be defined in its primitives. You need to describe the reverse computing logic that uses forward input, forward output, and output gradient to get the input gradient in bprop. Reverse computation logic can be composed of built-in operators or custom reverse operators.</p>
<p>The following points should be paid attention to when defining operators’ bprop functions:</p>
<ul class="simple">
<li><p>The order of input parameters of bprop function is defined as positive input, positive output and output gradient. If the operator is a multi-output operator, the forward output and output gradient will be provided in the form of tuples.</p></li>
<li><p>The form of the return values of bprop function is arranged as a tuple composed of input gradient, and the order of elements in the tuple is consistent with that of forward input parameters. Even if there is only one input gradient, the return value must be in the form of tuples.</p></li>
</ul>
<p>For example, the bprop primitives of <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="n">invert_permutation</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">InvertPermutation</span><span class="p">()</span>
<span class="n">transpose</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
<span class="n">zeros_like</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">()</span>
<span class="nd">@bprop_getters</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_bprop_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate bprop for Transpose&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">invert_permutation</span><span class="p">(</span><span class="n">perm</span><span class="p">)),</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">bprop</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Transpose</span></code> bprop operator uses <code class="docutils literal notranslate"><span class="pre">InvertPermutation</span></code> operator, which also needs a complete process of primitives, registration and implementation like <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> operator.</p></li>
</ul>
<p>Define the bprop case in document <code class="docutils literal notranslate"><span class="pre">test_transpose.py</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="k">class</span> <span class="nc">Grad</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Grad</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">sens</span><span class="p">):</span>
        <span class="n">gout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gout</span>

<span class="k">def</span> <span class="nf">test_grad_net</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">Grad</span><span class="p">(</span><span class="n">Net</span><span class="p">())</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sens</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dx: &quot;</span><span class="p">,</span> <span class="n">dx</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>-s<span class="w"> </span>test_transpose.py::test_grad_net
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>dx:  [[0. 2. 4.]
     [1. 3. 5.]]
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_operator_ascend.html" class="btn btn-neutral float-left" title="Custom Operators (Ascend)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="migrate_script.html" class="btn btn-neutral float-right" title="Migrating Training Scripts from Third Party Frameworks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>