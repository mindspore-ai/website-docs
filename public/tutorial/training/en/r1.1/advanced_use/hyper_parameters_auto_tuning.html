<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Use Mindoptimizer to Tune Hyperparameters &mdash; MindSpore r1.1 documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance Profiling(Ascend)" href="performance_profiling.html" />
    <link rel="prev" title="Viewing Lineage and Scalars Comparision" href="lineage_and_scalars_comparision.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Use</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/publish_model.html">Publishing Models using MindSpore Hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Process Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="convert_dataset.html">Converting Dataset to MindRecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_operator.html">Custom Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrate_script.html">Migrating Training Scripts from Third Party Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_deep_probability_programming.html">Deep Probabilistic Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="achieve_high_order_differentiation.html">Achieve High Order Differentiation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="visualization_tutorials.html">Training Process Visualization</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="summary_record.html">Collecting Summary Record</a></li>
<li class="toctree-l2"><a class="reference internal" href="dashboard.html">Viewing Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="lineage_and_scalars_comparision.html">Viewing Lineage and Scalars Comparision</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Use Mindoptimizer to Tune Hyperparameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuration-file-rules">Configuration File Rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#usage-examples">Usage Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="#notices">Notices</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_profiling.html">Performance Profiling(Ascend)</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_profiling_gpu.html">Performance Profiling（GPU）</a></li>
<li class="toctree-l2"><a class="reference internal" href="debugger.html">Using Debugger</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_explaination.html">Explain Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindinsight_commands.html">MindInsight Commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_cache.html">Application of Single-Node Tensor Cache</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Compression</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="improve_model_security_nad.html">Improving Model Security with NAD Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_differential_privacy.html">Protecting User Privacy with Differential Privacy Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_fuzzing.html">Testing Model Security Using Fuzz Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_membership_inference.html">Using Membership Inference to Test Model Security</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Use MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="visualization_tutorials.html">Training Process Visualization</a> &raquo;</li>
      <li>Use Mindoptimizer to Tune Hyperparameters</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_use/hyper_parameters_auto_tuning.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="use-mindoptimizer-to-tune-hyperparameters">
<h1>Use Mindoptimizer to Tune Hyperparameters<a class="headerlink" href="#use-mindoptimizer-to-tune-hyperparameters" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Optimization</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/training/source_en/advanced_use/hyper_parameters_auto_tuning.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>There are two kinds of parameters in machine learning. One is the model internal parameters, relying on training data and algorithms to tune the model parameters. And the other is the model external setting parameters, they need to be manually configured, such parameters are called hyperparameters. Because different hyperparameters impact the performance of model, hyperparameters are highly important in training tasks. Traditional methods require manual analysis of hyperparameters, manual debugging, and configuration, which consumes time and effort. MindInsight parameter tuning command can be used for automatic parameter tuning. Based on the parameter tuning configuration information provided by users, parameters can be automatically configured and model training can be performed.</p>
<p>MindInsight provides <code class="docutils literal notranslate"><span class="pre">mindoptimizer</span></code>. This tuning command can extract past training summaries from the training log according to the user configuration, analyze past training records and recommend hyperpameters, and finally automate training scripts. When using it, users need to configure information such as the scope of hyperparameters in yaml format. And then users need to replace the hyperparameters in the training script according to the tutorial, with the aim of synchronizing the auto-recommended hyperparameters into the training script. Currently, only the Gauss process tuning method is supported, and other methods are gradually supported.</p>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<p>This tool is a submodule of MindInsight. After MindInsight is installed, you can use the MindInsight parameter tuning command. For details about how to install MindInsight, see the <a class="reference external" href="https://gitee.com/mindspore/mindinsight/blob/r1.1/README.md#">installation guide</a>.</p>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline"></a></h2>
<p>MindInsight provides parameters tuning command. The command-line interface (CLI) provides the following commands:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>usage: mindoptimizer [-h] [--version] [--config CONFIG]
                     [--iter ITER]

optional arguments:
  -h, --help             Shows the help message and exits.
  --version              Shows the program version and exits.
  --config CONFIG        Specifies the configuration file for parameter tuning.
                         The file format is yaml.
  --iter ITER            Specifies the times of automatic training.
                         Automatically recommended parameters are used every time
                         before the training is performed.
                         The default value of ITER is 1.
</pre></div>
</div>
</section>
<section id="configuration-file-rules">
<h2>Configuration File Rules<a class="headerlink" href="#configuration-file-rules" title="Permalink to this headline"></a></h2>
<p>The file format of the configuration file is yaml, which requires configurations of running command, the root directory of training summaries, tuning method, optimization objectives, and hyperparameters information. It is necessary to configure the information about hyperparameters including the range of values, types and sources, etc. MindInsight extracts training records from the training summary based on configured hyperparameters and optimization objectives, such as learning and accuracy. They can be used by recommended algorithms to analyze their relationships and better recommend hyperparameters.</p>
<ol class="arabic">
<li><p>Configure the running command</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">command</span></code> to configure running command, such as <code class="docutils literal notranslate"><span class="pre">command:</span> <span class="pre">python</span> <span class="pre">train.py</span></code>. The running command is executed directly after the tuning program recommends hyperparameters.</p>
</li>
<li><p>Configure the root directory of training summaries</p>
<p>The <code class="docutils literal notranslate"><span class="pre">summary_base_dir</span></code> is the root directory of training summaries. It is also used for the extraction of training records, which makes hyperparameters better recommended. At the same time, it is recommended that users add <code class="docutils literal notranslate"><span class="pre">SummaryColletor</span></code> in their training scripts to collect training information, you can view the <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.1/advanced_use/summary_record.html">summary collection tutorial</a>. The tuning command generates a subdirectory path based on the configured <code class="docutils literal notranslate"><span class="pre">summary_base_dir</span></code>, which can be configured to record the training record at <code class="docutils literal notranslate"><span class="pre">SummaryColletor</span></code>. Therefore, after training, the training information is recorded in the subdirecte of the root directory of training summaries, and the training information can be used as a training record to recommend the next required hyperparameters. Configure the <code class="docutils literal notranslate"><span class="pre">summary_base_dir</span></code>, such as <code class="docutils literal notranslate"><span class="pre">/home/summaries</span></code>.</p>
</li>
<li><p>Configure the parameter tuning method</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">name</span></code> to specify the name of an acquisition function, and <code class="docutils literal notranslate"><span class="pre">args</span></code> tp specify parameters of the acquisition function.</p>
<p>The current algorithm is Gaussian process regressor(GP). The acquisition functhon of GP is optional, and its range is in [<code class="docutils literal notranslate"><span class="pre">ucb</span></code>, <code class="docutils literal notranslate"><span class="pre">pi</span></code>,<code class="docutils literal notranslate"><span class="pre">ei</span></code>]. The default value is <code class="docutils literal notranslate"><span class="pre">ucb</span></code>.</p>
<ul class="simple">
<li><p>Upper confidence bound (UCB)</p></li>
<li><p>Probability of improvement (PI)</p></li>
<li><p>Expected improvement (EI)</p></li>
</ul>
<p>For example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">tuner</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gp</span>
<span class="w">    </span><span class="nt">args</span><span class="p">:</span>
<span class="w">        </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ucb</span>
</pre></div>
</div>
</li>
<li><p>Configure the parameter tuning target</p>
<p>You can select loss or self-defined metrics as the target.</p>
<p>Configuration description:</p>
<ul class="simple">
<li><p>group: This parameter is optional. The value can be <code class="docutils literal notranslate"><span class="pre">system_defined</span></code> or <code class="docutils literal notranslate"><span class="pre">metric</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">system_defined</span></code>. Use <code class="docutils literal notranslate"><span class="pre">group</span></code> to configure the group in which the optimization target is located, such as the system custom collection field, which is the <code class="docutils literal notranslate"><span class="pre">system_defined</span></code> group. However, other evaluation metrics used in <code class="docutils literal notranslate"><span class="pre">Model()</span></code>, such as <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Model(net,</span> <span class="pre">loss_fn=loss,</span> <span class="pre">optimizer=None,</span> <span class="pre">metrics={'Accuracy'})</span></code>. <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code> belongs to the metrics, so the group is <code class="docutils literal notranslate"><span class="pre">metric</span></code>.</p></li>
<li><p>goal: This parameter is optional. The value can be <code class="docutils literal notranslate"><span class="pre">minimize</span></code> or <code class="docutils literal notranslate"><span class="pre">maximize</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">minimize</span></code>. Use <code class="docutils literal notranslate"><span class="pre">goal</span></code> to indicate the optimization direction of the target. For example, if <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code> is higher, the performance of model is better, so the <code class="docutils literal notranslate"><span class="pre">goal</span></code> needs to be configured as ‘maximize’.</p></li>
</ul>
<p>Config <code class="docutils literal notranslate"><span class="pre">loss</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">target</span><span class="p">:</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">name:loss</span>
</pre></div>
</div>
<p>Config <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code> in metrics:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">target</span><span class="p">:</span>
<span class="w">    </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">metric</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Accuracy</span>
<span class="w">    </span><span class="nt">goal</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">maximize</span>
</pre></div>
</div>
</li>
<li><p>Configure hyperparameters bounds, choice, type, and source</p>
<p>Configuration fields for hyperparameters consist of <code class="docutils literal notranslate"><span class="pre">bounds</span></code>, <code class="docutils literal notranslate"><span class="pre">choices</span></code>, <code class="docutils literal notranslate"><span class="pre">type</span></code>, and <code class="docutils literal notranslate"><span class="pre">source</span></code>. The fields of hyperparameters configured here are used for extraction of training summaries and recommendation of hyperparameters. In addition, <code class="docutils literal notranslate"><span class="pre">bounds</span></code>, <code class="docutils literal notranslate"><span class="pre">choice</span></code>, and <code class="docutils literal notranslate"><span class="pre">type</span></code> affect the recommendation of hyperparameters. <code class="docutils literal notranslate"><span class="pre">bounds</span></code> are configured as the upper and lower boundaries of the hyperparameters, <code class="docutils literal notranslate"><span class="pre">choice</span></code> indicates that which recommended values are selected, and <code class="docutils literal notranslate"><span class="pre">type</span></code> is the type of parameter configured.</p>
<p>The tunable fields currently collected by the system customization include <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">epoch</span></code>. Other parameters are user-defined parameters will be automatically collected in the training summary during training if the source is configured as <code class="docutils literal notranslate"><span class="pre">user_defined</span></code>.</p>
<ul class="simple">
<li><p>bounds: a list. The number of elements is 2. The first number is the lower bound min, and the second number is the upper bound max. The value range is [min, max). The method for generating a random number is <code class="docutils literal notranslate"><span class="pre">numpy.random.uniform()</span></code>.</p></li>
<li><p>choice: a list. The number of values is not limited. Values are selected from the elements in the list.</p></li>
<li><p>type: This parameter is mandatory and should be set to <code class="docutils literal notranslate"><span class="pre">int</span></code> or <code class="docutils literal notranslate"><span class="pre">float</span></code>.</p></li>
<li><p>source: This parameter is optional. The value should be <code class="docutils literal notranslate"><span class="pre">system_defined</span></code> or <code class="docutils literal notranslate"><span class="pre">user_defined</span></code>. If the name of parameter exists in system-defined field, the default value is <code class="docutils literal notranslate"><span class="pre">system_defined</span></code>, otherwise, the default value is <code class="docutils literal notranslate"><span class="pre">user_defined</span></code>.</p></li>
</ul>
<blockquote>
<div><p>You need to choose either <code class="docutils literal notranslate"><span class="pre">bounds</span></code> or <code class="docutils literal notranslate"><span class="pre">choice</span></code>. If you have configured <code class="docutils literal notranslate"><span class="pre">choice</span></code>, values are selected from the configured list only, and if you have configured both <code class="docutils literal notranslate"><span class="pre">choice</span></code> and <code class="docutils literal notranslate"><span class="pre">type</span></code>, <code class="docutils literal notranslate"><span class="pre">type</span></code> does not take effect.</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="usage-examples">
<h2>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this headline"></a></h2>
<p>If you want to optimize the <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">momentum</span></code>, and the optimization objective is <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code>, configure the YAML file as follows:</p>
<ol class="arabic">
<li><p>Configure config.yaml</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sh /home/example/run_alexnet_ascend.sh</span>
<span class="nt">summary_base_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/home/summaries</span>
<span class="nt">tuner</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gp</span>
<span class="nt">target</span><span class="p">:</span>
<span class="w">    </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">metric</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Accuracy</span>
<span class="w">    </span><span class="nt">goal</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">maximize</span>
<span class="nt">parameters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span>
<span class="w">        </span><span class="nt">bounds</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0.00001</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">0.001</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">float</span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span>
<span class="w">        </span><span class="nt">choice</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">32</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">64</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">128</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">256</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">int</span>
<span class="w">    </span><span class="nt">momentum</span><span class="p">:</span>
<span class="w">        </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">user_defined</span>
<span class="w">        </span><span class="nt">choice</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0.8</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">0.9</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">float</span>
</pre></div>
</div>
<blockquote>
<div><p>The name of <code class="docutils literal notranslate"><span class="pre">momentum</span></code> is not the same as that of the variable defined by the system. Therefore, you do not need to set the source field.</p>
</div></blockquote>
<p><strong>If the fields with the same name exist in the YAML file, the last one will be selected. Do not use the following method.</strong></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">parameters</span><span class="p">:</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span>
<span class="w">        </span><span class="nt">bounds</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0.0005</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">0.001</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">float</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span>
<span class="w">        </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">user_defined</span>
<span class="w">        </span><span class="nt">bounds</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0.00002</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">0.0001</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">float</span>
</pre></div>
</div>
</li>
<li><p>Instantiate the <code class="docutils literal notranslate"><span class="pre">HyperConfig</span></code> object in the training script</p>
<p>(1) After instantiating <code class="docutils literal notranslate"><span class="pre">HyperConfig</span></code>, use the parameter variables of the <code class="docutils literal notranslate"><span class="pre">HyperConfig</span></code> instance as the values of the corresponding parameters in the training script.<br />
(2) Please add <code class="docutils literal notranslate"><span class="pre">SummaryCollector</span></code> to collect lineage information, including hyperparameters and evaluation metrics.</p>
<p>For example, the training script in <a class="reference external" href="https://www.mindspore.cn/doc/note/en/r1.1/network_list_ms.html">Model Zoo</a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset_cifar10</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">get_lr_cifar10</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">ckpoint_cb</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">()]</span>
</pre></div>
</div>
<p>After the modification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindinsight.optimizer</span> <span class="kn">import</span> <span class="n">HyperConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">HyperConfig</span><span class="p">()</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">params</span>

<span class="c1"># Replace batch_size with params.batch_size.</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset_cifar10</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="c1"># Replace cfg.learning_rate with params.learning_rate.</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">get_lr_cifar10</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">))</span>
<span class="c1"># Replace cfg.momentum with params.momentum.</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>

<span class="c1"># Instantiate SummaryCollector and add it to callback to automatically collect training information.</span>
<span class="n">summary_cb</span> <span class="o">=</span> <span class="n">SummaryCollector</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_dir</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">ckpoint_cb</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">summary_cb</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>Execution</p>
<p>Please make sure that the training script can be executed correctly before performing automatic tuning.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mindoptimizer<span class="w"> </span>--config<span class="w"> </span>./config.yaml<span class="w"> </span>--iter<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
<blockquote>
<div><p>Please fill in the training command to execute the training in the configuration file, and run the automatic tuning program in the directory where the training command can be successfully run.</p>
</div></blockquote>
</li>
<li><p>Visualization</p>
<p>Enable MindInsight based on summary_base_dir configured in config.yaml. For details about the visualization method, see the <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.1/advanced_use/mindinsight_commands.html#start-the-service">MindInsight start tutorial</a>.</p>
</li>
</ol>
</section>
<section id="notices">
<h2>Notices<a class="headerlink" href="#notices" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>The training script is written and maintained by users. This tool does not automatically modify the training script. If the training script is incorrect, an error occurs when this tool is used to support the training script.</p></li>
<li><p>This tool does not process or modify the printed information during the running process.</p></li>
<li><p>Ensure that the parameter tuning process is trustworthy. If a parameter configuration error or script execution error occurs, the parameter tuning process will be terminated. You can locate the fault based on the displayed information.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lineage_and_scalars_comparision.html" class="btn btn-neutral float-left" title="Viewing Lineage and Scalars Comparision" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="performance_profiling.html" class="btn btn-neutral float-right" title="Performance Profiling(Ascend)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>