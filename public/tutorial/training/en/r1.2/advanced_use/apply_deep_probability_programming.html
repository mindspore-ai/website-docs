<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep Probabilistic Programming &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Implementing High-order Automatic Differentiation" href="implement_high_order_differentiation.html" />
    <link rel="prev" title="Migrating Training Scripts from Third Party Frameworks" href="migrate_3rd_scripts.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Use</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">Defining the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/publish_model.html">Publishing Models using MindSpore Hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Process Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="convert_dataset.html">Converting Dataset to MindRecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build Networks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="custom_loss_function.html">Customizing and Using Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_operator.html">Custom Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrate_script.html">Migrating Training Scripts from Third Party Frameworks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep Probabilistic Programming</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-bnn">Using BNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#processing-the-dataset">Processing the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-bnn">Defining the BNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-loss-function-and-optimizer">Defining the Loss Function and Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-network">Training the Network</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#using-the-vae">Using the VAE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-vae">Defining the VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Defining the Loss Function and Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#processing-data">Processing Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-new-samples-or-rebuilding-input-samples">Generating New Samples or Rebuilding Input Samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#one-click-conversion-from-dnn-to-bnn">One-click Conversion from DNN to BNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-dnn-model">Defining the DNN Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Defining the Loss Function and Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#instantiating-transformtobnn">Instantiating TransformToBNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#function-1-converting-the-entire-model">Function 1: Converting the Entire Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#function-2-converting-a-layer-of-a-specified-type">Function 2: Converting a Layer of a Specified Type</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#using-the-uncertainty-evaluation-toolbox">Using the Uncertainty Evaluation Toolbox</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="implement_high_order_differentiation.html">Implementing High-order Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantum_neural_network.html">Quantum Neural Network</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tutorials.html">Training Process Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_cache.html">Application of Single-Node Tensor Cache</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Compression</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="improve_model_security_nad.html">Improving Model Security with NAD Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_differential_privacy.html">Protecting User Privacy with Differential Privacy Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_suppress_privacy.html">Protecting User Privacy with Suppress Privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_fuzzing.html">Testing Model Security Using Fuzz Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_membership_inference.html">Using Membership Inference to Test Model Security</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Deep Probabilistic Programming</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_use/apply_deep_probability_programming.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="deep-probabilistic-programming">
<h1>Deep Probabilistic Programming<a class="headerlink" href="#deep-probabilistic-programming" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Whole</span> <span class="pre">Process</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.2/tutorials/training/source_en/advanced_use/apply_deep_probability_programming.md" target="_blank"><img src="../_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>A deep learning model has a strong fitting capability, and the Bayesian theory has a good explainability. MindSpore Deep Probabilistic Programming (MDP) combines deep learning and Bayesian learning. By setting a network weight to distribution and introducing latent space distribution, MDP can sample the distribution and forward propagation, which introduces uncertainty and enhances the robustness and explainability of a model. MDP contains general-purpose and professional probabilistic learning programming languages. It is applicable to professional users as well as beginners because the probabilistic programming supports the logic for developing deep learning models. In addition, it provides a toolbox for deep probabilistic learning to expand Bayesian application functions.</p>
<p>The following describes applications of deep probabilistic programming in MindSpore. Before performing the practice, ensure that MindSpore 0.7.0-beta or a later version has been installed. The contents are as follows:</p>
<ol class="arabic simple">
<li><p>Describe how to use the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/mindspore/nn/probability/bnn_layers">bnn_layers module</a> to implement the Bayesian neural network (BNN).</p></li>
<li><p>Describe how to use the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/mindspore/nn/probability/infer/variational">variational module</a> and <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/mindspore/nn/probability/dpn">dpn module</a> to implement the Variational Autoencoder (VAE).</p></li>
<li><p>Describe how to use the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/mindspore/nn/probability/transforms">transforms module</a> to implement one-click conversion from deep neural network (DNN) to BNN.</p></li>
<li><p>Describe how to use the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/mindspore/nn/probability/toolbox/uncertainty_evaluation.py">toolbox module</a> to implement uncertainty evaluation.</p></li>
</ol>
</section>
<section id="using-bnn">
<h2>Using BNN<a class="headerlink" href="#using-bnn" title="Permalink to this headline"></a></h2>
<p>BNN is a basic model composed of probabilistic model and neural network. Its weight is not a definite value, but a distribution. The following example describes how to use the bnn_layers module in MDP to implement a BNN, and then use the BNN to implement a simple image classification function. The overall process is as follows:</p>
<ol class="arabic simple">
<li><p>Process the MNIST dataset.</p></li>
<li><p>Define the Bayes LeNet.</p></li>
<li><p>Define the loss function and optimizer.</p></li>
<li><p>Load and train the dataset.</p></li>
</ol>
<blockquote>
<div><p>This example is for the GPU or Ascend 910 AI processor platform. You can download the complete sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/bnn_layers">https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/bnn_layers</a>.<br />
BNN only supports GRAPH mode now, please set <code class="docutils literal notranslate"><span class="pre">context.set_context(mode=context.GRAPH_MODE)</span></code> in your code.</p>
</div></blockquote>
<section id="processing-the-dataset">
<h3>Processing the Dataset<a class="headerlink" href="#processing-the-dataset" title="Permalink to this headline"></a></h3>
<p>The MNIST dataset is used in this example. The data processing is the same as that of <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.2/quick_start/quick_start.html">Implementing an Image Classification Application</a> in the tutorial.</p>
</section>
<section id="defining-the-bnn">
<h3>Defining the BNN<a class="headerlink" href="#defining-the-bnn" title="Permalink to this headline"></a></h3>
<p>Bayesian LeNet is used in this example. The method of building a BNN by using the bnn_layers module is the same as that of building a common neural network. Note that <code class="docutils literal notranslate"><span class="pre">bnn_layers</span></code> and common neural network layers can be combined with each other.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.probability</span> <span class="kn">import</span> <span class="n">bnn_layers</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="k">class</span> <span class="nc">BNNLeNet5</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    bayesian Lenet network</span>

<span class="sd">    Args:</span>
<span class="sd">        num_class (int): Num classes. Default: 10.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output tensor</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; BNNLeNet5(num_class=10)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BNNLeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span> <span class="o">=</span> <span class="n">num_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">ConvReparam</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">ConvReparam</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">DenseReparam</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">DenseReparam</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">DenseReparam</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="defining-the-loss-function-and-optimizer">
<h3>Defining the Loss Function and Optimizer<a class="headerlink" href="#defining-the-loss-function-and-optimizer" title="Permalink to this headline"></a></h3>
<p>A loss function and an optimizer need to be defined. The loss function is a training objective of the deep learning, and is also referred to as an objective function. The loss function indicates the distance between a logit of a neural network and a label, and is scalar data.</p>
<p>Common loss functions include mean square error, L2 loss, Hinge loss, and cross entropy. Cross entropy is usually used for image classification.</p>
<p>The optimizer is used for neural network solution (training). Because of the large scale of neural network parameters, the stochastic gradient descent (SGD) algorithm and its improved algorithm are used in deep learning to solve the problem. MindSpore encapsulates common optimizers, such as <code class="docutils literal notranslate"><span class="pre">SGD</span></code>, <code class="docutils literal notranslate"><span class="pre">Adam</span></code>, and <code class="docutils literal notranslate"><span class="pre">Momemtum</span></code>. In this example, the <code class="docutils literal notranslate"><span class="pre">Adam</span></code> optimizer is used. Generally, two parameters need to be set: learning rate (<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>) and weight attenuation (<code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>).</p>
<p>An example of the code for defining the loss function and optimizer in MindSpore is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># loss function definition</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="c1"># optimization definition</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-network">
<h3>Training the Network<a class="headerlink" href="#training-the-network" title="Permalink to this headline"></a></h3>
<p>The training process of BNN is similar to that of DNN. The only difference is that <code class="docutils literal notranslate"><span class="pre">WithLossCell</span></code> is replaced with <code class="docutils literal notranslate"><span class="pre">WithBNNLossCell</span></code> applicable to BNN. In addition to the <code class="docutils literal notranslate"><span class="pre">backbone</span></code> and <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> parameters, the <code class="docutils literal notranslate"><span class="pre">dnn_factor</span></code> and <code class="docutils literal notranslate"><span class="pre">bnn_factor</span></code> parameters are added to <code class="docutils literal notranslate"><span class="pre">WithBNNLossCell</span></code>. <code class="docutils literal notranslate"><span class="pre">dnn_factor</span></code> is a coefficient of the overall network loss computed by a loss function, and <code class="docutils literal notranslate"><span class="pre">bnn_factor</span></code> is a coefficient of the KL divergence of each Bayesian layer. The two parameters are used to balance the overall network loss and the KL divergence of the Bayesian layer, preventing the overall network loss from being covered by a large KL divergence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">WithBNNLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">dnn_factor</span><span class="o">=</span><span class="mi">60000</span><span class="p">,</span> <span class="n">bnn_factor</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span>
<span class="n">train_bnn_network</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">train_bnn_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;./mnist_data/train&#39;</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;./mnist_data/test&#39;</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">epoch</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">train_bnn_network</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">train_set</span><span class="p">)</span>

    <span class="n">valid_acc</span> <span class="o">=</span> <span class="n">validate_model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">test_set</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{}</span><span class="s1"> </span><span class="se">\t</span><span class="s1">Training Loss: </span><span class="si">{:.4f}</span><span class="s1"> </span><span class="se">\t</span><span class="s1">Training Accuracy: </span><span class="si">{:.4f}</span><span class="s1"> </span><span class="se">\t</span><span class="s1">validation Accuracy: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span>
          <span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">valid_acc</span><span class="p">))</span>
</pre></div>
</div>
<p>The code examples of <code class="docutils literal notranslate"><span class="pre">train_model</span></code> and <code class="docutils literal notranslate"><span class="pre">validate_model</span></code> in MindSpore are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">train_net</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()):</span>
        <span class="n">train_x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_net</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
        <span class="n">log_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
        <span class="n">accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>

    <span class="n">loss_sum</span> <span class="o">=</span> <span class="n">loss_sum</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span>
    <span class="n">acc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss_sum</span><span class="p">,</span> <span class="n">acc_mean</span>


<span class="k">def</span> <span class="nf">validate_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()):</span>
        <span class="n">train_x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
        <span class="n">log_output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
        <span class="n">accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>

    <span class="n">acc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">acc_mean</span>
</pre></div>
</div>
</section>
</section>
<section id="using-the-vae">
<h2>Using the VAE<a class="headerlink" href="#using-the-vae" title="Permalink to this headline"></a></h2>
<p>The following describes how to use the variational and dpn modules in MDP to implement VAE. VAE is a typical depth probabilistic model that applies variational inference to learn the representation of latent variables. The model can not only compress input data, but also generate new images of this type. The overall process is as follows:</p>
<ol class="arabic simple">
<li><p>Define a VAE.</p></li>
<li><p>Define the loss function and optimizer.</p></li>
<li><p>Process data.</p></li>
<li><p>Train the network.</p></li>
<li><p>Generate new samples or rebuild input samples.</p></li>
</ol>
<blockquote>
<div><p>This example is for the GPU or Ascend 910 AI processor platform. You can download the complete sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/dpn">https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/dpn</a>.</p>
</div></blockquote>
<section id="defining-the-vae">
<h3>Defining the VAE<a class="headerlink" href="#defining-the-vae" title="Permalink to this headline"></a></h3>
<p>Using the dpn module to build a VAE is simple. You only need to customize the encoder and decoder (DNN model) and call the <code class="docutils literal notranslate"><span class="pre">VAE</span></code> API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">IMAGE_SHAPE</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>


<span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">()</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>Defining the Loss Function and Optimizer<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>A loss function and an optimizer need to be defined. The loss function used in this example is <code class="docutils literal notranslate"><span class="pre">ELBO</span></code>, which is a loss function dedicated to variational inference. The optimizer used in this example is <code class="docutils literal notranslate"><span class="pre">Adam</span></code>.
An example of the code for defining the loss function and optimizer in MindSpore is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># loss function definition</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">ELBO</span><span class="p">(</span><span class="n">latent_prior</span><span class="o">=</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="n">output_prior</span><span class="o">=</span><span class="s1">&#39;Normal&#39;</span><span class="p">)</span>

<span class="c1"># optimization definition</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">vae</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="processing-data">
<h3>Processing Data<a class="headerlink" href="#processing-data" title="Permalink to this headline"></a></h3>
<p>The MNIST dataset is used in this example. The data processing is the same as that of <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.2/quick_start/quick_start.html">Implementing an Image Classification Application</a> in the tutorial.</p>
</section>
<section id="id2">
<h3>Training the Network<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">SVI</span></code> API in the variational module to train a VAE network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.probability.infer</span> <span class="kn">import</span> <span class="n">SVI</span>

<span class="n">vi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">=</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">vi</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">trained_loss</span> <span class="o">=</span> <span class="n">vi</span><span class="o">.</span><span class="n">get_train_loss</span><span class="p">()</span>
</pre></div>
</div>
<p>You can use <code class="docutils literal notranslate"><span class="pre">vi.run</span></code> to obtain a trained network and use <code class="docutils literal notranslate"><span class="pre">vi.get_train_loss</span></code> to obtain the loss after training.</p>
</section>
<section id="generating-new-samples-or-rebuilding-input-samples">
<h3>Generating New Samples or Rebuilding Input Samples<a class="headerlink" href="#generating-new-samples-or-rebuilding-input-samples" title="Permalink to this headline"></a></h3>
<p>With a trained VAE network, we can generate new samples or rebuild input samples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGE_SHAPE</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">generated_sample</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">generate_sample</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">IMAGE_SHAPE</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">ds_train</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="n">sample_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">reconstructed_sample</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">reconstruct_sample</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="one-click-conversion-from-dnn-to-bnn">
<h2>One-click Conversion from DNN to BNN<a class="headerlink" href="#one-click-conversion-from-dnn-to-bnn" title="Permalink to this headline"></a></h2>
<p>For DNN researchers unfamiliar with the Bayesian model, MDP provides an advanced API <code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> to convert the DNN model into the BNN model with one click. Currently, the API can be used in the LeNet, ResNet, MobileNet and VGG models. This example describes how to use the <code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> API in the transforms module to convert DNNs into BNNs with one click. The overall process is as follows:</p>
<ol class="arabic simple">
<li><p>Define a DNN model.</p></li>
<li><p>Define the loss function and optimizer.</p></li>
<li><p>Function 1: Convert the entire model.</p></li>
<li><p>Function 2: Convert a layer of a specified type.</p></li>
</ol>
<blockquote>
<div><p>This example is for the GPU or Ascend 910 AI processor platform. You can download the complete sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/transforms">https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/transforms</a>.</p>
</div></blockquote>
<section id="defining-the-dnn-model">
<h3>Defining the DNN Model<a class="headerlink" href="#defining-the-dnn-model" title="Permalink to this headline"></a></h3>
<p>LeNet is used as a DNN model in this example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">TruncatedNormal</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for conv layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                     <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                     <span class="n">weight_init</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fc_with_initialize</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for fc layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">weight_variable</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LeNet5</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Lenet network</span>

<span class="sd">    Args:</span>
<span class="sd">        num_class (int): Num classes. Default: 10.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output tensor</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; LeNet5(num_class=10)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span> <span class="o">=</span> <span class="n">num_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>The following shows the LeNet architecture.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LeNet5
  (conv1) Conv2dinput_channels=1, output_channels=6, kernel_size=(5, 5),stride=(1, 1),  pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False
  (conv2) Conv2dinput_channels=6, output_channels=16, kernel_size=(5, 5),stride=(1, 1),  pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False
  (fc1) Densein_channels=400, out_channels=120, weight=Parameter (name=fc1.weight), has_bias=True, bias=Parameter (name=fc1.bias)
  (fc2) Densein_channels=120, out_channels=84, weight=Parameter (name=fc2.weight), has_bias=True, bias=Parameter (name=fc2.bias)
  (fc3) Densein_channels=84, out_channels=10, weight=Parameter (name=fc3.weight), has_bias=True, bias=Parameter (name=fc3.bias)
  (relu) ReLU
  (max_pool2d) MaxPool2dkernel_size=2, stride=2, pad_mode=VALID
  (flatten) Flatten
</pre></div>
</div>
</section>
<section id="id3">
<h3>Defining the Loss Function and Optimizer<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>A loss function and an optimizer need to be defined. In this example, the cross entropy loss is used as the loss function, and <code class="docutils literal notranslate"><span class="pre">Adam</span></code> is used as the optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>

<span class="c1"># loss function definition</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="c1"># optimization definition</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>

<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="n">train_network</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="instantiating-transformtobnn">
<h3>Instantiating TransformToBNN<a class="headerlink" href="#instantiating-transformtobnn" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function of <code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> is defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformToBNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainable_dnn</span><span class="p">,</span> <span class="n">dnn_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bnn_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">trainable_dnn</span><span class="o">.</span><span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">trainable_dnn</span><span class="o">.</span><span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">net_with_loss</span><span class="o">.</span><span class="n">backbone_network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="s2">&quot;_loss_fn&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dnn_factor</span> <span class="o">=</span> <span class="n">dnn_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bnn_factor</span> <span class="o">=</span> <span class="n">bnn_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bnn_loss_file</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">trainable_bnn</span></code> parameter is a trainable DNN model packaged by <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>, <code class="docutils literal notranslate"><span class="pre">dnn_factor</span></code> and <code class="docutils literal notranslate"><span class="pre">bnn_factor</span></code> are the coefficient of the overall network loss computed by the loss function and the coefficient of the KL divergence of each Bayesian layer, respectively.
The code for instantiating <code class="docutils literal notranslate"><span class="pre">TransformToBNN</span></code> in MindSpore is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.probability</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">bnn_transformer</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">TransformToBNN</span><span class="p">(</span><span class="n">train_network</span><span class="p">,</span> <span class="mi">60000</span><span class="p">,</span> <span class="mf">0.000001</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="function-1-converting-the-entire-model">
<h3>Function 1: Converting the Entire Model<a class="headerlink" href="#function-1-converting-the-entire-model" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">transform_to_bnn_model</span></code> method can convert the entire DNN model into a BNN model. The definition is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">transform_to_bnn_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">get_dense_args</span><span class="o">=</span><span class="k">lambda</span> <span class="n">dp</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;in_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                          <span class="s2">&quot;out_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">activation</span><span class="p">},</span>
                               <span class="n">get_conv_args</span><span class="o">=</span><span class="k">lambda</span> <span class="n">dp</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;in_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="s2">&quot;out_channels&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                                                         <span class="s2">&quot;pad_mode&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                                                         <span class="s2">&quot;stride&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                         <span class="s2">&quot;padding&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="s2">&quot;dilation&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                         <span class="s2">&quot;group&quot;</span><span class="p">:</span> <span class="n">dp</span><span class="o">.</span><span class="n">group</span><span class="p">},</span>
                               <span class="n">add_dense_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">add_conv_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform the whole DNN model to BNN model, and wrap BNN model by TrainOneStepCell.</span>

<span class="sd">        Args:</span>
<span class="sd">            get_dense_args (function): The arguments gotten from the DNN full connection layer. Default: lambda dp:</span>
<span class="sd">                {&quot;in_channels&quot;: dp.in_channels, &quot;out_channels&quot;: dp.out_channels, &quot;has_bias&quot;: dp.has_bias}.</span>
<span class="sd">            get_conv_args (function): The arguments gotten from the DNN convolutional layer. Default: lambda dp:</span>
<span class="sd">                {&quot;in_channels&quot;: dp.in_channels, &quot;out_channels&quot;: dp.out_channels, &quot;pad_mode&quot;: dp.pad_mode,</span>
<span class="sd">                &quot;kernel_size&quot;: dp.kernel_size, &quot;stride&quot;: dp.stride, &quot;has_bias&quot;: dp.has_bias}.</span>
<span class="sd">            add_dense_args (dict): The new arguments added to BNN full connection layer. Default: {}.</span>
<span class="sd">            add_conv_args (dict): The new arguments added to BNN convolutional layer. Default: {}.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Cell, a trainable BNN model wrapped by TrainOneStepCell.</span>
<span class="sd">       &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">get_dense_args</span></code> parameter specifies parameters to be obtained from the fully connected layer of a DNN model, and the <code class="docutils literal notranslate"><span class="pre">get_conv_args</span></code> parameter specifies parameters to be obtained from the convolutional layer of the DNN model, the <code class="docutils literal notranslate"><span class="pre">add_dense_args</span></code> and <code class="docutils literal notranslate"><span class="pre">add_conv_args</span></code> parameters specify new parameter values to be specified for the BNN layer. Note that parameters in <code class="docutils literal notranslate"><span class="pre">add_dense_args</span></code> cannot be the same as those in <code class="docutils literal notranslate"><span class="pre">get_dense_args</span></code>. The same rule applies to <code class="docutils literal notranslate"><span class="pre">add_conv_args</span></code> and <code class="docutils literal notranslate"><span class="pre">get_conv_args</span></code>.</p>
<p>The code for converting the entire DNN model into a BNN model in MindSpore is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_bnn_network</span> <span class="o">=</span> <span class="n">bnn_transformer</span><span class="o">.</span><span class="n">transform_to_bnn_model</span><span class="p">()</span>
</pre></div>
</div>
<p>The structure of the converted model is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LeNet5
  (conv1) ConvReparam
    in_channels=1, out_channels=6, kernel_size=(5, 5), stride=(1, 1),  pad_mode=valid, padding=0, dilation=(1, 1), group=1, weight_mean=Parameter (name=conv1.weight_posterior.mean), weight_std=Parameter (name=conv1.weight_posterior.untransformed_std), has_bias=False
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (conv2) ConvReparam
    in_channels=6, out_channels=16, kernel_size=(5, 5), stride=(1, 1),  pad_mode=valid, padding=0, dilation=(1, 1), group=1, weight_mean=Parameter (name=conv2.weight_posterior.mean), weight_std=Parameter (name=conv2.weight_posterior.untransformed_std), has_bias=False
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (fc1) DenseReparam
    in_channels=400, out_channels=120, weight_mean=Parameter (name=fc1.weight_posterior.mean), weight_std=Parameter (name=fc1.weight_posterior.untransformed_std), has_bias=True, bias_mean=Parameter (name=fc1.bias_posterior.mean), bias_std=Parameter (name=fc1.bias_posterior.untransformed_std)
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None

    (bias_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (bias_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (fc2) DenseReparam
    in_channels=120, out_channels=84, weight_mean=Parameter (name=fc2.weight_posterior.mean), weight_std=Parameter (name=fc2.weight_posterior.untransformed_std), has_bias=True, bias_mean=Parameter (name=fc2.bias_posterior.mean), bias_std=Parameter (name=fc2.bias_posterior.untransformed_std)
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None

    (bias_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (bias_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (fc3) DenseReparam
    in_channels=84, out_channels=10, weight_mean=Parameter (name=fc3.weight_posterior.mean), weight_std=Parameter (name=fc3.weight_posterior.untransformed_std), has_bias=True, bias_mean=Parameter (name=fc3.bias_posterior.mean), bias_std=Parameter (name=fc3.bias_posterior.untransformed_std)
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None

    (bias_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (bias_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (relu) ReLU
  (max_pool2d) MaxPool2dkernel_size=2, stride=2, pad_mode=VALID
  (flatten) Flatten
</pre></div>
</div>
<p>The convolutional layer and fully connected layer on the entire LeNet are converted into the corresponding Bayesian layers.</p>
</section>
<section id="function-2-converting-a-layer-of-a-specified-type">
<h3>Function 2: Converting a Layer of a Specified Type<a class="headerlink" href="#function-2-converting-a-layer-of-a-specified-type" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">transform_to_bnn_layer</span></code> method can convert a layer of a specified type (nn.Dense or nn.Conv2d) in the DNN model into a corresponding Bayesian layer. The definition is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="k">def</span> <span class="nf">transform_to_bnn_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dnn_layer</span><span class="p">,</span> <span class="n">bnn_layer</span><span class="p">,</span> <span class="n">get_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform a specific type of layers in DNN model to corresponding BNN layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            dnn_layer_type (Cell): The type of DNN layer to be transformed to BNN layer. The optional values are</span>
<span class="sd">            nn.Dense, nn.Conv2d.</span>
<span class="sd">            bnn_layer_type (Cell): The type of BNN layer to be transformed to. The optional values are</span>
<span class="sd">                DenseReparameterization, ConvReparameterization.</span>
<span class="sd">            get_args (dict): The arguments gotten from the DNN layer. Default: None.</span>
<span class="sd">            add_args (dict): The new arguments added to BNN layer. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Cell, a trainable model wrapped by TrainOneStepCell, whose sprcific type of layer is transformed to the corresponding bayesian layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">dnn_layer</span></code> parameter specifies a type of a DNN layer to be converted into a BNN layer, and the <code class="docutils literal notranslate"><span class="pre">bnn_layer</span></code> parameter specifies a type of a BNN layer to be converted into a DNN layer, <code class="docutils literal notranslate"><span class="pre">get_args</span></code> and <code class="docutils literal notranslate"><span class="pre">add_args</span></code> specify parameters obtained from the DNN layer and the parameters to be re-assigned to the BNN layer, respectively.</p>
<p>The code for converting a Dense layer in a DNN model into a corresponding Bayesian layer <code class="docutils literal notranslate"><span class="pre">DenseReparam</span></code> in MindSpore is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_bnn_network</span> <span class="o">=</span> <span class="n">bnn_transformer</span><span class="o">.</span><span class="n">transform_to_bnn_layer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">,</span> <span class="n">bnn_layers</span><span class="o">.</span><span class="n">DenseReparam</span><span class="p">)</span>
</pre></div>
</div>
<p>The network structure after the conversion is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LeNet5
  (conv1) Conv2dinput_channels=1, output_channels=6, kernel_size=(5, 5),stride=(1, 1),  pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False
  (conv2) Conv2dinput_channels=6, output_channels=16, kernel_size=(5, 5),stride=(1, 1),  pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False
  (fc1) DenseReparam
    in_channels=400, out_channels=120, weight_mean=Parameter (name=fc1.weight_posterior.mean), weight_std=Parameter (name=fc1.weight_posterior.untransformed_std), has_bias=True, bias_mean=Parameter (name=fc1.bias_posterior.mean), bias_std=Parameter (name=fc1.bias_posterior.untransformed_std)
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None

    (bias_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (bias_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (fc2) DenseReparam
    in_channels=120, out_channels=84, weight_mean=Parameter (name=fc2.weight_posterior.mean), weight_std=Parameter (name=fc2.weight_posterior.untransformed_std), has_bias=True, bias_mean=Parameter (name=fc2.bias_posterior.mean), bias_std=Parameter (name=fc2.bias_posterior.untransformed_std)
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None

    (bias_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (bias_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (fc3) DenseReparam
    in_channels=84, out_channels=10, weight_mean=Parameter (name=fc3.weight_posterior.mean), weight_std=Parameter (name=fc3.weight_posterior.untransformed_std), has_bias=True, bias_mean=Parameter (name=fc3.bias_posterior.mean), bias_std=Parameter (name=fc3.bias_posterior.untransformed_std)
    (weight_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (weight_posterior) NormalPosterior
      (normal) Normalbatch_shape = None

    (bias_prior) NormalPrior
      (normal) Normalmean = 0.0, standard deviation = 0.1

    (bias_posterior) NormalPosterior
      (normal) Normalbatch_shape = None


  (relu) ReLU
  (max_pool2d) MaxPool2dkernel_size=2, stride=2, pad_mode=VALID
  (flatten) Flatten
</pre></div>
</div>
<p>As shown in the preceding information, the convolutional layer on the LeNet remains unchanged, and the fully connected layer becomes the corresponding Bayesian layer <code class="docutils literal notranslate"><span class="pre">DenseReparam</span></code>.</p>
</section>
</section>
<section id="using-the-uncertainty-evaluation-toolbox">
<h2>Using the Uncertainty Evaluation Toolbox<a class="headerlink" href="#using-the-uncertainty-evaluation-toolbox" title="Permalink to this headline"></a></h2>
<p>One of advantages of BNN is that uncertainty can be obtained. MDP provides a toolbox for uncertainty evaluation at the upper layer. Users can easily use the toolbox to compute uncertainty. Uncertainty means an uncertain degree of a prediction result of a deep learning model. Currently, most deep learning algorithm can only provide prediction results but cannot determine the result reliability. There are two types of uncertainties: aleatoric uncertainty and epistemic uncertainty.</p>
<ul class="simple">
<li><p>Aleatoric uncertainty: Internal noises in data, that is, unavoidable errors. This uncertainty cannot be reduced by adding sampling data.</p></li>
<li><p>Epistemic uncertainty: An inaccurate evaluation of input data by a model due to reasons such as poor training or insufficient training data. This may be reduced by adding training data.</p></li>
</ul>
<p>The uncertainty evaluation toolbox is applicable to mainstream deep learning models, such as regression and classification. During inference, developers can use the toolbox to obtain any aleatoric uncertainty and epistemic uncertainty by training models and training datasets and specifying tasks and samples to be evaluated. Developers can understand models and datasets based on uncertainty information.</p>
<blockquote>
<div><p>This example is for the GPU or Ascend 910 AI processor platform. You can download the complete sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/toolbox">https://gitee.com/mindspore/mindspore/tree/r1.2/tests/st/probability/toolbox</a>.</p>
</div></blockquote>
<p>The classification task is used as an example. The model is LeNet, the dataset is MNIST, and the data processing is the same as that of <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.2/quick_start/quick_start.html">Implementing an Image Classification Application</a> in the tutorial. To evaluate the uncertainty of the test example, use the toolbox as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.probability.toolbox.uncertainty_evaluation</span> <span class="kn">import</span> <span class="n">UncertaintyEvaluation</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">load_checkpoint</span><span class="p">,</span> <span class="n">load_param_into_net</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s1">&#39;checkpoint_lenet.ckpt&#39;</span><span class="p">)</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="c1"># get train and eval dataset</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;workspace/mnist/train&#39;</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s1">&#39;workspace/mnist/test&#39;</span><span class="p">)</span>
<span class="n">evaluation</span> <span class="o">=</span> <span class="n">UncertaintyEvaluation</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">network</span><span class="p">,</span>
                                   <span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span>
                                   <span class="n">task_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span>
                                   <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                   <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">epi_uncer_model_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                   <span class="n">ale_uncer_model_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                   <span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">eval_data</span> <span class="ow">in</span> <span class="n">ds_eval</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="n">eval_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">eval_data</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">epistemic_uncertainty</span> <span class="o">=</span> <span class="n">evaluation</span><span class="o">.</span><span class="n">eval_epistemic_uncertainty</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>
    <span class="n">aleatoric_uncertainty</span> <span class="o">=</span> <span class="n">evaluation</span><span class="o">.</span><span class="n">eval_aleatoric_uncertainty</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="migrate_3rd_scripts.html" class="btn btn-neutral float-left" title="Migrating Training Scripts from Third Party Frameworks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="implement_high_order_differentiation.html" class="btn btn-neutral float-right" title="Implementing High-order Automatic Differentiation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>