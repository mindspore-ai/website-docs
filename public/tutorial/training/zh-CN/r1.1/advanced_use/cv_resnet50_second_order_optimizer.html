<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>在ResNet-50网络上应用二阶优化实践 &mdash; MindSpore r1.1 documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/training.js"></script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="使用MobileNetV2网络实现微调（Fine Tune）" href="cv_mobilenetv2_fine_tune.html" />
    <link rel="prev" title="使用ResNet-50网络实现图像分类" href="cv_resnet50.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">实现一个图片分类应用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/linear_regression.html">实现简单线性函数拟合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_video.html">手把手安装和体验</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">基础使用</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preparation.html">加载数据集</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/defining_the_network.html">定义网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/save_model.html">保存模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/load_model_for_inference_and_transfer.html">加载模型用于推理或迁移学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/publish_model.html">发布模型</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">处理数据</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="convert_dataset.html">转换数据集为MindRecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">优化数据处理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">构建网络</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_operator.html">自定义算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrate_script.html">迁移第三方框架训练脚本</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_deep_probability_programming.html">深度概率编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="achieve_high_order_differentiation.html">实现高阶自动微分</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">调试网络</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">使用PyNative模式调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">自定义调试信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tutorials.html">使用可视化组件MindInsight</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_augmentation.html">应用自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">训练时验证模型</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">优化训练性能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_tutorials.html">分布式并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">使能自动混合精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">应用梯度累积算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_cache.html">应用单节点数据缓存</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">压缩模型</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">应用感知量化训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型安全和隐私</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="improve_model_security_nad.html">使用NAD算法提升模型安全性</a></li>
<li class="toctree-l1"><a class="reference internal" href="protect_user_privacy_with_differential_privacy.html">应用差分隐私机制保护用户隐私</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_fuzzing.html">使用fuzz testing模块测试模型安全性</a></li>
<li class="toctree-l1"><a class="reference internal" href="test_model_security_membership_inference.html">使用成员推理测试模型安全性</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">应用实践</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="cv.html">机器视觉</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cv_resnet50.html">使用ResNet-50网络实现图像分类</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">在ResNet-50网络上应用二阶优化实践</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">准备环节</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">准备数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">配置分布式环境变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">加载处理数据集</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">定义网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#thor">定义损失函数及THOR优化器</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id7">定义损失函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">定义优化器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id9">训练网络</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">配置模型保存</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">配置训练网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">运行脚本</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id15">模型推理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id16">定义推理网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id17">执行推理</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cv_mobilenetv2_fine_tune.html">使用MobileNetV2网络实现微调（Fine Tune）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">高性能计算</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">在云上使用MindSpore</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="cv.html">机器视觉</a> &raquo;</li>
      <li>在ResNet-50网络上应用二阶优化实践</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_use/cv_resnet50_second_order_optimizer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="resnet-50">
<h1>在ResNet-50网络上应用二阶优化实践<a class="headerlink" href="#resnet-50" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">模型开发</span></code> <code class="docutils literal notranslate"><span class="pre">模型调优</span></code> <code class="docutils literal notranslate"><span class="pre">高级</span></code>
<a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.1/tutorials/training/source_zh_cn/advanced_use/cv_resnet50_second_order_optimizer.md"><img alt="查看源文件" src="../_images/logo_source.png" /></a>  </p>
<section id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>常见的优化算法可分为一阶优化算法和二阶优化算法。经典的一阶优化算法如SGD等，计算量小、计算速度快，但是收敛的速度慢，所需的迭代次数多。而二阶优化算法使用目标函数的二阶导数来加速收敛，能更快地收敛到模型最优值，所需要的迭代次数少，但由于二阶优化算法过高的计算成本，导致其总体执行时间仍然慢于一阶，故目前在深度神经网络训练中二阶优化算法的应用并不普遍。二阶优化算法的主要计算成本在于二阶信息矩阵（Hessian矩阵、<a class="reference external" href="https://arxiv.org/pdf/1808.07172.pdf">FIM矩阵</a>等）的求逆运算，时间复杂度约为<span class="math notranslate nohighlight">\(O(n^3)\)</span>。</p>
<p>MindSpore开发团队在现有的自然梯度算法的基础上，对FIM矩阵采用近似、切分等优化加速手段，极大的降低了逆矩阵的计算复杂度，开发出了可用的二阶优化器THOR。使用8块Ascend 910 AI处理器，THOR可以在72min内完成ResNet50-v1.5网络和ImageNet数据集的训练，相比于SGD+Momentum速度提升了近一倍。</p>
<p>本篇教程将主要介绍如何在Ascend 910 以及GPU上，使用MindSpore提供的二阶优化器THOR训练ResNet50-v1.5网络和ImageNet数据集。</p>
<blockquote>
<div><p>你可以在这里下载完整的示例代码：
<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.1/model_zoo/official/cv/resnet_thor">https://gitee.com/mindspore/mindspore/tree/r1.1/model_zoo/official/cv/resnet_thor</a> 。</p>
</div></blockquote>
<p>示例代码目录结构</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>├──<span class="w"> </span>resnet_thor
<span class="w">    </span>├──<span class="w"> </span>README.md
<span class="w">    </span>├──<span class="w"> </span>scripts
<span class="w">        </span>├──<span class="w"> </span>run_distribute_train.sh<span class="w">         </span><span class="c1"># launch distributed training for Ascend 910</span>
<span class="w">        </span>└──<span class="w"> </span>run_eval.sh<span class="w">                     </span><span class="c1"># launch inference for Ascend 910</span>
<span class="w">        </span>├──<span class="w"> </span>run_distribute_train_gpu.sh<span class="w">     </span><span class="c1"># launch distributed training for GPU</span>
<span class="w">        </span>└──<span class="w"> </span>run_eval_gpu.sh<span class="w">                 </span><span class="c1"># launch inference for GPU</span>
<span class="w">    </span>├──<span class="w"> </span>src
<span class="w">        </span>├──<span class="w"> </span>crossentropy.py<span class="w">                 </span><span class="c1"># CrossEntropy loss function</span>
<span class="w">        </span>├──<span class="w"> </span>config.py<span class="w">                       </span><span class="c1"># parameter configuration</span>
<span class="w">        </span>├──<span class="w"> </span>dataset_helper.py<span class="w">               </span><span class="c1"># dataset helper for minddata dataset</span>
<span class="w">        </span>├──<span class="w"> </span>grad_reducer_thor.py<span class="w">            </span><span class="c1"># grad reduce for thor</span>
<span class="w">        </span>├──<span class="w"> </span>model_thor.py<span class="w">                   </span><span class="c1"># model for train</span>
<span class="w">        </span>├──<span class="w"> </span>resnet_thor.py<span class="w">                  </span><span class="c1"># resnet50_thor backone</span>
<span class="w">        </span>├──<span class="w"> </span>thor.py<span class="w">                         </span><span class="c1"># thor optimizer</span>
<span class="w">        </span>├──<span class="w"> </span>thor_layer.py<span class="w">                   </span><span class="c1"># thor layer</span>
<span class="w">        </span>└──<span class="w"> </span>dataset.py<span class="w">                      </span><span class="c1"># data preprocessing</span>
<span class="w">    </span>├──<span class="w"> </span>eval.py<span class="w">                             </span><span class="c1"># infer script</span>
<span class="w">    </span>├──<span class="w"> </span>train.py<span class="w">                            </span><span class="c1"># train script</span>
<span class="w">    </span>├──<span class="w"> </span>export.py<span class="w">                           </span><span class="c1"># export checkpoint file into air file</span>
<span class="w">    </span>└──<span class="w"> </span>mindspore_hub_conf.py<span class="w">               </span><span class="c1"># config file for mindspore hub repository</span>
</pre></div>
</div>
<p>整体执行流程如下：</p>
<ol class="arabic simple">
<li><p>准备ImageNet数据集，处理需要的数据集；</p></li>
<li><p>定义ResNet50网络；</p></li>
<li><p>定义损失函数和THOR优化器；</p></li>
<li><p>加载数据集并进行训练，训练完成后，查看结果及保存模型文件；</p></li>
<li><p>加载保存的模型，进行推理。</p></li>
</ol>
</section>
<section id="id2">
<h2>准备环节<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>实践前，确保已经正确安装MindSpore。如果没有，可以通过<a class="reference external" href="https://www.mindspore.cn/install">MindSpore安装页面</a>安装MindSpore。</p>
<section id="id3">
<h3>准备数据集<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>下载完整的ImageNet2012数据集，将数据集解压分别存放到本地工作区的<code class="docutils literal notranslate"><span class="pre">ImageNet2012/ilsvrc</span></code>、<code class="docutils literal notranslate"><span class="pre">ImageNet2012/ilsvrc_eval</span></code>路径下。</p>
<p>目录结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ImageNet2012
    ├─ilsvrc
    │      n03676483
    │      n04067472
    │      n01622779
    │      ......
    └─ilsvrc_eval
    │      n03018349
    │      n02504013
    │      n07871810
    │      ......
</pre></div>
</div>
</section>
<section id="id4">
<h3>配置分布式环境变量<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<section id="ascend-910">
<h4>Ascend 910<a class="headerlink" href="#ascend-910" title="Permalink to this headline"></a></h4>
<p>Ascend 910 AI处理器的分布式环境变量配置参考<a class="reference external" href="https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_ascend.html#id4">分布式并行训练 (Ascend)</a>。</p>
</section>
<section id="gpu">
<h4>GPU<a class="headerlink" href="#gpu" title="Permalink to this headline"></a></h4>
<p>GPU的分布式环境配置参考<a class="reference external" href="https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_gpu.html#id4">分布式并行训练 (GPU)</a>。</p>
</section>
</section>
</section>
<section id="id5">
<h2>加载处理数据集<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h2>
<p>分布式训练时，通过并行的方式加载数据集，同时通过MindSpore提供的数据增强接口对数据集进行处理。加载处理数据集的脚本在源码的<code class="docutils literal notranslate"><span class="pre">src/dataset.py</span></code>脚本中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision.c_transforms</span> <span class="k">as</span> <span class="nn">C</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms.c_transforms</span> <span class="k">as</span> <span class="nn">C2</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">do_train</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">device_num</span><span class="p">,</span> <span class="n">rank_id</span> <span class="o">=</span> <span class="n">_get_rank_info</span><span class="p">()</span>
        <span class="n">num_parallels</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">init</span><span class="p">()</span>
        <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
        <span class="n">device_num</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="n">num_parallels</span> <span class="o">=</span> <span class="mi">4</span>

    <span class="k">if</span> <span class="n">device_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallels</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallels</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                     <span class="n">num_shards</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.456</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.406</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.224</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.225</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>

    <span class="c1"># define map operations</span>
    <span class="k">if</span> <span class="n">do_train</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">C</span><span class="o">.</span><span class="n">RandomCropDecodeResize</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.333</span><span class="p">)),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">C</span><span class="o">.</span><span class="n">Decode</span><span class="p">(),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>

    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">C2</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallels</span><span class="p">)</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallels</span><span class="p">)</span>

    <span class="c1"># apply batch operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># apply dataset repeat operation</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_set</span>
</pre></div>
</div>
<blockquote>
<div><p>MindSpore支持进行多种数据处理和增强的操作，各种操作往往组合使用，具体可以参考<a class="reference external" href="https://www.mindspore.cn/doc/programming_guide/zh-CN/r1.1/pipeline.html">数据处理</a>和<a class="reference external" href="https://www.mindspore.cn/doc/programming_guide/zh-CN/r1.1/augmentation.html">数据增强</a>章节。</p>
</div></blockquote>
</section>
<section id="id6">
<h2>定义网络<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<p>本示例中使用的网络模型为ResNet50-v1.5，先定义<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.1/model_zoo/official/cv/resnet/src/resnet.py">ResNet50网络</a>，然后使用二阶优化器自定义的算子替换<code class="docutils literal notranslate"><span class="pre">Conv2d</span></code>和
和<code class="docutils literal notranslate"><span class="pre">Dense</span></code>算子。定义好的网络模型在在源码<code class="docutils literal notranslate"><span class="pre">src/resnet_thor.py</span></code>脚本中，自定义的算子<code class="docutils literal notranslate"><span class="pre">Conv2d_thor</span></code>和<code class="docutils literal notranslate"><span class="pre">Dense_thor</span></code>在<code class="docutils literal notranslate"><span class="pre">src/thor_layer.py</span></code>脚本中。</p>
<ul class="simple">
<li><p>使用<code class="docutils literal notranslate"><span class="pre">Conv2d_thor</span></code>替换原网络模型中的<code class="docutils literal notranslate"><span class="pre">Conv2d</span></code></p></li>
<li><p>使用<code class="docutils literal notranslate"><span class="pre">Dense_thor</span></code>替换原网络模型中的<code class="docutils literal notranslate"><span class="pre">Dense</span></code></p></li>
</ul>
<blockquote>
<div><p>使用THOR自定义的算子<code class="docutils literal notranslate"><span class="pre">Conv2d_thor</span></code>和<code class="docutils literal notranslate"><span class="pre">Dense_thor</span></code>是为了保存模型训练中的二阶矩阵信息，新定义的网络与原网络模型的backbone一致。</p>
</div></blockquote>
<p>网络构建完成以后，在<code class="docutils literal notranslate"><span class="pre">__main__</span></code>函数中调用定义好的ResNet50：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">src.resnet_thor</span> <span class="kn">import</span> <span class="n">resnet50</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define the net</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">,</span> <span class="n">damping</span><span class="o">=</span><span class="n">damping</span><span class="p">,</span> <span class="n">loss_scale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span>
                   <span class="n">frequency</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">frequency</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="thor">
<h2>定义损失函数及THOR优化器<a class="headerlink" href="#thor" title="Permalink to this headline"></a></h2>
<section id="id7">
<h3>定义损失函数<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<p>MindSpore支持的损失函数有<code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>、<code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>、<code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>等。THOR优化器需要使用<code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>损失函数。</p>
<p>损失函数的实现步骤在<code class="docutils literal notranslate"><span class="pre">src/crossentropy.py</span></code>脚本中。这里使用了深度网络模型训练中的一个常用trick：label smoothing，通过对真实标签做平滑处理，提高模型对分类错误标签的容忍度，从而可以增加模型的泛化能力。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropy</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CrossEntropy&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smooth_factor</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CrossEntropy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">smooth_factor</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">smooth_factor</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">one_hot_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logit</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>在<code class="docutils literal notranslate"><span class="pre">__main__</span></code>函数中调用定义好的损失函数：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">src.crossentropy</span> <span class="kn">import</span> <span class="n">CrossEntropy</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define the loss function</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">use_label_smooth</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="p">(</span><span class="n">smooth_factor</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>定义优化器<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<p>THOR优化器的参数更新公式如下：</p>
<div class="math notranslate nohighlight">
\[ \theta^{t+1} = \theta^t + \alpha F^{-1}\nabla E\]</div>
<p>参数更新公式中各参数的含义如下：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span>：网络中的可训参数；</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>：迭代次数；</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>：学习率值，参数的更新步长；</p></li>
<li><p><span class="math notranslate nohighlight">\(F^{-1}\)</span>：FIM矩阵，在网络中计算获得；</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla E\)</span>：一阶梯度值。</p></li>
</ul>
<p>从参数更新公式中可以看出，THOR优化器需要额外计算的是每一层的FIM矩阵，每一层的FIM矩阵就是之前在自定义的网络模型中计算获得的。FIM矩阵可以对每一层参数更新的步长和方向进行自适应的调整，加速收敛的同时可以降低调参的复杂度。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">src.thor</span> <span class="kn">import</span> <span class="n">THOR</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">src.thor</span> <span class="kn">import</span> <span class="n">THOR_GPU</span> <span class="k">as</span> <span class="n">THOR</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># learning rate setting</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">get_model_lr</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_init</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_decay</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_end_epoch</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">decay_epochs</span><span class="o">=</span><span class="mi">39</span><span class="p">)</span>
    <span class="c1"># define the optimizer</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">THOR</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span>
               <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;matrix_A&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span>
               <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;matrix_G&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span>
               <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;A_inv_max&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span>
               <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;G_inv_max&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span>
               <span class="n">config</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="id9">
<h2>训练网络<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<section id="id10">
<h3>配置模型保存<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<p>MindSpore提供了callback机制，可以在训练过程中执行自定义逻辑，这里使用框架提供的<code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>函数。
<code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>可以保存网络模型和参数，以便进行后续的fine-tuning操作。
<code class="docutils literal notranslate"><span class="pre">TimeMonitor</span></code>、<code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code>是MindSpore官方提供的callback函数，可以分别用于监控训练过程中单步迭代时间和<code class="docutils literal notranslate"><span class="pre">loss</span></code>值的变化。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">TimeMonitor</span><span class="p">,</span> <span class="n">LossMonitor</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define callbacks</span>
    <span class="n">time_cb</span> <span class="o">=</span> <span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">cb</span> <span class="o">=</span> <span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">loss_cb</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">:</span>
        <span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint_epochs</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span>
                                     <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">keep_checkpoint_max</span><span class="p">)</span>
        <span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">ckpt_save_dir</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
        <span class="n">cb</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ckpt_cb</span><span class="p">]</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="id11">
<h3>配置训练网络<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h3>
<p>通过MindSpore提供的<code class="docutils literal notranslate"><span class="pre">model.train</span></code>接口可以方便地进行网络的训练。THOR优化器通过降低二阶矩阵更新频率，来减少计算量，提升计算速度，故重新定义一个Model_Thor类，继承MindSpore提供的Model类。在Model_Thor类中增加二阶矩阵更新频率控制参数，用户可以通过调整该参数，优化整体的性能。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">FixedLossScaleManager</span>
<span class="kn">from</span> <span class="nn">src.model_thor</span> <span class="kn">import</span> <span class="n">Model_Thor</span> <span class="k">as</span> <span class="n">Model</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="n">loss_scale</span> <span class="o">=</span> <span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">amp_level</span><span class="o">=</span><span class="s1">&#39;O2&#39;</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span>
                      <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">},</span> <span class="n">frequency</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">frequency</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">},</span>
                      <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">frequency</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="id12">
<h3>运行脚本<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h3>
<p>训练脚本定义完成之后，调<code class="docutils literal notranslate"><span class="pre">scripts</span></code>目录下的shell脚本，启动分布式训练进程。</p>
<section id="id13">
<h4>Ascend 910<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h4>
<p>目前MindSpore分布式在Ascend上执行采用单卡单进程运行方式，即每张卡上运行1个进程，进程数量与使用的卡的数量一致。进程均放在后台执行，每个进程创建1个目录，目录名称为<code class="docutils literal notranslate"><span class="pre">train_parallel</span></code>+ <code class="docutils literal notranslate"><span class="pre">device_id</span></code>，用来保存日志信息，算子编译信息以及训练的checkpoint文件。下面以使用8张卡的分布式训练脚本为例，演示如何运行脚本：</p>
<p>使用以下命令运行脚本：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>run_distribute_train.sh<span class="w"> </span>&lt;RANK_TABLE_FILE&gt;<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;DEVICE_NUM&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>、<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">DEVICE_NUM</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>：组网信息文件的路径。(rank table文件的生成，参考<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.1/model_zoo/utils/hccl_tools">HCCL_TOOL</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：训练数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE_NUM</span></code>：实际的运行卡数。</p></li>
</ul>
<p>其余环境变量请参考安装教程中的配置项。</p>
<p>训练过程中loss打印示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>...
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">4</span>.4182425
epoch:<span class="w"> </span><span class="m">2</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">3</span>.740064
epoch:<span class="w"> </span><span class="m">3</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">4</span>.0546017
epoch:<span class="w"> </span><span class="m">4</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">3</span>.7598825
epoch:<span class="w"> </span><span class="m">5</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">3</span>.3744206
...
epoch:<span class="w"> </span><span class="m">40</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">1</span>.6907625
epoch:<span class="w"> </span><span class="m">41</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">1</span>.8217756
epoch:<span class="w"> </span><span class="m">42</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">1</span>.6453942
...
</pre></div>
</div>
<p>训练完后，每张卡训练产生的checkpoint文件保存在各自训练目录下，<code class="docutils literal notranslate"><span class="pre">device_0</span></code>产生的checkpoint文件示例如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>└─train_parallel0
<span class="w">    </span>├─resnet-1_5004.ckpt
<span class="w">    </span>├─resnet-2_5004.ckpt
<span class="w">    </span>│<span class="w">      </span>......
<span class="w">    </span>├─resnet-42_5004.ckpt
<span class="w">    </span>│<span class="w">      </span>......
</pre></div>
</div>
<p>其中，
<code class="docutils literal notranslate"><span class="pre">*.ckpt</span></code>：指保存的模型参数文件。checkpoint文件名称具体含义：<em>网络名称</em>-<em>epoch数</em>_<em>step数</em>.ckpt。</p>
</section>
<section id="id14">
<h4>GPU<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h4>
<p>在GPU硬件平台上，MindSpore采用OpenMPI的<code class="docutils literal notranslate"><span class="pre">mpirun</span></code>进行分布式训练，进程创建1个目录，目录名称为<code class="docutils literal notranslate"><span class="pre">train_parallel</span></code>，用来保存日志信息和训练的checkpoint文件。下面以使用8张卡的分布式训练脚本为例，演示如何运行脚本：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>run_distribute_train_gpu.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;DEVICE_NUM&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">DEVICE_NUM</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：训练数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE_NUM</span></code>：实际的运行卡数。</p></li>
</ul>
<p>在GPU训练时，无需设置<code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code>环境变量，因此在主训练脚本中不需要调用<code class="docutils literal notranslate"><span class="pre">int(os.getenv('DEVICE_ID'))</span></code>来获取卡的物理序号，同时<code class="docutils literal notranslate"><span class="pre">context</span></code>中也无需传入<code class="docutils literal notranslate"><span class="pre">device_id</span></code>。我们需要将device_target设置为GPU，并需要调用<code class="docutils literal notranslate"><span class="pre">init()</span></code>来使能NCCL。</p>
<p>训练过程中loss打印示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>...
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">4</span>.2546034
epoch:<span class="w"> </span><span class="m">2</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">4</span>.0819564
epoch:<span class="w"> </span><span class="m">3</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">3</span>.7005644
epoch:<span class="w"> </span><span class="m">4</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">3</span>.2668946
epoch:<span class="w"> </span><span class="m">5</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">3</span>.023509
...
epoch:<span class="w"> </span><span class="m">36</span><span class="w"> </span>step:<span class="w"> </span><span class="m">5004</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">1</span>.645802
...
</pre></div>
</div>
<p>训练完后，保存的模型文件示例如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>└─train_parallel
<span class="w">    </span>├─ckpt_0
<span class="w">        </span>├─resnet-1_5004.ckpt
<span class="w">        </span>├─resnet-2_5004.ckpt
<span class="w">        </span>│<span class="w">      </span>......
<span class="w">        </span>├─resnet-36_5004.ckpt
<span class="w">        </span>│<span class="w">      </span>......
<span class="w">    </span>......
<span class="w">    </span>├─ckpt_7
<span class="w">        </span>├─resnet-1_5004.ckpt
<span class="w">        </span>├─resnet-2_5004.ckpt
<span class="w">        </span>│<span class="w">      </span>......
<span class="w">        </span>├─resnet-36_5004.ckpt
<span class="w">        </span>│<span class="w">      </span>......
</pre></div>
</div>
</section>
</section>
</section>
<section id="id15">
<h2>模型推理<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h2>
<p>使用训练过程中保存的checkpoint文件进行推理，验证模型的泛化能力。首先通过<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code>接口加载模型文件，然后调用<code class="docutils literal notranslate"><span class="pre">Model</span></code>的<code class="docutils literal notranslate"><span class="pre">eval</span></code>接口对输入图片类别作出预测，再与输入图片的真实类别做比较，得出最终的预测精度值。</p>
<section id="id16">
<h3>定义推理网络<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>使用<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code>接口加载模型文件。</p></li>
<li><p>使用<code class="docutils literal notranslate"><span class="pre">model.eval</span></code>接口读入测试数据集，进行推理。</p></li>
<li><p>计算得出预测精度值。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">load_checkpoint</span><span class="p">,</span> <span class="n">load_param_into_net</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define net</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">thor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># load checkpoint</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">args_opt</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;damping&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># define model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>

    <span class="c1"># eval model</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;result:&quot;</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="s2">&quot;ckpt=&quot;</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id17">
<h3>执行推理<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<p>推理网络定义完成之后，调用<code class="docutils literal notranslate"><span class="pre">scripts</span></code>目录下的shell脚本，进行推理。</p>
<section id="id18">
<h4>Ascend 910<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h4>
<p>在Ascend 910硬件平台上，推理的执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>run_eval.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CHECKPOINT_PATH&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：推理数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>：保存的checkpoint路径。</p></li>
</ul>
<p>目前推理使用的是单卡（默认device 0）进行推理，推理的结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result: {&#39;top_5_accuracy&#39;: 0.9295574583866837, &#39;top_1_accuracy&#39;: 0.761443661971831} ckpt=train_parallel0/resnet-42_5004.ckpt
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">top_5_accuracy</span></code>：对于一个输入图片，如果预测概率排名前五的标签中包含真实标签，即认为分类正确；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_1_accuracy</span></code>：对于一个输入图片，如果预测概率最大的标签与真实标签相同，即认为分类正确。</p></li>
</ul>
</section>
<section id="id19">
<h4>GPU<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h4>
<p>在GPU硬件平台上，推理的执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>run_eval_gpu.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CHECKPOINT_PATH&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：推理数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>：保存的checkpoint路径。</p></li>
</ul>
<p>推理的结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result: {&#39;top_5_accuracy&#39;: 0.9287972151088348, &#39;top_1_accuracy&#39;: 0.7597031049935979} ckpt=train_parallel/resnet-36_5004.ckpt
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cv_resnet50.html" class="btn btn-neutral float-left" title="使用ResNet-50网络实现图像分类" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="cv_mobilenetv2_fine_tune.html" class="btn btn-neutral float-right" title="使用MobileNetV2网络实现微调（Fine Tune）" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>