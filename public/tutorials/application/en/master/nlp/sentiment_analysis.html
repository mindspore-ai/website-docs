<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sentiment Classification Implemented by RNN &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LSTM+CRF Sequence Labeling" href="sequence_labeling.html" />
    <link rel="prev" title="SSD for Object Detection" href="../cv/ssd.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">CV</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cv/resnet50.html">ResNet-50 for Image Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/transfer_learning.html">ResNet50 Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/fgsm.html">FGSM Network Adversarial Attack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/vit.html">Vision Transformer Image Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/cnnctc.html">CNN and CTC for Recognizing Text from Images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/fcn8s.html">FCN for Image Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/shufflenet.html">ShuffleNet for Image Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cv/ssd.html">SSD for Object Detection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NLP</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Sentiment Classification Implemented by RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-downloading-module">Data Downloading Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-imdb-dataset">Loading the IMDB Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-pre-trained-word-vectors">Loading Pre-trained Word Vectors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-preprocessing">Dataset Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-building">Model Building</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#embedding">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loss-function-and-optimizer">Loss Function and Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-logic">Training Logic</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation-metrics-and-logic">Evaluation Metrics and Logic</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-training-and-saving">Model Training and Saving</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-loading-and-testing">Model Loading and Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#custom-input-test">Custom Input Test</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sequence_labeling.html">LSTM+CRF Sequence Labeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="sequence_to_sequence.html">Text Translation Implemented by Seq2Seq Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Generative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../generative/gan.html">GAN for Image Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generative/dcgan.html">Generating Cartoon Head Portrait via DCGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generative/pix2pix.html">Pix2Pix for Image Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generative/cyclegan.html">CycleGAN for Image Style Migration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generative/diffusion.html">Diffusion Model</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Sentiment Classification Implemented by RNN</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/nlp/sentiment_analysis.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="sentiment-classification-implemented-by-rnn">
<h1>Sentiment Classification Implemented by RNN<a class="headerlink" href="#sentiment-classification-implemented-by-rnn" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/application/source_en/nlp/sentiment_analysis.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Sentiment classification is a classic task in natural language processing. It is a typical classification problem. The following uses MindSpore to implement an RNN-based sentimental classification model to achieve the following effects:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: This film is terrible
Correct label: Negative
Forecast label: Negative

Input: This film is great
Correct label: Positive
Forecast label: Positive
</pre></div>
</div>
</section>
<section id="data-preparation">
<h2>Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline"></a></h2>
<p>This section uses the classic <a class="reference external" href="https://ai.stanford.edu/~amaas/data/sentiment/">IMDB Movie Review Dataset</a> for sentimental classification. The dataset contains positive and negative data. The following is an example:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>Review</p></th>
<th class="text-center head"><p>Label</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>“Quitting” may be as much about exiting a pre-ordained identity as about drug withdrawal. As a rural guy coming to Beijing, class and success must have struck this young artist face on as an appeal to separate from his roots and far surpass his peasant parents’ acting success. Troubles arise, however, when the new man is too new, when it demands too big a departure from family, history, nature, and personal identity. The ensuing splits, and confusion between the imaginary and the real and the dissonance between the ordinary and the heroic are the stuff of a gut check on the one hand or a complete escape from self on the other.</p></td>
<td class="text-center"><p>Negative</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>This movie is amazing because the fact that the real people portray themselves and their real life experience and do such a good job it’s like they’re almost living the past over again. Jia Hongsheng plays himself an actor who quit everything except music and drugs struggling with depression and searching for the meaning of life while being angry at everyone especially the people who care for him most.</p></td>
<td class="text-center"><p>Positive</p></td>
</tr>
</tbody>
</table>
<p>In addition, the pre-trained word vectors are used to encode natural language words to obtain semantic features of text. In this section, the Global Vectors for Word Representation (<a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a>) are selected as embeddings.</p>
<section id="data-downloading-module">
<h3>Data Downloading Module<a class="headerlink" href="#data-downloading-module" title="Permalink to this headline"></a></h3>
<p>To facilitate the download of datasets and pre-trained word vectors, a data download module is designed to implement a visualized download process and save the data to a specified path. The data download module uses the <code class="docutils literal notranslate"><span class="pre">requests</span></code> library to send HTTP requests and uses the <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> library to visualize the download percentage. To ensure download security, temporary files are downloaded in I/O mode, saved to a specified path, and returned.</p>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> and <code class="docutils literal notranslate"><span class="pre">requests</span></code> libraries need to be manually installed. The command is <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">tqdm</span> <span class="pre">requests</span></code>.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">IO</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Set the storage path to `home_path/.mindspore_examples`.</span>
<span class="n">cache_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()</span> <span class="o">/</span> <span class="s1">&#39;.mindspore_examples&#39;</span>

<span class="k">def</span> <span class="nf">http_get</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">temp_file</span><span class="p">:</span> <span class="n">IO</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Download data by using the requests library and visualize the process by using the tqdm library.&quot;&quot;&quot;</span>
    <span class="n">req</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">content_length</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;Content-Length&#39;</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">content_length</span><span class="p">)</span> <span class="k">if</span> <span class="n">content_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">progress</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">unit</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">total</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">req</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">chunk</span><span class="p">:</span>
            <span class="n">progress</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">))</span>
            <span class="n">temp_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
    <span class="n">progress</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Download data and save it with the specified name.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">)</span>
    <span class="n">cache_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
    <span class="n">cache_exist</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cache_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_exist</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_file</span><span class="p">:</span>
            <span class="n">http_get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">temp_file</span><span class="p">)</span>
            <span class="n">temp_file</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
            <span class="n">temp_file</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cache_path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">cache_file</span><span class="p">:</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copyfileobj</span><span class="p">(</span><span class="n">temp_file</span><span class="p">,</span> <span class="n">cache_file</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cache_path</span>
</pre></div>
</div>
<p>After the data download module is complete, download the IMDB dataset for testing. The HUAWEI CLOUD image is used to improve the download speed. The download process and storage path are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_path</span> <span class="o">=</span> <span class="n">download</span><span class="p">(</span><span class="s1">&#39;aclImdb_v1.tar.gz&#39;</span><span class="p">,</span> <span class="s1">&#39;https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz&#39;</span><span class="p">)</span>
<span class="n">imdb_path</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;/root/.mindspore_examples/aclImdb_v1.tar.gz&#39;
</pre></div>
</div>
</section>
<section id="loading-the-imdb-dataset">
<h3>Loading the IMDB Dataset<a class="headerlink" href="#loading-the-imdb-dataset" title="Permalink to this headline"></a></h3>
<p>The downloaded IMDB dataset is a <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code> file. Use the <code class="docutils literal notranslate"><span class="pre">tarfile</span></code> library of Python to read the dataset and store all data and labels separately. The decompression directory of the original IMDB dataset is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── aclImdb
│   ├── imdbEr.txt
│   ├── imdb.vocab
│   ├── README
│   ├── test
│   └── train
│         ├── neg
│         ├── pos
...
</pre></div>
</div>
<p>The dataset has been divided into two parts: train and test. Each part contains the neg and pos folders. You need to use the train and test parts to read and process data and labels, respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">tarfile</span>

<span class="k">class</span> <span class="nc">IMDBData</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;IMDB dataset loader.</span>

<span class="sd">    Load the IMDB dataset and process it as a Python iteration object.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;pos&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;neg&quot;</span><span class="p">:</span> <span class="mi">0</span>
    <span class="p">}</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">docs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_load</span><span class="p">(</span><span class="s2">&quot;pos&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load</span><span class="p">(</span><span class="s2">&quot;neg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;aclImdb/</span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2">/.*\.txt$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
        <span class="c1"># Load data to the memory.</span>
        <span class="k">with</span> <span class="n">tarfile</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">)</span> <span class="k">as</span> <span class="n">tarf</span><span class="p">:</span>
            <span class="n">tf</span> <span class="o">=</span> <span class="n">tarf</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
            <span class="k">while</span> <span class="n">tf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pattern</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">name</span><span class="p">)):</span>
                    <span class="c1"># Segment text, remove punctuations and special characters, and convert text to lowercase.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">tarf</span><span class="o">.</span><span class="n">extractfile</span><span class="p">(</span><span class="n">tf</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">b</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\r</span><span class="s2">&quot;</span><span class="p">))</span>
                                         <span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">b</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">))</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">label_map</span><span class="p">[</span><span class="n">label</span><span class="p">]])</span>
                <span class="n">tf</span> <span class="o">=</span> <span class="n">tarf</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
<p>After the IMDB dataset loader is completed, load the training dataset for testing and output the number of datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_train</span> <span class="o">=</span> <span class="n">IMDBData</span><span class="p">(</span><span class="n">imdb_path</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">imdb_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>25000
</pre></div>
</div>
<p>After the IMDB dataset is loaded to the memory and built as an iteration object, you can use the <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> API provided by <code class="docutils literal notranslate"><span class="pre">mindspore.dataset</span></code> to load the dataset iteration object and then perform data processing. The following encapsulates a function to load train and test using <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code>, and set <code class="docutils literal notranslate"><span class="pre">column_name</span></code> of the text and label in the dataset to <code class="docutils literal notranslate"><span class="pre">text</span></code> and <code class="docutils literal notranslate"><span class="pre">label</span></code>, respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">load_imdb</span><span class="p">(</span><span class="n">imdb_path</span><span class="p">):</span>
    <span class="n">imdb_train</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">IMDBData</span><span class="p">(</span><span class="n">imdb_path</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">imdb_test</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">IMDBData</span><span class="p">(</span><span class="n">imdb_path</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">imdb_train</span><span class="p">,</span> <span class="n">imdb_test</span>
</pre></div>
</div>
<p>Load the IMDB dataset. You can see that <code class="docutils literal notranslate"><span class="pre">imdb_train</span></code> is a GeneratorDataset object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_train</span><span class="p">,</span> <span class="n">imdb_test</span> <span class="o">=</span> <span class="n">load_imdb</span><span class="p">(</span><span class="n">imdb_path</span><span class="p">)</span>
<span class="n">imdb_train</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7fa6cd168ed0&gt;
</pre></div>
</div>
</section>
<section id="loading-pre-trained-word-vectors">
<h3>Loading Pre-trained Word Vectors<a class="headerlink" href="#loading-pre-trained-word-vectors" title="Permalink to this headline"></a></h3>
<p>A pre-trained word vector is a numerical representation of an input word. The <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer uses the table lookup mode to input the index in the vocabulary corresponding to the word to obtain the corresponding expression vector.
Therefore, before model build, word vectors and vocabulary required by the Embedding layer need to be built. Here, we use the classic pre-trained word vectors, GloVe.
The data format is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>Word</p></th>
<th class="text-center head"><p>Vector</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>the</p></td>
<td class="text-center"><p>0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 …</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>,</p></td>
<td class="text-center"><p>0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 …</p></td>
</tr>
</tbody>
</table>
<p>The words in the first column are used as the vocabulary, and <code class="docutils literal notranslate"><span class="pre">dataset.text.Vocab</span></code> is used to load the words in sequence. In addition, the vector of each row is read and converted into <code class="docutils literal notranslate"><span class="pre">numpy.array</span></code> for the <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> to load weights. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">load_glove</span><span class="p">(</span><span class="n">glove_path</span><span class="p">):</span>
    <span class="n">glove_100d_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">,</span> <span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">glove_100d_path</span><span class="p">):</span>
        <span class="n">glove_zip</span> <span class="o">=</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">glove_path</span><span class="p">)</span>
        <span class="n">glove_zip</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">glove_100d_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">gf</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">glove</span> <span class="ow">in</span> <span class="n">gf</span><span class="p">:</span>
            <span class="n">word</span><span class="p">,</span> <span class="n">embedding</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">))</span>
    <span class="c1"># Add the embeddings corresponding to the special placeholders &lt;unk&gt; and &lt;pad&gt;.</span>
    <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
    <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">],</span> <span class="n">special_first</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">embeddings</span>
</pre></div>
</div>
<p>The dataset may contain words that are not covered by the vocabulary. Therefore, the <code class="docutils literal notranslate"><span class="pre">&lt;unk&gt;</span></code> token needs to be added. In addition, because the input lengths are different, the <code class="docutils literal notranslate"><span class="pre">&lt;pad&gt;</span></code> tokens need to be added to short text when the text is packed into a batch. The length of the completed vocabulary is the length of the original vocabulary plus 2.</p>
<p>Download the GloVe to generate and load a vocabulary and a word vector weight matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">glove_path</span> <span class="o">=</span> <span class="n">download</span><span class="p">(</span><span class="s1">&#39;glove.6B.zip&#39;</span><span class="p">,</span> <span class="s1">&#39;https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip&#39;</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">load_glove</span><span class="p">(</span><span class="n">glove_path</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">vocab</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>400002
</pre></div>
</div>
<p>Use a vocabulary to convert <code class="docutils literal notranslate"><span class="pre">the</span></code> into an index ID, and query a word vector corresponding to the word vector matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">tokens_to_ids</span><span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">idx</span><span class="p">,</span> <span class="n">embedding</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(0,
 array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,
        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,
         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,
        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,
         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,
        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,
         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,
         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,
        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,
        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,
        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,
        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,
        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,
        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,
        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,
         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,
        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32))
</pre></div>
</div>
</section>
</section>
<section id="dataset-preprocessing">
<h2>Dataset Preprocessing<a class="headerlink" href="#dataset-preprocessing" title="Permalink to this headline"></a></h2>
<p>Word segmentation is performed on the IMDB dataset loaded by the loader, but the dataset does not meet the requirements for building training data. Therefore, extra preprocessing is required. The preprocessing is as follows:</p>
<ul class="simple">
<li><p>Use the Vocab to convert all tokens to index IDs.</p></li>
<li><p>The length of the text sequence is unified. If the length is insufficient, <code class="docutils literal notranslate"><span class="pre">&lt;pad&gt;</span></code> is used to supplement the length. If the length exceeds the limit, the excess part is truncated.</p></li>
</ul>
<p>Here, the API provided in <code class="docutils literal notranslate"><span class="pre">mindspore.dataset</span></code> is used for preprocessing. The APIs used here are designed for MindSpore high-performance data engines. The operations corresponding to each API are considered as a part of the data pipeline. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/design/data_engine.html">MindSpore Data Engine</a>.</p>
<p>For the table query operation from a token to an index ID, use the <code class="docutils literal notranslate"><span class="pre">text.Lookup</span></code> API to load the built vocabulary and specify <code class="docutils literal notranslate"><span class="pre">unknown_token</span></code>. The <code class="docutils literal notranslate"><span class="pre">PadEnd</span></code> API is used to unify the length of the text sequence. This API defines the maximum length and padding value (<code class="docutils literal notranslate"><span class="pre">pad_value</span></code>). In this example, the maximum length is 500, and the padding value corresponds to the index ID of <code class="docutils literal notranslate"><span class="pre">&lt;pad&gt;</span></code> in the vocabulary.</p>
<blockquote>
<div><p>In addition to pre-processing the <code class="docutils literal notranslate"><span class="pre">text</span></code> data in the dataset, the <code class="docutils literal notranslate"><span class="pre">label</span></code> data needs to be converted to the float32 format to meet the subsequent model training requirements.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">lookup_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">Lookup</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">)</span>
<span class="n">pad_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">PadEnd</span><span class="p">([</span><span class="mi">500</span><span class="p">],</span> <span class="n">pad_value</span><span class="o">=</span><span class="n">vocab</span><span class="o">.</span><span class="n">tokens_to_ids</span><span class="p">(</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">))</span>
<span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>After the preprocessing is complete, you need to add data to the dataset processing pipeline and use the <code class="docutils literal notranslate"><span class="pre">map</span></code> API to add operations to the specified column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_train</span> <span class="o">=</span> <span class="n">imdb_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="p">[</span><span class="n">lookup_op</span><span class="p">,</span> <span class="n">pad_op</span><span class="p">],</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
<span class="n">imdb_train</span> <span class="o">=</span> <span class="n">imdb_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="p">[</span><span class="n">type_cast_op</span><span class="p">],</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>

<span class="n">imdb_test</span> <span class="o">=</span> <span class="n">imdb_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="p">[</span><span class="n">lookup_op</span><span class="p">,</span> <span class="n">pad_op</span><span class="p">],</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
<span class="n">imdb_test</span> <span class="o">=</span> <span class="n">imdb_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="p">[</span><span class="n">type_cast_op</span><span class="p">],</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>The IMDB dataset does not contain the validation set. Therefore, you need to manually divide the dataset into training and validation parts, with the ratio of 0.7 to 0.3.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_train</span><span class="p">,</span> <span class="n">imdb_valid</span> <span class="o">=</span> <span class="n">imdb_train</span><span class="o">.</span><span class="n">split</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
</pre></div>
</div>
<p>Finally, specify the batch size of the dataset by using the <code class="docutils literal notranslate"><span class="pre">batch</span></code> API and determine whether to discard the remaining data that cannot be exactly divided by the batch size.</p>
<blockquote>
<div><p>Call the <code class="docutils literal notranslate"><span class="pre">map</span></code>, <code class="docutils literal notranslate"><span class="pre">split</span></code>, and <code class="docutils literal notranslate"><span class="pre">batch</span></code> APIs of the dataset to add corresponding operations to the dataset processing pipeline. The return value is of the new dataset type. Currently, only the pipeline operation is defined. During execution, the data processing pipeline is executed to obtain the processed data and send the data to the model for training.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_train</span> <span class="o">=</span> <span class="n">imdb_train</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">imdb_valid</span> <span class="o">=</span> <span class="n">imdb_valid</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-building">
<h2>Model Building<a class="headerlink" href="#model-building" title="Permalink to this headline"></a></h2>
<p>After the dataset is processed, we design the model structure for sentimental classification. First, the input text (that is, the serialized index ID list) needs to be converted into a vectorized representation through table lookup. In this case, the <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer needs to be used to load the GloVe, and then the RNN is used to perform feature extraction. Finally, the RNN is connected to a fully-connected layer, that is, <code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code>, to convert the feature into a size that is the same as the number of classifications for subsequent model optimization training. The overall model structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nn.Embedding -&gt; nn.RNN -&gt; nn.Dense
</pre></div>
</div>
<p>The LSTM(Long short-term memory) variant that can avoid the RNN gradient vanishing problem is used as the feature extraction layer. The model is described as follows:</p>
<section id="embedding">
<h3>Embedding<a class="headerlink" href="#embedding" title="Permalink to this headline"></a></h3>
<p>The Embedding layer may also be referred to as an EmbeddingLookup layer. A function of the Embedding layer is to use an index ID to search for a vector of an ID corresponding to the weight matrix. When an input is a sequence including index IDs, a matrix with a same length is searched for and returned. For example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>embedding = nn.Embedding(1000, 100) # The size of the vocabulary (the value range of index) is 1000, and the size of the vector is 100.
input shape: (1, 16)                # The sequence length is 16.
output shape: (1, 16, 100)
</pre></div>
</div>
<p>Here, the processed GloVe word vector matrix is used. <code class="docutils literal notranslate"><span class="pre">embedding_table</span></code> of <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> is set to the pre-trained word vector matrix. The vocabulary size <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> is 400002, and <code class="docutils literal notranslate"><span class="pre">embedding_size</span></code> is the size of the selected <code class="docutils literal notranslate"><span class="pre">glove.6B.100d</span></code> vector, that is, 100.</p>
</section>
<section id="recurrent-neural-network-rnn">
<h3>Recurrent Neural Network (RNN)<a class="headerlink" href="#recurrent-neural-network-rnn" title="Permalink to this headline"></a></h3>
<p>RNN is a type of neural network that uses sequence data as an input, performs recursion in the evolution direction of a sequence, and connects all nodes (circulating units) in a chain. The following figure shows the general RNN structure.</p>
<p><img alt="RNN-0" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/application/source_zh_cn/nlp/images/0-RNN-0.png" /></p>
<blockquote>
<div><p>The left part of the figure shows an RNN Cell cycle, and the right part shows the RNN chain connection. Actually, there is only one Cell parameter regardless of a single RNN Cell or an RNN network, and the parameter is updated in continuous cyclic calculation.</p>
</div></blockquote>
<p>The recurrent feature of the RNN matches the sequence feature (a sentence is a sequence composed of words) of the natural language text. Therefore, the RNN is widely used in the research of natural language processing. The following figure shows the disassembled RNN structure.</p>
<p><img alt="RNN" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/application/source_zh_cn/nlp/images/0-RNN.png" /></p>
<p>A structure of a single RNN Cell is simple, causing the gradient vanishing problem. Specifically, when a sequence in the RNN is relatively long, information of a sequence header is basically lost at a tail of the sequence. To solve this problem, the LSTM(Long short-term memory) is proposed. The gating mechanism is used to control the retention and discarding of information flows in each cycle. The following figure shows the disassembled LSTM structure.</p>
<p><img alt="LSTM" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/application/source_zh_cn/nlp/images/0-LSTM.png" /></p>
<p>In this section, the LSTM variant instead of the classic RNN is used for feature extraction to avoid the gradient vanishing problem and obtain a better model effect. The formula corresponding to <code class="docutils literal notranslate"><span class="pre">nn.LSTM</span></code> in MindSpore is as follows:</p>
<div class="math notranslate nohighlight">
\[h_{0:t}, (h_t, c_t) = \text{LSTM}(x_{0:t}, (h_0, c_0))\]</div>
<p>Herein, <code class="docutils literal notranslate"><span class="pre">nn.LSTM</span></code> hides a cycle of the entire recurrent neural network on a sequence time step. After the input sequence and the initial state are sent, you can obtain a matrix formed by splicing hidden states of each time step and a hidden state corresponding to the last time step. We use the hidden state of the last time step as the encoding feature of the input sentence and send it to the next layer.</p>
<blockquote>
<div><p>Time step: Each cycle calculated by the recurrent neural network is a time step. When a text sequence is sent, a time step corresponds to a word. Therefore, in this example, the output <span class="math notranslate nohighlight">\(h_{0:t}\)</span> of the LSTM corresponds to the hidden state set of each word, and <span class="math notranslate nohighlight">\(h_t\)</span> and <span class="math notranslate nohighlight">\(c_t\)</span> correspond to the hidden state corresponding to the last word.</p>
</div></blockquote>
</section>
<section id="dense">
<h3>Dense<a class="headerlink" href="#dense" title="Permalink to this headline"></a></h3>
<p>After the sentence feature is obtained through LSTM encoding, the sentence feature is sent to a fully-connected layer, that is, <code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code>. The feature dimension is converted into dimension 1 required for binary classification. The output after passing through the Dense layer is the model prediction result.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Uniform</span><span class="p">,</span> <span class="n">HeUniform</span>

<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                 <span class="n">bidirectional</span><span class="p">,</span> <span class="n">pad_idx</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embedding_table</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">pad_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span>
                           <span class="n">hidden_dim</span><span class="p">,</span>
                           <span class="n">num_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
                           <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
                           <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">HeUniform</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">bias_init</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">bias_init</span><span class="o">=</span><span class="n">bias_init</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</section>
<section id="loss-function-and-optimizer">
<h3>Loss Function and Optimizer<a class="headerlink" href="#loss-function-and-optimizer" title="Permalink to this headline"></a></h3>
<p>After the model body is built, instantiate the network based on the specified parameters, select the loss function and optimizer. For a feature of the sentimental classification problem in this section, that is, a binary classification problem for predicting positive or negative, <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code> (binary cross entropy loss function) is selected.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">bidirectional</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">pad_idx</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">tokens_to_ids</span><span class="p">(</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">pad_idx</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-logic">
<h3>Training Logic<a class="headerlink" href="#training-logic" title="Permalink to this headline"></a></h3>
<p>After the model is built, design the training logic. Generally, the training logic consists of the following steps:</p>
<ol class="arabic simple">
<li><p>Read the data of a batch.</p></li>
<li><p>Send the data to the network for forward computation and backward propagation, and update the weight.</p></li>
<li><p>Return the loss.</p></li>
</ol>
<p>Based on this logic, use the <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> library to design an epoch training function for visualization of the training process and loss.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span>
    <span class="n">loss_total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">step_total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total</span><span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
        <span class="n">t</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">():</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span>
            <span class="n">loss_total</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">step_total</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">t</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss_total</span><span class="o">/</span><span class="n">step_total</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="evaluation-metrics-and-logic">
<h3>Evaluation Metrics and Logic<a class="headerlink" href="#evaluation-metrics-and-logic" title="Permalink to this headline"></a></h3>
<p>After the training logic is complete, you need to evaluate the model. That is, compare the prediction result of the model with the correct label of the test set to obtain the prediction accuracy. Because sentimental classification of the IMDB is a binary classification problem, you can directly round off the predicted value to obtain a classification label (0 or 1), and then determine whether the classification label is equal to a correct label. The following describes the implementation of the function for calculating the binary classification accuracy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">binary_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the accuracy of each batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Round off the predicted value.</span>
    <span class="n">rounded_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">rounded_preds</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>
</div>
<p>After the accuracy calculation function is available, similar to the training logic, the evaluation logic is designed in the following steps:</p>
<ol class="arabic simple">
<li><p>Read the data of a batch.</p></li>
<li><p>Send the data to the network for forward computation to obtain the prediction result.</p></li>
<li><p>Calculate the accuracy.</p></li>
</ol>
<p>Similar to the training logic, <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> is used to visualize the loss and process. In addition, the loss evaluation result is returned for determining the model quality when the model is saved.</p>
<blockquote>
<div><p>During the evaluation, the model used is the network body that does not contain the loss function and optimizer.
Before evaluation, you need to use <code class="docutils literal notranslate"><span class="pre">model.set_train(False)</span></code> to set the model to the evaluation state. In this case, Dropout does not take effect.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">step_total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total</span><span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
        <span class="n">t</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">():</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>

            <span class="n">acc</span> <span class="o">=</span> <span class="n">binary_accuracy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">epoch_acc</span> <span class="o">+=</span> <span class="n">acc</span>

            <span class="n">step_total</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">t</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">epoch_loss</span><span class="o">/</span><span class="n">step_total</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">epoch_acc</span><span class="o">/</span><span class="n">step_total</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">total</span>
</pre></div>
</div>
</section>
</section>
<section id="model-training-and-saving">
<h2>Model Training and Saving<a class="headerlink" href="#model-training-and-saving" title="Permalink to this headline"></a></h2>
<p>The model building, training, and evaluation logic design are complete. The following describes how to train a model. In this example, the number of training epochs is set to 5. In addition, maintain the <code class="docutils literal notranslate"><span class="pre">best_valid_loss</span></code> variable for saving the optimal model. Based on the loss value of each epoch of evaluation, select the epoch with the minimum loss value and save the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">best_valid_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="n">ckpt_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">,</span> <span class="s1">&#39;sentiment-analysis.ckpt&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">imdb_train</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">imdb_valid</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">valid_loss</span> <span class="o">&lt;</span> <span class="n">best_valid_loss</span><span class="p">:</span>
        <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="n">valid_loss</span>
        <span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ckpt_file_name</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273/273 [00:30&lt;00:00,  9.00it/s, loss=0.674]
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:12&lt;00:00,  9.43it/s, acc=0.511, loss=0.692]
Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273/273 [00:24&lt;00:00, 11.04it/s, loss=0.683]
Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:11&lt;00:00, 10.06it/s, acc=0.674, loss=0.614]
Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273/273 [00:24&lt;00:00, 11.06it/s, loss=0.623]
Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:11&lt;00:00, 10.12it/s, acc=0.799, loss=0.458]
Epoch 3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273/273 [00:24&lt;00:00, 10.95it/s, loss=0.408]
Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:11&lt;00:00, 10.19it/s, acc=0.875, loss=0.306]
Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273/273 [00:24&lt;00:00, 11.03it/s, loss=0.305]
Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:11&lt;00:00,  9.93it/s, acc=0.899, loss=0.251]
</pre></div>
</div>
<p>You can see that the loss decreases gradually in each epoch and the accuracy of the verification set increases gradually.</p>
</section>
<section id="model-loading-and-testing">
<h2>Model Loading and Testing<a class="headerlink" href="#model-loading-and-testing" title="Permalink to this headline"></a></h2>
<p>After model training is complete, you need to test or deploy the model. In this case, you need to load the saved optimal model (that is, checkpoint) for subsequent tests. The checkpoint loading and network weight loading APIs provided by MindSpore are used to load the saved model checkpoint to the memory and load the checkpoint to the model.</p>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> API returns the weight name that does not match the checkpoint in the model. If the weight name matches the checkpoint, an empty list is returned.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt_file_name</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[]
</pre></div>
</div>
<p>Batch the test set, and then use the evaluation method to evaluate the effect of the model on the test set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_test</span> <span class="o">=</span> <span class="n">imdb_test</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">imdb_test</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:25&lt;00:00, 15.10it/s, acc=0.857, loss=0.357]
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0.356888720432244
</pre></div>
</div>
</section>
<section id="custom-input-test">
<h2>Custom Input Test<a class="headerlink" href="#custom-input-test" title="Permalink to this headline"></a></h2>
<p>Finally, we design a prediction function to implement the effect described at the beginning. Enter a comment to obtain the sentimental classification of the comment. Specifically, the following steps are included:</p>
<ol class="arabic simple">
<li><p>Perform word segmentation on an input sentence.</p></li>
<li><p>Obtain index ID sequence by using the vocabulary.</p></li>
<li><p>Convert the index IDs sequence into tensors.</p></li>
<li><p>Send tensors to the model to obtain the prediction result.</p></li>
<li><p>Print the prediction result.</p></li>
</ol>
<p>The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">score_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;Positive&quot;</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;Negative&quot;</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">predict_sentiment</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">indexed</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">tokens_to_ids</span><span class="p">(</span><span class="n">tokenized</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">indexed</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score_map</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()))]</span>
</pre></div>
</div>
<p>Finally, predict the examples in the preceding section. It shows that the model can classify the sentiments of the statements.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predict_sentiment</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="s2">&quot;This film is terrible&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;Negative&#39;
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predict_sentiment</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="s2">&quot;This film is great&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;Positive&#39;
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../cv/ssd.html" class="btn btn-neutral float-left" title="SSD for Object Detection" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sequence_labeling.html" class="btn btn-neutral float-right" title="LSTM+CRF Sequence Labeling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>