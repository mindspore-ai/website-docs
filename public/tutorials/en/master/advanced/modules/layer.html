

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Cell and Parameter &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Parameter Initialization" href="initializer.html" />
    <link rel="prev" title="Model Module Customization" href="../modules.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/save_load.html">Saving and Loading the Model</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../modules.html">Model Module Customization</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Cell and Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="initializer.html">Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer.html">Optimizer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../derivation.html">Advanced Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compute_graph.html">Computational Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mixed_precision.html">Automatic Mix Precision</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../modules.html">Model Module Customization</a> &raquo;</li>
        
      <li>Cell and Parameter</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/advanced/modules/layer.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/source_en/advanced/modules/layer.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="cell-and-parameter">
<h1>Cell and Parameter<a class="headerlink" href="#cell-and-parameter" title="Permalink to this headline">¶</a></h1>
<p>Cell, as the basic unit of neural network construction, corresponds to the concept of neural network layer, and the abstract encapsulation of Tensor computation operation can represent the neural network structure more accurately and clearly. In addition to the basic Tensor computation flow definition, the neural network layer contains functions such as parameter management and state management. Parameter is the core of neural network training and is usually used as an internal member variable of the neural network layer. In this section, we systematically introduce parameters, neural network layers and their related usage.</p>
<div class="section" id="parameter">
<h2>Parameter<a class="headerlink" href="#parameter" title="Permalink to this headline">¶</a></h2>
<p>Parameter is a special class of Tensor, which is a variable whose value can be updated during model training. MindSpore provides the <code class="docutils literal notranslate"><span class="pre">mindspore.Parameter</span></code> class for Parameter construction. In order to distinguish between Parameter for different purposes, two different categories of Parameter are defined below. In order to distinguish between Parameter for different purposes, two different categories of Parameter are defined below:</p>
<ul class="simple">
<li><p>Trainable parameter. Tensor that is updated after the gradient is obtained according to the backward propagation algorithm during model training, and <code class="docutils literal notranslate"><span class="pre">required_grad</span></code> needs to be set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>Untrainable parameters. Tensor that does not participate in backward propagation needs to update values (e.g. <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">var</span></code> variables in BatchNorm), when <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> needs to be set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
<blockquote>
<div><p>Parameter is set to <code class="docutils literal notranslate"><span class="pre">required_grad=True</span></code> by default.</p>
</div></blockquote>
<p>We construct a simple fully-connected layer as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="c1"># weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span> <span class="c1"># bias</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">z</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
</pre></div>
</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of <code class="docutils literal notranslate"><span class="pre">Cell</span></code>, we define two parameters <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> and configure <code class="docutils literal notranslate"><span class="pre">name</span></code> for namespace management. Use <code class="docutils literal notranslate"><span class="pre">self.attr</span></code> in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method to call directly to participate in Tensor operations.</p>
<div class="section" id="obtaining-parameter">
<h3>Obtaining Parameter<a class="headerlink" href="#obtaining-parameter" title="Permalink to this headline">¶</a></h3>
<p>After constructing the neural network layer by using Cell+Parameter, we can use various methods to obtain the Parameter managed by Cell.</p>
<div class="section" id="obtaining-a-single-parameter">
<h4>Obtaining a Single Parameter<a class="headerlink" href="#obtaining-a-single-parameter" title="Permalink to this headline">¶</a></h4>
<p>To get a particular parameter individually, just call a member variable of a Python class directly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[-1.2192779  -0.36789745  0.0946381 ]
</pre></div>
</div>
</div>
<div class="section" id="obtaining-a-trainable-parameter">
<h4>Obtaining a Trainable Parameter<a class="headerlink" href="#obtaining-a-trainable-parameter" title="Permalink to this headline">¶</a></h4>
<p>Trainable parameters can be obtained by using the <code class="docutils literal notranslate"><span class="pre">Cell.trainable_params</span></code> method, and this interface is usually called when configuring the optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[Parameter (name=w, shape=(5, 3), dtype=Float32, requires_grad=True), Parameter (name=b, shape=(3,), dtype=Float32, requires_grad=True)]
</pre></div>
</div>
</div>
<div class="section" id="obtaining-all-parameters">
<h4>Obtaining All Parameters<a class="headerlink" href="#obtaining-all-parameters" title="Permalink to this headline">¶</a></h4>
<p>Use the <code class="docutils literal notranslate"><span class="pre">Cell.get_parameters()</span></code> method to get all parameters, at which point a Python iterator will be returned.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;generator&#39;&gt;
</pre></div>
</div>
<p>Or you can call <code class="docutils literal notranslate"><span class="pre">Cell.parameters_and_names</span></code> to return the parameter names and parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>w:
[[ 4.15680408e-02 -1.20311625e-01  5.02573885e-02]
 [ 1.22175144e-04 -1.34980649e-01  1.17642188e+00]
 [ 7.57667869e-02 -1.74758151e-01 -5.19092619e-01]
 [-1.67846107e+00  3.27240258e-01 -2.06452996e-01]
 [ 5.72323874e-02 -8.27963874e-02  5.94243526e-01]]
b:
[-1.2192779  -0.36789745  0.0946381 ]
</pre></div>
</div>
</div>
</div>
<div class="section" id="modifying-the-parameter">
<h3>Modifying the Parameter<a class="headerlink" href="#modifying-the-parameter" title="Permalink to this headline">¶</a></h3>
<div class="section" id="modify-parameter-values-directly">
<h4>Modify Parameter Values Directly<a class="headerlink" href="#modify-parameter-values-directly" title="Permalink to this headline">¶</a></h4>
<p>Parameter is a special kind of Tensor, so its value can be modified by using the Tensor index modification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ 1.         -0.36789745  0.0946381 ]
</pre></div>
</div>
</div>
<div class="section" id="overriding-the-modified-parameter-values">
<h4>Overriding the Modified Parameter Values<a class="headerlink" href="#overriding-the-modified-parameter-values" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">Parameter.set_data</span></code> method can be called to override the Parameter by using a Tensor with the same Shape. This method is commonly used for <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/modules/initializer.html">Cell traversal initialization</a> by using Initializer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[3. 4. 5.]
</pre></div>
</div>
</div>
<div class="section" id="modifying-parameter-values-during-runtime">
<h4>Modifying Parameter Values During Runtime<a class="headerlink" href="#modifying-parameter-values-during-runtime" title="Permalink to this headline">¶</a></h4>
<p>The main role of parameters is to update their values during model training, which involves parameter modification during runtime after backward propagation to obtain gradients, or when untrainable parameters need to be updated. Due to the compiled design of MindSpore’s <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/compute_graph.html">computational graph</a>, it is necessary at this point to use the <code class="docutils literal notranslate"><span class="pre">mindspore.ops.assign</span></code> interface to assign parameters. This method is commonly used in <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/modules/optimizer.html">Custom Optimizer</a> scenarios. The following is a simple sample modification of parameter values during runtime:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">modify_parameter</span><span class="p">():</span>
    <span class="n">b_hat</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">b_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="n">modify_parameter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[7. 8. 9.]
</pre></div>
</div>
</div>
</div>
<div class="section" id="parameter-tuple">
<h3>Parameter Tuple<a class="headerlink" href="#parameter-tuple" title="Permalink to this headline">¶</a></h3>
<p>ParameterTuple, variable tuple, used to store multiple Parameter, is inherited from tuple tuples, and provides cloning function.</p>
<p>The following example provides the ParameterTuple creation method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ParameterTuple</span>
<span class="c1"># Creation</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">default_input</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">default_input</span><span class="o">=</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">default_input</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>

<span class="c1"># Clone from params and change the name to &quot;params_copy&quot;</span>
<span class="n">params_copy</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="s2">&quot;params_copy&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">params_copy</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(Parameter (name=x, shape=(2, 3), dtype=Int64, requires_grad=True), Parameter (name=y, shape=(1, 2, 3), dtype=Float32, requires_grad=True), Parameter (name=z, shape=(), dtype=Float32, requires_grad=True))
(Parameter (name=params_copy.x, shape=(2, 3), dtype=Int64, requires_grad=True), Parameter (name=params_copy.y, shape=(1, 2, 3), dtype=Float32, requires_grad=True), Parameter (name=params_copy.z, shape=(), dtype=Float32, requires_grad=True))
</pre></div>
</div>
</div>
</div>
<div class="section" id="cell-training-state-change">
<h2>Cell Training State Change<a class="headerlink" href="#cell-training-state-change" title="Permalink to this headline">¶</a></h2>
<p>Some Tensor operations in neural networks do not behave the same during training and inference, e.g., <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code> performs random dropout during training but not during inference, and <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm</span></code> requires updating the <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">var</span></code> variables during training and fixing their values unchanged during inference. So we can set the state of the neural network through the <code class="docutils literal notranslate"><span class="pre">Cell.set_train</span></code> interface.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">set_train</span></code> is set to True, the neural network state is <code class="docutils literal notranslate"><span class="pre">train</span></code>, and the default value of <code class="docutils literal notranslate"><span class="pre">set_train</span></code> interface is <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">phase</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>train
</pre></div>
</div>
<p>When <code class="docutils literal notranslate"><span class="pre">set_train</span></code> is set to False, the neural network state is <code class="docutils literal notranslate"><span class="pre">predict</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">phase</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>predict
</pre></div>
</div>
</div>
<div class="section" id="custom-neural-network-layers">
<h2>Custom Neural Network Layers<a class="headerlink" href="#custom-neural-network-layers" title="Permalink to this headline">¶</a></h2>
<p>Normally, the neural network layer interface and function interface provided by MindSpore can meet the model construction requirements, but since the AI field is constantly updating, it is possible to encounter new network structures without built-in modules. At this point, we can customize the neural network layer through the function interface provided by MindSpore, Primitive operator, and can use the <code class="docutils literal notranslate"><span class="pre">Cell.bprop</span></code> method to customize the reverse. The following are the details of each of the three customization methods.</p>
<div class="section" id="constructing-neural-network-layers-by-using-the-function-interface">
<h3>Constructing Neural Network Layers by Using the Function Interface<a class="headerlink" href="#constructing-neural-network-layers-by-using-the-function-interface" title="Permalink to this headline">¶</a></h3>
<p>MindSpore provides a large number of basic function interfaces, which can be used to construct complex Tensor operations, encapsulated as neural network layers. The following is an example of <code class="docutils literal notranslate"><span class="pre">Threshold</span></code> with the following equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y =\begin{cases}
   x, &amp;\text{ if } x &gt; \text{threshold} \\
   \text{value}, &amp;\text{ otherwise }
   \end{cases}
\end{split}\]</div>
<p>It can be seen that <code class="docutils literal notranslate"><span class="pre">Threshold</span></code> determines whether the value of the Tensor is greater than the <code class="docutils literal notranslate"><span class="pre">threshold</span></code> value, keeps the value whose judgment result is <code class="docutils literal notranslate"><span class="pre">True</span></code>, and replaces the value whose judgment result is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Therefore, the corresponding implementation is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Threshold</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">ops.gt</span></code>, <code class="docutils literal notranslate"><span class="pre">ops.fill</span></code>, and <code class="docutils literal notranslate"><span class="pre">ops.select</span></code> are used to implement judgment and replacement respectively. The following custom <code class="docutils literal notranslate"><span class="pre">Threshold</span></code> layer is implemented:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Tensor(shape=[3], dtype=Float32, value= [ 2.00000000e+01,  2.00000003e-01,  3.00000012e-01])
</pre></div>
</div>
<p>It can be seen that <code class="docutils literal notranslate"><span class="pre">inputs[0]</span> <span class="pre">=</span> <span class="pre">threshold</span></code>, so it is replaced with <code class="docutils literal notranslate"><span class="pre">20</span></code>.</p>
</div>
<div class="section" id="custom-cell-reverse">
<h3>Custom Cell Reverse<a class="headerlink" href="#custom-cell-reverse" title="Permalink to this headline">¶</a></h3>
<p>In special scenarios, we not only need to customize the forward logic of the neural network layer, but also want to manually control the computation of its reverse, which we can define through the <code class="docutils literal notranslate"><span class="pre">Cell.bprop</span></code> interface. The function will be used in scenarios such as new neural network structure design and backward propagation speed optimization. In the following, we take <code class="docutils literal notranslate"><span class="pre">Dropout2d</span></code> as an example to introduce custom Cell reverse.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Dropout2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">keep_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">out</span>
        <span class="n">dy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dout</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">)</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">dy</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dy</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">)</span>

<span class="n">dropout_2d</span> <span class="o">=</span> <span class="n">Dropout2d</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">dropout_2d</span><span class="o">.</span><span class="n">bprop_debug</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">bprop</span></code> method has three separate input parameters:</p>
<ul class="simple">
<li><p><em>x</em>: Forward input. When there are multiple forward inputs, the same number of inputs are required.</p></li>
<li><p><em>out</em>: Forward input.</p></li>
<li><p><em>dout</em>: When backward propagation is performed, the current Cell executes the previous reverse result.</p></li>
</ul>
<p>Generally we need to calculate the reverse result according to the reverse derivative formula based on the forward output and the reverse result of the front layer, and return it. The reverse calculation of <code class="docutils literal notranslate"><span class="pre">Dropout2d</span></code> requires masking the reverse result of the front layer based on the <code class="docutils literal notranslate"><span class="pre">mask</span></code> matrix of the forward output, and then scaling according to <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>. The final implementation can get the correct calculation result.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="initializer.html" class="btn btn-neutral float-right" title="Parameter Initialization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../modules.html" class="btn btn-neutral float-left" title="Model Module Customization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>