<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Accelerating with Static Graphs &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Advanced Encapsulation: Model" href="../advanced/model.html" />
    <link rel="prev" title="Saving and Loading the Model" href="save_load.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading the Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Accelerating with Static Graphs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-graph-mode">Dynamic Graph Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#static-graph-mode">Static Graph Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scenarios-for-static-graph-mode">Scenarios for Static Graph Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#static-graph-mode-startup-method">Static Graph Mode Startup Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#decorator-based-startup-method">Decorator-based Startup Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="#context-based-startup-method">Context-based Startup Method</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#syntax-constraints-for-static-graph">Syntax Constraints for Static Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jitconfig-configuration-option">JitConfig Configuration Option</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-programming-techniques-for-static-graphs">Advanced Programming Techniques for Static Graphs</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/modules.html">Model Module Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/derivation.html">Advanced Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_graph_expert_programming.html">Advanced Programming Techniques for Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/mixed_precision.html">Automatic Mix Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/error_analysis.html">Error Reporting Analysis</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Accelerating with Static Graphs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/beginner/accelerate_with_static_graph.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="accelerating-with-static-graphs">
<h1>Accelerating with Static Graphs<a class="headerlink" href="#accelerating-with-static-graphs" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/source_en/beginner/accelerate_with_static_graph.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<p><a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/introduction.html">Introduction</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/quick_start.html">Quick Start</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/tensor.html">Tensor</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/dataset.html">Dataset</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/transforms.html">Transforms</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/model.html">Model</a> || <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/source_en/beginner/autograd.md">Autograd</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/train.html">Train</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/save_load.html">Save and Load</a> || <strong>Accelerating with Static Graphs</strong></p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>The AI compilation framework is divided into two modes of operation, namely, dynamic graph mode and static graph mode. MindSpore runs in dynamic graph mode by default, but it also supports manual switching to static graph mode. The details of the two modes are as follows:</p>
<section id="dynamic-graph-mode">
<h3>Dynamic Graph Mode<a class="headerlink" href="#dynamic-graph-mode" title="Permalink to this headline"></a></h3>
<p>Dynamic graphs are characterized by the construction of the computational graph and computation occurring at the same time (Define by run), which is in line with Python interpreted execution. When defining a Tensor in the computational graph, its value is computed and determined, so it is more convenient to debug the model, and can be able to get the value of the intermediate results in real time, but it is difficult to optimize the whole computational graph due to the fact that all the nodes need to be saved.</p>
<p>In MindSpore, dynamic graph mode is also known as PyNative mode. Due to the interpreted execution of dynamic graphs, it is recommended to use dynamic graph mode for debugging during script development and network process debugging.
If you need to manually control the framework to use PyNative mode, you can configure it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>  <span class="c1"># Dynamic graph mode configuration using set_context</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 ...
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]
 [-0.00134926 -0.13563682 -0.02863023 -0.05452826  0.03290743 -0.12423715
  -0.0582641  -0.10854103 -0.08558805  0.06099342]]
</pre></div>
</div>
</section>
<section id="static-graph-mode">
<h3>Static Graph Mode<a class="headerlink" href="#static-graph-mode" title="Permalink to this headline"></a></h3>
<p>Compared to dynamic graphs, static graphs are characterized by separating the construction of the computational graph from the actual computation (Define and run). For more information on how the static graph model works, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#overview">Static Graph Syntax Support</a>.</p>
<p>In MindSpore, the static graph mode is also known as Graph mode. In Graph mode, based on techniques such as graph optimization and whole computational graph sinking, the compiler can globally optimize for graphs and obtain better performance, so it is more suitable for scenarios where the network is fixed and high performance is required.</p>
<p>If you need to manually control the framework to use static graph mode, you can build the network with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>  <span class="c1"># Static graph mode configuration using set_context</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 ...
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]
 [ 0.05363735  0.05117104 -0.03343301  0.06347139  0.07546629  0.03263091
   0.02790363  0.06269836  0.01838502  0.04387159]]
</pre></div>
</div>
</section>
</section>
<section id="scenarios-for-static-graph-mode">
<h2>Scenarios for Static Graph Mode<a class="headerlink" href="#scenarios-for-static-graph-mode" title="Permalink to this headline"></a></h2>
<p>The MindSpore compiler is focused on the computation of Tensor data and its differential processing. Therefore operations using the MindSpore API and based on Tensor objects are more suitable for static graph compilation optimization. Other operations can be partially compiled into the graph, but the actual optimization is limited. In addition, the static graph mode compiles first and then executes, resulting in compilation time consumption. As a result, there may be no need to use static graph acceleration if the function does not need to be executed repeatedly.</p>
<p>For an example of using static graphs for network compilation, see <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/model.html">Network Build</a>.</p>
</section>
<section id="static-graph-mode-startup-method">
<h2>Static Graph Mode Startup Method<a class="headerlink" href="#static-graph-mode-startup-method" title="Permalink to this headline"></a></h2>
<p>Usually, due to the flexibility of dynamic graphs, we choose to use PyNative mode for free neural network construction for model innovation and optimization. But when performance acceleration is needed, we need to accelerate the neural network partially or as a whole. MindSpore provides two ways of switching to graph mode, the decorator-based startup method and the global context-based startup method.</p>
<section id="decorator-based-startup-method">
<h3>Decorator-based Startup Method<a class="headerlink" href="#decorator-based-startup-method" title="Permalink to this headline"></a></h3>
<p>MindSpore provides a jit decorator that can be used to modify Python functions or member functions of Python classes so that they can be compiled into computational graphs, which improves the speed of operation through graph optimization and other techniques. At this point we can simply accelerate the graph compilation for the modules we want to optimize for performance, while the rest of the model, which still uses interpreted execution, does not lose the flexibility of dynamic graphs. Regardless of whether the global context is set to static graph mode or dynamic graph mode, the part modified by the jit will always run in static graph mode.</p>
<p>When you need to accelerate the compilation of some of Tensor operations, you can use the jit decorator on the function it defines, and the module is automatically compiled into a static graph when the function is called. Note that jit decorators can only be used to modify functions, not classes. The example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>  <span class="c1"># Use the ms.jit decorator to make the decorated function run in static graph mode</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 ...
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]
 [-0.12126954  0.06986676 -0.2230821  -0.07087803 -0.01003947  0.01063392
   0.10143848 -0.0200909  -0.09724037  0.0114444 ]]
</pre></div>
</div>
<p>In addition to using modifiers, jit methods can also be called using function transformations, as shown in the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">run_with_jit</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">run</span><span class="p">)</span>  <span class="c1"># Transforming a function to execute as a static graph by calling jit</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>When we need to accelerate a part of the neural network, we can use the jit modifier directly on the construct method, and the module is automatically compiled as a static graph when the instantiated object is called. The example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>  <span class="c1"># Use the ms.jit decorator to make the decorated function run in static graph mode</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 ...
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]
 [ 0.10522258  0.06597593 -0.09440921 -0.04883489  0.07194916  0.1343117
  -0.06813788  0.01986085  0.0216996  -0.05345828]]
</pre></div>
</div>
</section>
<section id="context-based-startup-method">
<h3>Context-based Startup Method<a class="headerlink" href="#context-based-startup-method" title="Permalink to this headline"></a></h3>
<p>The context mode is a global setting mode. The code example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>  <span class="c1"># Configuration for running static graph mode using set_context</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_relu_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 ...
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]
 [ 0.08501796 -0.04404321 -0.05165704  0.00357929  0.00051521  0.00946456
   0.02748473 -0.19415936 -0.00278988  0.04024826]]
</pre></div>
</div>
</section>
</section>
<section id="syntax-constraints-for-static-graph">
<h2>Syntax Constraints for Static Graph<a class="headerlink" href="#syntax-constraints-for-static-graph" title="Permalink to this headline"></a></h2>
<p>In Graph mode, Python code is not executed by the Python interpreter, but the code is compiled into a static computational graph and then the static computational graph is executed. As a result, the compiler cannot support the global Python syntax. MindSpore static graph compiler maintains a subset of common Python syntax to support neural network construction and training. For more details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="jitconfig-configuration-option">
<h2>JitConfig Configuration Option<a class="headerlink" href="#jitconfig-configuration-option" title="Permalink to this headline"></a></h2>
<p>In graph mode, the compilation process can be customized by using the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.JitConfig.html#mindspore.JitConfig">JitConfig</a> configuration option. Currently JitConfig supports the following configuration parameters:</p>
<ul class="simple">
<li><p>jit_level: Used to control the optimization level.</p></li>
<li><p>exec_mode: Used to control the model execution.</p></li>
<li><p>jit_syntax_level: Set the static graph syntax support level. See <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#overview">Static Graph Syntax Support</a> for details.</p></li>
</ul>
</section>
<section id="advanced-programming-techniques-for-static-graphs">
<h2>Advanced Programming Techniques for Static Graphs<a class="headerlink" href="#advanced-programming-techniques-for-static-graphs" title="Permalink to this headline"></a></h2>
<p>Using static graph advanced programming techniques can effectively improve the compilation efficiency as well as the execution efficiency, and can make the program run more stably. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/static_graph_expert_programming.html">Advanced Programming Techniques with Static Graphs</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="save_load.html" class="btn btn-neutral float-left" title="Saving and Loading the Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../advanced/model.html" class="btn btn-neutral float-right" title="Advanced Encapsulation: Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>