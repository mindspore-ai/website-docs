<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Accelerating with Static Graphs &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script src="../_static/js/theme.js"></script>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Advanced Encapsulation: Model" href="../advanced/model.html" />
    <link rel="prev" title="Saving and Loading the Model" href="save_load.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading the Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Accelerating with Static Graphs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-graph-mode">Dynamic Graph Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#static-graph-mode">Static Graph Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scenarios-for-static-graph-mode">Scenarios for Static Graph Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#static-graph-mode-startup-method">Static Graph Mode Startup Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#decorator-based-startup-method">Decorator-based Startup Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="#context-based-startup-method">Context-based Startup Method</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#syntax-constraints-for-static-graph">Syntax Constraints for Static Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jitconfig-configuration-option">JitConfig Configuration Option</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-programming-techniques-for-static-graphs">Advanced Programming Techniques for Static Graphs</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/modules.html">Model Module Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/derivation.html">Advanced Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_graph_expert_programming.html">Advanced Programming Techniques for Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/mixed_precision.html">Automatic Mix Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/error_analysis.html">Error Reporting Analysis</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Accelerating with Static Graphs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/beginner/accelerate_with_static_graph.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="accelerating-with-static-graphs">
<h1>Accelerating with Static Graphs<a class="headerlink" href="#accelerating-with-static-graphs" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/source_en/beginner/accelerate_with_static_graph.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<p><a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/introduction.html">Introduction</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/quick_start.html">Quick Start</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/tensor.html">Tensor</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/dataset.html">Dataset</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/transforms.html">Transforms</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/model.html">Model</a> || <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/source_en/beginner/autograd.md">Autograd</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/train.html">Train</a> || <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/save_load.html">Save and Load</a> || <strong>Accelerating with Static Graphs</strong></p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>The AI compilation framework is divided into two modes of operation, namely, dynamic graph mode and static graph mode. MindSpore runs in dynamic graph mode by default, but it also supports manual switching to static graph mode. The details of the two modes are as follows:</p>
<section id="dynamic-graph-mode">
<h3>Dynamic Graph Mode<a class="headerlink" href="#dynamic-graph-mode" title="Permalink to this headline"></a></h3>
<p>Dynamic graphs are characterized by the construction of the computational graph and computation occurring at the same time (Define by run), which is in line with Python interpreted execution. When defining a Tensor in the computational graph, its value is computed and determined, so it is more convenient to debug the model, and can be able to get the value of the intermediate results in real time, but it is difficult to optimize the whole computational graph due to the fact that all the nodes need to be saved.</p>
<p>In MindSpore, dynamic graph mode is also known as PyNative mode. Due to the interpreted execution of dynamic graphs, it is recommended to use dynamic graph mode for debugging during script development and network process debugging.
If you need to manually control the framework to use PyNative mode, you can configure it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
</pre></div>
</div>
<p>In PyNative mode, the underlying operator corresponding to all computational nodes is executed using a single Kernel, so that printing and debugging of computational results can be done arbitrarily. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="c1"># weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span> <span class="c1"># bias</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;matmul: &#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;add bias: &#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out: &quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>We simply define a Tensor with a shape of (5,) as input and observe the output. You can see that the <code class="docutils literal notranslate"><span class="pre">print</span></code> statement inserted in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method prints out the intermediate results in real time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>matmul:  [-1.8809001   2.0400267   0.32370526]
add bias:  [-1.6770952   1.5087128   0.15726662]
out:  [-1.6770952   1.5087128   0.15726662]
</pre></div>
</div>
</section>
<section id="static-graph-mode">
<h3>Static Graph Mode<a class="headerlink" href="#static-graph-mode" title="Permalink to this headline"></a></h3>
<p>Compared to dynamic graphs, static graphs are characterized by separating the construction of the computational graph from the actual computation (Define and run). In the construction phase, the original computational graph is optimized and adjusted according to the complete computational flow, and compiled to obtain a more memory-saving and less computationally intensive computational graph. Since the structure of the graph does not change after compilation, it is called “static graph” . In the computation phase, the compiled computation graph is executed according to the input data to get the computation result. Compared to dynamic graphs, static graphs have richer information about the global information and more optimizations can be done, but their intermediate process is a black box for users, and they cannot get the intermediate computation results in real time like dynamic graphs.</p>
<p>In MindSpore, the static graph mode is also known as Graph mode. In Graph mode, based on techniques such as graph optimization and whole computational graph sinking, the compiler can globally optimize for graphs and obtain better performance, so it is more suitable for scenarios where the network is fixed and high performance is required.</p>
<p>In static graph mode, MindSpore converts Python source code into Intermediate Representation (IR) by means of source conversion, and optimizes the IR graph on this basis, and finally executes the optimized graph on the hardware device.MindSpore uses functional IR based on the graph representation that is called MindIR. For more details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/design/all_scenarios.html#mindspore-ir-mindir">Intermediate Representation MindIR</a>.</p>
<p>MindSpore static graph execution process actually consists of two steps, corresponding to the Define and Run phases of the static graph, but in practice, it is not perceived when the instantiated Cell object is called, and MindSpore encapsulates both phases in the Cell’s <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method, so the actual calling process is:</p>
<p><code class="docutils literal notranslate"><span class="pre">model(inputs)</span> <span class="pre">=</span> <span class="pre">model.compile(inputs)</span> <span class="pre">+</span> <span class="pre">model.construct(inputs)</span></code>, where <code class="docutils literal notranslate"><span class="pre">model</span></code> instantiates the Cell object.</p>
<p>Below we explicitly call the <code class="docutils literal notranslate"><span class="pre">compile</span></code> method for an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;out: &#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>matmul:
Tensor(shape=[3], dtype=Float32, value=[-4.01971531e+00 -5.79053342e-01  3.41115999e+00])
add bias:
Tensor(shape=[3], dtype=Float32, value=[-3.94732714e+00 -1.46257186e+00  4.50144434e+00])
out:  [-3.9473271 -1.4625719  4.5014443]
</pre></div>
</div>
</section>
</section>
<section id="scenarios-for-static-graph-mode">
<h2>Scenarios for Static Graph Mode<a class="headerlink" href="#scenarios-for-static-graph-mode" title="Permalink to this headline"></a></h2>
<p>The MindSpore compiler is focused on the computation of Tensor data and its differential processing. Therefore operations using the MindSpore API and based on Tensor objects are more suitable for static graph compilation optimization. Other operations can be partially compiled into the graph, but the actual optimization is limited. In addition, the static graph mode compiles first and then executes, resulting in compilation time consumption. As a result, there may be no need to use static graph acceleration if the function does not need to be executed repeatedly.</p>
<p>For an example of using static graphs for network compilation, see <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/model.html">Network Build</a>.</p>
</section>
<section id="static-graph-mode-startup-method">
<h2>Static Graph Mode Startup Method<a class="headerlink" href="#static-graph-mode-startup-method" title="Permalink to this headline"></a></h2>
<p>Usually, due to the flexibility of dynamic graphs, we choose to use PyNative mode for free neural network construction for model innovation and optimization. But when performance acceleration is needed, we need to accelerate the neural network partially or as a whole. MindSpore provides two ways of switching to graph mode, the decorator-based startup method and the global context-based startup method.</p>
<section id="decorator-based-startup-method">
<h3>Decorator-based Startup Method<a class="headerlink" href="#decorator-based-startup-method" title="Permalink to this headline"></a></h3>
<p>MindSpore provides a jit decorator that can be used to modify Python functions or member functions of Python classes so that they can be compiled into computational graphs, which improves the speed of operation through graph optimization and other techniques. At this point we can simply accelerate the graph compilation for the modules we want to optimize for performance, while the rest of the model, which still uses interpreted execution, does not lose the flexibility of dynamic graphs.</p>
<p>When you need to accelerate the compilation of some of Tensor operations, you can use the jit decorator on the function it defines, and the module is automatically compiled into a static graph when the function is called. The example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
</pre></div>
</div>
<p>When we need to accelerate a part of the neural network, we can use the jit decorator directly on the construct method, and the module is automatically compiled as a static graph when the instantiated object is called. The example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>MindSpore supports combining the forward computation, backward propagation, and gradient optimization updating of neural network training into one computational graph for compilation and optimization, which is called whole graph compilation. In this case, you only need to construct the neural network training logic as a function, and use the jit decorator on the function to achieve the effect of whole graph compilation. The following is an example using a simple fully connected network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

<span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>As shown in the above code, after encapsulating the neural network forward execution and loss function into forward_fn, the function transformation is performed to obtain the gradient calculation function. Then encapsulate the gradient calculation function and optimizer call into train_step function, and use jit to modify it. When calling train_step function, it will perform static graph compilation, get the whole graph and execute it.</p>
<p>In addition to using decorator, jit methods can also be called using function transformations, as shown in the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_step</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="context-based-startup-method">
<h3>Context-based Startup Method<a class="headerlink" href="#context-based-startup-method" title="Permalink to this headline"></a></h3>
<p>The context mode is a global setting mode. The code example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="syntax-constraints-for-static-graph">
<h2>Syntax Constraints for Static Graph<a class="headerlink" href="#syntax-constraints-for-static-graph" title="Permalink to this headline"></a></h2>
<p>In Graph mode, Python code is not executed by the Python interpreter, but the code is compiled into a static computational graph and then the static computational graph is executed. As a result, the compiler cannot support the global Python syntax. MindSpore static graph compiler maintains a subset of common Python syntax to support neural network construction and training. For more details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="jitconfig-configuration-option">
<h2>JitConfig Configuration Option<a class="headerlink" href="#jitconfig-configuration-option" title="Permalink to this headline"></a></h2>
<p>In graph mode, the compilation process can be customized by using the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.JitConfig.html#mindspore.JitConfig">JitConfig</a> configuration option. Currently JitConfig supports the following configuration parameters:</p>
<ul class="simple">
<li><p>jit_level: Used to control the optimization level.</p></li>
<li><p>exec_mode: Used to control the model execution.</p></li>
<li><p>jit_syntax_level: Set the static graph syntax support level. See <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#overview">Static Graph Syntax Support</a> for details.</p></li>
</ul>
</section>
<section id="advanced-programming-techniques-for-static-graphs">
<h2>Advanced Programming Techniques for Static Graphs<a class="headerlink" href="#advanced-programming-techniques-for-static-graphs" title="Permalink to this headline"></a></h2>
<p>Using static graph advanced programming techniques can effectively improve the compilation efficiency as well as the execution efficiency, and can make the program run more stably. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/static_graph_expert_programming.html">Advanced Programming Techniques with Static Graphs</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="save_load.html" class="btn btn-neutral float-left" title="Saving and Loading the Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../advanced/model.html" class="btn btn-neutral float-right" title="Advanced Encapsulation: Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>