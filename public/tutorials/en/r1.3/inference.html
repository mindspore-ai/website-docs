<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Inference &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Saving and Loading the Model" href="save_load_model.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start for Beginners</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading and Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Building a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimizing Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load_model.html">Saving and Loading the Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ascend-ai-processor-inference">Ascend AI Processor Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#inference-code">Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-script">Build Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-inference-code">Building Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-inference-and-viewing-the-result">Performing Inference and Viewing the Result</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mobile-device-inference">Mobile Device Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-conversion">Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#environment-building-and-running">Environment Building and Running</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#building-and-running-the-linux-system">Building and Running the Linux System</a></li>
<li class="toctree-l4"><a class="reference internal" href="#building-and-running-the-windows-system">Building and Running the Windows System</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#inference-code-parsing">Inference Code Parsing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-loading">Model Loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-build">Model Build</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-inference">Model Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#releasing-memory">Releasing Memory</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/inference.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="inference">
<h1>Inference<a class="headerlink" href="#inference" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/tutorials/source_en/inference.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png" /></a></p>
<p>This is the last tutorial. To better adapt to different inference devices, inference is classified into Ascend AI Processor inference and mobile device inference.</p>
<section id="ascend-ai-processor-inference">
<h2>Ascend AI Processor Inference<a class="headerlink" href="#ascend-ai-processor-inference" title="Permalink to this headline"></a></h2>
<p>An Ascend AI Processor is an energy-efficient and highly integrated AI processor oriented to edge scenarios. It can implement multiple data analysis and inference computing, such as image and video analysis, and can be widely used in scenarios such as intelligent surveillance, robots, drones, and video servers. The following describes how to use MindSpore to perform inference on the Ascend AI Processors.</p>
<section id="inference-code">
<h3>Inference Code<a class="headerlink" href="#inference-code" title="Permalink to this headline"></a></h3>
<p>Create a directory to store the inference code project, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/mindspore_sample/ascend910_resnet50_preprocess_sample</span></code>. You can download the <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/ascend910_resnet50_preprocess_sample">sample code</a> from the official website. The <code class="docutils literal notranslate"><span class="pre">model</span></code> directory is used to store the exported <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model file, and the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory is used to store the images to be classified. The directory structure of the inference code project is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ascend910_resnet50_preprocess_sample
    ├── CMakeLists.txt                   // Build script
    ├── README.md                         // Usage description
    ├── main.cc                           // Main function
    ├── model
    │   └── resnet50_imagenet.mindir      // MindIR model file
    └── test_data
        ├── ILSVRC2012_val_00002138.JPEG  // Input sample image 1
        ├── ILSVRC2012_val_00003014.JPEG  // Input sample image 2
        ├── ...                           // Input sample image n.
</pre></div>
</div>
<p>Namespaces that reference <code class="docutils literal notranslate"><span class="pre">mindspore</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore::dataset</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">ms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="p">;</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">ds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="o">::</span><span class="nn">dataset</span><span class="p">;</span>
</pre></div>
</div>
<p>Initialize the environment, specify the hardware platform used for inference, and set DeviceID.</p>
<p>Set the hardware to Ascend 910 and set DeviceID to 0. The code example is as follows:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend910_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Ascend910DeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="n">ascend910_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend910_info</span><span class="p">);</span>
</pre></div>
</div>
<p>Load the model file.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load the MindIR model.</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Status</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">resnet_file</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>
<span class="c1">// Build a model using a graph.</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">resnet50</span><span class="p">;</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
</pre></div>
</div>
<p>Obtain the input information required by the model.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>Load the image file.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// ReadFile is a function used to read images.</span>
<span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="nf">ReadFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">file</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">image_file</span><span class="p">);</span>
</pre></div>
</div>
<p>Preprocess images.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Use the CPU operator provided by MindData to preprocess images.</span>

<span class="c1">// Create an operator to encode the input into the RGB format.</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">decode</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Decode</span><span class="p">());</span>
<span class="c1">// Create an operator to resize the image to the specified size.</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Resize</span><span class="p">({</span><span class="mi">256</span><span class="p">}));</span>
<span class="c1">// Create an operator to normalize the input of the operator.</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">normalize</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Normalize</span><span class="p">(</span>
<span class="w">    </span><span class="p">{</span><span class="mf">0.485</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.456</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.406</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mf">0.229</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.224</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.225</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">}));</span>
<span class="c1">// Create an operator to perform central cropping.</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">center_crop</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">CenterCrop</span><span class="p">({</span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">}));</span>
<span class="c1">// Create an operator to transform shape (H, W, C) into shape (C, H, W).</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hwc2chw</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">HWC2CHW</span><span class="p">());</span>

<span class="c1">// Define a MindData data preprocessing function that contains the preceding operators in sequence.</span>
<span class="n">ds</span><span class="o">::</span><span class="n">Execute</span><span class="w"> </span><span class="nf">preprocessor</span><span class="p">({</span><span class="n">decode</span><span class="p">,</span><span class="w"> </span><span class="n">resize</span><span class="p">,</span><span class="w"> </span><span class="n">normalize</span><span class="p">,</span><span class="w"> </span><span class="n">center_crop</span><span class="p">,</span><span class="w"> </span><span class="n">hwc2chw</span><span class="p">});</span>

<span class="c1">// Call the data preprocessing function to obtain the processed image.</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">preprocessor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">image</span><span class="p">);</span>
</pre></div>
</div>
<p>Start inference.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create an output vector.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="c1">// Create an input vector.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span>
<span class="w">                    </span><span class="n">image</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">image</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span>
<span class="c1">// Call the Predict function of the model for inference.</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
<p>Obtain the inference result.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Maximum value of the output probability.</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Image: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">image_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; infer result: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">GetMax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="build-script">
<h3>Build Script<a class="headerlink" href="#build-script" title="Permalink to this headline"></a></h3>
<p>Add the header file search path for the compiler:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">option</span><span class="p">(</span><span class="s">MINDSPORE_PATH</span><span class="w"> </span><span class="s2">&quot;mindspore install path&quot;</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/include</span><span class="p">)</span>
</pre></div>
</div>
<p>Search for the required dynamic library in MindSpore.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">find_library</span><span class="p">(</span><span class="s">MS_LIB</span><span class="w"> </span><span class="s">libmindspore.so</span><span class="w"> </span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>
<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span><span class="w"> </span><span class="s">MD_LIB</span><span class="w"> </span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/_c_dataengine*</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the specified source file to generate the target executable file and link the target file to the MindSpore library.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">resnet50_sample</span><span class="w"> </span><span class="s">main.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">resnet50_sample</span><span class="w"> </span><span class="o">${</span><span class="nv">MS_LIB</span><span class="o">}</span><span class="w"> </span><span class="o">${</span><span class="nv">MD_LIB</span><span class="o">}</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>For details, see
<a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/sample_code/ascend910_resnet50_preprocess_sample/CMakeLists.txt">https://gitee.com/mindspore/docs/blob/r1.3/docs/sample_code/ascend910_resnet50_preprocess_sample/CMakeLists.txt</a></p>
</div></blockquote>
</section>
<section id="building-inference-code">
<h3>Building Inference Code<a class="headerlink" href="#building-inference-code" title="Permalink to this headline"></a></h3>
<p>Go to the project directory <code class="docutils literal notranslate"><span class="pre">ascend910_resnet50_preprocess_sample</span></code> and set the following environment variables:</p>
<blockquote>
<div><p>If the device is Ascend 310, go to the project directory <code class="docutils literal notranslate"><span class="pre">ascend310_resnet50_preprocess_sample</span></code>. The following code uses Ascend 910 as an example.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Control the log print level. 0 indicates DEBUG, 1 indicates INFO, 2 indicates WARNING (default value), and 3 indicates ERROR.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>

<span class="c1"># Select the Conda environment.</span>
<span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend<span class="w"> </span><span class="c1"># Root directory of the running package</span>

<span class="c1"># Library on which the running package depends</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/fwkacllib/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/driver/lib64/common:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/driver/lib64/driver:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/opp/op_impl/built-in/ai_core/tbe/op_tiling:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># Libraries on which MindSpore depends</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="sb">`</span>pip3<span class="w"> </span>show<span class="w"> </span>mindspore-ascend<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Location<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $2&quot;/mindspore/lib&quot;}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>realpath<span class="sb">`</span>:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># Configure necessary environment variables.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TBE_IMPL_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe<span class="w">            </span><span class="c1"># Path of the TBE operator</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp<span class="w">                                       </span><span class="c1"># OPP path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/fwkacllib/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span><span class="w">                 </span><span class="c1"># Path of the TBE operator build tool</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">TBE_IMPL_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span><span class="w">                                                       </span><span class="c1"># Python library that TBE depends on</span>
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">cmake</span></code> command. In the command, <code class="docutils literal notranslate"><span class="pre">pip3</span></code> needs to be modified based on the actual situation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>.<span class="w"> </span>-DMINDSPORE_PATH<span class="o">=</span><span class="sb">`</span>pip3<span class="w"> </span>show<span class="w"> </span>mindspore-ascend<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Location<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $2&quot;/mindspore&quot;}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>realpath<span class="sb">`</span>
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">make</span></code> command for building.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
<p>After building, the executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file is generated in <code class="docutils literal notranslate"><span class="pre">ascend910_resnet50_preprocess_sample</span></code>.</p>
</section>
<section id="performing-inference-and-viewing-the-result">
<h3>Performing Inference and Viewing the Result<a class="headerlink" href="#performing-inference-and-viewing-the-result" title="Permalink to this headline"></a></h3>
<p>After the preceding operations are complete, you can learn how to perform inference.</p>
<p>Log in to the Ascend 910 environment, and create the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory to store the <code class="docutils literal notranslate"><span class="pre">resnet50_imagenet.mindir</span></code> file, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/mindspore_sample/ascend910_resnet50_preprocess_sample/model</span></code>.
Create the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory to store images, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/mindspore_sample/ascend910_resnet50_preprocess_sample/test_data</span></code>.
Then, perform the inference.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./resnet50_sample
</pre></div>
</div>
<p>Inference is performed on all images stored in the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory. For example, if there are 2 images whose label is 0 in the <a class="reference external" href="http://image-net.org/download-images">ImageNet2012</a> validation set, the inference result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Image: ./test_data/ILSVRC2012_val_00002138.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00003014.JPEG infer result: 0
</pre></div>
</div>
</section>
</section>
<section id="mobile-device-inference">
<h2>Mobile Device Inference<a class="headerlink" href="#mobile-device-inference" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite is the device part of the device-edge-cloud AI framework MindSpore and can implement intelligent applications on mobile devices such as phones. MindSpore Lite provides a high-performance inference engine and ultra-lightweight solution. It supports mobile phone operating systems such as iOS and Android, LiteOS-embedded operating systems, various intelligent devices such as mobile phones, large screens, tablets, and IoT devices, and MindSpore/TensorFlow Lite/Caffe/ONNX model applications.</p>
<p>The following provides a demo that runs on the Windows and Linux operating systems and is built based on the C++ API to help users get familiar with the on-device inference process. The demo uses the shuffled data as the input data, performs the inference on the MobileNetV2 model, and directly displays the output data on the computer.</p>
<blockquote>
<div><p>For details about the complete instance running on the mobile phone, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/quick_start/quick_start.html">Android Application Development Based on JNI</a>.</p>
</div></blockquote>
<section id="model-conversion">
<h3>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline"></a></h3>
<p>The format of a model needs to be converted before the model is used for inference on the device. Currently, MindSpore Lite supports four types of AI frameworks: MindSpore, TensorFlow Lite, Caffe, and ONNX.</p>
<p>The following uses the <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mobilenetv2_openimage_lite/mobilenetv2.mindir">mobilenetv2.mindir</a> model trained by MindSpore as an example to describe how to generate the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model used in the demo.</p>
<blockquote>
<div><p>The following describes the conversion process. Skip it if you only need to run the demo.</p>
<p>The following describes only the model used by the demo. For details about how to use the conversion tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/converter_tool.html#">Converting Models for Inference</a>.</p>
</div></blockquote>
<ul>
<li><p>Download the conversion tool.</p>
<p>Download the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/downloads.html">conversion tool package</a> based on the OS in use, decompress the package to a local directory, obtain the <code class="docutils literal notranslate"><span class="pre">converter</span></code> tool, and configure environment variables.</p>
</li>
<li><p>Use the conversion tool.</p>
<ul>
<li><p>For Linux</p>
<p>Go to the directory where the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> executable file is located, place the downloaded <code class="docutils literal notranslate"><span class="pre">mobilenetv2.mindir</span></code> model in the same path, and run the following command on the PC to convert the model:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">converter_lite</span><span class="w"> </span><span class="o">--</span><span class="n">fmk</span><span class="o">=</span><span class="n">MINDIR</span><span class="w"> </span><span class="o">--</span><span class="n">modelFile</span><span class="o">=</span><span class="n">mobilenetv2</span><span class="p">.</span><span class="n">mindir</span><span class="w"> </span><span class="o">--</span><span class="n">outputFile</span><span class="o">=</span><span class="n">mobilenetv2</span>
</pre></div>
</div>
</li>
<li><p>For Windows</p>
<p>Go to the directory where the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> executable file is located, place the downloaded <code class="docutils literal notranslate"><span class="pre">mobilenetv2.mindir</span></code> model in the same path, and run the following command on the PC to convert the model:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">call</span><span class="w"> </span><span class="n">converter_lite</span><span class="w"> </span><span class="o">--</span><span class="n">fmk</span><span class="o">=</span><span class="n">MINDIR</span><span class="w"> </span><span class="o">--</span><span class="n">modelFile</span><span class="o">=</span><span class="n">mobilenetv2</span><span class="p">.</span><span class="n">mindir</span><span class="w"> </span><span class="o">--</span><span class="n">outputFile</span><span class="o">=</span><span class="n">mobilenetv2</span>
</pre></div>
</div>
</li>
<li><p>Parameter description</p>
<p>During the command execution, three parameters are set. <code class="docutils literal notranslate"><span class="pre">--fmk</span></code> indicates the original format of the input model. In this example, this parameter is set to <code class="docutils literal notranslate"><span class="pre">MINDIR</span></code>, which is the export format of the MindSpore framework training model. <code class="docutils literal notranslate"><span class="pre">--modelFile</span></code> indicates the path of the input model. <code class="docutils literal notranslate"><span class="pre">--outputFile</span></code> indicates the output path of the model. The suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code> is automatically added to the converted model.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="environment-building-and-running">
<h3>Environment Building and Running<a class="headerlink" href="#environment-building-and-running" title="Permalink to this headline"></a></h3>
<section id="building-and-running-the-linux-system">
<h4>Building and Running the Linux System<a class="headerlink" href="#building-and-running-the-linux-system" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Build</p>
<p>Run the build script in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp</span></code> directory to automatically download related files and build the demo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
</li>
<li><p>Inference</p>
<p>After the build is complete, go to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/build</span></code> directory and run the following command to perform MindSpore Lite inference on the MobileNetV2 model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mindspore_quick_start_cpp<span class="w"> </span>../model/mobilenetv2.ms
</pre></div>
</div>
<p>After the execution is complete, the following information is displayed, including the tensor name, tensor size, number of output tensors, and the first 50 pieces of data:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:Default/head-MobileNetV2Head/Softmax-op204 tensor size is:4000 tensor elements num is:1000
output data is:5.26823e-05 0.00049752 0.000296722 0.000377607 0.000177048 .......
</pre></div>
</div>
</li>
</ul>
</section>
<section id="building-and-running-the-windows-system">
<h4>Building and Running the Windows System<a class="headerlink" href="#building-and-running-the-windows-system" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Build</p>
<ul>
<li><p>Download the library: Manually download the MindSpore Lite model inference framework <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/downloads.html">mindspore-lite-{version}-win-x64.zip</a> whose hardware platform is CPU and operating system is Windows-x64. Copy the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.a</span></code> file in the decompressed <code class="docutils literal notranslate"><span class="pre">inference/lib</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/lib</span></code> directory. Copy the <code class="docutils literal notranslate"><span class="pre">inference/include</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/include</span></code> directory.</p></li>
<li><p>Download the model: Manually download the model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mobilenetv2_imagenet/mobilenetv2.ms">mobilenetv2.ms</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/model</span></code> directory.</p>
<blockquote>
<div><p>You can use the mobilenetv2.ms model file obtained in “Model Conversion”.</p>
</div></blockquote>
</li>
<li><p>Build: Run the build script in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp</span></code> directory to automatically download related files and build the demo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>build.bat
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Inference</p>
<p>After the build is complete, go to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/build</span></code> directory and run the following command to perform MindSpore Lite inference on the MobileNetV2 model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>./mindspore_quick_start_cpp.exe<span class="w"> </span>../model/mobilenetv2.ms
</pre></div>
</div>
<p>After the execution is complete, the following information is displayed, including the tensor name, tensor size, number of output tensors, and the first 50 pieces of data:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:Default/head-MobileNetV2Head/Softmax-op204 tensor size is:4000 tensor elements num is:1000
output data is:5.26823e-05 0.00049752 0.000296722 0.000377607 0.000177048 .......
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="inference-code-parsing">
<h3>Inference Code Parsing<a class="headerlink" href="#inference-code-parsing" title="Permalink to this headline"></a></h3>
<p>The following analyzes the inference process in the demo source code and shows how to use the C++ API.</p>
<section id="model-loading">
<h4>Model Loading<a class="headerlink" href="#model-loading" title="Permalink to this headline"></a></h4>
<p>Read the MindSpore Lite model from the file system and use the <code class="docutils literal notranslate"><span class="pre">mindspore::lite::Model::Import</span></code> function to import the model for parsing.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Read the model file.</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Load the model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Model</span><span class="o">::</span><span class="n">Import</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Import model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="model-build">
<h4>Model Build<a class="headerlink" href="#model-build" title="Permalink to this headline"></a></h4>
<p>Model build includes configuration context creation, session creation, and graph build.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="w"> </span><span class="o">*</span><span class="nf">Compile</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="o">*</span><span class="n">model</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Initialize the context.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed while.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Create a session.</span>
<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="w"> </span><span class="o">*</span><span class="n">session</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">session</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CreateSession failed while running.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Graph build.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Compile failed while running.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Note: If model-&gt;Free() is used, the model cannot be built again.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Free</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="model-inference">
<h4>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this headline"></a></h4>
<p>Model inference includes data input, inference execution, and output obtaining. In this example, the input data is generated from randomly built data, and the output result after inference is displayed.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Run</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="w"> </span><span class="o">*</span><span class="n">session</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Obtain the input data.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Generate Random Input Data failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Run.</span>
<span class="w">  </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Obtain the output data.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">out_tensors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">first</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">()</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">50</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="releasing-memory">
<h4>Releasing Memory<a class="headerlink" href="#releasing-memory" title="Permalink to this headline"></a></h4>
<p>If the MindSpore Lite inference framework is not required, you need to release the created <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> and <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete the model cache.</span>
<span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="c1">// Delete the session cache.</span>
<span class="k">delete</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="save_load_model.html" class="btn btn-neutral float-left" title="Saving and Loading the Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>