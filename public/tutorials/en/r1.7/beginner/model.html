<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Building a Neural Network &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Automatic Differentiation" href="autograd.html" />
    <link rel="prev" title="Data Processing" href="dataset.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quickstart: Handwritten Digit Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Data Processing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Building a Neural Network</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#lenet-5-model">LeNet-5 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#defining-a-model-class">Defining a Model Class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-layers">Model Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nn-conv2d">nn.Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nn-relu">nn.ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nn-maxpool2d">nn.MaxPool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nn-flatten">nn.Flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nn-dense">nn.Dense</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-parameters">Model Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quickly-building-a-lenet-5-model">Quickly Building a LeNet-5 Model</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading the Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="infer.html">Inference and Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dataset.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/train.html">Training and Evaluation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Building a Neural Network</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/beginner/model.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="building-a-neural-network">
<h1>Building a Neural Network<a class="headerlink" href="#building-a-neural-network" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.7/tutorials/source_en/beginner/model.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source_en.png" /></a></p>
<p>A neural network model consists of multiple data operation layers. <code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code> provides various basic network modules. The following uses LeNet-5 as an example to first describe how to build a neural network model by using <code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code>, and then describes how to build a LeNet-5 model by using <code class="docutils literal notranslate"><span class="pre">mindvision.classification.models</span></code>.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">mindvision.classification.models</span></code> is a network model interface developed based on <code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code>, providing some classic and commonly used network models for the convenience of users.</p>
</div></blockquote>
<section id="lenet-5-model">
<h2>LeNet-5 Model<a class="headerlink" href="#lenet-5-model" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="https://ieeexplore.ieee.org/document/726791">LeNet-5</a> is a typical convolutional neural network proposed by professor Yann LeCun in 1998, which achieves 99.4% accuracy on the MNIST dataset and is the first classic in the field of CNN. The model structure is shown in the following figure:</p>
<p><img alt="LeNet-5" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/tutorials/source_zh_cn/beginner/images/lenet.png" /></p>
<p>Except the input layer, LeNet contains seven layers: three convolutional layers, two subsampling layers, and two fully-connected layers.</p>
</section>
<section id="defining-a-model-class">
<h2>Defining a Model Class<a class="headerlink" href="#defining-a-model-class" title="Permalink to this headline"></a></h2>
<p>In the preceding figure, C indicates the convolutional layer layer, S indicates the sampling layer, and F indicates the fully-connected layer.</p>
<p>The input size of an image is fixed at <span class="math notranslate nohighlight">\(32 x 32\)</span>. To achieve a good convolution effect, the number must be in the center of the image. Therefore, the input <span class="math notranslate nohighlight">\(32 x 32\)</span> is the result after the image is filled with <span class="math notranslate nohighlight">\(28 x 28\)</span>. Unlike the three-channel input images of the CNN network, the input images of LeNet are only normalized binary images. The output of the network is the prediction probability of digits 0 to 9, which can be understood as the probability that the input image belongs to digits 0 to 9.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class of MindSpore is the base class for building all networks and the basic unit of a network. When a neural network is required, you need to inherit the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class and overwrite the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="docutils literal notranslate"><span class="pre">construct</span></code> methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">LeNet5</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LeNet-5 network structure</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Convolutional layer, where the number of input channels is num_channel, the number of output channels is 6, and the convolutional kernel size is 5 x 5.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
        <span class="c1"># Convolutional layer, where the number of input channels is 6, the number of output channels is 16, and the convolutional kernel size is 5 x 5.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
        <span class="c1"># Fully-connected layer, where the number of inputs is 16 x 5 x 5 and the number of outputs is 120.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="c1"># Fully-connected layer, where the number of inputs is 120 and the number of outputs is 84.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="c1"># Fully-connected layer, where the number of inputs is 84 and the number of classes is num_class.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">num_class</span><span class="p">)</span>
        <span class="c1"># ReLU activation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="c1"># Pooling layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># Multidimensional arrays are flattened into one-dimensional arrays.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Use the defined operation to build a forward network.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Next, build the neural network model defined above and look at the structure of the network model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LeNet5&lt;
  (conv1): Conv2d&lt;input_channels=1, output_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;
  (conv2): Conv2d&lt;input_channels=6, output_channels=16, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;
  (fc1): Dense&lt;input_channels=400, output_channels=120, has_bias=True&gt;
  (fc2): Dense&lt;input_channels=120, output_channels=84, has_bias=True&gt;
  (fc3): Dense&lt;input_channels=84, output_channels=10, has_bias=True&gt;
  (relu): ReLU&lt;&gt;
  (max_pool2d): MaxPool2d&lt;kernel_size=2, stride=2, pad_mode=VALID&gt;
  (flatten): Flatten&lt;&gt;
&gt;
</pre></div>
</div>
</section>
<section id="model-layers">
<h2>Model Layers<a class="headerlink" href="#model-layers" title="Permalink to this headline"></a></h2>
<p>The following describes the key member functions of the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class used in LeNet-5, and then describes how to use the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class to access model parameters through the instantiation network. For more information about the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/mindspore.nn.html">mindspore.nn interface</a>.</p>
<section id="nn-conv2d">
<h3>nn.Conv2d<a class="headerlink" href="#nn-conv2d" title="Permalink to this headline"></a></h3>
<p>Add the <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> layer and add a convolution function to the network to help the neural network extract features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>

<span class="c1"># The number of channels input is 1, the number of channels of output is 6, the convolutional kernel size is 5 x 5, and the parameters are initialized using the normal operator, and the pixels are not filled.</span>
<span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">conv2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(1, 6, 32, 32)
</pre></div>
</div>
</section>
<section id="nn-relu">
<h3>nn.ReLU<a class="headerlink" href="#nn-relu" title="Permalink to this headline"></a></h3>
<p>Add the <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> layer and add a non-linear activation function to the network to help the neural network learn various complex features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[0. 2. 0. 2. 0.]
</pre></div>
</div>
</section>
<section id="nn-maxpool2d">
<h3>nn.MaxPool2d<a class="headerlink" href="#nn-maxpool2d" title="Permalink to this headline"></a></h3>
<p>Initialize the <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d</span></code> layer and down-sample the 6 x 28 x 28 array to a 6 x 7 x 7 array.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(1, 6, 7, 7)
</pre></div>
</div>
</section>
<section id="nn-flatten">
<h3>nn.Flatten<a class="headerlink" href="#nn-flatten" title="Permalink to this headline"></a></h3>
<p>Initialize the <code class="docutils literal notranslate"><span class="pre">nn.Flatten</span></code> layer and convert the 1 x 16 x 5 x 5 array into 400 consecutive arrays.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(1, 400)
</pre></div>
</div>
</section>
<section id="nn-dense">
<h3>nn.Dense<a class="headerlink" href="#nn-dense" title="Permalink to this headline"></a></h3>
<p>Initialize the <code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code> layer and perform linear transformation on the input matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">400</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(1, 120)
</pre></div>
</div>
</section>
</section>
<section id="model-parameters">
<h2>Model Parameters<a class="headerlink" href="#model-parameters" title="Permalink to this headline"></a></h2>
<p>After instantiation is performed on the convolutional layer and the fully-connected layer in the network, there are a weight parameter and an offset parameter. These parameters are continuously optimized in a training process. During training, you can use <code class="docutils literal notranslate"><span class="pre">get_parameters()</span></code> to view the name, shape, and data type of each network layer, and whether backward calculation is performed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layer:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, shape:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dtype:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, requeires_grad:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">requires_grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>layer:backbone.conv1.weight, shape:(6, 1, 5, 5), dtype:Float32, requeires_grad:True
layer:backbone.conv2.weight, shape:(16, 6, 5, 5), dtype:Float32, requeires_grad:True
layer:backbone.fc1.weight, shape:(120, 400), dtype:Float32, requeires_grad:True
layer:backbone.fc1.bias, shape:(120,), dtype:Float32, requeires_grad:True
layer:backbone.fc2.weight, shape:(84, 120), dtype:Float32, requeires_grad:True
layer:backbone.fc2.bias, shape:(84,), dtype:Float32, requeires_grad:True
layer:backbone.fc3.weight, shape:(10, 84), dtype:Float32, requeires_grad:True
layer:backbone.fc3.bias, shape:(10,), dtype:Float32, requeires_grad:True
</pre></div>
</div>
</section>
<section id="quickly-building-a-lenet-5-model">
<h2>Quickly Building a LeNet-5 Model<a class="headerlink" href="#quickly-building-a-lenet-5-model" title="Permalink to this headline"></a></h2>
<p>The preceding describes how to use <code class="docutils literal notranslate"><span class="pre">mindspore.nn.cell</span></code> to build a LeNet-5 model. The built network model API is available in <code class="docutils literal notranslate"><span class="pre">mindvision.classification.models</span></code>. You can also use the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> API to directly build a LeNet-5 model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindvision.classification.models</span> <span class="kn">import</span> <span class="n">lenet</span>

<span class="c1"># `num_classes` indicates the number of classes, and `pretrained` determines whether to train with the trained model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lenet</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layer:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, shape:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dtype:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, requeires_grad:</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">requires_grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>layer:backbone.conv1.weight, shape:(6, 1, 5, 5), dtype:Float32, requeires_grad:True
layer:backbone.conv2.weight, shape:(16, 6, 5, 5), dtype:Float32, requeires_grad:True
layer:backbone.fc1.weight, shape:(120, 400), dtype:Float32, requeires_grad:True
layer:backbone.fc1.bias, shape:(120,), dtype:Float32, requeires_grad:True
layer:backbone.fc2.weight, shape:(84, 120), dtype:Float32, requeires_grad:True
layer:backbone.fc2.bias, shape:(84,), dtype:Float32, requeires_grad:True
layer:backbone.fc3.weight, shape:(10, 84), dtype:Float32, requeires_grad:True
layer:backbone.fc3.bias, shape:(10,), dtype:Float32, requeires_grad:True
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dataset.html" class="btn btn-neutral float-left" title="Data Processing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="autograd.html" class="btn btn-neutral float-right" title="Automatic Differentiation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>