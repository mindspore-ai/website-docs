<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Inference and Deployment &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Processing" href="../advanced/dataset.html" />
    <link rel="prev" title="Saving and Loading the Model" href="save_load.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quickstart: Handwritten Digit Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Building a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading the Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Inference and Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation-and-loading">Data Preparation and Loading</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#downloading-a-dataset">Downloading a Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset">Loading the Dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-training">Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principles-of-the-mobilenet-v2-model">Principles of the MobileNet V2 Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloading-the-pre-trained-model">Downloading the Pre-trained Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mobilenet-v2-model-fine-tuning">MobileNet V2 Model Fine-tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-training-and-evaluation">Model Training and Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#visualizing-model-predictions">Visualizing Model Predictions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-export">Model Export</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inference-and-deployment-on-the-mobile-phone">Inference and Deployment on the Mobile Phone</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#converting-the-file-format">Converting the File Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#application-deployment">Application Deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#application-experience">Application Experience</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#customizing-the-model-label-files">Customizing the Model Label Files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#labels-and-model-files-deployed-to-mobile-phones">Labels and Model Files Deployed to Mobile Phones</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dataset.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/train.html">Training and Evaluation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Inference and Deployment</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/beginner/infer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="inference-and-deployment">
<h1>Inference and Deployment<a class="headerlink" href="#inference-and-deployment" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/source_en/beginner/infer.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png" /></a></p>
<p>This chapter uses the <code class="docutils literal notranslate"><span class="pre">mobilenet_v2</span></code> network fine-tuning approach in <a class="reference external" href="https://mindspore.cn/vision/docs/en/r0.1/index.html">MindSpore Vision</a> to develop an AI application to classify dogs and croissants, and deploy the trained network model on the Android phone to perform inference and deployment.</p>
<section id="data-preparation-and-loading">
<h2>Data Preparation and Loading<a class="headerlink" href="#data-preparation-and-loading" title="Permalink to this headline"></a></h2>
<section id="downloading-a-dataset">
<h3>Downloading a Dataset<a class="headerlink" href="#downloading-a-dataset" title="Permalink to this headline"></a></h3>
<p>First, you need to download the <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/beginner/DogCroissants.zip">dog and croissants classification dataset</a> which contains two classes, dog and croissants. Each class contains about 150 training images, 20 verification images, and 1 inference image.</p>
<p>The dataset is as follows:</p>
<p><img alt="datset-dog" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/datset_dog.png" /></p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">DownLoad</span></code> interface in <a class="reference external" href="https://mindspore.cn/vision/docs/en/r0.1/index.html">MindSpore Vision</a> to download and decompress the dataset to the specified path. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindvision.dataset</span> <span class="kn">import</span> <span class="n">DownLoad</span>

<span class="n">dataset_url</span> <span class="o">=</span> <span class="s2">&quot;https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/beginner/DogCroissants.zip&quot;</span>
<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;./datasets&quot;</span>

<span class="n">dl</span> <span class="o">=</span> <span class="n">DownLoad</span><span class="p">()</span>
<span class="c1"># Download and decompress the dataset.</span>
<span class="n">dl</span><span class="o">.</span><span class="n">download_and_extract_archive</span><span class="p">(</span><span class="n">dataset_url</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>The directory structure of the dataset is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>datasets
└── DogCroissants
    ├── infer
    │   ├── croissants.jpg
    │   └── dog.jpg
    ├── train
    │   ├── croissants
    │   └── dog
    └── val
        ├── croissants
        └── dog
</pre></div>
</div>
</section>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h3>
<p>Define the <code class="docutils literal notranslate"><span class="pre">create_dataset</span></code> function to load the dog and croissant dataset, perform image argumentation on the dataset, and set batch_size of the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="mi">224</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">class_indexing</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;croissants&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>

    <span class="c1"># Image augmentation</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.456</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.406</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.224</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.225</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">RandomCropDecodeResize</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.333</span><span class="p">)),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Decode</span><span class="p">(),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="c1"># Set the value of the batch_size. Discard the samples if the number of samples last fetched is less than the value of batch_size.</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
<p>Load the training dataset and validation dataset for subsequent model training and validation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the training dataset.</span>
<span class="n">train_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/DogCroissants/train&quot;</span>
<span class="n">dataset_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Load the validation dataset.</span>
<span class="n">val_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/DogCroissants/val&quot;</span>
<span class="n">dataset_val</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">val_path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="model-training">
<h2>Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline"></a></h2>
<p>In this case, we use a pre-trained model to fine-tune the model on the dog and croissant classification dataset, and convert the trained CKPT model file to the MINDIR format for subsequent deployment on the mobile phone.</p>
<section id="principles-of-the-mobilenet-v2-model">
<h3>Principles of the MobileNet V2 Model<a class="headerlink" href="#principles-of-the-mobilenet-v2-model" title="Permalink to this headline"></a></h3>
<p>MobileNet is a lightweight CNN proposed by the Google team in 2017 to focus on mobile, embedded, or IoT devices. Compared with traditional convolutional neural networks, MobileNet uses depthwise separable convolution to greatly reduce the model parameters and computation amount with a slight decrease in accuracy. In addition, the width coefficient <span class="math notranslate nohighlight">\(\alpha\)</span> and resolution coefficient <span class="math notranslate nohighlight">\(\beta\)</span> are introduced to meet the requirements of different application scenarios.</p>
<p>Because a large amount of data is lost when the ReLU activation function in the MobileNet processes low-dimensional feature information, the MobileNetV2 proposes to use an inverted residual block and Linear Bottlenecks to design the network, to improve accuracy of the model and make the optimized model smaller.</p>
<p><img alt="mobilenet" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/mobilenet.png" /></p>
<p>In the inverted residual block structure, the 1 x 1 convolution is used for dimension increase, the 3 x 3 DepthWise convolution is used, and the 1 x 1 convolution is used for dimension reduction. This structure is opposite to the residual block structure. For the residual block, the 1 x 1 convolution is first used for dimension reduction, then the 3 x 3 convolution is used, and finally the 1 x 1 convolution is used for dimension increase.</p>
<blockquote>
<div><p>For details, see the <a class="reference external" href="https://arxiv.org/pdf/1801.04381.pdf">MobileNet V2 paper.</a></p>
</div></blockquote>
</section>
<section id="downloading-the-pre-trained-model">
<h3>Downloading the Pre-trained Model<a class="headerlink" href="#downloading-the-pre-trained-model" title="Permalink to this headline"></a></h3>
<p>Download the <a class="reference external" href="https://download.mindspore.cn/vision/classification/mobilenet_v2_1.0_224.ckpt">ckpt file of the MobileNetV2 pre-trained model</a> required by the case. The width coefficient of the pre-training model is <span class="math notranslate nohighlight">\(\alpha = 1.0\)</span>, and the input image size is (224, 224). Save the downloaded pre-trained model to the current directory. Use <code class="docutils literal notranslate"><span class="pre">DownLoad</span></code> in MindSpore Vision to download the pre-trained model file to the current directory. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindvision.dataset</span> <span class="kn">import</span> <span class="n">DownLoad</span>

<span class="n">models_url</span> <span class="o">=</span> <span class="s2">&quot;https://download.mindspore.cn/vision/classification/mobilenet_v2_1.0_224.ckpt&quot;</span>

<span class="n">dl</span> <span class="o">=</span> <span class="n">DownLoad</span><span class="p">()</span>
<span class="c1"># Download the pre-trained model file.</span>
<span class="n">dl</span><span class="o">.</span><span class="n">download_url</span><span class="p">(</span><span class="n">models_url</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mobilenet-v2-model-fine-tuning">
<h3>MobileNet V2 Model Fine-tuning<a class="headerlink" href="#mobilenet-v2-model-fine-tuning" title="Permalink to this headline"></a></h3>
<p>This chapter uses MobileNet V2 pre-trained model for fine-tuning, and uses the dog and croissant classification dataset to retrain the model to update the model parameter by deleting the last parameter of the 1 x 1 convolution layer for classification in the MobileNet V2 pre-trained model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="kn">from</span> <span class="nn">mindvision.classification.models</span> <span class="kn">import</span> <span class="n">mobilenet_v2</span>
<span class="kn">from</span> <span class="nn">mindvision.engine.loss</span> <span class="kn">import</span> <span class="n">CrossEntropySmooth</span>

<span class="c1"># Create a model, in which the number of target classifications is 2 and the input image size is (224,224).</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>

<span class="c1"># Save model parameters to param_dict.</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;./mobilenet_v2_1.0_224.ckpt&quot;</span><span class="p">)</span>

<span class="c1"># Obtain the parameter name of the last convolutional layer of the mobilenet_v2 network.</span>
<span class="n">filter_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">network</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()]</span>

<span class="c1"># Delete the last convolutional layer of the pre-trained model.</span>
<span class="k">def</span> <span class="nf">filter_ckpt_parameter</span><span class="p">(</span><span class="n">origin_dict</span><span class="p">,</span> <span class="n">param_filter</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">origin_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">param_filter</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Delete parameter from checkpoint: &quot;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
                <span class="k">del</span> <span class="n">origin_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="k">break</span>

<span class="n">filter_ckpt_parameter</span><span class="p">(</span><span class="n">param_dict</span><span class="p">,</span> <span class="n">filter_list</span><span class="p">)</span>

<span class="c1"># Load the pre-trained model parameters as the network initialization weight.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="c1"># Define the optimizer.</span>
<span class="n">network_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Define the loss function.</span>
<span class="n">network_loss</span> <span class="o">=</span> <span class="n">CrossEntropySmooth</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">smooth_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">classes_num</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Define evaluation metrics.</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()}</span>

<span class="c1"># Initialize the model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">network_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">network_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[WARNING] ME(375486:140361546602304,MainProcess): [mindspore/train/serialization.py:644] 2 parameters in the &#39;net&#39; are not loaded, because they are not in the &#39;parameter_dict&#39;.
[WARNING] ME(375486:140361546602304,MainProcess): [mindspore/train/serialization.py:646] head.classifier.weight is not loaded.
[WARNING] ME(375486:140361546602304,MainProcess): [mindspore/train/serialization.py:646] head.classifier.bias is not loaded.

Delete parameter from checkpoint:  head.classifier.weight
Delete parameter from checkpoint:  head.classifier.bias
Delete parameter from checkpoint:  moments.head.classifier.weight
Delete parameter from checkpoint:  moments.head.classifier.bias
</pre></div>
</div>
<blockquote>
<div><p>The preceding warning is generated because the last convolutional layer parameter of the pre-trained model needs to be deleted for model fine-tuning. When the pre-trained model is loaded, the system displays a message indicating that the <code class="docutils literal notranslate"><span class="pre">head.classifier</span></code> parameter is not loaded. The <code class="docutils literal notranslate"><span class="pre">head.classifier</span></code> parameter uses the initial value during model creation.</p>
</div></blockquote>
</section>
<section id="model-training-and-evaluation">
<h3>Model Training and Evaluation<a class="headerlink" href="#model-training-and-evaluation" title="Permalink to this headline"></a></h3>
<p>Train and evaluate the network, and use the <code class="docutils literal notranslate"><span class="pre">mindvision.engine.callback.ValAccMonitor</span></code> interface in MindSpore Vision to print the loss value and the evaluation accuracy of the training. After the training is completed, save the CKPT file with the highest evaluation accuracy, <code class="docutils literal notranslate"><span class="pre">best.ckpt</span></code>, in the current directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindvision.engine.callback</span> <span class="kn">import</span> <span class="n">ValAccMonitor</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Train and verify the model. After the training is completed, save the CKPT file with the highest evaluation accuracy, `best.ckpt`, in the current directory.</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span>
            <span class="n">dataset_train</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ValAccMonitor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset_val</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">TimeMonitor</span><span class="p">()])</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>--------------------
Epoch: [  0 /  10], Train Loss: [0.388], Accuracy:  0.975
epoch time: 7390.423 ms, per step time: 254.842 ms
--------------------
Epoch: [  1 /  10], Train Loss: [0.378], Accuracy:  0.975
epoch time: 1876.590 ms, per step time: 64.710 ms
--------------------
Epoch: [  2 /  10], Train Loss: [0.372], Accuracy:  1.000
epoch time: 2103.431 ms, per step time: 72.532 ms
--------------------
Epoch: [  3 /  10], Train Loss: [0.346], Accuracy:  1.000
epoch time: 2246.303 ms, per step time: 77.459 ms
--------------------
Epoch: [  4 /  10], Train Loss: [0.376], Accuracy:  1.000
epoch time: 2164.527 ms, per step time: 74.639 ms
--------------------
Epoch: [  5 /  10], Train Loss: [0.353], Accuracy:  1.000
epoch time: 2191.490 ms, per step time: 75.569 ms
--------------------
Epoch: [  6 /  10], Train Loss: [0.414], Accuracy:  1.000
epoch time: 2183.388 ms, per step time: 75.289 ms
--------------------
Epoch: [  7 /  10], Train Loss: [0.362], Accuracy:  1.000
epoch time: 2219.950 ms, per step time: 76.550 ms
--------------------
Epoch: [  8 /  10], Train Loss: [0.354], Accuracy:  1.000
epoch time: 2174.555 ms, per step time: 74.985 ms
--------------------
Epoch: [ 9 /  10], Train Loss: [0.364], Accuracy:  1.000
epoch time: 2190.957 ms, per step time: 75.550 ms
================================================================================
End of validation the best Accuracy is:  1.000, save the best ckpt file in ./best.ckpt
</pre></div>
</div>
</section>
<section id="visualizing-model-predictions">
<h3>Visualizing Model Predictions<a class="headerlink" href="#visualizing-model-predictions" title="Permalink to this headline"></a></h3>
<p>Define the <code class="docutils literal notranslate"><span class="pre">visualize_model</span></code> function, use the model with the highest validation accuracy described above to predict the input image, and visualize the prediction result.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">def</span> <span class="nf">visualize_model</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

    <span class="c1"># Normalization processing</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.456</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.406</span> <span class="o">*</span> <span class="mi">255</span><span class="p">])</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.224</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.225</span> <span class="o">*</span> <span class="mi">255</span><span class="p">])</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="p">(</span><span class="n">image</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Convert the image channel from (h, w, c) to (c, h, w).</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Extend the data dimension to (1, c, h, w)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Define and load the network.</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;./best.ckpt&quot;</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="c1"># Use the model for prediction.</span>
    <span class="n">pre</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pre</span><span class="p">)</span>

    <span class="n">class_name</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;Croissants&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;Dog&quot;</span><span class="p">}</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predict: </span><span class="si">{</span><span class="n">class_name</span><span class="p">[</span><span class="n">result</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">image1</span> <span class="o">=</span> <span class="s2">&quot;./datasets/DogCroissants/infer/croissants.jpg&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">visualize_model</span><span class="p">(</span><span class="n">image1</span><span class="p">)</span>

<span class="n">image2</span> <span class="o">=</span> <span class="s2">&quot;./datasets/DogCroissants/infer/dog.jpg&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">visualize_model</span><span class="p">(</span><span class="n">image2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="model-export">
<h3>Model Export<a class="headerlink" href="#model-export" title="Permalink to this headline"></a></h3>
<p>After model training is complete, the trained network model (CKPT file) is converted into the MindIR format for subsequent inference on the mobile phone. The <code class="docutils literal notranslate"><span class="pre">mobilenet_v2_1.0_224.mindir</span></code> file is generated in the current directory through the <code class="docutils literal notranslate"><span class="pre">export</span></code> interface.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="c1"># Define and load the network parameters.</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;best.ckpt&quot;</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="c1"># Export the model from the CKPT format to the MINDIR format.</span>
<span class="n">input_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_np</span><span class="p">),</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">&quot;mobilenet_v2_1.0_224&quot;</span><span class="p">,</span> <span class="n">file_format</span><span class="o">=</span><span class="s2">&quot;MINDIR&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="inference-and-deployment-on-the-mobile-phone">
<h2>Inference and Deployment on the Mobile Phone<a class="headerlink" href="#inference-and-deployment-on-the-mobile-phone" title="Permalink to this headline"></a></h2>
<p>To implement the inference function of the model file on the mobile phone, perform the following steps:</p>
<ul class="simple">
<li><p>Convert file format: Convert MindIR file format to the MindSpore Lite recognizable file on the Android phone.</p></li>
<li><p>Application deployment: Deploy the app APK on the mobile phone, that is, download a MindSpore Vision suite Android APK.</p></li>
<li><p>Application experience: After importing the MS model file to the mobile phone, experience the function of recognizing dogs and croissants.</p></li>
</ul>
<section id="converting-the-file-format">
<h3>Converting the File Format<a class="headerlink" href="#converting-the-file-format" title="Permalink to this headline"></a></h3>
<p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r1.8/use/converter_tool.html">conversion tool</a> applied on the device side to convert the mobilenet_v2_1.0_224.mindir file generated during the training process into the mobilenet_v2_1.0_224.ms file which can be recognized by the MindSpore Lite on-device inference framework.</p>
<p>The following describes how to convert the model file format:</p>
<ol class="arabic">
<li><p>Use MindSpore Lite Converter to convert the file format in Linux. <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/downloads.html">Linux-x86_64 tool download link</a></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and decompress the software package and set the path of the software package. {converter_path} indicates the path of the decompressed tool package, and PACKAGE_ROOT_PATH indicates the environment variable.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PACKAGE_ROOT_PATH</span><span class="o">={</span>converter_path<span class="o">}</span>

<span class="c1"># Add the dynamic link library required by the converter to the environment variable LD_LIBRARY_PATH.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># Run the conversion command on the mindspore-lite-linux-x64/tools/converter/converter.</span>
./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>mobilenet_v2_1.0_224.mindir<span class="w">  </span>--outputFile<span class="o">=</span>mobilenet_v2_1.0_224
</pre></div>
</div>
</li>
<li><p>Use MindSpore Lite Converter to convert the file format in Windows. <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/downloads.html">Windows-x64 tool download link</a></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and decompress the software package and set the path of the software package. {converter_path} indicates the path of the decompressed tool package, and PACKAGE_ROOT_PATH indicates the environment variable.</span>
<span class="nb">set</span><span class="w"> </span><span class="nv">PACKAGE_ROOT_PATH</span><span class="o">={</span>converter_path<span class="o">}</span>

<span class="c1"># Add the dynamic link library required by the converter to the environment variable PATH.</span>
<span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>%PACKAGE_ROOT_PATH%<span class="se">\t</span>ools<span class="se">\c</span>onverter<span class="se">\l</span>ib<span class="p">;</span>%PATH%

<span class="c1"># Run the following command in the mindspore-lite-win-x64\tools\converter\converter directory:</span>
call<span class="w"> </span>converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>mobilenet_v2_1.0_224.mindir<span class="w"> </span>--outputFile<span class="o">=</span>mobilenet_v2_1.0_224
</pre></div>
</div>
</li>
</ol>
<p>After the conversion is successful, <code class="docutils literal notranslate"><span class="pre">CONVERT</span> <span class="pre">RESULT</span> <span class="pre">SUCCESS:0</span></code> is displayed, and the <code class="docutils literal notranslate"><span class="pre">mobilenet_v2_1.0_224.ms</span></code> file is generated in the current directory.</p>
<blockquote>
<div><p>For details about how to download MindSpore Lite Converter in other environments, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/downloads.html">Download MindSpore Lite</a>.</p>
</div></blockquote>
</section>
<section id="application-deployment">
<h3>Application Deployment<a class="headerlink" href="#application-deployment" title="Permalink to this headline"></a></h3>
<p>Download <a class="reference external" href="https://download.mindspore.cn/vision/android/mindvision-0.1.0.apk">Android app APK</a> of the MindSpore Vision Suite, or download it by scanning the QR code on phone.</p>
<p><img alt="qr" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/app_qr_code.png" /></p>
<p>Install the APK on your phone, and its app name appears as <code class="docutils literal notranslate"><span class="pre">MindSpore</span> <span class="pre">Vision</span></code>.</p>
<blockquote>
<div><p>The MindSpore Vision APK is used as an example of the visual development tool. It provides basic UI functions such as photographing and image selection, and provides AI application demos such as classification, and detection.</p>
</div></blockquote>
<p>Open the app, tap the <code class="docutils literal notranslate"><span class="pre">classification</span></code> module on the home screen, and then tap the middle button to take photos or tap the image album button on the top bar to select an image for classification.</p>
<p><img alt="main" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/app1.png" /></p>
<p>By default, the MindSpore Vision <code class="docutils literal notranslate"><span class="pre">classification</span></code> module has a built-in general AI network model for image identification and classification.</p>
<p><img alt="result" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/app2.png" /></p>
</section>
<section id="application-experience">
<h3>Application Experience<a class="headerlink" href="#application-experience" title="Permalink to this headline"></a></h3>
<p>Finally, the custom network model <code class="docutils literal notranslate"><span class="pre">mobilenet_v2_1.0_224.ms</span></code> trained above is deployed to the Android mobile phone to experience the recognition function of dogs and croissants.</p>
<section id="customizing-the-model-label-files">
<h4>Customizing the Model Label Files<a class="headerlink" href="#customizing-the-model-label-files" title="Permalink to this headline"></a></h4>
<p>To deploy a custom model, you need to define the information required by the network model in the following format, that is, customize a label file, and create a label file in JSON format named <code class="docutils literal notranslate"><span class="pre">custom.json</span></code> on the local computer.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{
    &quot;title&quot;: &#39;dog and croissants&#39;,
    &quot;file&quot;: &#39;mobilenet_v2_1.0_224.ms&#39;,
    &quot;label&quot;: [&#39;croissants&#39;, &#39;dog&#39;]
}
</pre></div>
</div>
<p>The JSON label file must contain the <code class="docutils literal notranslate"><span class="pre">title</span></code>, <code class="docutils literal notranslate"><span class="pre">file</span></code>, and <code class="docutils literal notranslate"><span class="pre">label</span></code> key fields, which are described as follows:</p>
<ul class="simple">
<li><p>title: custom module titles (dog and croissants).</p></li>
<li><p>file: the name of the model file converted above.</p></li>
<li><p>label: <code class="docutils literal notranslate"><span class="pre">array</span></code> information about the custom label.</p></li>
</ul>
</section>
<section id="labels-and-model-files-deployed-to-mobile-phones">
<h4>Labels and Model Files Deployed to Mobile Phones<a class="headerlink" href="#labels-and-model-files-deployed-to-mobile-phones" title="Permalink to this headline"></a></h4>
<p>On the home page of the <code class="docutils literal notranslate"><span class="pre">MindSpore</span> <span class="pre">Vision</span> <span class="pre">APK</span></code>, hold down the <code class="docutils literal notranslate"><span class="pre">classification</span></code> button to enter the custom classification mode and select the labels and model files to be deployed.</p>
<p>To implement the identification function of the dogs and croissants on the mobile phone, you need to place the label file <code class="docutils literal notranslate"><span class="pre">custom.json</span></code> and model file <code class="docutils literal notranslate"><span class="pre">mobilenet_v2_1.0_224.ms</span></code> to the specified directory on the mobile phone. The <code class="docutils literal notranslate"><span class="pre">Android/data/Download/</span></code> folder is used as an example. Place the label file and model file in the preceding mobile phone address, as shown in the following figure. Click the custom button. The system file function is displayed. Click icon in the upper left corner and find the directory where the JSON label file and model file are stored, and select the corresponding JSON file.</p>
<p><img alt="step" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/app3.png" /></p>
<p>After the label and model file are deployed on the mobile phone, you can click the middle button to take photos and obtain images, or click the image button on the upper side bar to select an image album for images. In this way, the dogs and croissants can be classified and identified.</p>
<p><img alt="result1" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/beginner/images/app4.png" /></p>
<blockquote>
<div><p>This chapter only covers the simple deployment process on the mobile phone. For more information about inference, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/index.html">MindSpore Lite</a>.</p>
</div></blockquote>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="save_load.html" class="btn btn-neutral float-left" title="Saving and Loading the Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../advanced/dataset.html" class="btn btn-neutral float-right" title="Data Processing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>