

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Saving and Loading the Model &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference and Deployment" href="infer.html" />
    <link rel="prev" title="Model Training" href="train.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quickstart: Handwritten Digit Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Building a Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Model Training</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Saving and Loading the Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-training">Model Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#saving-the-model">Saving the Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#directly-saving-the-model">Directly Saving the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#saving-the-model-during-training">Saving the Model During Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loading-the-model">Loading the Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#validating-the-model">Validating the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#for-transfer-learning">For Transfer Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="infer.html">Inference and Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dataset.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/train.html">Training and Evaluation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Saving and Loading the Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/beginner/save_load.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="saving-and-loading-the-model">
<h1>Saving and Loading the Model<a class="headerlink" href="#saving-and-loading-the-model" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/source_en/beginner/save_load.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<p>The previous section describes how to adjust hyperparameters and train network models. During network model training, we want to save the intermediate and final results for fine-tuning and subsequent model deployment and inference. This section describes how to save and load a model.</p>
<section id="model-training">
<h2>Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline"></a></h2>
<p>The following uses the MNIST dataset as an example to describe how to save and load a network model. First, obtain the MNIST dataset and train the model. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="kn">from</span> <span class="nn">mindvision.engine.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>
<span class="kn">from</span> <span class="nn">mindvision.classification.dataset</span> <span class="kn">import</span> <span class="n">Mnist</span>
<span class="kn">from</span> <span class="nn">mindvision.classification.models</span> <span class="kn">import</span> <span class="n">lenet</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Training epochs</span>

<span class="c1"># 1. Build a dataset.</span>
<span class="n">download_train</span> <span class="o">=</span> <span class="n">Mnist</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;./mnist&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dataset_train</span> <span class="o">=</span> <span class="n">download_train</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

<span class="c1"># 2. Define a neural network.</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">lenet</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># 3.1 Define a loss function.</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="c1"># 3.2 Define an optimizer function.</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="c1"># 3.3 Initialize model parameters.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">net_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;accuracy&#39;</span><span class="p">})</span>

<span class="c1"># 4. Train the neural network.</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">dataset_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1875</span><span class="p">)])</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Epoch:[  0/ 10], step:[ 1875/ 1875], loss:[0.148/1.210], time:2.021 ms, lr:0.01000
Epoch time: 4251.808 ms, per step time: 2.268 ms, avg loss: 1.210
Epoch:[  1/ 10], step:[ 1875/ 1875], loss:[0.049/0.081], time:2.048 ms, lr:0.01000
Epoch time: 4301.405 ms, per step time: 2.294 ms, avg loss: 0.081
Epoch:[  2/ 10], step:[ 1875/ 1875], loss:[0.014/0.050], time:1.992 ms, lr:0.01000
Epoch time: 4278.799 ms, per step time: 2.282 ms, avg loss: 0.050
Epoch:[  3/ 10], step:[ 1875/ 1875], loss:[0.035/0.038], time:2.254 ms, lr:0.01000
Epoch time: 4380.553 ms, per step time: 2.336 ms, avg loss: 0.038
Epoch:[  4/ 10], step:[ 1875/ 1875], loss:[0.130/0.031], time:1.932 ms, lr:0.01000
Epoch time: 4287.547 ms, per step time: 2.287 ms, avg loss: 0.031
Epoch:[  5/ 10], step:[ 1875/ 1875], loss:[0.003/0.027], time:1.981 ms, lr:0.01000
Epoch time: 4377.000 ms, per step time: 2.334 ms, avg loss: 0.027
Epoch:[  6/ 10], step:[ 1875/ 1875], loss:[0.004/0.023], time:2.167 ms, lr:0.01000
Epoch time: 4687.250 ms, per step time: 2.500 ms, avg loss: 0.023
Epoch:[  7/ 10], step:[ 1875/ 1875], loss:[0.004/0.020], time:2.226 ms, lr:0.01000
Epoch time: 4685.529 ms, per step time: 2.499 ms, avg loss: 0.020
Epoch:[  8/ 10], step:[ 1875/ 1875], loss:[0.000/0.016], time:2.275 ms, lr:0.01000
Epoch time: 4651.129 ms, per step time: 2.481 ms, avg loss: 0.016
Epoch:[  9/ 10], step:[ 1875/ 1875], loss:[0.022/0.015], time:2.177 ms, lr:0.01000
Epoch time: 4623.760 ms, per step time: 2.466 ms, avg loss: 0.015
</pre></div>
</div>
<p>The preceding print result shows that the loss values tend to converge as the number of training epochs increases.</p>
</section>
<section id="saving-the-model">
<h2>Saving the Model<a class="headerlink" href="#saving-the-model" title="Permalink to this headline"></a></h2>
<p>After the network training is complete, save the network model as a file. There are two types of APIs for saving models:</p>
<ol class="arabic simple">
<li><p>One is to simply save the network model before and after training. The advantage is that the interface is easy to use, but only the network model status when the command is executed is retained.</p></li>
<li><p>The other one is to save the interface during network model training. MindSpore automatically saves the number of epochs and number of steps set during training. That is, the intermediate weight parameters generated during the model training process are also saved to facilitate network fine-tuning and stop training.</p></li>
</ol>
<section id="directly-saving-the-model">
<h3>Directly Saving the Model<a class="headerlink" href="#directly-saving-the-model" title="Permalink to this headline"></a></h3>
<p>Use the save_checkpoint provided by MindSpore to save the model, pass it to the network, and save the path.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="c1"># The defined network model is net, which is used before or after training.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="s2">&quot;./MyNet.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In the preceding command, <code class="docutils literal notranslate"><span class="pre">network</span></code> indicates the training network, and <code class="docutils literal notranslate"><span class="pre">&quot;./MyNet.ckpt&quot;</span></code> indicates the path for storing the network model.</p>
</section>
<section id="saving-the-model-during-training">
<h3>Saving the Model During Training<a class="headerlink" href="#saving-the-model-during-training" title="Permalink to this headline"></a></h3>
<p>During model training, you can use the <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">model.train</span></code> to transfer the <a class="reference external" href="https://mindspore.cn/docs/en/r1.8/api_python/mindspore/mindspore.ModelCheckpoint.html#mindspore.ModelCheckpoint">ModelCheckpoint</a> object (usually used together with <a class="reference external" href="https://mindspore.cn/docs/en/r1.8/api_python/mindspore/mindspore.CheckpointConfig.html#mindspore.CheckpointConfig">CheckpointConfig</a>) to save model parameters and generate a checkpoint (CKPT) file.</p>
<p>You can set <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> to configure the Checkpoint policy as required. The following describes the usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="c1"># Set the value of epoch_num.</span>
<span class="n">epoch_num</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Set the model saving parameters.</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">1875</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Apply the model saving parameters.</span>
<span class="n">ckpoint</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;lenet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./lenet&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_num</span><span class="p">,</span> <span class="n">dataset_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ckpoint</span><span class="p">])</span>
</pre></div>
</div>
<p>In the preceding code, you need to initialize a <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> class object to set the saving policy.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">save_checkpoint_steps</span></code> indicates the interval (in steps) for saving the checkpoint file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keep_checkpoint_max</span></code> indicates the maximum number of checkpoint files that can be retained.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefix</span></code> indicates the prefix of the generated checkpoint file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">directory</span></code> indicates the directory for storing files.</p></li>
</ul>
<p>Create a <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> object and pass it to the <code class="docutils literal notranslate"><span class="pre">model.train</span></code> method. Then the checkpoint function can be used during training.</p>
<p>The generated checkpoint file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>lenet-graph.meta # Computational graph after compiled.
lenet-1_1875.ckpt  # The extension of the checkpoint file is &#39;.ckpt&#39;.
lenet-2_1875.ckpt  # The file name indicates the epoch where the parameter is stored and the number of steps. Here are the model parameters for the 1875th step of the second epoch.
lenet-3_1875.ckpt  # The file name indicates that the model parameters generated during the 1875th step of the third epoch are saved.
...
</pre></div>
</div>
<p>If you use the same prefix and run the training script for multiple times, checkpoint files with the same name may be generated. To help users distinguish files generated each time, MindSpore adds underscores (_) and digits to the end of the user-defined prefix. If you want to delete the <code class="docutils literal notranslate"><span class="pre">.ckpt</span></code> file, delete the <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file at the same time.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">lenet_3-2_1875.ckpt</span></code> indicates the checkpoint file generated during the 1875th step of the second epoch after the script is executed for the third time.</p>
</section>
</section>
<section id="loading-the-model">
<h2>Loading the Model<a class="headerlink" href="#loading-the-model" title="Permalink to this headline"></a></h2>
<p>To load the model weight, you need to create an instance of the same model and then use the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> and <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> methods to load parameters.</p>
<p>The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="kn">from</span> <span class="nn">mindvision.classification.dataset</span> <span class="kn">import</span> <span class="n">Mnist</span>
<span class="kn">from</span> <span class="nn">mindvision.classification.models</span> <span class="kn">import</span> <span class="n">lenet</span>

<span class="c1"># Save the model parameters to the parameter dictionary. The model parameters saved during the training are loaded.</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;./lenet/lenet-5_1875.ckpt&quot;</span><span class="p">)</span>

<span class="c1"># Redefine a LeNet neural network.</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">lenet</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Load parameters to the network.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="c1"># Redefine an optimizer function.</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">net_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">})</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> method loads the network parameters in the parameter file to the <code class="docutils literal notranslate"><span class="pre">param_dict</span></code> dictionary.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> method loads the parameters in the <code class="docutils literal notranslate"><span class="pre">param_dict</span></code> dictionary to the network or optimizer. After the loading, parameters in the network are stored by the checkpoint.</p></li>
</ul>
<section id="validating-the-model">
<h3>Validating the Model<a class="headerlink" href="#validating-the-model" title="Permalink to this headline"></a></h3>
<p>After the preceding modules load the parameters to the network, you can call the <code class="docutils literal notranslate"><span class="pre">eval</span></code> function to verify the inference in the inference scenario. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Call eval() for inference.</span>
<span class="n">download_eval</span> <span class="o">=</span> <span class="n">Mnist</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;./mnist&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dataset_eval</span> <span class="o">=</span> <span class="n">download_eval</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset_eval</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;accuracy&#39;: 0.9866786858974359}
</pre></div>
</div>
</section>
<section id="for-transfer-learning">
<h3>For Transfer Learning<a class="headerlink" href="#for-transfer-learning" title="Permalink to this headline"></a></h3>
<p>For task interrupt retraining and fine-tuning scenarios, the <code class="docutils literal notranslate"><span class="pre">train</span></code> function can be called to perform transfer learning. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a training dataset.</span>
<span class="n">download_train</span> <span class="o">=</span> <span class="n">Mnist</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;./mnist&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dataset_train</span> <span class="o">=</span> <span class="n">download_train</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

<span class="c1"># Network model calls train() for training.</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_num</span><span class="p">,</span> <span class="n">dataset_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1875</span><span class="p">)])</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Epoch:[  0/  5], step:[ 1875/ 1875], loss:[0.000/0.010], time:2.193 ms, lr:0.01000
Epoch time: 4106.620 ms, per step time: 2.190 ms, avg loss: 0.010
Epoch:[  1/  5], step:[ 1875/ 1875], loss:[0.000/0.009], time:2.036 ms, lr:0.01000
Epoch time: 4233.697 ms, per step time: 2.258 ms, avg loss: 0.009
Epoch:[  2/  5], step:[ 1875/ 1875], loss:[0.000/0.010], time:2.045 ms, lr:0.01000
Epoch time: 4246.248 ms, per step time: 2.265 ms, avg loss: 0.010
Epoch:[  3/  5], step:[ 1875/ 1875], loss:[0.000/0.008], time:2.001 ms, lr:0.01000
Epoch time: 4235.036 ms, per step time: 2.259 ms, avg loss: 0.008
Epoch:[  4/  5], step:[ 1875/ 1875], loss:[0.002/0.008], time:2.039 ms, lr:0.01000
Epoch time: 4354.482 ms, per step time: 2.322 ms, avg loss: 0.008
</pre></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="train.html" class="btn btn-neutral float-left" title="Model Training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="infer.html" class="btn btn-neutral float-right" title="Inference and Deployment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>