<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Combination of Dynamic and Static Graphs &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Dynamic and Static Graphs" href="mode.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/save_load.html">Saving and Loading the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Module Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../derivation.html">Advanced Automatic Differentiation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../compute_graph.html">Computation Graphs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="mode.html">Dynamic and Static Graphs</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Combination of Dynamic and Static Graphs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#implementation-principle">Implementation Principle</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ms-function-modifier"><code class="docutils literal notranslate"><span class="pre">ms_function</span></code> Modifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#precautions">Precautions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../compute_graph.html">Computation Graphs</a> &raquo;</li>
      <li>Combination of Dynamic and Static Graphs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/advanced/compute_graph/combine.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="combination-of-dynamic-and-static-graphs">
<h1>Combination of Dynamic and Static Graphs<a class="headerlink" href="#combination-of-dynamic-and-static-graphs" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.9/tutorials/source_en/advanced/compute_graph/combine.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/resource/_static/logo_source_en.png"></a></p>
<p>Currently, dynamic and static graphs are supported in the industry. Dynamic graphs are executed through explanation, with dynamic syntax affinity and flexible expression. Static graphs are executed through just in time (JIT) build, which focuses on static syntax and has many syntax constraints. The build process of the dynamic graph is different from that of the static graph. As a result, the syntax constraints are also different.</p>
<p>For dynamic and static graph modes, MindSpore first unifies the API expression and uses the same APIs in the two modes. Then, it unifies the underlying differentiation mechanism of dynamic and static graphs.</p>
<p><img alt="dynamic" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/tutorials/source_en/advanced/compute_graph/images/framework1.png" /></p>
<section id="implementation-principle">
<h2>Implementation Principle<a class="headerlink" href="#implementation-principle" title="Permalink to this headline"></a></h2>
<p>MindSpore allows you to use the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier to modify objects that need to be executed using static graphs, achieving combination of dynamic and static graphs. The following uses a simple combination example to describe the implementation principle. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a class to implement the x self addition.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a class to implement the x self multiplication.&quot;&quot;&quot;</span>
    <span class="nd">@ms_function</span>  <span class="c1"># Use ms_function to modify the function. This function is executed in static graph mode.</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Test</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define a class to implement Add(x), Mul(x), and then Add(x).&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Test</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">Mul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;init x:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Test</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">x:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    init x:
     [[1. 1. 1.]
     [1. 1. 1.]
     [1. 1. 1.]]

    x:
     [[1024. 1024. 1024.]
     [1024. 1024. 1024.]
     [1024. 1024. 1024.]]
</pre></div>
</div>
<p>According to the preceding information, after the test operation, the final value of x is a 3*3 matrix whose each element is 8. The following figure shows the build method of this test case according to the execution sequence.</p>
<p><img alt="msfunction" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/tutorials/source_en/advanced/compute_graph/images/ms_function.png" /></p>
<p>Functions modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> are built and executed in static graph mode. If the network involves reverse derivation, the part modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> is also used to generate a backward graph in the form of an entire graph. The backward graph is connected to backward graphs of operators before and after the graph and then delivered for execution. The cache policy is the same as that of the static graph. When the input shape and type information of the same function object is the same, the built graph structure is cached.</p>
</section>
<section id="ms-function-modifier">
<h2><code class="docutils literal notranslate"><span class="pre">ms_function</span></code> Modifier<a class="headerlink" href="#ms-function-modifier" title="Permalink to this headline"></a></h2>
<p>To improve the execution speed of forward computing tasks in dynamic graph mode, MindSpore provides the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier. You can modify Python functions or member functions of Python classes to build them into computational graphs. Technologies such as graph optimization are used to improve the running speed.</p>
<section id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this headline"></a></h3>
<p>MindSpore supports  static build in dynamic graphs. You can use the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier to modify the function objects that need to be executed using static graphs to implement mixed execution of dynamic and static graphs.</p>
<section id="1-modifying-independent-function">
<h4>1. Modifying Independent Function<a class="headerlink" href="#1-modifying-independent-function" title="Permalink to this headline"></a></h4>
<p>When using the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier, you can modify an independently defined function so that it can run in static graph mode. The following is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="c1"># Set the running mode to dynamic graph mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="c1"># Use the modifier to specify the execution in static graph mode.</span>
<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">add_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">add_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [5. 7. 9.]
</pre></div>
</div>
<p>In the preceding sample code, although the running mode is set to dynamic graph mode at the beginning, the <code class="docutils literal notranslate"><span class="pre">add_func(x,</span> <span class="pre">y)</span></code> function is modified using the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier. Therefore, the <code class="docutils literal notranslate"><span class="pre">add_func(x,</span> <span class="pre">y)</span></code> function still runs in static graph mode.</p>
</section>
<section id="2-modifying-the-member-functions-of-a-class">
<h4>2. Modifying the Member Functions of a Class<a class="headerlink" href="#2-modifying-the-member-functions-of-a-class" title="Permalink to this headline"></a></h4>
<p>When using the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier, you can modify the member methods of the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> subclass, <code class="docutils literal notranslate"><span class="pre">ms_class</span></code> class, or common user defined class. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="c1"># Set the running mode to dynamic graph mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>

    <span class="nd">@ms_function</span> <span class="c1"># Use the modifier to specify the execution in static graph mode.</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">grad_ops</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Define the derivation operation.</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()</span>
<span class="n">grad_out</span> <span class="o">=</span> <span class="n">grad_ops</span><span class="p">(</span><span class="n">net</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Infer result:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gradient result:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Grad x Tensor1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Derivation of x</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Grad y Tensor2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Derivation of y</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    Infer result:
     [5. 7. 9.]
    Gradient result:
    Grad x Tensor1:
     [1. 1. 1.]
    Grad y Tensor2:
     [1. 1. 1.]
</pre></div>
</div>
<p>According to the preceding information, the sum of x and y is [5, 7, 9]. The derivation result of x is the same as that of y, that is, [1, 1, 1].</p>
</section>
</section>
<section id="precautions">
<h3>Precautions<a class="headerlink" href="#precautions" title="Permalink to this headline"></a></h3>
<p>When using <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> to modify functions to improve execution efficiency, pay attention to the following points:</p>
<ol class="arabic">
<li><p>Functions modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> must be within the syntax scope supported by static graph build, including but not limited to data types.</p></li>
<li><p>The control flow syntax supported by a function modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> is the same as that supported by the static graph. An acceleration effect is achieved only for a control flow structure with a fixed loop count or a branch condition.</p></li>
<li><p>When the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> function is used in PyNative mode, the parts that are not modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> support breakpoint debugging, and the parts modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> do not support breakpoint debugging because they are built in static graph mode.</p></li>
<li><p>Functions modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> are built and executed in static graph mode. Therefore, <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> does not support the Hook operator in the modified functions or the customized Bprop function.</p></li>
<li><p>Functions modified by <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> are affected by side effects of static graph functions. Side effects of a function refer to the additional effects on the main function in addition to the return value of the function, for example, modifying global variables (variables other than the function) and modifying function parameters.</p>
<p>Scenario 1:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="c1"># pylint: disable=W0612</span>

<span class="n">value</span> <span class="o">=</span> <span class="mi">5</span>

<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">value</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    5
</pre></div>
</div>
<p>In this scenario, <code class="docutils literal notranslate"><span class="pre">value</span></code> is a global variable and is modified in the <code class="docutils literal notranslate"><span class="pre">func</span></code> function. In this case, if <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> is used to modify the <code class="docutils literal notranslate"><span class="pre">func</span></code> function, the global variable <code class="docutils literal notranslate"><span class="pre">value</span></code> is not changed. The reason is that statements irrelevant to return values are optimized during static graph build.</p>
<p>Scenario 2:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Func</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="nd">@ms_function</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">Func</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out1:&quot;</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">func</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out2:&quot;</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    out1: [6. 7. 8.]
    out2: [6. 7. 8.]
</pre></div>
</div>
<p>According to the preceding information, after the member variable <code class="docutils literal notranslate"><span class="pre">value</span></code> of the <code class="docutils literal notranslate"><span class="pre">func</span></code> class is changed to 1, the <code class="docutils literal notranslate"><span class="pre">construct</span></code> operation of the member function is not affected. In this scenario, <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> is used to modify the <code class="docutils literal notranslate"><span class="pre">construct</span></code> member function of the <code class="docutils literal notranslate"><span class="pre">func</span></code> object. When <code class="docutils literal notranslate"><span class="pre">construct</span></code> is executed, it is built and executed in static graph mode. The static graph caches the build result. Therefore, when <code class="docutils literal notranslate"><span class="pre">func</span></code> is called for the second time, the modification of <code class="docutils literal notranslate"><span class="pre">value</span></code> does not take effect.</p>
</li>
<li><p>If a function with the <code class="docutils literal notranslate"><span class="pre">ms_function</span></code> modifier contains operators (such as <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> and <code class="docutils literal notranslate"><span class="pre">Add</span></code>) that do not require parameter training, these operators can be directly called in the modified function. If the modified function contains operators (such as <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> operators) that require parameter training, these operators must be instantiated outside the modified function. The following uses sample code to describe the two scenarios.</p>
<p>Scenario 1: Directly call an operator (<code class="docutils literal notranslate"><span class="pre">mindspore.ops.Add</span></code> in the example) that does not require parameter training in the modified function. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>

<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">add_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">add_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">y:&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">z:&quot;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    x: [1. 2. 3.]
    y: [4. 5. 6.]
    z: [5. 7. 9.]
</pre></div>
</div>
<p>Scenario 2: The operator (<code class="docutils literal notranslate"><span class="pre">mindspore.nn.Conv2d</span></code> in the example) that requires parameter training must be instantiated outside the modified function. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ms_function</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="c1"># Instantiate the conv_obj operator in the conv_fn function.</span>
<span class="n">conv_obj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">conv_obj</span><span class="o">.</span><span class="n">init_parameters_data</span><span class="p">()</span>

<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">conv_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">conv_obj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">conv_fn</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [[[[ 0.00829158 -0.02994147]
    [-0.09116832 -0.00181637]]

    [[-0.00519348 -0.02172063]
    [-0.04015012 -0.02083161]]

    [[ 0.00608188 -0.01443425]
    [-0.01468289  0.01200477]]

    [[ 0.00845292  0.00044869]
    [-0.00361492  0.01993337]]]]
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mode.html" class="btn btn-neutral float-left" title="Dynamic and Static Graphs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>