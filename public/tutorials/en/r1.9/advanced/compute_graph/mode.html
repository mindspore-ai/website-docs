<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Dynamic and Static Graphs &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Combination of Dynamic and Static Graphs" href="combine.html" />
    <link rel="prev" title="Computation Graphs" href="../compute_graph.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/save_load.html">Saving and Loading the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Module Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../derivation.html">Advanced Automatic Differentiation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../compute_graph.html">Computation Graphs</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Dynamic and Static Graphs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction-to-dynamic-and-static-graphs">Introduction to Dynamic and Static Graphs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mode-selection">Mode Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mode-switching">Mode Switching</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#static-graph">Static Graph</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#execution-principle-of-the-static-graph-mode">Execution Principle of the Static Graph Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#code-example-in-static-graph-mode">Code Example in Static Graph Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#control-flow-in-static-graph-mode">Control Flow in Static Graph Mode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-graph">Dynamic Graph</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#execution-principle-of-the-dynamic-graph-mode">Execution Principle of the Dynamic Graph Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#principle-of-automatic-differentiation-in-dynamic-graph-mode">Principle of Automatic Differentiation in Dynamic Graph Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#control-flow-in-dynamic-graph-mode">Control Flow in Dynamic Graph Mode</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="combine.html">Combination of Dynamic and Static Graphs</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../compute_graph.html">Computation Graphs</a> &raquo;</li>
      <li>Dynamic and Static Graphs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/advanced/compute_graph/mode.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="dynamic-and-static-graphs">
<h1>Dynamic and Static Graphs<a class="headerlink" href="#dynamic-and-static-graphs" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.9/tutorials/source_en/advanced/compute_graph/mode.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/resource/_static/logo_source_en.png" /></a></p>
<p>Currently, there are two execution modes of a mainstream deep learning framework: a static graph mode (Graph) and a dynamic graph mode (PyNative).</p>
<ul class="simple">
<li><p>In static graph mode, when the program is built and executed, the graph structure of the neural network is generated first, and then the computation operations involved in the graph are performed. Therefore, in static graph mode, the compiler can achieve better execution performance by using technologies such as graph optimization, which facilitates large-scale deployment and cross-platform running.</p></li>
<li><p>In dynamic graph mode, the program is executed line by line according to the code writing sequence. In the forward execution process, the backward execution graph is dynamically generated according to the backward propagation principle. In this mode, the compiler delivers the operators in the neural network to the device one by one for computing, facilitating users to build and debug the neural network model.</p></li>
</ul>
<section id="introduction-to-dynamic-and-static-graphs">
<h2>Introduction to Dynamic and Static Graphs<a class="headerlink" href="#introduction-to-dynamic-and-static-graphs" title="Permalink to this headline"></a></h2>
<p>MindSpore provides a unified encoding mode for static and dynamic graphs, significantly enhancing compatibility between both types of graphs. This enables you to switch between the static and dynamic graph modes by changing only one line of code, eliminating the need to develop multiple sets of code. By default, MindSpore uses the static graph mode, and the dynamic graph mode is used for debugging.</p>
<blockquote>
<div><p>When switching the running mode from dynamic graph to static graph, pay attention to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/note/static_graph_syntax_support.html">static graph syntax support</a>.</p>
</div></blockquote>
<section id="mode-selection">
<h3>Mode Selection<a class="headerlink" href="#mode-selection" title="Permalink to this headline"></a></h3>
<p>You can configure the <code class="docutils literal notranslate"><span class="pre">context</span></code> parameter to control the program running mode. The differences between the dynamic graph mode and the static graph mode are as follows:</p>
<ul class="simple">
<li><p><strong>Application scenario:</strong> The network structure of a static graph needs to be built at the beginning, and then the framework optimizes and executes the entire graph. This mode is applicable to scenarios where the network is fixed and high performance is required. Operators are executed line by line on a dynamic graph. Single operator, common functions, and networks can be executed, and gradients can be computed separately.</p></li>
<li><p><strong>Network execution:</strong> When the same network and operator are executed in static graph mode and dynamic graph mode, the accuracy effect is the same. The static graph mode uses technologies such as graph optimization and entire computational graph offloading. The static graph mode has higher network performance and efficiency, while the dynamic graph mode facilitates debugging and optimization.</p></li>
<li><p><strong>Code debugging:</strong> The dynamic graph mode is recommended for script development and network process debugging. In dynamic graph mode, you can easily set breakpoints and obtain intermediate results of network execution. You can also debug the network in pdb mode. In static graph mode, breakpoints cannot be set. You can only specify an operator for printing and view the output result after the network execution is complete.</p></li>
</ul>
</section>
<section id="mode-switching">
<h3>Mode Switching<a class="headerlink" href="#mode-switching" title="Permalink to this headline"></a></h3>
<p>During mode switching, you need to set the running mode in the context. Define the network model <code class="docutils literal notranslate"><span class="pre">MyNet</span></code> and the data used in subsequent code snippets for subsequent switching and display of the dynamic and static graph modes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">class</span> <span class="nc">MyNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Customize the network to implement the addition of two tensors.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
<p>Set the running mode to static graph mode.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">MyNet</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [5. 7. 9.]
</pre></div>
</div>
<p>When MindSpore is in static graph mode, you can switch to the dynamic graph mode by setting <code class="docutils literal notranslate"><span class="pre">mode=ms.PYNATIVE_MODE</span></code>. Similarly, when MindSpore is in dynamic graph mode, you can switch to the static graph mode by setting<code class="docutils literal notranslate"><span class="pre">mode=ms.GRAPH_MODE</span></code>. Pay attention to <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/note/static_graph_syntax_support.html">static graph syntax support</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">MyNet</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [5. 7. 9.]
</pre></div>
</div>
</section>
</section>
<section id="static-graph">
<h2>Static Graph<a class="headerlink" href="#static-graph" title="Permalink to this headline"></a></h2>
<p>In MindSpore, the static graph mode is also called the Graph mode, which is applicable to scenarios where the network is fixed and high performance is required. You can set the input parameter <code class="docutils literal notranslate"><span class="pre">mode</span></code> to <code class="docutils literal notranslate"><span class="pre">GRAPH_MODE</span></code> in the <code class="docutils literal notranslate"><span class="pre">set_context</span></code> API to set the static graph mode.</p>
<p>In static graph mode, the compiler can perform global optimization on graphs based on technologies such as graph optimization and entire computational graph offloading. Therefore, good performance can be obtained when the compiler is executed in static graph mode. However, the execution graph is converted from the source code. Therefore, not all Python syntax is supported in static graph mode. There are some special constraints. For details about the support, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
<section id="execution-principle-of-the-static-graph-mode">
<h3>Execution Principle of the Static Graph Mode<a class="headerlink" href="#execution-principle-of-the-static-graph-mode" title="Permalink to this headline"></a></h3>
<p>In static graph mode, MindSpore converts the Python source code into an intermediate representation (IR), optimizes the IR graph based on the IR, and executes the optimized graph on the hardware device.</p>
<p>MindSpore uses a graph-based functional IR called MindIR. The static graph mode is built and optimized based on MindIR. When using the static graph mode, you need to use the <a class="reference external" href="https://mindspore.cn/docs/en/r1.9/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell">nn.Cell</a> class and writ execution code in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function.</p>
</section>
<section id="code-example-in-static-graph-mode">
<h3>Code Example in Static Graph Mode<a class="headerlink" href="#code-example-in-static-graph-mode" title="Permalink to this headline"></a></h3>
<p>The code of the static graph mode is as follows. The neural network model implements the computation operation of <span class="math notranslate nohighlight">\(f(x, y)=x*y\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the running mode to static graph mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Customize the network to implement the multiplication of two tensors.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s2">&quot;&quot;&quot;Define execution code.&quot;&quot;</span>
<span class="s2">        return self.mul(x, y)</span>

<span class="s2">x = ms.Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))</span>
<span class="s2">y = ms.Tensor(np.array([4.0, 5.0, 6.0]).astype(np.float32))</span>

<span class="s2">net = Net()</span>

<span class="s2">print(net(x, y))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [ 4. 10. 18.]
</pre></div>
</div>
</section>
<section id="control-flow-in-static-graph-mode">
<h3>Control Flow in Static Graph Mode<a class="headerlink" href="#control-flow-in-static-graph-mode" title="Permalink to this headline"></a></h3>
<p>For details about control flows in static graph mode, see <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r1.9/network/control_flow.html">Process Control Statements</a>.</p>
</section>
</section>
<section id="dynamic-graph">
<h2>Dynamic Graph<a class="headerlink" href="#dynamic-graph" title="Permalink to this headline"></a></h2>
<p>In MindSpore, the dynamic graph mode is also called the PyNative mode. You can set the input parameter <code class="docutils literal notranslate"><span class="pre">mode</span></code> to <code class="docutils literal notranslate"><span class="pre">PYNATIVE_MODE</span></code> in the <code class="docutils literal notranslate"><span class="pre">set_context</span></code> API to set the dynamic graph mode.</p>
<p>During script development and network process debugging, you are advised to use the dynamic graph mode for debugging. The dynamic graph mode supports single-operator execution, common function execution, network execution, and independent gradient computation.</p>
<section id="execution-principle-of-the-dynamic-graph-mode">
<h3>Execution Principle of the Dynamic Graph Mode<a class="headerlink" href="#execution-principle-of-the-dynamic-graph-mode" title="Permalink to this headline"></a></h3>
<p>In dynamic graph mode, you can use complete Python APIs. In addition, when APIs provided by MindSpore are used, the framework executes operator API operations on the corresponding hardware platform based on the selected hardware platform (Ascend/GPU/CPU) or environment information, and returns the corresponding result.</p>
<p>The overall execution process of the framework is as follows:</p>
<p><img alt="process" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/tutorials/source_en/advanced/compute_graph/images/framework2.png" /></p>
<p>The front-end Python API is called to the framework layer and finally computed on the corresponding hardware device.</p>
<p>The following uses the <code class="docutils literal notranslate"><span class="pre">ops.mul</span></code> operator to replace the network model that needs to be defined in static graph mode to implement the computation of <span class="math notranslate nohighlight">\(f(x, y)=x*y\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the running mode to dynamic graph mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [ 4. 10. 18.]
</pre></div>
</div>
<p>In the preceding sample code, when the <code class="docutils literal notranslate"><span class="pre">ops.mul(x,</span> <span class="pre">y)</span></code> API is called, the Python API at the MindSpore expression layer is called to the C++ layer of the MindSpore framework using <a class="reference external" href="https://pybind11.readthedocs.io/en/stable/basics.html">Pybind11</a> and converted into the C++ API. Then, the framework automatically selects the corresponding hardware device based on the MindSpore installation environment information and performs the add operation on the hardware device.</p>
<p>According to the preceding principles, in PyNative mode, Python script code is executed based on the Python syntax. During the execution, Python APIs at the MindSpore expression layer are executed on different hardware based on user settings to accelerate performance.</p>
<p>Therefore, in dynamic graph mode, you can use Python syntax and debugging methods as required.</p>
</section>
<section id="principle-of-automatic-differentiation-in-dynamic-graph-mode">
<h3>Principle of Automatic Differentiation in Dynamic Graph Mode<a class="headerlink" href="#principle-of-automatic-differentiation-in-dynamic-graph-mode" title="Permalink to this headline"></a></h3>
<p>In a dynamic graph, the forward propagation process is executed based on the Python syntax, and the backward propagation process is implemented based on tensors.</p>
<p>Therefore, during the forward propagation process, all operations applied to tensors are recorded and computed backward, and all backward propagation processes are connected to form an overall backward propagation graph. Finally, the backward graph is executed on the device and the gradient is computed.</p>
<p>The following uses a simple sample code to describe the principle of automatic differentiation in dynamic graph mode. Multiply the matrix x by a fixed parameter z, and then perform matrix multiplication with y:</p>
<div class="math notranslate nohighlight">
\[f(x, y)=(x * z) * y \tag{1}\]</div>
<p>The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the running mode to dynamic graph mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Customize a network.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define the derivation of x.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWrtX</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [[4.5099998 2.7       3.6000001]
     [4.5099998 2.7       3.6000001]]
</pre></div>
</div>
<blockquote>
<div><p>The accuracy may vary depending on the computing platform. Therefore, the execution results of the preceding code vary slightly on different platforms. For details about the derivation of the formula and the explanation of the preceding printed results, see <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.9/advanced/derivation.html#">Automatic Derivation</a>.</p>
</div></blockquote>
<p><img alt="forward" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/tutorials/source_zh_cn/advanced/compute_graph/images/forward_backward.png" /></p>
<p>It can be learned from the preceding dynamic graph mode that, in a forward propagation process, MindSpore records a computation process of Mul, and a backward MulGrad operator is obtained according to a definition of a backward bprop corresponding to Mul.</p>
<p>The bprop definition of the Mul operator is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.ops._grad.grad_base</span> <span class="kn">import</span> <span class="n">bprop_getters</span>

<span class="nd">@bprop_getters</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_bprop_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Grad definition for `Mul` operation.&quot;&quot;&quot;</span>
    <span class="n">mul_func</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">bc_dx</span> <span class="o">=</span> <span class="n">mul_func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="n">bc_dy</span> <span class="o">=</span> <span class="n">mul_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">binop_grad_common</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bc_dx</span><span class="p">,</span> <span class="n">bc_dy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">bprop</span>
</pre></div>
</div>
<p>You can see that two backward propagation gradient values of the input and output are required to compute the backward input of Mul. In this case, you can connect z to MulGrad based on the actual input value. The rest can be deduced by analogy. For the next operator Matmul, the MatmulGrad information is obtained accordingly, and then the context gradient propagation is connected based on the input and output of bprop.</p>
<p>Similarly, for derivation of input y, the same process may be used for derivation.</p>
</section>
<section id="control-flow-in-dynamic-graph-mode">
<h3>Control Flow in Dynamic Graph Mode<a class="headerlink" href="#control-flow-in-dynamic-graph-mode" title="Permalink to this headline"></a></h3>
<p>In MindSpore, the control flow syntax is not specially processed. Instead, the control flow syntax is directly executed based on the Python syntax, and automatic differentiation operations are performed on the expanded execution operators.</p>
<p>For example, in a for loop, the Python source code is executed first in the dynamic graph, and then the statements in the for loop are continuously executed based on the number of loops, and automatic differentiation operations are performed on the operators.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the running mode to dynamic graph mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Customize a network.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [4. 5. 6.]
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../compute_graph.html" class="btn btn-neutral float-left" title="Computation Graphs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="combine.html" class="btn btn-neutral float-right" title="Combination of Dynamic and Static Graphs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>