

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Advanced Automatic Differentiation &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Computation Graphs" href="compute_graph.html" />
    <link rel="prev" title="Graph Data Loading and Processing" href="dataset/augment_graph_data.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/save_load.html">Saving and Loading the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Module Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Advanced Automatic Differentiation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#first-order-derivation">First-order Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#computing-the-first-order-derivative-for-input">Computing the First-order Derivative for Input</a></li>
<li class="toctree-l3"><a class="reference internal" href="#computing-the-derivative-for-weight">Computing the Derivative for Weight</a></li>
<li class="toctree-l3"><a class="reference internal" href="#returning-auxiliary-variables">Returning Auxiliary Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stopping-gradient-computation">Stopping Gradient Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#high-order-derivation">High-order Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#single-input-single-output-high-order-derivative">Single-input Single-output High-order Derivative</a></li>
<li class="toctree-l3"><a class="reference internal" href="#single-input-multi-output-high-order-derivative">Single-input Multi-output High-order Derivative</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-input-multiple-output-high-order-derivative">Multiple-Input Multiple-Output High-Order Derivative</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compute_graph.html">Computation Graphs</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Advanced Automatic Differentiation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced/derivation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="advanced-automatic-differentiation">
<h1>Advanced Automatic Differentiation<a class="headerlink" href="#advanced-automatic-differentiation" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.9/tutorials/source_en/advanced/derivation.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/resource/_static/logo_source_en.png"></a></p>
<p>The <code class="docutils literal notranslate"><span class="pre">grad</span></code> and <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code> provided by the <code class="docutils literal notranslate"><span class="pre">mindspore.ops</span></code> module generate the gradients of the network model. <code class="docutils literal notranslate"><span class="pre">grad</span></code> computes the network gradient, and <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code> computes both the forward output and the gradient of the network. This article focuses on how to use the main functions of the <code class="docutils literal notranslate"><span class="pre">grad</span></code>, including first-order and second-order derivations, derivation of the input or network weights separately, returning auxiliary variables, and stopping calculating the gradient.</p>
<blockquote>
<div><p>For more information about the derivative interface, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/api_python/ops/mindspore.ops.grad.html">API documentation</a>.</p>
</div></blockquote>
<section id="first-order-derivation">
<h2>First-order Derivation<a class="headerlink" href="#first-order-derivation" title="Permalink to this headline"></a></h2>
<p>Method: <code class="docutils literal notranslate"><span class="pre">mindspore.ops.grad</span></code>. The parameter usage is as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">fn</span></code>: the function or network to be derived.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">grad_position</span></code>: specifies the index of the input position to be derived. If the index is int type, it means to derive for a single input; if tuple type, it means to derive for the position of the index within the tuple, where the index starts from 0; and if None, it means not to derive for the input. In this scenario, <code class="docutils literal notranslate"><span class="pre">weights</span></code> is non-None. Default: 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weights</span></code>: the network variables that need to return the gradients in the training network. Generally the network variables can be obtained by <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">net.trainable_params()</span></code>. Default: None.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">has_aux</span></code>: symbol for whether to return auxiliary arguments. If True, the number of <code class="docutils literal notranslate"><span class="pre">fn</span></code> outputs must be more than one, where only the first output of <code class="docutils literal notranslate"><span class="pre">fn</span></code> is involved in the derivation and the other output values will be returned directly. Default: False.</p></li>
</ul>
<p>The following is a brief introduction to the use of the <code class="docutils literal notranslate"><span class="pre">grad</span></code> by first constructing a customized network model <code class="docutils literal notranslate"><span class="pre">Net</span></code> and then performing a first-order derivative on it:</p>
<div class="math notranslate nohighlight">
\[f(x, y)=(x * z) * y \tag{1}\]</div>
<p>First, define the network model <code class="docutils literal notranslate"><span class="pre">Net</span></code>, input <code class="docutils literal notranslate"><span class="pre">x</span></code>, and input <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="c1"># Define the inputs x and y.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">5.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<section id="computing-the-first-order-derivative-for-input">
<h3>Computing the First-order Derivative for Input<a class="headerlink" href="#computing-the-first-order-derivative-for-input" title="Permalink to this headline"></a></h3>
<p>To derive the inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, set <code class="docutils literal notranslate"><span class="pre">grad_position</span></code> to (0, 1):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial x}=2 * x * y * z \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial y}=x * x * z \tag{3}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    (Tensor(shape=[1], dtype=Float32, value= [ 3.00000000e+01]), Tensor(shape=[1], dtype=Float32, value= [ 9.00000000e+00]))
</pre></div>
</div>
</section>
<section id="computing-the-derivative-for-weight">
<h3>Computing the Derivative for Weight<a class="headerlink" href="#computing-the-derivative-for-weight" title="Permalink to this headline"></a></h3>
<p>Derive for the weight <code class="docutils literal notranslate"><span class="pre">z</span></code>, where it is not necessary to derive for the inputs, and set <code class="docutils literal notranslate"><span class="pre">grad_position</span></code> to None:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial z}=x * x * y \tag{4}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">params</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    (Tensor(shape=[1], dtype=Float32, value= [ 4.50000000e+01]),)
</pre></div>
</div>
</section>
<section id="returning-auxiliary-variables">
<h3>Returning Auxiliary Variables<a class="headerlink" href="#returning-auxiliary-variables" title="Permalink to this headline"></a></h3>
<p>Simultaneous derivation for the inputs and weights, where only the first output is involved in the derivation, with the following sample code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>


<span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>

<span class="c1"># Aux value does not contribute to the gradient.</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">inputs_gradient</span><span class="p">,</span> <span class="p">(</span><span class="n">aux_logits</span><span class="p">,)</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs_gradient</span><span class="p">),</span> <span class="n">aux_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    16, (16, 1)
</pre></div>
</div>
</section>
<section id="stopping-gradient-computation">
<h3>Stopping Gradient Computation<a class="headerlink" href="#stopping-gradient-computation" title="Permalink to this headline"></a></h3>
<p>You can use <code class="docutils literal notranslate"><span class="pre">stop_gradient</span></code> to stop computing the gradient of a specified operator to eliminate the impact of the operator on the gradient.</p>
<p>Based on the matrix multiplication network model used for the first-order derivation, add an operator <code class="docutils literal notranslate"><span class="pre">out2</span></code> and disable the gradient computation to obtain the customized network <code class="docutils literal notranslate"><span class="pre">Net2</span></code>. Then, check the derivation result of the input.</p>
<p>The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span> <span class="c1"># Stop computing the gradient of the out2 operator.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out1</span> <span class="o">+</span> <span class="n">out2</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [5.0]
</pre></div>
</div>
<p>According to the preceding information, <code class="docutils literal notranslate"><span class="pre">stop_gradient</span></code> is set for <code class="docutils literal notranslate"><span class="pre">out2</span></code>. Therefore, <code class="docutils literal notranslate"><span class="pre">out2</span></code> does not contribute to gradient computation. The output result is the same as that when <code class="docutils literal notranslate"><span class="pre">out2</span></code> is not added.</p>
<p>Delete <code class="docutils literal notranslate"><span class="pre">out2</span> <span class="pre">=</span> <span class="pre">stop_gradient(out2)</span></code> and check the output result. An example of the code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
        <span class="c1"># out2 = stop_gradient(out2)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out1</span> <span class="o">+</span> <span class="n">out2</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [10.0]
</pre></div>
</div>
<p>According to the printed result, after the gradient of the <code class="docutils literal notranslate"><span class="pre">out2</span></code> operator is computed, the gradients generated by the <code class="docutils literal notranslate"><span class="pre">out2</span></code> and <code class="docutils literal notranslate"><span class="pre">out1</span></code> operators are the same. Therefore, the value of each item in the result is twice the original value (accuracy error exists).</p>
</section>
</section>
<section id="high-order-derivation">
<h2>High-order Derivation<a class="headerlink" href="#high-order-derivation" title="Permalink to this headline"></a></h2>
<p>High-order differentiation is used in domains such as AI-supported scientific computing and second-order optimization. For example, in the molecular dynamics simulation, when the potential energy is trained using the neural network, the derivative of the neural network output to the input needs to be computed in the loss function, and then the second-order cross derivative of the loss function to the input and the weight exists in backward propagation.</p>
<p>In addition, the second-order derivative of the output to the input exists in a differential equation solved by AI (such as PINNs). Another example is that in order to enable the neural network to converge quickly in the second-order optimization, the second-order derivative of the loss function to the weight needs to be computed using the Newton method.</p>
<p>MindSpore can support high-order derivatives by computing derivatives for multiple times. The following uses several examples to describe how to compute derivatives.</p>
<section id="single-input-single-output-high-order-derivative">
<h3>Single-input Single-output High-order Derivative<a class="headerlink" href="#single-input-single-output-high-order-derivative" title="Permalink to this headline"></a></h3>
<p>For example, the formula of the Sin operator is as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = sin(x) \tag{1}\]</div>
<p>The first derivative is:</p>
<div class="math notranslate nohighlight">
\[f'(x) = cos(x) \tag{2}\]</div>
<p>The second derivative is:</p>
<div class="math notranslate nohighlight">
\[f''(x) = cos'(x) = -sin(x) \tag{3}\]</div>
<p>The second derivative (-Sin) is implemented as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Feedforward network model&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.1415926</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">firstgrad</span> <span class="o">=</span> <span class="n">Grad</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">secondgrad</span> <span class="o">=</span> <span class="n">GradSec</span><span class="p">(</span><span class="n">firstgrad</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">secondgrad</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># Print the result.</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [-0.]
</pre></div>
</div>
<p>The preceding print result shows that the value of <span class="math notranslate nohighlight">\(-sin(3.1415926)\)</span> is close to <span class="math notranslate nohighlight">\(0\)</span>.</p>
</section>
<section id="single-input-multi-output-high-order-derivative">
<h3>Single-input Multi-output High-order Derivative<a class="headerlink" href="#single-input-multi-output-high-order-derivative" title="Permalink to this headline"></a></h3>
<p>Compute the derivation of the following formula:</p>
<div class="math notranslate nohighlight">
\[f(x) = (f_1(x), f_2(x)) \tag{1}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[f_1(x) = sin(x) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[f_2(x) = cos(x) \tag{3}\]</div>
<p>MindSpore uses the reverse-mode automatic differentiation mechanism during gradient computation. The output result is summed and then the derivative of the input is computed. Therefore, the first derivative is:</p>
<div class="math notranslate nohighlight">
\[f'(x) = cos(x)  -sin(x) \tag{4}\]</div>
<p>The second derivative is:</p>
<div class="math notranslate nohighlight">
\[f''(x) = -sin(x) - cos(x) \tag{5}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Feedforward network model&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.1415926</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">firstgrad</span> <span class="o">=</span> <span class="n">Grad</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">secondgrad</span> <span class="o">=</span> <span class="n">GradSec</span><span class="p">(</span><span class="n">firstgrad</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">secondgrad</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># Print the result.</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [1.]
</pre></div>
</div>
<p>The preceding print result shows that the value of <span class="math notranslate nohighlight">\(-sin(3.1415926) - cos(3.1415926)\)</span> is close to <span class="math notranslate nohighlight">\(1\)</span>.</p>
</section>
<section id="multiple-input-multiple-output-high-order-derivative">
<h3>Multiple-Input Multiple-Output High-Order Derivative<a class="headerlink" href="#multiple-input-multiple-output-high-order-derivative" title="Permalink to this headline"></a></h3>
<p>Compute the derivation of the following formula:</p>
<div class="math notranslate nohighlight">
\[f(x, y) = (f_1(x, y), f_2(x, y)) \tag{1}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[f_1(x, y) = sin(x) - cos(y)  \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[f_2(x, y) = cos(x) - sin(y)  \tag{3}\]</div>
<p>MindSpore uses the reverse-mode automatic differentiation mechanism during gradient computation. The output result is summed and then the derivative of the input is computed.</p>
<p>Sum:</p>
<div class="math notranslate nohighlight">
\[\sum{output} = sin(x) + cos(x) - sin(y) - cos(y) \tag{4}\]</div>
<p>The first derivative of output sum with respect to input <span class="math notranslate nohighlight">\(x\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\mathrm{d}\sum{output}}{\mathrm{d}x} = cos(x) - sin(x) \tag{5}\]</div>
<p>The second derivative of output sum with respect to input <span class="math notranslate nohighlight">\(x\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\mathrm{d}\sum{output}^{2}}{\mathrm{d}^{2}x} = -sin(x) - cos(x) \tag{6}\]</div>
<p>The first derivative of output sum with respect to input <span class="math notranslate nohighlight">\(y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\mathrm{d}\sum{output}}{\mathrm{d}y} = -cos(y) + sin(y) \tag{7}\]</div>
<p>The second derivative of output sum with respect to input <span class="math notranslate nohighlight">\(y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\mathrm{d}\sum{output}^{2}}{\mathrm{d}^{2}y} = sin(y) + cos(y) \tag{8}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Feedforward network model&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.1415926</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.1415926</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">firstgrad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">secondgrad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">firstgrad</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">secondgrad</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Print the result.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [1.]
    [-1.]
</pre></div>
</div>
<p>According to the preceding result, the value of the second derivative <span class="math notranslate nohighlight">\(-sin(3.1415926) - cos(3.1415926)\)</span> of the output to the input <span class="math notranslate nohighlight">\(x\)</span> is close to <span class="math notranslate nohighlight">\(1\)</span>, and the value of the second derivative <span class="math notranslate nohighlight">\(sin(3.1415926) + cos(3.1415926)\)</span> of the output to the input <span class="math notranslate nohighlight">\(y\)</span> is close to <span class="math notranslate nohighlight">\(-1\)</span>.</p>
<blockquote>
<div><p>The accuracy may vary depending on the computing platform. Therefore, the execution results of the code in this section vary slightly on different platforms.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dataset/augment_graph_data.html" class="btn btn-neutral float-left" title="Graph Data Loading and Processing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="compute_graph.html" class="btn btn-neutral float-right" title="Computation Graphs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>