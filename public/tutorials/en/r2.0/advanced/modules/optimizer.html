<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimizer &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Advanced Data Processing" href="../dataset.html" />
    <link rel="prev" title="Loss Function" href="loss.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introduction.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dataset.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transforms.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/model.html">Building a Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/autograd.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/train.html">Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/save_load.html">Saving and Loading the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../model.html">Advanced Encapsulation: Model</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../modules.html">Model Module Customization</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="layer.html">Cell and Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="initializer.html">Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Optimizer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">Advanced Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../derivation.html">Advanced Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compute_graph.html">Computational Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mixed_precision.html">Automatic Mix Precision</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../modules.html">Model Module Customization</a> &raquo;</li>
      <li>Optimizer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/advanced/modules/optimizer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.0/tutorials/source_en/advanced/modules/optimizer.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png" /></a></p>
<section id="optimizer">
<h1>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this headline"></a></h1>
<p>During model training, the optimizer is used to update network parameters. A proper optimizer can effectively reduce the training time and improve model performance.</p>
<p>The most basic optimizer is the stochastic gradient descent (SGD) algorithm. Many optimizers are improved based on the SGD to achieve the target function to converge to the global optimal point more quickly and effectively. The <code class="docutils literal notranslate"><span class="pre">nn</span></code> module in MindSpore provides common optimizers, such as <code class="docutils literal notranslate"><span class="pre">nn.SGD</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Adam</span></code>, and <code class="docutils literal notranslate"><span class="pre">nn.Momentum</span></code>. The following describes how to configure the optimizer provided by MindSpore and how to customize the optimizer.</p>
<p><img alt="learningrate.png" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/tutorials/source_zh_cn/advanced/modules/images/learning_rate.png" /></p>
<blockquote>
<div><p>For details about the optimizer provided by MindSpore, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/mindspore.nn.html#optimizer">Optimizer API</a>.</p>
</div></blockquote>
<section id="configuring-the-optimizer">
<h2>Configuring the Optimizer<a class="headerlink" href="#configuring-the-optimizer" title="Permalink to this headline"></a></h2>
<p>When using the optimizer provided by MindSpore, you need to specify the network parameter <code class="docutils literal notranslate"><span class="pre">params</span></code> to be optimized, and then set other main parameters of the optimizer, such as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> and <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>.</p>
<p>If you want to set options for different network parameters separately, for example, set different learning rates for convolutional and non-convolutional parameters, you can use the parameter grouping method to set the optimizer.</p>
<section id="parameter-configuration">
<h3>Parameter Configuration<a class="headerlink" href="#parameter-configuration" title="Permalink to this headline"></a></h3>
<p>When building an optimizer instance, you need to use the optimizer parameter <code class="docutils literal notranslate"><span class="pre">params</span></code> to configure the weights to be trained and updated on the model network.</p>
<p><code class="docutils literal notranslate"><span class="pre">Parameter</span></code> contains a Boolean class attribute <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>, which is used to indicate whether network parameters in the model need to be updated. The default value of <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> of most network parameters is True, while the default value of <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> of a few network parameters is False, for example, <code class="docutils literal notranslate"><span class="pre">moving_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">moving_variance</span></code> in BatchNorm.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">trainable_params</span></code> method in MindSpore shields the attribute whose <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> is False in <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>. When configuring the input parameter <code class="docutils literal notranslate"><span class="pre">params</span></code> for the optimizer, you can use the <code class="docutils literal notranslate"><span class="pre">net.trainable_params()</span></code> method to specify the network parameters to be optimized and updated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="s1">&#39;param&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="c1"># Parameters to be updated for the configuration optimizer</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [Parameter (name=param, shape=(1,), dtype=Float32, requires_grad=True), Parameter (name=conv.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True)]
</pre></div>
</div>
<p>You can manually change the default value of the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute of <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> in the network weight to determine which parameters need to be updated.</p>
<p>As shown in the following example, use the <code class="docutils literal notranslate"><span class="pre">net.get_parameters()</span></code> method to obtain all parameters on the network and manually change the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute of the convolutional parameter to False. During the training, only non-convolutional parameters are updated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conv_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
<span class="k">for</span> <span class="n">conv_param</span> <span class="ow">in</span> <span class="n">conv_params</span><span class="p">:</span>
    <span class="n">conv_param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [Parameter (name=param, shape=(1,), dtype=Float32, requires_grad=True)]
</pre></div>
</div>
</section>
<section id="learning-rate">
<h3>Learning Rate<a class="headerlink" href="#learning-rate" title="Permalink to this headline"></a></h3>
<p>As a common hyperparameter in machine learning and deep learning, the learning rate has an important impact on whether the target function can converge to the local minimum value and when to converge to the minimum value. If the learning rate is too high, the target function may fluctuate greatly and it is difficult to converge to the optimal value. If the learning rate is too low, the convergence process takes a long time. In addition to setting a fixed learning rate, MindSpore also supports setting a dynamic learning rate. These methods can significantly improve the convergence efficiency on a deep learning network.</p>
<section id="fixed-learning-rate">
<h4>Fixed Learning Rate<a class="headerlink" href="#fixed-learning-rate" title="Permalink to this headline"></a></h4>
<p>When a fixed learning rate is used, the <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> input by the optimizer is a floating-point tensor or scalar tensor.</p>
<p>Take <code class="docutils literal notranslate"><span class="pre">nn.Momentum</span></code> as an example. The fixed learning rate is 0.01. The following is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the learning rate to 0.01.</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="dynamic-learning-rate">
<h4>Dynamic Learning Rate<a class="headerlink" href="#dynamic-learning-rate" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code> provides the dynamic learning rate module, which is classified into the Dynamic LR function and LearningRateSchedule class. The Dynamic LR function pre-generates a learning rate list whose length is <code class="docutils literal notranslate"><span class="pre">total_step</span></code> and transfers the list to the optimizer for use. During training, the value of the ith learning rate is used as the learning rate of the current step in step <code class="docutils literal notranslate"><span class="pre">i</span></code>. The value of <code class="docutils literal notranslate"><span class="pre">total_step</span></code> cannot be less than the total number of training steps. The LearningRateSchedule class transfers the instance to the optimizer, and the optimizer computes the current learning rate based on the current step.</p>
<ul>
<li><p>Dynamic LR function</p>
<p>Currently, the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/mindspore.nn.html#dynamic-lr-function">Dynamic LR function</a> can compute the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.cosine_decay_lr</span></code>) based on the cosine decay function, the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.exponential_decay_lr</span></code>) based on the exponential decay function, the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.inverse_decay_lr</span></code>) based on the counterclockwise decay function, and the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.natural_exp_decay_lr</span></code>) based on the natural exponential decay function, the piecewise constant learning rate (<code class="docutils literal notranslate"><span class="pre">nn.piecewise_constant_lr</span></code>), the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.polynomial_decay_lr</span></code>) based on the polynomial decay function, and the warm-up learning rate (<code class="docutils literal notranslate"><span class="pre">nn.warmup_lr</span></code>).</p>
<p>The following uses <code class="docutils literal notranslate"><span class="pre">nn.piecewise_constant_lr</span></code> as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">milestone</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">piecewise_constant_lr</span><span class="p">(</span><span class="n">milestone</span><span class="p">,</span> <span class="n">learning_rates</span><span class="p">)</span>

<span class="c1"># Print the learning rate.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="c1"># The optimizer sets the network parameters to be optimized and the piecewise constant learning rate.</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [0.1, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
</pre></div>
</div>
</li>
<li><p>LearningRateSchedule Class</p>
<p>Currently, the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/mindspore.nn.html#learningrateschedule-class">LearningRateSchedule class</a> can compute the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.CosineDecayLR</span></code>) based on the cosine decay function, the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.ExponentialDecayLR</span></code>) based on the exponential decay function, the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.InverseDecayLR</span></code>) based on the counterclockwise decay function, the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.NaturalExpDecayLR</span></code>) based on the natural exponential decay function, the learning rate (<code class="docutils literal notranslate"><span class="pre">nn.PolynomialDecayLR</span></code>) based on the polynomial decay function, and warm-up learning rate (<code class="docutils literal notranslate"><span class="pre">nn.WarmUpLR</span></code>).</p>
<p>In the following example, the learning rate <code class="docutils literal notranslate"><span class="pre">nn.ExponentialDecayLR</span></code> is computed based on the exponential decay function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Initial value of the learning rate</span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.9</span>     <span class="c1"># Decay rate</span>
<span class="n">decay_steps</span> <span class="o">=</span> <span class="mi">4</span>      <span class="c1">#Number of decay steps</span>
<span class="n">step_per_epoch</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">exponential_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ExponentialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">decay_steps</span><span class="p">):</span>
<span class="n">step</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exponential_decay_lr</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;step</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, lr:</span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="c1"># The optimizer sets the learning rate and computes the learning rate based on the exponential decay function.</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">exponential_decay_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    step1, lr:0.1
    step2, lr:0.097400375
    step3, lr:0.094868325
    step4, lr:0.09240211
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="weight-decay">
<h3>Weight Decay<a class="headerlink" href="#weight-decay" title="Permalink to this headline"></a></h3>
<p>Weight decay, also referred to as L2 regularization, is a method for mitigating overfitting of a deep neural network.</p>
<p>Generally, the value range of <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> is <span class="math notranslate nohighlight">\( [0,1) \)</span>, and the default value is 0.0, indicating that the weight decay policy is not used.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                        <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition, MindSpore supports dynamic weight decay. In this case, <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> is a customized Cell called <code class="docutils literal notranslate"><span class="pre">weight_decay_schedule</span></code>. During training, the optimizer calls the instance of the Cell and transfers <code class="docutils literal notranslate"><span class="pre">global_step</span></code> to compute the <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> value of the current step. <code class="docutils literal notranslate"><span class="pre">global_step</span></code> is an internally maintained variable. The value of <code class="docutils literal notranslate"><span class="pre">global_step</span></code> increases by 1 each time a step is trained. Note that the <code class="docutils literal notranslate"><span class="pre">construct</span></code> of the customized <code class="docutils literal notranslate"><span class="pre">weight_decay_schedule</span></code> receives only one input. The following is an example of exponential decay during training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">class</span> <span class="nc">ExponentialWeightDecay</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialWeightDecay</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="n">decay_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span> <span class="o">=</span> <span class="n">decay_steps</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="c1"># The `construct` can have only one input. During training, the global step is automatically transferred for computation.</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="n">weight_decay</span> <span class="o">=</span> <span class="n">ExponentialWeightDecay</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                        <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hyperparameter-grouping">
<h3>Hyperparameter Grouping<a class="headerlink" href="#hyperparameter-grouping" title="Permalink to this headline"></a></h3>
<p>The optimizer can also set options for different parameters separately. In this case, a dictionary list is transferred instead of variables. Each dictionary corresponds to a group of parameter values. Available keys in the dictionary include <code class="docutils literal notranslate"><span class="pre">params</span></code>, <code class="docutils literal notranslate"><span class="pre">lr</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>, and <code class="docutils literal notranslate"><span class="pre">grad_centralizaiton</span></code>, and <code class="docutils literal notranslate"><span class="pre">value</span></code> indicates the corresponding value.</p>
<p><code class="docutils literal notranslate"><span class="pre">params</span></code> is mandatory, and other parameters are optional. If <code class="docutils literal notranslate"><span class="pre">params</span></code> is not configured, the parameter values set when the optimizer is defined are used. During grouping, the learning rate can be a fixed learning rate or a dynamic learning rate, and <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> can be a fixed value.</p>
<p>In the following example, different learning rates and weight decay parameters are set for convolutional and non-convolutional parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="c1"># Convolutional parameter</span>
<span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="c1"># Non-convolutional parameter</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>

<span class="c1"># Fixed learning rate</span>
<span class="n">fix_lr</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># Computation of Learning Rate Based on Polynomial Decay Function</span>
<span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>      <span class="c1"># Initial learning rate</span>
                                           <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="c1"># Final the learning rate</span>
                                           <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>          <span class="c1">#Number of decay steps</span>
                                           <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>              <span class="c1"># Polynomial power</span>

<span class="c1"># The convolutional parameter uses a fixed learning rate of 0.001, and the weight decay is 0.01.</span>
<span class="c1"># The non-convolutional parameter uses a dynamic learning rate, and the weight decay is 0.0.</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">fix_lr</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">polynomial_decay_lr</span><span class="p">}]</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>Except a few optimizers (such as AdaFactor and FTRL), MindSpore supports grouping of learning rates. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/mindspore.nn.html#optimizer">Optimizer API</a>.</p>
</div></blockquote>
</section>
</section>
<section id="customized-optimizer">
<h2>Customized Optimizer<a class="headerlink" href="#customized-optimizer" title="Permalink to this headline"></a></h2>
<p>In addition to the optimizers provided by MindSpore, you can customize optimizers.</p>
<p>When customizing an optimizer, you need to inherit the optimizer base class <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/nn/mindspore.nn.Optimizer.html#mindspore.nn.Optimizer">nn.Optimizer</a> and rewrite the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="docutils literal notranslate"><span class="pre">construct</span></code> methods to set the parameter update policy.</p>
<p>The following example implements the customized optimizer Momentum (SGD algorithm with momentum):</p>
<div class="math notranslate nohighlight">
\[ v_{t+1} = v_t×u+grad \tag{1} \]</div>
<div class="math notranslate nohighlight">
\[p_{t+1} = p_t - lr*v_{t+1} \tag{2} \]</div>
<p><span class="math notranslate nohighlight">\(grad\)</span>, <span class="math notranslate nohighlight">\(lr\)</span>, <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(v\)</span>, and <span class="math notranslate nohighlight">\(u\)</span> respectively represent a gradient, a learning rate, a weight parameter, a momentum parameter, and an initial speed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Momentum</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define the optimizer.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Momentum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;momentum&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moments</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;moments&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The input of construct is gradient. Gradients are automatically transferred during training.&quot;&quot;&quot;</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="c1"># Weight parameter to be updated</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
            <span class="c1"># Update the moments value.</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">moments</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">+</span> <span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">update</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">lr</span> <span class="c1"># SGD algorithm with momentum</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">update</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">params</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="c1"># Set the parameter to be optimized and the learning rate of the optimizer to 0.01.</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">mindSpore.ops</span></code> also encapsulates optimizer operators for users to define optimizers, such as <code class="docutils literal notranslate"><span class="pre">ops.ApplyCenteredRMSProp</span></code>, <code class="docutils literal notranslate"><span class="pre">ops.ApplyMomentum</span></code>, and <code class="docutils literal notranslate"><span class="pre">ops.ApplyRMSProp</span></code>. The following example uses the <code class="docutils literal notranslate"><span class="pre">ApplyMomentum</span></code> operator to customize the optimizer Momentum:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Momentum</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define the optimizer.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Momentum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moments</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;moments&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ApplyMomentum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
        <span class="c1"># Weight parameter to be updated</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">success</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">mom</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
            <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">mom</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">success</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="c1"># Set the parameter to be optimized and the learning rate of the optimizer to 0.01.</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="loss.html" class="btn btn-neutral float-left" title="Loss Function" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../dataset.html" class="btn btn-neutral float-right" title="Advanced Data Processing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>