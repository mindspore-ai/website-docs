

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Fixed Randomness to Reproduce Run Results of Script &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance Tuning" href="performance_optimization.html" />
    <link rel="prev" title="Applying PyNative Mode" href="pynative.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="function_debug.html">Function Debug</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="error_analyze.html">Error Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_debug.html">Custom Debugging Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindir.html">Reading IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="pynative_debug.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="pynative.html">Applying PyNative Mode</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Fixed Randomness to Reproduce Run Results of Script</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/master/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Fault Recovery</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="function_debug.html">Function Debug</a> &raquo;</li>
        
      <li>Fixed Randomness to Reproduce Run Results of Script</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/debug/fixing_randomness.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="fixed-randomness-to-reproduce-run-results-of-script">
<h1>Fixed Randomness to Reproduce Run Results of Script<a class="headerlink" href="#fixed-randomness-to-reproduce-run-results-of-script" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/debug/fixing_randomness.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<p>The purpose of fixed randomness is to reproduce the run results of the script and assist in locating the problem. After fixing the randomness, the loss curve produced by the two trainings under the same conditions should be basically the same, and you can perform debugging multiple times to easily find the cause of the loss curve abnormality without worrying about the problem phenomenon of the last debugging and no longer appearing in this run.</p>
<p>Note that even after all fixable randomities have been fixed, it may not be possible to accurately reproduce the results of the run on MindSpore. Especially when the MindSpore version (commit id) is used at the same time, or the machine executing the script is not the same machine, or the AI training accelerator executing the script is not the same physical device, even using the same seed may not be able to reproduce the running results.</p>
<p>After fixing the randomness, there may be a decrease in running performance, so it is recommended that after the problem is fixed, the randomness is unsettled and the relevant script changes are removed so as not to affect the normal running performance of the script.</p>
<p>This document applies to Graph mode on Ascend.</p>
<div class="section" id="steps-to-fix-the-randomness-of-the-mindspore-script">
<h2>Steps to Fix the Randomness of the MindSpore Script<a class="headerlink" href="#steps-to-fix-the-randomness-of-the-mindspore-script" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Insert code at the beginning of the script you want to execute to pin the global random number of seeds.</p>
<p>Random number of seeds that need to be fixed include MindSpore global random number seeds <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.set_seed.html#mindspore.set_seed">mindspore.set_seed(1)</a>, numpy and other tripartite libraries of global random number of seeds <code class="docutils literal notranslate"><span class="pre">numpy.random.seed(1)</span></code> , and Python random number of seeds <code class="docutils literal notranslate"><span class="pre">random.seed(1)</span></code> etc. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">import</span> <span class="nn">mindspore</span>

<span class="n">mindspore</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Fix hyperparameter.</p>
<p>It is recommended to specify each hyperparameter with a clear value, and if it involves dynamic learning rate, please ensure that the parameters of the generated dynamic learning rate are determined. Avoid using superparameters with randomness.</p>
</li>
<li><p>Fix initialization weights.</p>
<p>It is recommended to fix the initialization weights by loading a fixed checkpoint file. When loading checkpoint, make sure that the file is fully loaded, and cannot pop out some keys before loading.</p>
</li>
<li><p>Fix data processing methods and data order.</p>
<p>(1) Remove or replace all random data processing operations (for example removing <a class="reference external" href="https://mindspore.cn/docs/en/master/api_python/dataset_vision/mindspore.dataset.vision.RandomHorizontalFlip.html#mindspore.dataset.vision.RandomHorizontalFlip">RandomHorizontalFlip</a> and replace <a class="reference external" href="https://mindspore.cn/docs/en/master/api_python/dataset_vision/mindspore.dataset.vision.RandomCrop.html#mindspore.dataset.vision.RandomCrop">RandomCrop</a> with <a class="reference external" href="https://mindspore.cn/docs/en/master/api_python/dataset_vision/mindspore.dataset.vision.Crop.html#mindspore.dataset.vision.Crop">Crop</a>). Random operations refer to data processing operations with Random in all names.</p>
<p>(2) Set <code class="docutils literal notranslate"><span class="pre">shuffle=False</span></code> to turn off shuffle. Do not use the sampler of the dataset.</p>
<p>(3) Set the <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter to 1 to avoid the effect of parallel data processing on the data order.</p>
<p>(4) If you need to start training from an iteration, you can use the <code class="docutils literal notranslate"><span class="pre">dataset.skip()</span></code> interface to skip the data from the previous iteration.
The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Fix network.</p>
<p>Remove operators with randomness in the network, such as DropOut operators and operators with Random in the name. If some random operators really can’t be removed, you should set a fixed random number seed (random number seed is recommended to choose a number other than 0). DropOut operator randomness is difficult to fix in some scenarios, and it is recommended to always delete it. The known random operators include: <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore.ops.primitive.html#random-generation-operator">Random Operators</a>, all the DropOut operators, such as <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Dropout.html#mindspore.ops.Dropout">Dropout</a>, <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Dropout2D.html#mindspore.ops.Dropout2D">Dropout2D</a>, <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Dropout3D.html#mindspore.ops.Dropout3D">Dropout3D</a>.</p>
<p>In addition, there are special operators on the Ascend backend that carry a slight randomness when calculating, which does not cause errors in the calculation results, but only causes the calculation results to produce a slight difference between the two calculations that enter the same. For networks containing these special operators, the difference in loss values between the two runs of the script due to error accumulation will increase significantly, and the criteria for judging whether the loss values provided in this article are consistent are not applicable. A list of special operators on the Ascend backend is found at the end of this article.</p>
</li>
<li><p>Confirm that the randomness was successfully fixed.</p>
<p>Run the training script twice in the same environment and check the loss curve to determine whether the randomness is successfully fixed. It is recommended to run the script in non-sink mode to get the loss value for each iteration of the script, and then you can compare the loss value of the first two iterations. The reason why it is not recommended to use the sinking mode is that the loss value of each epoch can generally only be obtained in the sinking mode, because there are many iterations experienced in an epoch, and the accumulation of randomness may make there is a significant gap in the loss value of the epoch granularity of the two runs, which cannot be used as the basis for whether the randomness is fixed or not.</p>
<p>Successful fixing of randomness requires the following two conditions:</p>
<p>(1) Run the script twice, and the loss value of the first iteration satisfies atol=1e-3, and <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.allclose.html">numpy.allclose()</a> is True. This indicates that the randomness of the forward propagation of the network has been fixed.</p>
<p>(2) Run the script twice, and the loss value of the second iteration satisfies atol=1e-3, and <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.allclose.html">numpy.allclose()</a> is True. This indicates that the randomness of forward and backpropagation of the network has been fixed.</p>
<p>If the above two conditions cannot be met at the same time, it should be checked whether the above fixed randomness steps are in place. If the operation of fixing randomness has been done, but the script is run twice, and the loss values of the first two iterations are still inconsistent, please <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues/new">Set New issue to ask MindSpore for help</a>.</p>
<p>We provide a <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/sample_code/mindinsight/fix_randomness/fix_randomness.py">sample code</a> that successfully fixed randomness, which performs 2 iterations of training. As can be seen by running this code twice, the loss value of the first iteration of the two trainings satisfies the numpy.allclose() function, and the loss value of the second iteration of the two trainings satisfies the numpy.allclose() function, indicating that the randomness of the network is fixed.</p>
</li>
</ol>
</div>
<div class="section" id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>This document is primarily for the <code class="docutils literal notranslate"><span class="pre">GRAPH_MODE</span></code> training script on the Ascend backend.</p></li>
<li><p>The list of special operators on the Ascend backend is as follows, which have a slight randomness when calculated:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.DynamicGRUV2.html#mindspore.ops.DynamicGRUV2">DynamicGRUV2</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.DynamicRNN.html#mindspore.ops.DynamicRNN">DynamicRNN</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.LayerNorm.html#mindspore.ops.LayerNorm">LayerNorm</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.NLLLoss.html#mindspore.ops.NLLLoss">NLLLoss</a></p></li>
<li><p>BNTrainingReduce: When you use the [BatchNorm](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.BatchNorm.html #mindspore.ops.BatchNorm) class operator, the BNTrainingReduce operator is used in forward calculations.</p></li>
<li><p>BNTrainingReduceGrad: When you use the [BatchNorm](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.BatchNorm.html #mindspore.ops.BatchNorm) class operator, the BNTrainingReduceGrad operator is used in the reverse calculation.</p></li>
<li><p>Conv2DBackFilter: When you use the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Conv2D.html#mindspore.ops.Conv2D">Conv2d</a> operator in the network, the reverse calculation uses the Conv2DBackFilter operator.</p></li>
<li><p>Conv3DBackFilter: When you use the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Conv3D.html#mindspore.ops.Conv3D">Conv3d</a> operator in your network, the reverse calculation uses the Conv3DBackFilter operator.</p></li>
<li><p>HcomAllreduce: When you use the [AllReduce](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.AllReduce.html #mindspore.ops.AllReduce) operator in your network, the HcomAllreduce operator may be used in forward computations.</p></li>
<li><p>MaxPool3dGrad: When you use the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.MaxPool3D.html#mindspore.ops.MaxPool3D">MaxPool3D</a> operator in the network, the MaxPool3dGrad operator is used in the reverse calculation.</p></li>
<li><p>ReduceAllD: When you use the [ReduceAll](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ReduceAll.html #mindspore.ops.ReduceAll) operator in your network, the ReduceAllD operator may be used in forward computations.</p></li>
<li><p>ReduceAnyD: When you use the [ReduceAny](https://www.mindspore.cn/docs/zh-CN/master/api_python/ops/mindspore.ops.ReduceAny.html #mindspore.ops.ReduceAny) operator, the ReduceAnyD operator may be used in forward calculations.</p></li>
<li><p>ReduceMaxD: When you use the [ReduceMax](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ReduceMax.html #mindspore.ops.ReduceMax) operator in your network, the ReduceMaxD operator may be used in forward computations.</p></li>
<li><p>ReduceMeanD: When you use the [ReduceMean](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ReduceMean.html #mindspore.ops.ReduceMean) operator in your network, the ReduceMeanD operator may be used in forward calculations.</p></li>
<li><p>ReduceMinD: When you use the [ReduceMin](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ReduceMin.html #mindspore.ops.ReduceMin) operator in your network, the ReduceMinD operator may be used in forward computations.</p></li>
<li><p>ReduceProdD: When you use the [ReduceProd](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ReduceProd.html #mindspore.ops.ReduceProd) operator, the ReduceProdD operator may be used in forward calculations.</p></li>
<li><p>ReduceSum and ReduceSumD: When you use the [ReduceSum](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ReduceSum.html #mindspore.ops.ReduceSum) operator in your network, the ReduceSum or ReduceSum operator may be used in forward calculations.</p></li>
<li><p>RoiAlignGrad: When you use the [ROIAlign](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ROIAlign.html #mindspore.ops.ROIAlign) operator, the StrikeDSliceGrad operator is used in the reverse calculation.</p></li>
<li><p>SquareSum: When you use the [SquareSumAll](https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.SquareSumAll.html #mindspore.ops.SquareSumAll) operator, the SquareSum operator is used in forward computations.</p></li>
<li><p>StridedSliceGrad: When you use the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.StridedSlice.html#mindspore.ops.StridedSlice">StridedSlice</a> operator, the TridEdSliceGrad operator is used in the reverse calculation.</p></li>
</ul>
</li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="performance_optimization.html" class="btn btn-neutral float-right" title="Performance Tuning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="pynative.html" class="btn btn-neutral float-left" title="Applying PyNative Mode" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>