<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Computing Jacobian and Hessian Matrices Using Functional Transformations &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Per-sample-gradients" href="per_sample_gradients.html" />
    <link rel="prev" title="Automatic Vectorization (Vmap)" href="../vmap/vmap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Computing Jacobian and Hessian Matrices Using Functional Transformations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#jacobian-matrices">Jacobian Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#computing-the-jacobian-matrix">Computing the Jacobian Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inverse-mode-computation-of-jacobian-matrix-vs-forward-mode-computation-of-jacobian-matrix">Inverse Mode Computation of Jacobian Matrix vs Forward Mode Computation of Jacobian Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hessian-matrix">Hessian Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#computing-the-hessian-matrix">Computing the Hessian Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#computing-the-batch-jacobian-matrix-and-the-batch-hessian-matrix">Computing the Batch Jacobian Matrix and the Batch Hessian Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#computing-the-hessian-vector-product">Computing the Hessian-vector Product</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Computing Jacobian and Hessian Matrices Using Functional Transformations</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/func_programming/Jacobians_Hessians.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="computing-jacobian-and-hessian-matrices-using-functional-transformations">
<h1>Computing Jacobian and Hessian Matrices Using Functional Transformations<a class="headerlink" href="#computing-jacobian-and-hessian-matrices-using-functional-transformations" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/func_programming/Jacobians_Hessians.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="jacobian-matrices">
<h2>Jacobian Matrices<a class="headerlink" href="#jacobian-matrices" title="Permalink to this headline"></a></h2>
<p>Before describing the methods provided by MindSpore to compute Jacobian matrices, an introduction to Jacobian matrices is first given.</p>
<p>We first define a mapping <span class="math notranslate nohighlight">\(\textbf{f}\)</span>:</p>
<div class="math notranslate nohighlight">
\[R^{n} \longrightarrow R^{m}\]</div>
<p>We use the notation <span class="math notranslate nohighlight">\(\longmapsto\)</span> here to denote the mapping between the elements of the set, and bold all the symbols representing vectors:</p>
<div class="math notranslate nohighlight">
\[\textbf{x} \longmapsto \textbf{f}(\textbf{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(\textbf{x} = (x_{1}, x_{2},\dots, x_{n})\)</span>，<span class="math notranslate nohighlight">\(\textbf{f(x)} = (f_{1}(\textbf{x}), f_{2}(\textbf{x}),\dots,f_{m}(\textbf{x}))\)</span>.</p>
<p>We denote the set consisting of all mappings from <span class="math notranslate nohighlight">\(R^{n}\)</span> to <span class="math notranslate nohighlight">\(R^{m}\)</span> as <span class="math notranslate nohighlight">\(F_{n}^{m}\)</span>.
Here, we refer to a mapping from one function (mapping) set to another function (mapping) set as an operation. It is easy to obtain that the gradient operation <span class="math notranslate nohighlight">\(\nabla\)</span> is an operation of n simultaneous partial derivatives on the set of functions <span class="math notranslate nohighlight">\(F_{n}^{1}\)</span>, defined as a mapping from the set of functions <span class="math notranslate nohighlight">\(F_{n}^{1}\)</span> to <span class="math notranslate nohighlight">\(F_{n}^{n}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\nabla：F_{n}^{1} \longrightarrow F_{n}^{n}\]</div>
<p>The generalized gradient operation, <span class="math notranslate nohighlight">\(\partial\)</span>, is defined as the simultaneous operation of n partial derivatives on the set of functions <span class="math notranslate nohighlight">\(F_{n}^{m}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\partial: F_{n}^{m} \longrightarrow F_{n}^{m \times n}\]</div>
<p>The Jacobian matrix is the result obtained by applying the operation <span class="math notranslate nohighlight">\(\partial\)</span> to <span class="math notranslate nohighlight">\(\textbf{f}\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[\textbf{f} \longmapsto \partial \textbf{f} = (\frac{\partial \textbf{f}}{\partial x_{1}}, \frac{\partial \textbf{f}}{\partial x_{2}}, \dots, \frac{\partial \textbf{f}}{\partial x_{n}})\]</div>
<p>Obtain the Jacobian matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}J_{f} = \begin{bmatrix}
  \frac{\partial f_{1}}{\partial x_{1}} &amp;\frac{\partial f_{1}}{\partial x_{2}} &amp;\dots &amp;\frac{\partial f_{1}}{\partial x_{n}}  \\
  \frac{\partial f_{2}}{\partial x_{1}} &amp;\frac{\partial f_{2}}{\partial x_{2}} &amp;\dots &amp;\frac{\partial f_{2}}{\partial x_{n}} \\
  \vdots &amp;\vdots &amp;\ddots &amp;\vdots \\
  \frac{\partial f_{m}}{\partial x_{1}} &amp;\frac{\partial f_{m}}{\partial x_{2}} &amp;\dots &amp;\frac{\partial f_{m}}{\partial x_{n}}
\end{bmatrix}\end{split}\]</div>
<p>Applications of Jacobian matrix: In the forward mode of automatic differentiation, for each forward propagation, we can work out one row of the Jacobian matrix. In the inverse mode of automatic differentiation, for each backward propagation, we can compute a row of the Jacobian matrix.</p>
</section>
<section id="computing-the-jacobian-matrix">
<h2>Computing the Jacobian Matrix<a class="headerlink" href="#computing-the-jacobian-matrix" title="Permalink to this headline"></a></h2>
<p>It is difficult to compute Jacobian matrices efficiently using standard automatic differentiation systems, but MindSpore provides methods that can compute Jacobian efficiently, which are described below.</p>
<p>First we define the function <code class="docutils literal notranslate"><span class="pre">forecast</span></code>, which is a simple linear function with a non-linear activation function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">jacrev</span><span class="p">,</span> <span class="n">jacfwd</span><span class="p">,</span> <span class="n">vmap</span><span class="p">,</span> <span class="n">vjp</span><span class="p">,</span> <span class="n">jvp</span><span class="p">,</span> <span class="n">grad</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">mindspore</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forecast</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
</pre></div>
</div>
<p>Next, we construct some data: a weight tensor <code class="docutils literal notranslate"><span class="pre">weight</span></code>, a bias tensor <code class="docutils literal notranslate"><span class="pre">bias</span></code>, and an input vector <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">forecast</span></code> does the following mapping transformation on the input vector <code class="docutils literal notranslate"><span class="pre">x</span></code>, <span class="math notranslate nohighlight">\(R^{D}\overset{}{\rightarrow}R^{D}\)</span>.
MindSpore computes the vector-jacobian product during automatic differentiation. To compute the full Jacobian matrix for the mapping <span class="math notranslate nohighlight">\(R^{D}\overset{}{\rightarrow}R^{D}\)</span>, we compute it one row at a time using different unit vectors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">partial_forecast</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>

<span class="n">_</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">partial_forecast</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_jac_matrix</span><span class="p">(</span><span class="n">unit_vectors</span><span class="p">):</span>
    <span class="n">jacobian_rows</span> <span class="o">=</span> <span class="p">[</span><span class="n">vjp_fn</span><span class="p">(</span><span class="n">vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">unit_vectors</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">jacobian_rows</span><span class="p">)</span>


<span class="n">unit_vectors</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">jacobian</span> <span class="o">=</span> <span class="n">compute_jac_matrix</span><span class="p">(</span><span class="n">unit_vectors</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jacobian</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(16, 16)
[-3.2045446e-05 -1.3530695e-05  1.8671712e-05 -9.6547810e-05
  5.9755850e-05 -5.1343523e-05  1.3528993e-05 -4.6988782e-05
 -4.5517798e-05 -6.1188715e-05 -1.6264191e-04  5.5033437e-05
 -4.3497541e-05  2.2357668e-05 -1.3188722e-04 -3.0677278e-05]
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">compute_jac_matrix</span></code>, calculating the jacobian matrix using a for loop row by row is not very efficient. MindSpore provides <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> to calculate the jacobian matrix, and the implementation of <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> makes use of <code class="docutils literal notranslate"><span class="pre">vmap</span></code>, which removes the for loop in <code class="docutils literal notranslate"><span class="pre">compute_jac_matrix</span></code> and vectorizes the entire calculation process. The argument <code class="docutils literal notranslate"><span class="pre">grad_position</span></code> to <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> specifies the Jacobian matrix with respect to which the output is computed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">jacrev</span>
<span class="n">jacrev_jacobian</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">jacrev_jacobian</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">jacobian</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p>The performance of <code class="docutils literal notranslate"><span class="pre">compute_jac_matrix</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> is compared. Typically, <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> has better performance because using <code class="docutils literal notranslate"><span class="pre">vmap</span></code> for vectorization will more fully utilize the hardware to compute multiple sets of data at the same time, reducing the computational overhead for better performance.</p>
<p>Let’s write a function that evaluates the performance of both methods on a microsecond scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">perf_compution</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">run_times</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">run_times</span><span class="p">):</span>
        <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">cost_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000000</span>
    <span class="k">return</span> <span class="n">cost_time</span>


<span class="n">run_times</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">xp</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">compute_jac_matrix_cost_time</span> <span class="o">=</span> <span class="n">perf_compution</span><span class="p">(</span><span class="n">compute_jac_matrix</span><span class="p">,</span> <span class="n">run_times</span><span class="p">,</span> <span class="n">xp</span><span class="p">)</span>
<span class="n">jac_fn</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">jacrev_cost_time</span> <span class="o">=</span> <span class="n">perf_compution</span><span class="p">(</span><span class="n">jac_fn</span><span class="p">,</span> <span class="n">run_times</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;compute_jac_matrix run </span><span class="si">{</span><span class="n">run_times</span><span class="si">}</span><span class="s2"> times, cost time </span><span class="si">{</span><span class="n">compute_jac_matrix_cost_time</span><span class="si">}</span><span class="s2"> microseconds.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;jacrev run </span><span class="si">{</span><span class="n">run_times</span><span class="si">}</span><span class="s2"> times, cost time </span><span class="si">{</span><span class="n">jacrev_cost_time</span><span class="si">}</span><span class="s2"> microseconds.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>compute_jac_matrix run 500 times, cost time 12942823.04868102 microseconds.
jacrev run 500 times, cost time 909309.7001314163 microseconds.
</pre></div>
</div>
<p>Run <code class="docutils literal notranslate"><span class="pre">compute_jac_matrix</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> 500 times respectively and count the time consumption.</p>
<p>The following calculates the percentage performance improvement of using <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> to compute the Jacobian matrix compared to using <code class="docutils literal notranslate"><span class="pre">compute_jac_matrix</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">perf_cmp</span><span class="p">(</span><span class="n">first</span><span class="p">,</span> <span class="n">first_descriptor</span><span class="p">,</span> <span class="n">second</span><span class="p">,</span> <span class="n">second_descriptor</span><span class="p">):</span>
    <span class="n">faster</span> <span class="o">=</span> <span class="n">second</span>
    <span class="n">slower</span> <span class="o">=</span> <span class="n">first</span>
    <span class="n">gain</span> <span class="o">=</span> <span class="p">(</span><span class="n">slower</span> <span class="o">-</span> <span class="n">faster</span><span class="p">)</span> <span class="o">/</span> <span class="n">slower</span>
    <span class="k">if</span> <span class="n">gain</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">gain</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">final_gain</span> <span class="o">=</span> <span class="n">gain</span><span class="o">*</span><span class="mi">100</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; Performance delta: </span><span class="si">{</span><span class="n">final_gain</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> percent improvement with </span><span class="si">{</span><span class="n">second_descriptor</span><span class="si">}</span><span class="s2">. &quot;</span><span class="p">)</span>

<span class="n">perf_cmp</span><span class="p">(</span><span class="n">compute_jac_matrix_cost_time</span><span class="p">,</span> <span class="s2">&quot;for loop&quot;</span><span class="p">,</span> <span class="n">jacrev_cost_time</span><span class="p">,</span> <span class="s2">&quot;jacrev&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> Performance delta: 92.9744 percent improvement with jacrev.
</pre></div>
</div>
<p>It is also possible to compute the Jacobian matrix of the output with respect to the model parameters weight and bias by specifying the parameter <code class="docutils literal notranslate"><span class="pre">grad_position</span></code> of <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">jacrev_weight</span><span class="p">,</span> <span class="n">jacrev_bias</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jacrev_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jacrev_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(16, 16, 16)
(16, 16)
</pre></div>
</div>
</section>
<section id="inverse-mode-computation-of-jacobian-matrix-vs-forward-mode-computation-of-jacobian-matrix">
<h2>Inverse Mode Computation of Jacobian Matrix vs Forward Mode Computation of Jacobian Matrix<a class="headerlink" href="#inverse-mode-computation-of-jacobian-matrix-vs-forward-mode-computation-of-jacobian-matrix" title="Permalink to this headline"></a></h2>
<p>MindSpore provides two APIs to compute Jacobian matrices: <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> and <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code>, respectively.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">jacrev</span></code>: Automatic differentiation using inverse mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">jacfwd</span></code>: Automatic differentiation using forward mode.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> are interchangeable, but they perform differently in different scenarios.</p>
<p>In general, if one needs to compute the Jacobian matrix of a function <span class="math notranslate nohighlight">\(R^{n}\overset{}{\rightarrow}R^{m}\)</span>, <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> performs better in terms of performance when the size of the output vector of that function is larger than the size of the input vector (i.e., m &gt; n), otherwise <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> performs better in terms of performance.</p>
<p>The following is a non-rigorous argument for this conclusion, in that the forward mode auto-differentiation (computing the Jacobian - vector product) is computed column by column, and in the reverse mode auto-differentiation (computing the vector - Jacobian product) is computed row by row for the Jacobian matrix. Assuming that the size of the Jacobian matrix to be computed is m rows and n columns, we recommend using <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> to compute the Jacobian matrix column-by-column if m &gt; n, and vice versa if m &lt; n, we recommend using <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> to compute the Jacobian matrix row-by-row.</p>
</section>
<section id="hessian-matrix">
<h2>Hessian Matrix<a class="headerlink" href="#hessian-matrix" title="Permalink to this headline"></a></h2>
<p>Before describing the methods provided by MindSpore to compute the Hessian matrix, the Hessian matrix is first introduced.</p>
<p>The Hessian matrix can be obtained from the composite of the gradient operation <span class="math notranslate nohighlight">\(\nabla\)</span> and the generalized gradient operation <span class="math notranslate nohighlight">\(\partial\)</span>, namely</p>
<div class="math notranslate nohighlight">
\[\nabla \circ \partial: F_{n}^{1} \longrightarrow F_{n}^{n} \longrightarrow F_{n \times n}^{n}\]</div>
<p>Applying this composite operation to f yields that</p>
<div class="math notranslate nohighlight">
\[f \longmapsto \nabla f \longmapsto J_{\nabla f}\]</div>
<p>The Hessian matrix is obtained:</p>
<div class="math notranslate nohighlight">
\[\begin{split}H_{f} = \begin{bmatrix}
  \frac{\partial (\nabla _{1}f)}{\partial x_{1}} &amp;\frac{\partial (\nabla _{1}f)}{\partial x_{2}} &amp;\dots &amp;\frac{\partial (\nabla _{1}f)}{\partial x_{n}}  \\
  \frac{\partial (\nabla _{2}f)}{\partial x_{1}} &amp;\frac{\partial (\nabla _{2}f)}{\partial x_{2}} &amp;\dots &amp;\frac{\partial (\nabla _{2}f)}{\partial x_{n}} \\
  \vdots &amp;\vdots &amp;\ddots &amp;\vdots \\
  \frac{\partial (\nabla _{n}f)}{\partial x_{1}} &amp;\frac{\partial (\nabla _{n}f)}{\partial x_{2}} &amp;\dots &amp;\frac{\partial (\nabla _{n}f)}{\partial x_{n}}
\end{bmatrix} = \begin{bmatrix}
  \frac{\partial ^2 f}{\partial x_{1}^{2}} &amp;\frac{\partial ^2 f}{\partial x_{2} \partial x_{1}} &amp;\dots &amp;\frac{\partial ^2 f}{\partial x_{n} \partial x_{1}}  \\
  \frac{\partial ^2 f}{\partial x_{1} \partial x_{2}} &amp;\frac{\partial ^2 f}{\partial x_{2}^{2}} &amp;\dots &amp;\frac{\partial ^2 f}{\partial x_{n} \partial x_{2}} \\
  \vdots &amp;\vdots &amp;\ddots &amp;\vdots \\
  \frac{\partial ^2 f}{\partial x_{1} \partial x_{n}} &amp;\frac{\partial ^2 f}{\partial x_{2} \partial x_{n}} &amp;\dots &amp;\frac{\partial ^2 f}{\partial x_{n}^{2}}
\end{bmatrix}\end{split}\]</div>
<p>It is easy to see that the Hessian matrix is a real symmetric matrix.</p>
<p>Application of Hessian matrix: using the Hessian matrix, we can explore the curvature of the neural network at a certain point, providing a numerical basis for whether the training converges.</p>
</section>
<section id="computing-the-hessian-matrix">
<h2>Computing the Hessian Matrix<a class="headerlink" href="#computing-the-hessian-matrix" title="Permalink to this headline"></a></h2>
<p>In MindSpore, we can compute the Hessian matrix by any combination of <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Din</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">Dout</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Dout</span><span class="p">,</span> <span class="n">Din</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Dout</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Din</span><span class="p">)</span>

<span class="n">hess1</span> <span class="o">=</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">hess2</span> <span class="o">=</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">hess3</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">hess4</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">hess1</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">hess2</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">hess2</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">hess3</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">hess3</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">hess4</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</section>
<section id="computing-the-batch-jacobian-matrix-and-the-batch-hessian-matrix">
<h2>Computing the Batch Jacobian Matrix and the Batch Hessian Matrix<a class="headerlink" href="#computing-the-batch-jacobian-matrix-and-the-batch-hessian-matrix" title="Permalink to this headline"></a></h2>
<p>In the examples given above, we are computing the Jacobian matrix of a single output vector with respect to a single input vector. In some cases, you may want to compute the Jacobian matrix of a batch of output vectors with respect to a batch of input vectors, or in other words, given a batch of input vectors with a shape of (b, n) and a function whose mapping relation is $<span class="math notranslate nohighlight">\(R^{n}\overset{}{\rightarrow}R^{m}\)</span>$. We would expect to get a batch of Jacobian matrices with a shape is (b, m, n).</p>
<p>We can use <code class="docutils literal notranslate"><span class="pre">vmap</span></code> to compute the batch Jacobian matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">Din</span> <span class="o">=</span> <span class="mi">31</span>
<span class="n">Dout</span> <span class="o">=</span> <span class="mi">33</span>

<span class="n">weight</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Dout</span><span class="p">,</span> <span class="n">Din</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Dout</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">Din</span><span class="p">)</span>

<span class="n">compute_batch_jacobian</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">batch_jacobian</span> <span class="o">=</span> <span class="n">compute_batch_jacobian</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_jacobian</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(64, 33, 31)
</pre></div>
</div>
<p>Computing the batch Hessian matrix is similar to computing the batch Jacobian matrix using <code class="docutils literal notranslate"><span class="pre">vmap</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hessian</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">forecast</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">compute_batch_hessian</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">hessian</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">batch_hessian</span> <span class="o">=</span> <span class="n">compute_batch_hessian</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_hessian</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(64, 33, 31, 31)
</pre></div>
</div>
</section>
<section id="computing-the-hessian-vector-product">
<h2>Computing the Hessian-vector Product<a class="headerlink" href="#computing-the-hessian-vector-product" title="Permalink to this headline"></a></h2>
<p>The most straightforward way to compute a Hessian-vector product (hvp) is to compute a complete Hessian matrix and dot-product it with vectors. However, MindSpore provides a better way to compute the Hessian-vector product without computing a complete Hessian matrix. Below we describe two ways to compute the Hessian-Vector product.</p>
<ul class="simple">
<li><p>Combine two reverse mode auto-differential.</p></li>
<li><p>Combine reverse mode auto-differentiation with forward mode auto-differentiation.</p></li>
</ul>
<p>The following first describes, in MindSpore, how to compute the Hessian-vector product, using a combination of reverse mode auto-differentiation and forward mode auto-differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hvp_revfwd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jvp</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="n">result_hvp_revfwd</span> <span class="o">=</span> <span class="n">hvp_revfwd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_hvp_revfwd</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(128,)
</pre></div>
</div>
<p>If forward auto-differentiation does not suffice, we can use a combination of reverse mode auto-differentiation and reverse mode auto-differentiation to compute the Hessian-vector product:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hvp_revrev</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="o">*</span><span class="n">vector</span><span class="p">)</span>

<span class="n">result_hvp_revrev</span> <span class="o">=</span> <span class="n">hvp_revrev</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,),</span> <span class="p">(</span><span class="n">vector</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_hvp_revrev</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(128,)
</pre></div>
</div>
<p>Computing the Hessian-vector product using the two methods above gives the same result.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result_hvp_revfwd</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">result_hvp_revrev</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../vmap/vmap.html" class="btn btn-neutral float-left" title="Automatic Vectorization (Vmap)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="per_sample_gradients.html" class="btn btn-neutral float-right" title="Per-sample-gradients" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>