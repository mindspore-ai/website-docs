<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Per-sample-gradients &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Auto Augmentation" href="../dataset/augment.html" />
    <link rel="prev" title="Computing Jacobian and Hessian Matrices Using Functional Transformations" href="Jacobians_Hessians.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Per-sample-gradients</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/func_programming/per_sample_gradients.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="per-sample-gradients">
<h1>Per-sample-gradients<a class="headerlink" href="#per-sample-gradients" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/func_programming/per_sample_gradients.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<p>Calculating per-sample-gradients means calculating the gradient of each sample in a batch sample. When training a neural network, many deep learning frameworks calculate the gradients of the batch samples and use the gradients of the batch samples to update the network parameters. per-sample-gradients can help us to better improve the training of the model by more accurately calculating the effect of each sample on the network parameters when training the neural network.</p>
<p>Calculating per-sample-gradients is a troblesome business in many deep learning computational frameworks because these frameworks directly accumulate the gradients of the entire batch of samples. Using these frameworks, we can think of a simple way to compute per-sample-gradients, i.e., to compute the loss of the predicted and labeled values for each of the batch samples and to compute the gradient of that loss with respect to the network parameters, but this method is clearly very inefficient.</p>
<p>MindSpore provides us with a more efficient way to calculate per-sample-gradients.</p>
<p>We illustrate the efficient method of computing per-sample-gradients with the example of TD(0) (Temporal Difference) algorithm, which is a reinforcement learning algorithm based on temporal difference that learns the optimal strategy in the absence of an environment model. In the TD(0) algorithm, the valued function estimates are updated according to the current rewards. The TD(0) algorithm is formulated as follows:</p>
<div class="math notranslate nohighlight">
\[V(S_{t}) = V(S_{t}) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_{t}))\]</div>
<p>where <span class="math notranslate nohighlight">\(V(S_{t})\)</span> is the current valued function estimate, <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate, <span class="math notranslate nohighlight">\(R_{t+1}\)</span> is the reward obtained after performing the action in state <span class="math notranslate nohighlight">\(S_{t}\)</span>, <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor, <span class="math notranslate nohighlight">\(V(S_{t+1})\)</span> is the valued function estimate for the next state <span class="math notranslate nohighlight">\(S_{t+1}\)</span>, <span class="math notranslate nohighlight">\(R_{t+1} + \ gamma V(S_{t+1})\)</span> is known as the TD target, and <span class="math notranslate nohighlight">\(R_{t+1} + \gamma V(S_{t+1}) - V(S_{t})\)</span> is known as the TD bias.</p>
<p>By continuously updating the valued function estimates using the TD(0) algorithm, the optimal policy can be learned incrementally to maximize the reward gained in the environment.</p>
<p>Combining jit, vmap and grad in MindSpore, we get a more efficient way to compute per-sample-gradients.</p>
<p>The method is described below, assuming that the estimate <span class="math notranslate nohighlight">\(v_{\theta}\)</span> at state <span class="math notranslate nohighlight">\(s_{t}\)</span> is parameterized by a linear function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">vmap</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">grad</span>


<span class="n">value_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">tensor_dot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
</pre></div>
</div>
<p>Consider the following scenario, transforming from state <span class="math notranslate nohighlight">\(s_{t}\)</span> to state <span class="math notranslate nohighlight">\(s_{t+1}\)</span> and in which we observe a reward of <span class="math notranslate nohighlight">\(r_{t+1}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s_t</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">r_tp1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">s_tp1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
</pre></div>
</div>
<p>The updating volume of the parameter <span class="math notranslate nohighlight">\({\theta}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\Delta{\theta}=(r_{t+1} + v_{\theta}(s_{t+1}) - v_{\theta}(s_{t}))\nabla v_{\theta}(s_{t})\]</div>
<p>The update of the parameter <span class="math notranslate nohighlight">\({\theta}\)</span> is not the gradient of any loss function, however, it can be considered as the gradient of the following pseudo-loss function (assuming that the effect of the target value <span class="math notranslate nohighlight">\(r_{t+1} + v_{\theta}(s_{t+1})\)</span> on the computation of the gradient of <span class="math notranslate nohighlight">\(L(\theta)\)</span> with respect to <span class="math notranslate nohighlight">\({\theta}\)</span> is ignored).</p>
<div class="math notranslate nohighlight">
\[L(\theta) = [r_{t+1} + v_{\theta}(s_{t+1}) - v_{\theta}(s_{t})]^{2}\]</div>
<p>When computing the update of the parameter <span class="math notranslate nohighlight">\({\theta}\)</span> (computing the gradient of <span class="math notranslate nohighlight">\(L(\theta)\)</span> with respect to <span class="math notranslate nohighlight">\({\theta}\)</span>), we need to eliminate the effect of the target value <span class="math notranslate nohighlight">\(r_{t+1} + v_{\theta}(s_{t+1})\)</span> on the computation of the gradient of <span class="math notranslate nohighlight">\({\theta}\)</span> using <code class="docutils literal notranslate"><span class="pre">ops.stop_gradient</span></code>, which can be made such that the target values <span class="math notranslate nohighlight">\(r_{t+1} + v_{\theta}(s_{t+1})\)</span> do not contribute to the derivation of <span class="math notranslate nohighlight">\({\theta}\)</span> during the derivation process to obtain the correct update of the parameter <span class="math notranslate nohighlight">\({\theta}\)</span>.</p>
<p>We give the implementation of the pseudo-loss function <span class="math notranslate nohighlight">\(L(\theta)\)</span> in MindSpore.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">td_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
    <span class="n">v_t</span> <span class="o">=</span> <span class="n">value_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_t</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">r_tp1</span> <span class="o">+</span> <span class="n">value_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tp1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">v_t</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Pass <code class="docutils literal notranslate"><span class="pre">td_loss</span></code> into <code class="docutils literal notranslate"><span class="pre">grad</span></code> and compute the gradient of td_loss with respect to <code class="docutils literal notranslate"><span class="pre">theta</span></code>, i.e., the update of <code class="docutils literal notranslate"><span class="pre">theta</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">td_update</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">td_loss</span><span class="p">)</span>
<span class="n">delta_theta</span> <span class="o">=</span> <span class="n">td_update</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">r_tp1</span><span class="p">,</span> <span class="n">s_tp1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delta_theta</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[-4. -8. -0.]
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">td_update</span></code> computes the gradient of td_loss with respect to the parameter <span class="math notranslate nohighlight">\({\theta}\)</span> based on only one sample. We can vectorize this function using <code class="docutils literal notranslate"><span class="pre">vmap</span></code> which will add a batch dimension to all inputs and outputs. Now, we give a batch of inputs and produce a batch of outputs, with each output element in the output batch corresponding to the corresponding input element in the input batch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batched_s_t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">])</span>
<span class="n">batched_r_tp1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">r_tp1</span><span class="p">,</span> <span class="n">r_tp1</span><span class="p">])</span>
<span class="n">batched_s_tp1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s_tp1</span><span class="p">,</span> <span class="n">s_tp1</span><span class="p">])</span>
<span class="n">batched_theta</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">theta</span><span class="p">,</span> <span class="n">theta</span><span class="p">])</span>

<span class="n">per_sample_grads</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">td_update</span><span class="p">)</span>
<span class="n">batch_theta</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">theta</span><span class="p">,</span> <span class="n">theta</span><span class="p">])</span>
<span class="n">delta_theta</span> <span class="o">=</span> <span class="n">per_sample_grads</span><span class="p">(</span><span class="n">batched_theta</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">,</span> <span class="n">batched_r_tp1</span><span class="p">,</span> <span class="n">batched_s_tp1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delta_theta</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[-4. -8.  0.]
 [-4. -8.  0.]]
</pre></div>
</div>
<p>In the above example, we need to manually pass a batch of <code class="docutils literal notranslate"><span class="pre">theta</span></code> for <code class="docutils literal notranslate"><span class="pre">per_sample_grads</span></code>, but in reality, we can pass just a single <code class="docutils literal notranslate"><span class="pre">theta</span></code>. To complete this, we pass the parameter <code class="docutils literal notranslate"><span class="pre">in_axes</span></code> to <code class="docutils literal notranslate"><span class="pre">vmap</span></code>, where the position corresponding to the parameter <code class="docutils literal notranslate"><span class="pre">theta</span></code> in <code class="docutils literal notranslate"><span class="pre">in_axes</span></code> is set to <code class="docutils literal notranslate"><span class="pre">None</span></code> and the positions corresponding to the other parameters are set to <code class="docutils literal notranslate"><span class="pre">0</span></code>. This allows us to add an additional axis only to parameters other than <code class="docutils literal notranslate"><span class="pre">theta</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inefficiecient_per_sample_grads</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">td_update</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">delta_theta</span> <span class="o">=</span> <span class="n">inefficiecient_per_sample_grads</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">,</span> <span class="n">batched_r_tp1</span><span class="p">,</span> <span class="n">batched_s_tp1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delta_theta</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[-4. -8.  0.]
 [-4. -8.  0.]]
</pre></div>
</div>
<p>Up to this point, the gradient for each sample is calculated correctly, but we can also make the calculation process a bit faster. We call <code class="docutils literal notranslate"><span class="pre">inefficiecient_per_sample_grads</span></code> using <code class="docutils literal notranslate"><span class="pre">jit</span></code>, which will compile <code class="docutils literal notranslate"><span class="pre">inefficiecient_per_sample_grads</span></code> into a callable MindSpore graph and improve the efficiency of its operation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">efficiecient_per_sample_grads</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">inefficiecient_per_sample_grads</span><span class="p">)</span>
<span class="n">delta_theta</span> <span class="o">=</span> <span class="n">efficiecient_per_sample_grads</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">,</span> <span class="n">batched_r_tp1</span><span class="p">,</span> <span class="n">batched_s_tp1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delta_theta</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[-4. -8.  0.]
 [-4. -8.  0.]]
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Jacobians_Hessians.html" class="btn btn-neutral float-left" title="Computing Jacobian and Hessian Matrices Using Functional Transformations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../dataset/augment.html" class="btn btn-neutral float-right" title="Auto Augmentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>