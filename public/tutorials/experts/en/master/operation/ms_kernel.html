<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MindSpore Hybrid Syntax Specification &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom Operator Registration" href="op_custom_adv.html" />
    <link rel="prev" title="Custom Operators (Custom-based)" href="op_custom.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">MindSpore Hybrid Syntax Specification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#syntax-specification">Syntax Specification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#variables">Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#expressions">Expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loop">Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduling-keywords">Scheduling keywords</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attribute">Attribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="#keywords">Keywords</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#frequent-error-messages-and-error-attributions">Frequent Error Messages and Error Attributions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>MindSpore Hybrid Syntax Specification</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/operation/ms_kernel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-hybrid-syntax-specification">
<h1>MindSpore Hybrid Syntax Specification<a class="headerlink" href="#mindspore-hybrid-syntax-specification" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/operation/ms_kernel.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The syntax of MindSpore Hybrid DSL is similar to Python syntax, such as function definitions, indentation, and annotations. Functions written in MindSpore Hybrid DSL can be used as ordinary <code class="docutils literal notranslate"><span class="pre">numpy</span></code> functions after adding <code class="docutils literal notranslate"><span class="pre">kernel</span></code> decorators, or they can be customized operators used for Custom.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">kernel</span>

<span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">outer_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">allocate</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">i2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">d</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span>
                <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">sin</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i2</span><span class="p">,</span> <span class="n">i1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="n">np_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">np_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">outer_product</span><span class="p">(</span><span class="n">np_x</span><span class="p">,</span> <span class="n">np_y</span><span class="p">))</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_x</span><span class="p">)</span>
<span class="n">input_y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_y</span><span class="p">)</span>

<span class="n">test_op_akg</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">outer_product</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">test_op_akg</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="syntax-specification">
<h2>Syntax Specification<a class="headerlink" href="#syntax-specification" title="Permalink to this headline"></a></h2>
<section id="variables">
<h3>Variables<a class="headerlink" href="#variables" title="Permalink to this headline"></a></h3>
<p>There are two kinds of variables in MindSpore Hybrid DSL: tensor variables and scalar variables.</p>
<p>Tensor variables, besides those in the inputs of the function, must be declared with <code class="docutils literal notranslate"><span class="pre">shape</span></code>和 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> before use.</p>
<ul class="simple">
<li><p>declare a output tensor by <code class="docutils literal notranslate"><span class="pre">output_tensor</span></code>, such as <code class="docutils literal notranslate"><span class="pre">output_tensor(shape,</span> <span class="pre">dtype)</span></code>.</p></li>
<li><p>declare an intermediate tensor by <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, such as <code class="docutils literal notranslate"><span class="pre">allocate(shape,</span> <span class="pre">dtype)</span></code>.</p></li>
</ul>
<p>Example of Tensor allocation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># We can use a and b directly as input tensors</span>

    <span class="c1"># d is a tensor with dtype fp16 and shape (2,), and will be used as an intermediate variable in the following code</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">allocate</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>
    <span class="c1"># c is a tensor with same dtype and shape as a, and will be used as a output function in the following code</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># assign value to c by d as the intermediate variable</span>
    <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># c as output</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Scalar variables will regard its first assignment as the declaration. The assignment can be either a number or an expression. The place of the first assignment of a scalar variable defines its scope, such as within a certain level of for loop. Using the variable outside its scope will lead to error.</p>
<p>Example of using Scalar variable:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="c1"># i loop</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="c1"># j loop</span>
            <span class="c1"># assign a number to Scalar d</span>
            <span class="n">d</span> <span class="o">=</span> <span class="mf">2.0</span>
            <span class="c1"># assign an expression to Scalar e</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="c1"># use scalars</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">e</span>

    <span class="c1"># Wrong: c[0, 0] = d</span>
    <span class="c1"># Can&#39;t use Scalar d outside its scope (j loop)</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Unlike native Python language, once a variable is defined, we can’t change its <code class="docutils literal notranslate"><span class="pre">shape</span></code>和 <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</section>
<section id="expressions">
<h3>Expressions<a class="headerlink" href="#expressions" title="Permalink to this headline"></a></h3>
<p>MindSpore Hybrid DSL supports basic math operators, including <code class="docutils literal notranslate"><span class="pre">+,</span> <span class="pre">-,</span> <span class="pre">*,</span> <span class="pre">/</span></code>, as well as self-assign operators, including <code class="docutils literal notranslate"><span class="pre">=,</span> <span class="pre">+=,</span> <span class="pre">-=,</span> <span class="pre">*=,</span> <span class="pre">/=</span></code>.
Users can write codes like writing Python expressions.</p>
<p><strong>All the expressions must be based on scalars. Computation for the tensors must include all indices, such as <code class="docutils literal notranslate"><span class="pre">C[i,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">A[i,</span> <span class="pre">j]</span> <span class="pre">+</span> <span class="pre">B[i,</span> <span class="pre">j]</span></code>. Currently, tensorized codes such as <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">A</span> <span class="pre">+</span> <span class="pre">B</span></code> are not supported.</strong></p>
<p>When writing assignment expressions, users must take care of the dtype of the expression and make them consistent on both sides of the equality. Otherwise, the error might be thrown on the stage of <strong>operator compilation</strong>. Any integer numbers in the expression will be treated as int32, while float numbers will be treated as float32. There is no implicit dtype casting in MindSpore Hybrid DSL, and all dtype casting must be written with dtype names as casting functions, including:</p>
<ul class="simple">
<li><p>int32</p></li>
<li><p>float16</p></li>
<li><p>float32</p></li>
<li><p>(only on gpu backend) int8, int16, int64, float64</p></li>
</ul>
<p>Example of dtype casting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="c1"># Wrong: c[0] = 0.1 c&#39;s dtype is fp16, while 0.1&#39;s dtype is fp32</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">float16</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># float16(0.1) cast the number 0.1 to dtype fp16</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">float16</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># float16(a[0, 0]) cast the number 0.1 to dtype fp16</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
</section>
<section id="loop">
<h3>Loop<a class="headerlink" href="#loop" title="Permalink to this headline"></a></h3>
<p>Currently, only the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is supported. <code class="docutils literal notranslate"><span class="pre">while</span></code>, <code class="docutils literal notranslate"><span class="pre">break</span></code>, and <code class="docutils literal notranslate"><span class="pre">continue</span></code> are illegal in MindSpore Hybrid DSL.</p>
<p>Loops are the same as those in Python. <code class="docutils literal notranslate"><span class="pre">range</span></code> and <code class="docutils literal notranslate"><span class="pre">grid</span></code> are supported to express extents of loops. <code class="docutils literal notranslate"><span class="pre">range</span></code> is for one-dimensional loops and accepts a number as the upper bound of the loop, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
                <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>The iteration space of the above loops is <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">i</span> <span class="pre">&lt;</span> <span class="pre">3,</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">j</span> <span class="pre">&lt;</span> <span class="pre">4,</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">k</span> <span class="pre">&lt;</span> <span class="pre">5</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">grid</span></code> is for multi-dimensional loops and accepts <code class="docutils literal notranslate"><span class="pre">tuple</span></code> as its input. For example, the above code can be also written as follows in <code class="docutils literal notranslate"><span class="pre">grid</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>Right now <code class="docutils literal notranslate"><span class="pre">arg</span></code> is equivalent to a three dimensional index <code class="docutils literal notranslate"><span class="pre">(i,j,k)</span></code>, with upper bound 4, 5, 6 respectively. We also have access to each element in <code class="docutils literal notranslate"><span class="pre">arg</span></code>, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>Then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>.</p>
</section>
<section id="scheduling-keywords">
<h3>Scheduling keywords<a class="headerlink" href="#scheduling-keywords" title="Permalink to this headline"></a></h3>
<p>From version 1.8, MindSpore Hybrid DSL provides scheduling keywords to describe the type of loops. On the Ascend backend, scheduling keywords will help the new DSA polyhedron scheduler generate codes. The scheduling keywords include <code class="docutils literal notranslate"><span class="pre">serial</span></code>, <code class="docutils literal notranslate"><span class="pre">vectorize</span></code>, <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, and <code class="docutils literal notranslate"><span class="pre">reduce</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">serial</span></code> indicates that the scheduler should keep the order of the loop and not apply loop transformations on such loops. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">serial_test</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">serial</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">serial</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">b</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">serial</span></code> indicates that there are dependence relations on <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code>. <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code> should be in ascending order during the scheduling.</p>
<p><code class="docutils literal notranslate"><span class="pre">vectorize</span></code> is usually used in the innermost loop, indicating the chance of generation vector instructions. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">vector_test</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">vectorize</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">vectorize</span></code> indicates that the innermost <code class="docutils literal notranslate"><span class="pre">j</span></code> loop conducts the same computation at each iteration and that the computation can be accelerated via vector instructions.</p>
<p><code class="docutils literal notranslate"><span class="pre">parallel</span></code> is usually used in the outermost loop, prompting the scheduler that the loop has the chance of parallel execution. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">parallel_test</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parallel</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">parallel</span></code> indicates that there is no dependency between each iteration of the <code class="docutils literal notranslate"><span class="pre">i</span></code> loop and that the computation can be accelerated via parallelization.</p>
<p><code class="docutils literal notranslate"><span class="pre">reduce</span></code> indicates that the loop is a reduction axis. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reduce_test</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">reduce</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">reduce</span></code> indicates that <code class="docutils literal notranslate"><span class="pre">k</span></code> is a reduction axis.</p>
<p>Notice that：</p>
<ul class="simple">
<li><p>The scheduling keywords will only influence the scheduling on the Ascend backend. On the CPU or GPU backend, the above scheduling keywords will be treated as the usual <code class="docutils literal notranslate"><span class="pre">for</span></code> keyword.</p></li>
<li><p>The scheduling keywords only provide hints to the scheduler. When the hints from the scheduling keywords contradict the analysis and validation result from the scheduler, the above scheduling keywords will be treated as the usual <code class="docutils literal notranslate"><span class="pre">for</span></code> keyword.</p></li>
</ul>
</section>
<section id="attribute">
<h3>Attribute<a class="headerlink" href="#attribute" title="Permalink to this headline"></a></h3>
<p>Currently we support only tensor’s <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">dtype</span></code> attributes, such as <code class="docutils literal notranslate"><span class="pre">a.shape</span></code>, and <code class="docutils literal notranslate"><span class="pre">c.dtype</span></code>.</p>
<p>The shape attribute of a Tensor is a <code class="docutils literal notranslate"><span class="pre">tuple</span></code>. We have access to its element with a <strong>fixed</strong> index, such as <code class="docutils literal notranslate"><span class="pre">a.shape[0]</span></code>.</p>
<p>Once <code class="docutils literal notranslate"><span class="pre">grid</span></code> accepts one Tensor’s <code class="docutils literal notranslate"><span class="pre">shape</span></code> attribute as its input, the dimension of the loops is the same as the dimension of the Tensor. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>If a is a two dimensional tensor, then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>, while if a is a three dimensional tensor, then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>.</p>
</section>
<section id="keywords">
<h3>Keywords<a class="headerlink" href="#keywords" title="Permalink to this headline"></a></h3>
<p>Currently, we support keywords including:</p>
<ul class="simple">
<li><p>Math keywords (all platform): <code class="docutils literal notranslate"><span class="pre">log</span></code>, <code class="docutils literal notranslate"><span class="pre">exp</span></code>, <code class="docutils literal notranslate"><span class="pre">sqrt</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, <code class="docutils literal notranslate"><span class="pre">power</span></code>, <code class="docutils literal notranslate"><span class="pre">floor</span></code></p></li>
<li><p>Memory allocation: <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, <code class="docutils literal notranslate"><span class="pre">output_tensor</span></code></p></li>
<li><p>Datatype keywords: <code class="docutils literal notranslate"><span class="pre">int32</span></code>, <code class="docutils literal notranslate"><span class="pre">float16</span></code>, <code class="docutils literal notranslate"><span class="pre">float32</span></code>, <code class="docutils literal notranslate"><span class="pre">float64</span></code></p></li>
<li><p>For keywords: <code class="docutils literal notranslate"><span class="pre">for</span></code>, <code class="docutils literal notranslate"><span class="pre">range</span></code>, <code class="docutils literal notranslate"><span class="pre">grid</span></code></p></li>
<li><p>Scheduling keywords: <code class="docutils literal notranslate"><span class="pre">serial</span></code>, <code class="docutils literal notranslate"><span class="pre">vec</span></code>, <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, <code class="docutils literal notranslate"><span class="pre">reduce</span></code></p></li>
<li><p>In current version, advanced keywords are provided for the CPU/GPU backend:</p>
<ul>
<li><p>Math keywords: <code class="docutils literal notranslate"><span class="pre">rsqrt</span></code>, <code class="docutils literal notranslate"><span class="pre">erf</span></code>, <code class="docutils literal notranslate"><span class="pre">isnan</span></code>, <code class="docutils literal notranslate"><span class="pre">sin</span></code>, <code class="docutils literal notranslate"><span class="pre">cos</span></code>, <code class="docutils literal notranslate"><span class="pre">isinf</span></code>, <code class="docutils literal notranslate"><span class="pre">isfinite</span></code>, <code class="docutils literal notranslate"><span class="pre">atan</span></code>, <code class="docutils literal notranslate"><span class="pre">atan2</span></code> (only on GPU), <code class="docutils literal notranslate"><span class="pre">expm1</span></code> (only on GPU), <code class="docutils literal notranslate"><span class="pre">floor</span></code>, <code class="docutils literal notranslate"><span class="pre">ceil</span></code>, <code class="docutils literal notranslate"><span class="pre">trunc</span></code>, <code class="docutils literal notranslate"><span class="pre">round</span></code>, <code class="docutils literal notranslate"><span class="pre">ceil_div</span></code></p></li>
<li><p>Datatype keywords: <code class="docutils literal notranslate"><span class="pre">int8</span></code>, <code class="docutils literal notranslate"><span class="pre">int16</span></code>, <code class="docutils literal notranslate"><span class="pre">int64</span></code></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="frequent-error-messages-and-error-attributions">
<h2>Frequent Error Messages and Error Attributions<a class="headerlink" href="#frequent-error-messages-and-error-attributions" title="Permalink to this headline"></a></h2>
<p>To help users effectively develop and locate bugs, MindSpore Hybrid DSL provides the following error messages, including:</p>
<ul class="simple">
<li><p>TypeError: There are Python keywords such as <code class="docutils literal notranslate"><span class="pre">while</span></code>, <code class="docutils literal notranslate"><span class="pre">break</span></code> and <code class="docutils literal notranslate"><span class="pre">continue</span></code> which are not supported by MindSpore Hybrid DSL.</p></li>
<li><p>ValueError:</p>
<ul>
<li><p>There are built-in function names which are not in the above support list;</p></li>
<li><p>Take properties that are not <code class="docutils literal notranslate"><span class="pre">shape</span></code> or <code class="docutils literal notranslate"><span class="pre">dtype</span></code> for tensors.</p></li>
</ul>
</li>
<li><p>Other frequent error messages:</p>
<ul>
<li><p>“SyntaxError”: DSL does not conform to the Python syntax (not the syntax defined by MindSpore Hybrid DSL), and is reported by the Python interpreter itself</p></li>
<li><p>“ValueError: Compile error” and “The pointer[kernel_mod] is null”: the kernel compiler fails in compiling the DSL. Check error messages from AKG for further information;</p></li>
<li><p>“Launch graph failed”: The compiled kernel fails in running. Check the error message from the hardware. For example, when the kernel fails in Ascend, there will be an “Ascend error occurred” message and corresponding hareware error messages.</p></li>
</ul>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="op_custom.html" class="btn btn-neutral float-left" title="Custom Operators (Custom-based)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="op_custom_adv.html" class="btn btn-neutral float-right" title="Custom Operator Registration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>