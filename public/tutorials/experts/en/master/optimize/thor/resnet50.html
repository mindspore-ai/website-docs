<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Applying Second-Order Optimization Practices on the ResNet-50 Network &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Automatic Vectorization (Vmap)" href="../../vmap/vmap.html" />
    <link rel="prev" title="Introduction to Second-order Optimizer THOR" href="intro.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../thor.html">Second-order Optimization</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">Introduction to Second-order Optimizer THOR</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Applying Second-Order Optimization Practices on the ResNet-50 Network</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#preparing-the-dataset">Preparing the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-distributed-environment-variables">Configuring Distributed Environment Variables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loading-and-processing-the-datasets">Loading and Processing the Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-networks">Defining the Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-loss-function-and-thor-optimizer">Defining the Loss Function and THOR Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-loss-function">Defining the Loss Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-optimizers">Defining the Optimizers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-networks">Training the Networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-model-saving">Configuring Model Saving</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-training-network">Configuring the Training Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-the-script">Running the Script</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-inference">Model Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-inference-network">Defining the Inference Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#executing-the-inference">Executing the Inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../thor.html">Second-order Optimization</a> &raquo;</li>
      <li>Applying Second-Order Optimization Practices on the ResNet-50 Network</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/optimize/thor/resnet50.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="applying-second-order-optimization-practices-on-the-resnet-50-network">
<h1>Applying Second-Order Optimization Practices on the ResNet-50 Network<a class="headerlink" href="#applying-second-order-optimization-practices-on-the-resnet-50-network" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/optimize/thor/resnet50.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Common optimization algorithms can be divided into first-order optimization algorithms and second-order optimization algorithms. Classical first-order optimization algorithms, such as SGD, have small volume of computation, with fast computation speed, but converge slowly and require many iterations. The second-order optimization algorithms use the second-order derivative of the objective function to accelerate the convergence, which can converge to the optimal value of the model faster and requires fewer iterations. However, the overall execution time of the second-order optimization algorithms is still slower than the first-order optimization algorithms due to its high computational cost, so the application of the second-order optimization algorithm in deep neural network training is not common at present. The main computational cost of the second-order optimization algorithms lies in the inverse operation of the second-order information matrix (Hessian matrix, <a class="reference external" href="https://arxiv.org/pdf/1808.07172.pdf">FIM matrix</a>, etc.), with a time complexity of about <span class="math notranslate nohighlight">\(O(n^3)\)</span>.</p>
<p>Based on the existing natural gradient algorithm, the MindSpore development team has developed a usable second-order optimizer THOR by using approximations, tiles and other optimization accelerations for the FIM matrix, which greatly reduces the computational complexity of the inverse matrix. Using eight Ascend 910 AI processors, THOR can complete training the ResNet50-v1.5 network and ImageNet dataset in 72min, nearly doubling the speed compared to SGD+Momentum.</p>
<p>This tutorial will focus on how to train ResNet50-v1.5 network and ImageNet dataset on Ascend 910 and GPU using THOR, a second-order optimizer provided by MindSpore.</p>
<blockquote>
<div><p>Download the complete sample code: <a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/cv/ResNet">Resnet</a>.</p>
</div></blockquote>
<p>The directory structure of sample code:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── resnet
    ├── README.md
    ├── scripts
        ├── run_distribute_train.sh         # launch distributed training for Ascend 910
        ├── run_eval.sh                     # launch inference for Ascend 910
        ├── run_distribute_train_gpu.sh     # launch distributed training for GPU
        ├── run_eval_gpu.sh                 # launch inference for GPU
    ├── src
        ├── dataset.py                      # data preprocessing
        ├── CrossEntropySmooth.py           # CrossEntropy loss function
        ├── lr_generator.py                 # generate learning rate for every step
        ├── resnet.py                       # ResNet50 backbone
        ├── model_utils
            ├── config.py                   # parameter configuration
    ├── eval.py                             # infer script
    ├── train.py                            # train script
</pre></div>
</div>
<p>The overall execution process is as follows:</p>
<ol class="arabic simple">
<li><p>Prepare ImageNet datasets and process the required datasets.</p></li>
<li><p>Define the ResNet50 network.</p></li>
<li><p>Define the loss function and THOR optimizer.</p></li>
<li><p>Load the dataset and train it, and view the results and save the model file after the training is completed.</p></li>
<li><p>Load the saved model for inference.</p></li>
</ol>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<p>Make sure MindSpore is properly installed before practicing. If not, you can install MindSpore through the <a class="reference external" href="https://www.mindspore.cn/install">MindSpore installation page</a>.</p>
<section id="preparing-the-dataset">
<h3>Preparing the Dataset<a class="headerlink" href="#preparing-the-dataset" title="Permalink to this headline"></a></h3>
<p>Download the complete ImageNet2012 dataset and unzip the dataset to <code class="docutils literal notranslate"><span class="pre">ImageNet2012/ilsvrc</span></code> and <code class="docutils literal notranslate"><span class="pre">ImageNet2012/ilsvrc_eval</span></code> paths in the local workspace respectively.</p>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ImageNet2012
    ├─ilsvrc
    │      n03676483
    │      n04067472
    │      n01622779
    │      ......
    └─ilsvrc_eval
    │      n03018349
    │      n02504013
    │      n07871810
    │      ......
</pre></div>
</div>
</section>
<section id="configuring-distributed-environment-variables">
<h3>Configuring Distributed Environment Variables<a class="headerlink" href="#configuring-distributed-environment-variables" title="Permalink to this headline"></a></h3>
<section id="ascend-910">
<h4>Ascend 910<a class="headerlink" href="#ascend-910" title="Permalink to this headline"></a></h4>
<p>Refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_ascend.html#configuring-distributed-environment-variables">Distributed Parallel Training (Ascend)</a> for the configuration of distributed environment variables for the Ascend 910 AI processor.</p>
</section>
<section id="gpu">
<h4>GPU<a class="headerlink" href="#gpu" title="Permalink to this headline"></a></h4>
<p>Refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_gpu.html#configuring-distributed-environment">Distributed Parallel Training (GPU)</a> for the configuration of distributed environment variables for the GPU.</p>
</section>
</section>
</section>
<section id="loading-and-processing-the-datasets">
<h2>Loading and Processing the Datasets<a class="headerlink" href="#loading-and-processing-the-datasets" title="Permalink to this headline"></a></h2>
<p>For distributed training, the dataset is loaded in a parallel manner, while the dataset is processed through the data augmentation interface provided by MindSpore. The script to load and process the datasets is in the <code class="docutils literal notranslate"><span class="pre">src/dataset.py</span></code> script in the source code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>


<span class="k">def</span> <span class="nf">create_dataset2</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">do_train</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">distribute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_session_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a training or evaluation ImageNet2012 dataset for ResNet50.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset_path(string): the path of dataset.</span>
<span class="sd">        do_train(bool): whether the dataset is used for training or evaluation.</span>
<span class="sd">        repeat_num(int): the repeat times of dataset. Default: 1</span>
<span class="sd">        batch_size(int): the batch size of dataset. Default: 32</span>
<span class="sd">        target(str): the device target. Default: Ascend</span>
<span class="sd">        distribute(bool): data for distribute or not. Default: False</span>
<span class="sd">        enable_cache(bool): whether tensor caching service is used for evaluation. Default: False</span>
<span class="sd">        cache_session_id(int): if enable_cache is set, cache session_id need to be provided. Default: None</span>

<span class="sd">    Returns:</span>
<span class="sd">        dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">device_num</span><span class="p">,</span> <span class="n">rank_id</span> <span class="o">=</span> <span class="n">_get_rank_info</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">distribute</span><span class="p">:</span>
            <span class="n">init</span><span class="p">()</span>
            <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
            <span class="n">device_num</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_num</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">device_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">num_shards</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.456</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.406</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.224</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.225</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>

    <span class="c1"># define map operations</span>
    <span class="k">if</span> <span class="n">do_train</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">RandomCropDecodeResize</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.333</span><span class="p">)),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Decode</span><span class="p">(),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>

    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="c1"># only enable cache for eval</span>
    <span class="k">if</span> <span class="n">do_train</span><span class="p">:</span>
        <span class="n">enable_cache</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">enable_cache</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_session_id</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;A cache session_id must be provided to use cache.&quot;</span><span class="p">)</span>
        <span class="n">eval_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">cache_session_id</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                <span class="n">cache</span><span class="o">=</span><span class="n">eval_cache</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="c1"># apply batch operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># apply dataset repeat operation</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_set</span>
</pre></div>
</div>
<blockquote>
<div><p>MindSpore supports a variety of data processing and augmentation operations, often in combination, as described in the <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/dataset.html">Data Processing</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/dataset.html">Data Augmentation</a> chapters.</p>
</div></blockquote>
</section>
<section id="defining-the-networks">
<h2>Defining the Networks<a class="headerlink" href="#defining-the-networks" title="Permalink to this headline"></a></h2>
<p>The network model used in this example is ResNet50-v1.5, defining the <a class="reference external" href="https://gitee.com/mindspore/models/blob/master/official/cv/ResNet/src/resnet.py">ResNet50 network</a>.</p>
<p>After the network is constructed, the defined ResNet50 is called in the <code class="docutils literal notranslate"><span class="pre">__main__</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">src.resnet</span> <span class="kn">import</span> <span class="n">resnet50</span> <span class="k">as</span> <span class="n">resnet</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define net</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="defining-the-loss-function-and-thor-optimizer">
<h2>Defining the Loss Function and THOR Optimizer<a class="headerlink" href="#defining-the-loss-function-and-thor-optimizer" title="Permalink to this headline"></a></h2>
<section id="defining-the-loss-function">
<h3>Defining the Loss Function<a class="headerlink" href="#defining-the-loss-function" title="Permalink to this headline"></a></h3>
<p>The loss functions supported by MindSpore are <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>, <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>, <code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>, etc. The THOR optimizer requires the <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code> loss function.</p>
<p>The steps to implement the loss function are in the <code class="docutils literal notranslate"><span class="pre">src/CrossEntropySmooth.py</span></code> script. A common trick in deep network model training is used here: label smoothing, which can increase the generalization ability of the model by smoothing the real labels and improving the tolerance of the model to misclassified labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropySmooth</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CrossEntropy&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">smooth_factor</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CrossEntropySmooth</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">smooth_factor</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">smooth_factor</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logit</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>Call the defined loss function in the <code class="docutils literal notranslate"><span class="pre">__main__</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">src.CrossEntropySmooth</span> <span class="kn">import</span> <span class="n">CrossEntropySmooth</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define the loss function</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">use_label_smooth</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropySmooth</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
                              <span class="n">smooth_factor</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="defining-the-optimizers">
<h3>Defining the Optimizers<a class="headerlink" href="#defining-the-optimizers" title="Permalink to this headline"></a></h3>
<p>The parameter update formula for the THOR optimizer is as follows:</p>
<div class="math notranslate nohighlight">
\[ \theta^{t+1} = \theta^t + \alpha F^{-1}\nabla E\]</div>
<p>The meaning of each parameter in the parameter update formula is as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span>: Trainable parameters in the network.</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>: The number of iterations.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>: The learning rate value, the update step of the parameter.</p></li>
<li><p><span class="math notranslate nohighlight">\(F^{-1}\)</span>: FIM matrix, obtained by calculation in the network.</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla E\)</span>: First-order gradient values.</p></li>
</ul>
<p>As can be seen from the parameter update formula, what the THOR optimizer needs to calculate additionally is the FIM matrix for each layer. The FIM matrix can be adaptively adjusted to the step size and direction in each layer of parameter updates, and reduce the complexity of parameters tuning while accelerating the convergence.</p>
<p>When calling the MindSpore-encapsulated second-order optimizer THOR, the optimizer automatically calls the transformation interface to convert the Conv2d layer and Dense layer in the previously defined ResNet50 network into the corresponding <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/nn/layer/thor_layer.py">Conv2dThor</a> and <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/nn/layer/thor_layer.py">DenseThor</a>.
And the computation and storage of the second-order information matrix can be done in Conv2dThor and DenseThor.</p>
<blockquote>
<div><p>The network backbone is the same before and after the THOR optimizer conversion, and the network parameters remain unchanged.</p>
</div></blockquote>
<p>Calling the THOR optimizer in the training master script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">thor</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># learning rate setting and damping setting</span>
    <span class="kn">from</span> <span class="nn">src.lr_generator</span> <span class="kn">import</span> <span class="n">get_thor_lr</span><span class="p">,</span> <span class="n">get_thor_damping</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">get_thor_lr</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_init</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_decay</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_end_epoch</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">decay_epochs</span><span class="o">=</span><span class="mi">39</span><span class="p">)</span>
    <span class="n">damping</span> <span class="o">=</span> <span class="n">get_thor_damping</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">damping_init</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">damping_decay</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
    <span class="c1"># define the optimizer</span>
    <span class="n">split_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">26</span><span class="p">,</span> <span class="mi">53</span><span class="p">]</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">thor</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">damping</span><span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span>
               <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">split_indices</span><span class="o">=</span><span class="n">split_indices</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">frequency</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="training-the-networks">
<h2>Training the Networks<a class="headerlink" href="#training-the-networks" title="Permalink to this headline"></a></h2>
<section id="configuring-model-saving">
<h3>Configuring Model Saving<a class="headerlink" href="#configuring-model-saving" title="Permalink to this headline"></a></h3>
<p>MindSpore provides a callback mechanism to execute custom logic during training, here using the <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> function provided by the framework.
<code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> can save the network model and parameters for subsequent fine-tuning operations.
<code class="docutils literal notranslate"><span class="pre">TimeMonitor</span></code>, <code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code> are the official callback functions provided by MindSpore, which can be used to monitor the changes of single-step iteration time and <code class="docutils literal notranslate"><span class="pre">loss</span></code> values during the training process respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define callbacks</span>
    <span class="n">time_cb</span> <span class="o">=</span> <span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">cb</span> <span class="o">=</span> <span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">loss_cb</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">:</span>
        <span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint_epochs</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span>
                                     <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">keep_checkpoint_max</span><span class="p">)</span>
        <span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">ckpt_save_dir</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
        <span class="n">cb</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ckpt_cb</span><span class="p">]</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="configuring-the-training-network">
<h3>Configuring the Training Network<a class="headerlink" href="#configuring-the-training-network" title="Permalink to this headline"></a></h3>
<p>Training of the network can be easily performed through the <code class="docutils literal notranslate"><span class="pre">model.train</span></code> interface provided by MindSpore. The THOR optimizer reduces the volume of computation and improves the computation speed by reducing the frequency of second-order matrix updates, so it redefines a <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/train/train_thor/model_thor.py">ModelThor</a> class and inherits the Model class provided by MindSpore. Obtaining the second-order matrix update frequency control parameter of THOR in the ModelThor class, users can optimize the overall performance by adjusting this parameter.
MindSpore provides a one-click conversion interface from Model class to ModelThor class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">amp</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">ConvertModelUtils</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="n">loss_scale</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
                  <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eval_network</span><span class="o">=</span><span class="n">dist_eval_network</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s2">&quot;Thor&quot;</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">ConvertModelUtils</span><span class="p">()</span><span class="o">.</span><span class="n">convert_to_thor_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
                                                          <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">},</span>
                                                          <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="running-the-script">
<h3>Running the Script<a class="headerlink" href="#running-the-script" title="Permalink to this headline"></a></h3>
<p>After the training script is defined, call the shell script in the <code class="docutils literal notranslate"><span class="pre">scripts</span></code> directory and start the distributed training process.</p>
<section id="ascend-910-1">
<h4>Ascend 910<a class="headerlink" href="#ascend-910-1" title="Permalink to this headline"></a></h4>
<p>The current MindSpore distributed executes in the running mode of single-card, single-process on Ascend, i.e., 1 process running on each card, with the number of processes matching the number of used cards. The processes are executed in the background and each process creates a directory called <code class="docutils literal notranslate"><span class="pre">train_parallel</span></code> + <code class="docutils literal notranslate"><span class="pre">device_id</span></code> to store log information, operator compilation information and training checkpoint files. The following is an example of a distributed training script by using 8 cards to demonstrate how to run the script.</p>
<p>Use the following command to run the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_distribute_train.sh<span class="w"> </span>&lt;RANK_TABLE_FILE&gt;<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span><span class="o">[</span>CONFIG_PATH<span class="o">]</span>
</pre></div>
</div>
<p>The script needs to pass in the variables <code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>, <code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>, where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>: The path of networking information file. (For the generation of rank table files, refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/master/utils/hccl_tools">HCCL_TOOL</a>.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>: The path of the training dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>: The path of configuration file.</p></li>
</ul>
<p>For the rest of the environment variables, please refer to the configuration items in the installation tutorial.</p>
<p>An example of loss printing during training is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
epoch: 1 step: 5004, loss is 4.4182425
epoch: 2 step: 5004, loss is 3.740064
epoch: 3 step: 5004, loss is 4.0546017
epoch: 4 step: 5004, loss is 3.7598825
epoch: 5 step: 5004, loss is 3.3744206
...
epoch: 40 step: 5004, loss is 1.6907625
epoch: 41 step: 5004, loss is 1.8217756
epoch: 42 step: 5004, loss is 1.6453942
...
</pre></div>
</div>
<p>After training, the checkpoint file generated by each card training is saved in the respective training directory. An example of the checkpoint file generated by <code class="docutils literal notranslate"><span class="pre">device_0</span></code> is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─train_parallel0
    ├─ckpt_0
        ├─resnet-1_5004.ckpt
        ├─resnet-2_5004.ckpt
        │      ......
        ├─resnet-42_5004.ckpt
        │      ......
</pre></div>
</div>
<p>where
<code class="docutils literal notranslate"><span class="pre">*.ckpt</span></code> refers to the saved model parameter file. The specific meanings of checkpoint file names: <em>network name</em>-<em>number of epoch</em>_<em>number of step</em>.ckpt.</p>
</section>
<section id="gpu-1">
<h4>GPU<a class="headerlink" href="#gpu-1" title="Permalink to this headline"></a></h4>
<p>On the GPU hardware platform, MindSpore uses OpenMPI <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> for distributed training. The process creates a directory called <code class="docutils literal notranslate"><span class="pre">train_parallel</span></code> to store log information and checkpoint files for training. The following is an example of a distributed training script using 8 cards to demonstrate how to run the script.</p>
<p>Use the following command to run the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_distribute_train_gpu.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CONFIG_PATH&gt;
</pre></div>
</div>
<p>The script needs to pass in the variables <code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>, where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>: Training dataset path.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>: Configuration file path.</p></li>
</ul>
<p>During GPU training, there is no need to set the <code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code> environment variable. So there is no need to call <code class="docutils literal notranslate"><span class="pre">int(os.getenv('DEVICE_ID'))</span></code> to get the physical serial number of the card in the main training script, and there is no need to pass <code class="docutils literal notranslate"><span class="pre">device_id</span></code> in the <code class="docutils literal notranslate"><span class="pre">context</span></code>. We need to set device_target to GPU and call <code class="docutils literal notranslate"><span class="pre">init()</span></code> to enable NCCL.</p>
<p>An example of loss printing during training is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
epoch: 1 step: 5004, loss is 4.2546034
epoch: 2 step: 5004, loss is 4.0819564
epoch: 3 step: 5004, loss is 3.7005644
epoch: 4 step: 5004, loss is 3.2668946
epoch: 5 step: 5004, loss is 3.023509
...
epoch: 36 step: 5004, loss is 1.645802
...
</pre></div>
</div>
<p>After training, an example of the saved model file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─train_parallel
    ├─ckpt_0
        ├─resnet-1_5004.ckpt
        ├─resnet-2_5004.ckpt
        │      ......
        ├─resnet-36_5004.ckpt
        │      ......
    ......
    ├─ckpt_7
        ├─resnet-1_5004.ckpt
        ├─resnet-2_5004.ckpt
        │      ......
        ├─resnet-36_5004.ckpt
        │      ......
</pre></div>
</div>
</section>
</section>
</section>
<section id="model-inference">
<h2>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this headline"></a></h2>
<p>Inference is performed by using the checkpoint file saved during the training process to verify the generalization ability of the model. First load the model file through the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> interface, call the <code class="docutils literal notranslate"><span class="pre">eval</span></code> interface of <code class="docutils literal notranslate"><span class="pre">Model</span></code> to make a prediction on the input image category, and then compare it with the real category of the input image, to get the final prediction accuracy value.</p>
<section id="defining-the-inference-network">
<h3>Defining the Inference Network<a class="headerlink" href="#defining-the-inference-network" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> interface to load the model file.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">model.eval</span></code> interface to read in the test dataset for inference.</p></li>
<li><p>Calculate the prediction accuracy value.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define net</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>

    <span class="c1"># load checkpoint</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">args_opt</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># define loss</span>
    <span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">dataset</span> <span class="o">==</span> <span class="s2">&quot;imagenet2012&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">use_label_smooth</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropySmooth</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                                  <span class="n">smooth_factor</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>

    <span class="c1"># define model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>

    <span class="c1"># eval model</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;result:&quot;</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="s2">&quot;ckpt=&quot;</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="executing-the-inference">
<h3>Executing the Inference<a class="headerlink" href="#executing-the-inference" title="Permalink to this headline"></a></h3>
<p>After the inference network is defined, the shell script in the <code class="docutils literal notranslate"><span class="pre">scripts</span></code> directory is called for inference.</p>
<section id="ascend-910-2">
<h4>Ascend 910<a class="headerlink" href="#ascend-910-2" title="Permalink to this headline"></a></h4>
<p>On the Ascend 910 hardware platform, the inference execution command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_eval.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CHECKPOINT_PATH&gt;<span class="w"> </span>&lt;CONFIG_PATH&gt;
</pre></div>
</div>
<p>The script needs to pass in the variables <code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>, <code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;CONFIG_PATH&gt;</span></code>, where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>: The inference dataset path.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>: The saved checkpoint path.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>: The configuration file path.</p></li>
</ul>
<p>The current inference is performed using a single card (default device 0), and the result of the inference is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result: {&#39;top_5_accuracy&#39;: 0.9295574583866837, &#39;top_1_accuracy&#39;: 0.761443661971831} ckpt=train_parallel0/resnet-42_5004.ckpt
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">top_5_accuracy</span></code>: For an input image, a classification is considered correct if the top five tags in the predicted probability ranking contain true tags.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_1_accuracy</span></code>: For an input image, if the label with the highest predicted probability is the same as the true label, the classification is considered correct.</p></li>
</ul>
</section>
<section id="gpu-2">
<h4>GPU<a class="headerlink" href="#gpu-2" title="Permalink to this headline"></a></h4>
<p>On the GPU hardware platform, the execution command for inference is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>bash<span class="w"> </span>run_eval_gpu.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CHECKPOINT_PATH&gt;<span class="w"> </span>&lt;CONFIG_PATH&gt;
</pre></div>
</div>
<p>The script needs to pass in the variables <code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>, <code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>, where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>: The inference dataset path.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>: The saved checkpoint path.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>: The configuration file path.</p></li>
</ul>
<p>The inference result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result: {&#39;top_5_accuracy&#39;: 0.9287972151088348, &#39;top_1_accuracy&#39;: 0.7597031049935979} ckpt=train_parallel/resnet-36_5004.ckpt
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="intro.html" class="btn btn-neutral float-left" title="Introduction to Second-order Optimizer THOR" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../vmap/vmap.html" class="btn btn-neutral float-right" title="Automatic Vectorization (Vmap)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>