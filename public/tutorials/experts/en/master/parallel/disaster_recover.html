<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Disaster Recovery in Dynamic Cluster Scenarios &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fault Recovery Based on Redundant Information" href="fault_recover.html" />
    <link rel="prev" title="Fault Recovery" href="recover.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="recover.html">Fault Recovery</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Disaster Recovery in Dynamic Cluster Scenarios</a></li>
<li class="toctree-l2"><a class="reference internal" href="fault_recover.html">Fault Recovery Based on Redundant Information</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="recover.html">Fault Recovery</a> &raquo;</li>
      <li>Disaster Recovery in Dynamic Cluster Scenarios</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/disaster_recover.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="disaster-recovery-in-dynamic-cluster-scenarios">
<h1>Disaster Recovery in Dynamic Cluster Scenarios<a class="headerlink" href="#disaster-recovery-in-dynamic-cluster-scenarios" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/disaster_recover.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The model training has high reliability and serviceability requirements for distributed training architecture. MindSpore dynamic cluster startup method supports data parallel disaster recovery. There is process abnormal exit in multi-card data parallel training scenarios cluster (multiple Workers and a Scheduler), after the process is pulled up again, the training task continues to be able to execute normally.</p>
<p>Specifically, in the graph mode, the data sink mode is used for training, and the data parallel mode is turned on. After the training cluster is started by dynamic cluster, if any process exits abnormally during the training process, it is guaranteed that the training can be continued after pulling up the corresponding script of the corresponding process under the same environment variables (<code class="docutils literal notranslate"><span class="pre">MS_ENABLE_RECOVERY</span></code> and <code class="docutils literal notranslate"><span class="pre">MS_RECOVERY_PATH</span></code>) and the accuracy convergence will not be affected.</p>
<blockquote>
<div><p>Disaster recovery in dynamic cluster scenarios only supports GPUs and needs to run in Graph mode.</p>
</div></blockquote>
<p>For more detailed instructions, see dynamic cluster environment variables in the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/env_var_list.html">Environment Variables List</a>.</p>
</section>
<section id="operation-practice">
<h2>Operation Practice<a class="headerlink" href="#operation-practice" title="Permalink to this headline"></a></h2>
<p>The following is an example of how to do this with Ascend:</p>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Download the full sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/disaster_recover">disaster_recover</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ disaster_recover
       ├── train.py
       ├── run.sh
       └── recover.sh
    ...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train.py</span></code> is the script that defines the network structure and the training process. <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the execution script and <code class="docutils literal notranslate"><span class="pre">recover.sh</span></code> is the recovery script after node failure.</p>
</section>
<section id="network-structure">
<h3>Network Structure<a class="headerlink" href="#network-structure" title="Permalink to this headline"></a></h3>
<p>The network structure and dataset loading is consistent with the example in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dynamic_cluster.html">Dynamic Cluster Startup Method</a>.</p>
</section>
<section id="defining-the-training-process">
<h3>Defining the Training Process<a class="headerlink" href="#defining-the-training-process" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">train</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># Configure the interval at which checkpoints are saved and the maximum number to be saved</span>
<span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># Configure the checkpoint storage path, and each process uses different paths</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt_of_rank/&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">get_rank</span><span class="p">()),</span> <span class="n">config</span><span class="o">=</span><span class="n">ckpt_config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">data_set</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">,</span> <span class="n">ckpoint_cb</span><span class="p">])</span>
</pre></div>
</div>
<p>Each worker is enabled to save checkpoints with different paths (e.g., the directory setting in the above example uses a rank id to ensure that the paths are not the same) to prevent checkpoint saving conflicts with the same name. Checkpoint is used for abnormal process recovery and normal process rollback, and the rollback of training means that each Worker in the cluster is restored to the state corresponding to the latest checkpoint, and at the same time, the data side is also rolled back to the corresponding step, and then continue to train.</p>
<p>The interval between checkpoints is configurable, and determines the granularity of disaster recovery. The smaller the interval, the smaller the number of steps back to the last checkpoint save, but frequent checkpoint saves may also affect the training efficiency, and larger intervals have the opposite effect. keep_checkpoint_max is set to at least 2 (to prevent checkpoint saves from failing).</p>
</section>
<section id="preparing-the-startup-script">
<h3>Preparing the Startup Script<a class="headerlink" href="#preparing-the-startup-script" title="Permalink to this headline"></a></h3>
<p>The script content <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is as follows, adding environment variables related to disaster recovery:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data.zip&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span>wget<span class="w"> </span>http://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip
<span class="w">    </span><span class="k">fi</span>
<span class="w">    </span>unzip<span class="w"> </span>MNIST_Data.zip
<span class="k">fi</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/MNIST_Data/train/

<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                </span><span class="c1"># Enable disaster recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>./recovery/<span class="w">        </span><span class="c1"># Set the disaster recovery file save path</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Loop start of 8 Worker training processes</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">          </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">  </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">       </span><span class="c1"># Set the Scheduler Port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">        </span><span class="c1"># Set the startup process to MS_WORKER role</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">            </span><span class="c1"># Set the process id, optional</span>
<span class="w">    </span>python<span class="w"> </span>./train.py<span class="w"> </span>&gt;<span class="w"> </span>device/worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># Start training scripts</span>
<span class="k">done</span>

<span class="c1"># Start 1 Scheduler process</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># Set the Scheduler Port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># Set the startup process to MS_SCHED role</span>
python<span class="w"> </span>./train.py<span class="w"> </span>&gt;<span class="w"> </span>device/scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># Start training scripts</span>
</pre></div>
</div>
<p>The environment variable <code class="docutils literal notranslate"><span class="pre">MS_ENABLE_RECOVERY=1</span></code> indicates that disaster recovery is enabled, and <code class="docutils literal notranslate"><span class="pre">MS_RECOVERY_PATH=.</span> <span class="pre">/recovery/</span></code> means configuring the path where persistence files are stored.</p>
<p>Before starting the Worker and Scheduler, you need to add the relevant environment variable settings, for example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MS_WORKER_NUM=8</span></code>: Configure the number of Worker processes to 8.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MS_SCHED_HOST=127.0.0.1</span></code>: Configure the Scheduler IP address to 127.0.0.1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MS_SCHED_PORT=8118</span></code>: Configure the port number of the Scheduler to be 8118.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MS_ROLE=MS_WORKER</span></code>: Configure the role of the current process. <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code> means the role is Worker, and <code class="docutils literal notranslate"><span class="pre">MS_SCHED</span></code> means the role is Scheduler.</p></li>
</ul>
<p>Execute the following command to start a single-machine 8-card data parallel training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh
</pre></div>
</div>
<p>Distributed training starts, and if the training process encounters an exception, such as a process exiting abnormally, the corresponding process is restarted and the training process can be resumed:
For example, if the Scheduler process exits abnormally in the middle of training, you can execute the following command to restart the Scheduler:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/MNIST_Data/train/
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                </span><span class="c1"># Enable disaster recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>./recovery/<span class="w">        </span><span class="c1"># Set the disaster recovery file save path</span>

<span class="c1"># Start 1 Scheduler process</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># Set the Scheduler Port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># Set the startup process to MS_SCHED role</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">             </span><span class="c1"># Set this node Node ID to &#39;sched&#39;</span>
python<span class="w"> </span>./train.py<span class="w"> </span>&gt;<span class="w"> </span>device/scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># Start training scripts</span>
</pre></div>
</div>
<p>Or execute the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>recover.sh
</pre></div>
</div>
<p>The grouping of Worker and Scheduler is automatically restored.</p>
<p>Worker processes with abnormal exit are handled in a similar way (Note: Worker processes with abnormal exit need to wait for 30s before pulling up again to resume training, and before that, the Scheduler rejects Workers with the same node id from re-registering in order to prevent network jitter and malicious registrations).</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="recover.html" class="btn btn-neutral float-left" title="Fault Recovery" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="fault_recover.html" class="btn btn-neutral float-right" title="Fault Recovery Based on Redundant Information" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>