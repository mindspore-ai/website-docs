<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Graph Partition &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Functional Operator Shading" href="pynative_shard_function_parallel.html" />
    <link rel="prev" title="Experimental Characteristics" href="others.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="others.html">Experimental Characteristics</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Graph Partition</a></li>
<li class="toctree-l2"><a class="reference internal" href="pynative_shard_function_parallel.html">Functional Operator Shading</a></li>
<li class="toctree-l2"><a class="reference internal" href="support_dynamic_shape_in_parallel.html">Distributed Parallel Supports Dynamic Shape</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/sdc.html">Accuracy-Sensitive Detection</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="others.html">Experimental Characteristics</a> &raquo;</li>
      <li>Distributed Graph Partition</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/distributed_graph_partition.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="distributed-graph-partition">
<h1>Distributed Graph Partition<a class="headerlink" href="#distributed-graph-partition" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/distributed_graph_partition.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In large model training tasks, users often use various types of parallel algorithms to distribute computational tasks to various nodes, to make full use of computational resources, e.g., through MindSpore <code class="docutils literal notranslate"><span class="pre">operator-level</span> <span class="pre">parallelism</span></code>, <code class="docutils literal notranslate"><span class="pre">pipeline</span> <span class="pre">parallelism</span></code> and other features. However, in some scenarios, the user needs to slice the graph into multiple subgraphs based on a custom algorithm and distribute them to different processes for distributed execution. MindSpore <code class="docutils literal notranslate"><span class="pre">distributed</span> <span class="pre">graph</span> <span class="pre">partition</span></code> feature provides an operator-granularity Python layer API to allow users to freely perform graph slicing and build distributed training/inference tasks based on custom algorithms.</p>
<blockquote>
<div><p>Distributed graph slicing does not support PyNative mode.</p>
</div></blockquote>
<p>Related interfaces:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.Cell.place(role,</span> <span class="pre">rank_id)</span></code>: Set a label for all operators in this Cell. This label tells the MindSpore compiler on which process this Cell is started. Each label consists of the process role <code class="docutils literal notranslate"><span class="pre">role</span></code> and <code class="docutils literal notranslate"><span class="pre">rank_id</span></code>, so by setting different labels for different Cells, these Cells will be started on different processes, allowing the user to perform tasks such as distributed training/inference.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mindspore.ops.Primitive.place(role,</span> <span class="pre">rank_id)</span></code>: Set labels for Primitive operator, with the same function as <code class="docutils literal notranslate"><span class="pre">mindspore.nn.Cell.place(role,</span> <span class="pre">rank_id)</span></code> interface.</p></li>
</ol>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>Distributed tasks need to be executed in a cluster. MindSpore reuses built-in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dynamic_cluster.html">Dynamic Networking module</a> of MindSpore in order to have better scalability and reliability in distributed graph partition scenarios.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">distributed</span> <span class="pre">graph</span> <span class="pre">partition</span></code>, each process represents a compute node (called <code class="docutils literal notranslate"><span class="pre">Worker</span></code>), and the scheduling node (called <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>) started by the <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">networking</span></code> module mentioned above allows discovering each compute node and thus forming a compute cluster.</p>
<p>After <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">networking</span></code>, MindSpore assigns <code class="docutils literal notranslate"><span class="pre">role</span></code> and <code class="docutils literal notranslate"><span class="pre">rank</span></code>, the <code class="docutils literal notranslate"><span class="pre">role</span></code> and <code class="docutils literal notranslate"><span class="pre">id</span></code> of each process, based on the user startup configuration, both of which form a unique <code class="docutils literal notranslate"><span class="pre">tag</span></code> for each process and are input to the Python layer API <code class="docutils literal notranslate"><span class="pre">place</span></code>. With the correspondence, the user can set process labels for any operator by calling the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface, and the MindSpore graph compiler module processes it to slice the computed graph into multiple subgraphs for distribution to different processes for execution. For the detailed use, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Primitive.html#mindspore.ops.Primitive.place">Primitive.place</a> and <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.place">Cell.place</a> interface document.
As an example, the compute topology after distributed graph partition might be as follows:</p>
<p><img alt="image" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/parallel/images/distributed_graph_partition.png" /></p>
<p>As shown in the figure above, each <code class="docutils literal notranslate"><span class="pre">Worker</span></code> has a portion of subgraphs that have been sliced by the user, with their own weights and inputs, and each <code class="docutils literal notranslate"><span class="pre">Worker</span></code> interacts with each other through the built-in <code class="docutils literal notranslate"><span class="pre">Rpc</span> <span class="pre">communication</span> <span class="pre">operator</span></code> for data interaction.</p>
<p>To ensure ease of use and user-friendliness, MindSpore also supports users to start dynamic networking and distributed training (without distinguishing between <code class="docutils literal notranslate"><span class="pre">Worker</span></code> and <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>) with a few modifications to a single script. See the following operation practice section for more details.</p>
</section>
<section id="operation-practice">
<h2>Operation Practice<a class="headerlink" href="#operation-practice" title="Permalink to this headline"></a></h2>
<p>Taking LeNet training on GPU based on MNIST dataset as an example, different parts of the graph in the training task are split to different computational nodes for execution.</p>
<blockquote>
<div><p>You can download complete code here:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_graph_partition">https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_graph_partition</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ distributed_graph_partition
       ├── lenet.py
       ├── run.sh
       └── train.py
</pre></div>
</div>
<blockquote>
<div><p>This tutorial does not involve starting across physical nodes, where all processes are on the same node. For MindSpore, there is no difference in the implementation of intra-node and cross-node distributed graph partition: after dynamic networking, graph slicing, and graph compilation processes, data interaction is performed via Rpc communication operators.</p>
</div></blockquote>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">C</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">CV</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.vision</span> <span class="kn">import</span> <span class="n">Inter</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span>
                   <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    create dataset for train or test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># define dataset</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">rescale_nml</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">0.3081</span>
    <span class="n">shift_nml</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="mf">0.1307</span> <span class="o">/</span> <span class="mf">0.3081</span>

    <span class="c1"># define map operations</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">Inter</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">)</span>  <span class="c1"># Bilinear mode</span>
    <span class="n">rescale_nml_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale_nml</span><span class="p">,</span> <span class="n">shift_nml</span><span class="p">)</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">hwc2chw_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># apply map operations on images</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">rescale_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">rescale_nml_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">hwc2chw_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>

    <span class="c1"># apply DatasetOps</span>
    <span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mnist_ds</span>
</pre></div>
</div>
<p>The above code creates the MNIST dataset.</p>
</section>
<section id="constructing-the-lenet-network">
<h3>Constructing the LeNet Network<a class="headerlink" href="#constructing-the-lenet-network" title="Permalink to this headline"></a></h3>
<p>In order to make slicing to a single-machine single-card task, we need to first construct a single-machine single-card copy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">TruncatedNormal</span>

<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for conv layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                     <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                     <span class="n">weight_init</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fc_with_initialize</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for fc layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">weight_variable</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span> <span class="o">=</span> <span class="n">num_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="calling-the-interface-for-distributed-graph-partition">
<h3>Calling the Interface for Distributed Graph Partition<a class="headerlink" href="#calling-the-interface-for-distributed-graph-partition" title="Permalink to this headline"></a></h3>
<p>For this training task we slice <code class="docutils literal notranslate"><span class="pre">fc1</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">fc2</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">fc3</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">conv1</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">3</span></code>, and <code class="docutils literal notranslate"><span class="pre">conv2</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">4</span></code>.</p>
<p>Distributed graph sharding can be completed by adding the following graph partition statement after initializing the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>The first parameter of the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface, <code class="docutils literal notranslate"><span class="pre">role</span></code>, is the process role, and the second parameter is the process <code class="docutils literal notranslate"><span class="pre">rank</span></code>, which means that the operator is executed on a process of such role. Currently the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface only supports the <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code> role, which represents the <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">X</span></code> process described above.</p>
<p>It follows that a user can quickly implement a distributed training task by simply describing a custom algorithm through a single-machine copy and then setting the label of the compute node where the operator is located through the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface. The advantages of this approach are:</p>
<p><strong>1.Instead of writing separate execution scripts for each compute node, users can execute distributed tasks with just one script, MindSpore.</strong></p>
<p><strong>2.Provides a more general and user-friendly interface. Through the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface, users can intuitively describe their distributed training algorithms</strong></p>
</section>
<section id="defining-the-optimizer-and-loss-function">
<h3>Defining the Optimizer and Loss Function<a class="headerlink" href="#defining-the-optimizer-and-loss-function" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">mom_optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mom_optimizer</span>
<span class="k">def</span> <span class="nf">get_loss</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="executing-the-training-code">
<h3>Executing the Training Code<a class="headerlink" href="#executing-the-training-code" title="Permalink to this headline"></a></h3>
<p>The entry script train.py of training code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">set_seed</span>
<span class="kn">from</span> <span class="nn">mindspore.train.metrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>


<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">get_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================= Start training =================&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">),</span> <span class="s1">&#39;train&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">TimeMonitor</span><span class="p">()],</span><span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================= Start testing =================&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">),</span> <span class="s1">&#39;test&#39;</span><span class="p">))</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">if</span> <span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy is:&quot;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code trains first and then infers, where all processes are executed in a distributed manner.</p>
<ul class="simple">
<li><p>In a distributed graph partition scenario, the user must call <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code>, an interface that is the entry point for MindSpore <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">networking</span></code> module, which is used to help form compute clusters, and initialize communication operators. If not called, MindSpore performs single-machine single-card training, i.e. the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface will not take effect.</p></li>
<li><p>Since only some subgraphs are executed on some processes, their inference precision or loss is not meaningful. The user only needs to focus on the precision on <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">0</span></code>.</p></li>
</ul>
</section>
<section id="preparation-for-training-shell-script">
<h3>Preparation for Training Shell Script<a class="headerlink" href="#preparation-for-training-shell-script" title="Permalink to this headline"></a></h3>
<section id="starting-scheduler-and-worker-processes">
<h4>Starting Scheduler and Worker Processes<a class="headerlink" href="#starting-scheduler-and-worker-processes" title="Permalink to this headline"></a></h4>
<p>Since multiple processes are started within a node, only one <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> process and multiple <code class="docutils literal notranslate"><span class="pre">Worker</span></code> processes need to be started via a Shell script. For the meaning of the environment variables in the script and their usage, refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dynamic_cluster.html">dynamic cluster environment variables</a>.</p>
<p>The run.sh execution script is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">execute_path</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span><span class="s2">/MNIST_Data&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span><span class="s2">/MNIST_Data.zip&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span>wget<span class="w"> </span>http://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip
<span class="w">    </span><span class="k">fi</span>
<span class="w">    </span>unzip<span class="w"> </span>MNIST_Data.zip
<span class="k">fi</span>

<span class="nv">self_path</span><span class="o">=</span><span class="k">$(</span>dirname<span class="w"> </span><span class="nv">$0</span><span class="k">)</span>

<span class="c1"># Set public environment.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/MNIST_Data/

<span class="c1"># Launch scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/
mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/
<span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
python<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/../train.py<span class="w"> </span>&gt;<span class="w"> </span>sched.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Launch workers.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="nv">$MS_WORKER_NUM</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">  </span>rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/
<span class="w">  </span>mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
<span class="w">  </span>python<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/../train.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Start training. The output is saved in sched and worker_* folder.&quot;</span>
</pre></div>
</div>
<p>In the above script, <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_WORKER_NUM=8</span></code> means that <code class="docutils literal notranslate"><span class="pre">8</span></code> <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code> processes need to be started for this distributed execution. <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_SCHED_HOST=127.0.0.1</span></code> means the address of <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> is <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>. <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_SCHED_PORT=8118</span></code> means the open port of <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> is 8118, and all processes will connect to this port for dynamic networking.</p>
<p>The above environment variables are exported for both <code class="docutils literal notranslate"><span class="pre">Worker</span></code> and <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> processes, and then the corresponding roles are exported separately to start the processes for the corresponding roles: after <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_ROLE=MS_SCHED</span></code>, start the Scheduler process, and after <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_ROLE=MS_WORKER</span></code>, start the MS_WORKER_NUM worker process cyclically.</p>
<blockquote>
<div><p>Note that each process is executed in the background, so there will be a wait for the process to exit statement at the end of the script.</p>
</div></blockquote>
<p>Executing instruction</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh
</pre></div>
</div>
</section>
</section>
<section id="viewing-the-execution-result">
<h3>Viewing the Execution Result<a class="headerlink" href="#viewing-the-execution-result" title="Permalink to this headline"></a></h3>
<p>View accuracy execution instruction:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>grep<span class="w"> </span>-rn<span class="w"> </span><span class="s2">&quot;Accuracy&quot;</span><span class="w"> </span>*/*.log
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Accuracy is: {&#39;Accuracy&#39;: 0.9888822115384616}
</pre></div>
</div>
<p>It shows that distributed graph Partition has no effect on LeNet training and inference results.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h2>
<p>MindSpore distributed graph partition feature provides users with <code class="docutils literal notranslate"><span class="pre">place</span></code> interface for operator granularity, which supports users to slice the compute graph according to custom algorithms and perform distributed training in various scenarios through dynamic networking.
The user only needs to make the following modifies to the single-machine single-card copy to initiate the task:</p>
<ul class="simple">
<li><p>Call the <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code> interface at the top of the single-machine single-card copy to start dynamic networking.</p></li>
<li><p>Call the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface of <code class="docutils literal notranslate"><span class="pre">ops.Primitive</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> for the operator or layer in the graph, based on the user algorithm, to set the label of the process where the operator is located.</p></li>
<li><p>Start <code class="docutils literal notranslate"><span class="pre">Worker</span></code> and <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> processes via shell scripts to execute distributed tasks.</p></li>
</ul>
<p>At present, the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface is in a limited state of support, and advanced usage is in the development stage. Users are welcome to make various comments or issues on the MindSpore website.
The <code class="docutils literal notranslate"><span class="pre">place</span></code> interface currently has the following limitations:</p>
<ul class="simple">
<li><p>The input parameter <code class="docutils literal notranslate"><span class="pre">role</span></code> is only supported to be set to <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code>. This is because each node in the distributed graph partition scenario is a compute node <code class="docutils literal notranslate"><span class="pre">Worker</span></code>, and setting other roles is not required for now.</p></li>
<li><p>Unable to mix with Parameter Server, Data Parallelism, and Auto Parallelism. The compute graph of each process after distributed graph partition is not consistent, and all three features have copies of inter-process graphs or operators that may cause unknown errors when executed overlaid with this feature. Mixed-use features will be supported in subsequent releases.</p></li>
<li><p>Control flow + distributed graph partition is in a limited support state and errors may be reported. This scenario will also be supported in subsequent releases.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="others.html" class="btn btn-neutral float-left" title="Experimental Characteristics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pynative_shard_function_parallel.html" class="btn btn-neutral float-right" title="Functional Operator Shading" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>