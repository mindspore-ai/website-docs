<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Dynamic Cluster Startup Method &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performing Distributed Training on K8S Clusters" href="ms_operator.html" />
    <link rel="prev" title="Distributed Parallel Startup Methods" href="startup_method.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Dynamic Cluster Startup Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="ms_operator.html">Performing Distributed Training on K8S Clusters</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="parallel_mode.html">Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="startup_method.html">Distributed Parallel Startup Methods</a> &raquo;</li>
      <li>Dynamic Cluster Startup Method</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/dynamic_cluster.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="dynamic-cluster-startup-method">
<h1>Dynamic Cluster Startup Method<a class="headerlink" href="#dynamic-cluster-startup-method" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/dynamic_cluster.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>For reliability requirements during training, MindSpore provides <strong>dynamic cluster</strong> features that enable users to start Ascend/GPU/CPU distributed training tasks without relying on any third-party library (OpenMPI) and without any modification to the training script. We recommend users to use this startup method in preference. Users can click <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/startup_method.html">Multi-Card Startup Method</a> to check the support of multi-card startup method on different platforms.</p>
<p>OpenMPI synchronizes data on the Host side and clustering between processes in a distributed training scenario. The MindSpore <strong>Dynamic Cluster</strong> feature replaces the OpenMPI capability by <strong>reusing the Parameter Server mode training architecture</strong>, which can be found in the <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/master/parallel/parameter_server_training.html">Parameter Server Mode</a> training tutorial.</p>
<p>The <strong>Dynami Cluster</strong> feature starts multiple MindSpore training processes as <code class="docutils literal notranslate"><span class="pre">Workers</span></code>, and starts an additional <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> for cluster and disaster recovery. The user only needs to make a few changes to the startup script to perform distributed training.</p>
<blockquote>
<div><p>Dynamic cluster startup scripts can be quickly migrated between multiple hardware platforms without additional modifications.</p>
</div></blockquote>
</section>
<section id="precautions">
<h2>Precautions<a class="headerlink" href="#precautions" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Dynamic cluster does not currently support <code class="docutils literal notranslate"><span class="pre">PyNative</span></code> mode.</p></li>
</ul>
</section>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline"></a></h2>
<p>Several environment variables need to be exported before the training script can be started for dynamic cluster, as shown in the following table:</p>
<table align="center">
    <tr>
        <th align="left">Environment Variables</th>
        <th align="left">Functions</th>
        <th align="left">Types</th>
        <th align="left">Values</th>
        <th align="left">Descriptions</th>
    </tr>
    <tr>
        <td align="left">MS_ROLE</td>
        <td align="left">Specify this process role.</td>
        <td align="left">String</td>
        <td align="left">
            <ul>
                <li>MS_SCHED: Represents a Scheduler process. Only one Scheduler is started for a training task, responsible for cluster, disaster recovery, etc. <b>No training code is executed</b>.</li>
                <li>MS_WORKER: Represents the Worker process, and generally sets the distributed training process to this role.</li>
                <li>MS_PSERVER: Represents Parameter Server process. This role is only available in Parameter Server mode. For more details, refer to <a href="https://mindspore.cn/tutorials/experts/en/master/parallel/parameter_server_training.html">Parameter Server Mode</a>.</li>
            </ul>
        </td>
        <td align="left">The Worker and Parameter Server processes will register with the Scheduler process to complete the cluster.</td>
    </tr>
    <tr>
        <td align="left">MS_SCHED_HOST</td>
        <td align="left">Specify IP address of Scheduler.</td>
        <td align="left">String</td>
        <td align="left">Legitimate IP address.</td>
        <td align="left">The current version does not support IPv6 addresses.</td>
    </tr>
    <tr>
        <td align="left">MS_SCHED_PORT</td>
        <td align="left">Specify the Scheduler binding port number.</td>
        <td align="left">Integer</td>
        <td align="left">Port number in the range of 1024 to 65535.</td>
        <td align="left"></td>
    </tr>
    <tr>
        <td align="left">MS_NODE_ID</td>
        <td align="left">Specify the ID of this process, unique within the cluster.</td>
        <td align="left">String</td>
        <td align="left">Represents the unique ID of this process, which is automatically generated by MindSpore by default.</td>
        <td align="left">
            MS_NODE_ID needs to be set in the following cases, but in general it does not need to be set and is automatically generated by MindSpore:
            <ul>
                <li>Start disaster recovery scenario: The current process ID needs to be obtained for disaster recovery so as to re-register with Scheduler.</li>
                <li>Start GLOG log redirection scenario: In order to ensure that each training process log is saved independently, you need to set the process ID as the log saving path suffix.</li>
                <li>Specify process rank id scenario: Users can specify the rank id of this process by setting MS_NODE_ID to some integer.</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td align="left">MS_WORKER_NUM</td>
        <td align="left">Specify the number of processes whose role is MS_WORKER.</td>
        <td align="left">Integer</td>
        <td align="left">The integer greater than 0.</td>
        <td align="left">
            The number of Worker processes started by the user should be equal to the value of this environment variable. If it is less than this value, the cluster will fail, while if it is more than this value, the Scheduler process will complete the cluster according to the Worker registration order, and the extra Worker processes will fail to start.
        </td>
    </tr>
    <tr>
        <td align="left">MS_SERVER_NUM</td>
        <td align="left">Specify the number of processes whose role is MS_PSERVER.</td>
        <td align="left">Integer</td>
        <td align="left">The integer greater than 0.</td>
        <td align="left">Only set Parameter Server training mode.</td>
    </tr>
    <tr>
        <td align="left">MS_ENABLE_RECOVERY</td>
        <td align="left">Turn on disaster recovery.</td>
        <td align="left">Integer</td>
        <td align="left">1 means on, and 0 means off. The default is 0.</td>
        <td align="left"></td>
    </tr>
    <tr>
        <td align="left">MS_RECOVERY_PATH</td>
        <td align="left">Persistent path folder.</td>
        <td align="left">String</td>
        <td align="left">Legitimate user directory.</td>
        <td align="left">The Worker and Scheduler processes perform the necessary persistence during execution, such as the node information used to recover the cluster and the intermediate state of the training service, and save it through files.</td>
    </tr>
    <tr>
        <td align="left">MS_HCCL_CM_INIT</td>
        <td align="left">Whether to initialize the HCCL using the CM method.</td>
        <td align="left">Integer</td>
        <td align="left">1 means on, and 0 means off. The default is 0.</td>
        <td align="left">It is recommended that this environment variable be on only on <b>Ascend hardware platforms with a high number of communication domains</b>. Turning on this environment variable reduces the memory footprint of the HCCL collective communication library, and training tasks are executed in the same way as `rank table` starts.</td>
    </tr>
</table>
<blockquote>
<div><p>The above environment variables should be set before each process starts and the contents of <code class="docutils literal notranslate"><span class="pre">MS_SCHED_HOST</span></code>, <code class="docutils literal notranslate"><span class="pre">MS_SCHED_PORT</span></code> and <code class="docutils literal notranslate"><span class="pre">MS_WORKER_NUM</span></code> should be consistent, otherwise the network will fail due to the inconsistent configuration of each process.</p>
</div></blockquote>
</section>
<section id="executing-training-tasks">
<h2>Executing Training Tasks<a class="headerlink" href="#executing-training-tasks" title="Permalink to this headline"></a></h2>
<p>Since the <strong>Dynamic Cluster</strong> startup script can be consistent across hardware platforms, the following is an example of how to write a startup script using 8-card distributed training on a GPU hardware platform only:</p>
<blockquote>
<div><p>The running directory of sample: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_training">distributed_training</a></p>
</div></blockquote>
<section id="1-preparing-python-training-scripts">
<h3>1. Preparing Python Training Scripts<a class="headerlink" href="#1-preparing-python-training-scripts" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
    <span class="n">init</span><span class="p">()</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=GRAPH_MODE</span></code>: To use distributed training, you need to specify the running mode as graph mode (the current version of <strong>Dynamic Cluster</strong> feature does not support PyNative mode).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init()</span></code>: Initializing the cluster. Initialize the collective communication library (NCCL in this case) according to the backend specified in the <code class="docutils literal notranslate"><span class="pre">set_context</span></code> interface, and complete the distributed training initialization operation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ms.ParallelMode.DATA_PARALLEL</span></code>: Set the training mode to data parallel mode.</p></li>
</ul>
<p>Dynamic cluster also supports <strong>secure encrypted channel</strong> features and supports <code class="docutils literal notranslate"><span class="pre">TLS/SSL</span></code> protocols to meet users’ security needs. By default, the secure encrypted channel is off. If you need to turn it on, you can call init() only after the secure encrypted channel is configured correctly through <code class="docutils literal notranslate"><span class="pre">set_ps_context</span></code>, otherwise the initialization of the cluster will fail. If you want to use the secure encrypted channel, please configure:</p>
<p><code class="docutils literal notranslate"><span class="pre">set_ps_context(config_file_path=&quot;/path/to/config_file.json&quot;,</span> <span class="pre">enable_ssl=True,</span> <span class="pre">client_password=&quot;123456&quot;,</span> <span class="pre">server_password=&quot;123456&quot;)</span></code></p>
<blockquote>
<div><p>For the detailed parameter configuration descriptions, refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">mindspore.set_ps_context</a> and and the <a class="reference internal" href="#security-authentication"><span class="std std-doc">Security Authentication</span></a> section of this document.</p>
</div></blockquote>
</section>
<section id="2-preparing-the-startup-script">
<h3>2. Preparing the Startup Script<a class="headerlink" href="#2-preparing-the-startup-script" title="Permalink to this headline"></a></h3>
<section id="single-machine-multi-card">
<h4>Single-Machine Multi-Card<a class="headerlink" href="#single-machine-multi-card" title="Permalink to this headline"></a></h4>
<p>The content of the single-machine multi-card startup script <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster.sh</span></code> is as follows. Before starting the Worker and Scheduler, you need to add the relevant environment variable settings:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Start 8 Worker training processes in a loop</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">          </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">  </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">       </span><span class="c1"># Set Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">        </span><span class="c1"># Set the started process to the MS_WORKER role</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">                      </span><span class="c1"># Set process id, optional</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">                             </span><span class="c1"># Start training script</span>
<span class="k">done</span>

<span class="c1"># Start 1 Scheduler process</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># Set Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># Set the started process to the MS_SCHED role</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># Start training script</span>
</pre></div>
</div>
<blockquote>
<div><p>The training scripts for the Scheduler and Worker processes are identical in content and startup method, because the internal processes of the two roles are handled differently in MindSpore. Users simply pull up the process in the normal training manner, without modifying the Python code by role. This is one of the reasons why dynamic cluster startup scripts can be consistent across multiple hardware platforms.</p>
</div></blockquote>
<p>A single-machine 8-card distributed training can be executed by executing the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
</section>
<section id="multi-machine-multi-card">
<h4>Multi-Machine Multi-Card<a class="headerlink" href="#multi-machine-multi-card" title="Permalink to this headline"></a></h4>
<p>The startup script needs to be split in the multi-machine training scenario. The following is an example of performing 2-machine 8-card training, with each machine executing the startup 4 Worker:</p>
<p>The script <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_1.sh</span></code> starts 1 <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> and <code class="docutils literal notranslate"><span class="pre">Worker1</span></code> to <code class="docutils literal notranslate"><span class="pre">Worker4</span></code> on node 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Start Worker1 to Worker4, 4 Worker training processes in a loop</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">4</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">                    </span><span class="c1"># Set the total number of Worker processes in the cluster to 8 (including other node processes)</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w">  </span><span class="c1"># Set the Scheduler IP address to the Node 1 IP address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">                 </span><span class="c1"># Set the Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">                  </span><span class="c1"># Set the startup process to the MS_WORKER role</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">                      </span><span class="c1"># Set process id, optional</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">                                       </span><span class="c1"># Start training script</span>
<span class="k">done</span>

<span class="c1"># Start 1 Scheduler process on node 1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">                        </span><span class="c1"># Set the total number of Worker processes in the cluster to 8 (including other node processes)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the Node 1 IP address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">                     </span><span class="c1"># Set the Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">                       </span><span class="c1"># Set the startup process to the MS_SCHED role</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># Start training script</span>
</pre></div>
</div>
<p>The script <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_2.sh</span></code> starts <code class="docutils literal notranslate"><span class="pre">Worker5</span></code> to <code class="docutils literal notranslate"><span class="pre">Worker8</span></code> on node 2 (without executing Scheduler):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Start Worker5 to Worker8, 4 Worker training processes in a loop</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">4</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">                    </span><span class="c1"># Set the total number of Worker processes in the cluster to 8 (including other node processes)</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w">  </span><span class="c1"># Set the Scheduler IP address to the Node 1 IP address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">                 </span><span class="c1"># Set the Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">                  </span><span class="c1"># Set the startup process to the MS_WORKER role</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">                      </span><span class="c1"># Set process id, optional</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">                                       </span><span class="c1"># Start training script</span>
<span class="k">done</span>
</pre></div>
</div>
<blockquote>
<div><p>The multi-machine task <code class="docutils literal notranslate"><span class="pre">MS_WORKER_NUM</span></code> should be the total number of Worker nodes in the cluster.
To keep the inter-node network connected, use the <code class="docutils literal notranslate"><span class="pre">telnet</span> <span class="pre">&lt;scheduler</span> <span class="pre">ip&gt;</span> <span class="pre">&lt;scheduler</span> <span class="pre">port&gt;</span></code> command to test whether this node is connected to the started Scheduler node.</p>
</div></blockquote>
<p>Execute on Node 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster_1.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
<p>Execute on Node 2:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster_2.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
<p>That is, you can perform 2-machine 8-card distributed training tasks.</p>
<blockquote>
<div><p>The above startup scripts are consistent across <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> and <code class="docutils literal notranslate"><span class="pre">CPU</span></code> hardware platforms, and only hardware-related code modifications such as <code class="docutils literal notranslate"><span class="pre">device_target</span></code> in the Python training scripts is performed, and we can execute dynamic cluster distributed training.</p>
</div></blockquote>
</section>
</section>
<section id="3-execution-results">
<h3>3. Execution Results<a class="headerlink" href="#3-execution-results" title="Permalink to this headline"></a></h3>
<p>The script will run in the background and the log file will be saved to the current directory. A total of 10 epochs are run, each of which has 234 steps. The results about the Loss part are saved in worker_*.log. After grep out the loss value, the example is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 234, loss is 2.0084016
epoch: 2 step: 234, loss is 1.6407638
epoch: 3 step: 234, loss is 1.6164391
epoch: 4 step: 234, loss is 1.6838071
epoch: 5 step: 234, loss is 1.6320667
epoch: 6 step: 234, loss is 1.3098773
epoch: 7 step: 234, loss is 1.3515002
epoch: 8 step: 234, loss is 1.2943741
epoch: 9 step: 234, loss is 1.2316195
epoch: 10 step: 234, loss is 1.1533381
</pre></div>
</div>
</section>
</section>
<section id="disaster-recovery">
<h2>Disaster Recovery<a class="headerlink" href="#disaster-recovery" title="Permalink to this headline"></a></h2>
<p>Model training requires high reliability and serviceability of distributed training architecture. MindSpore supports disaster recovery under data parallelism, and the training tasks continue to be executed normally after the processes in the cluster (multiple Workers and 1 Scheduler) of the multi-card data parallel training scenario exit abnormally and are pulled up again.</p>
<p>Scenario constraints:
In graph mode, <code class="docutils literal notranslate"><span class="pre">MindData</span></code> is used for data sink mode training. Data parallel mode is turned on, and the Worker process is pulled up using the non-<code class="docutils literal notranslate"><span class="pre">OpenMPI</span></code> approach described above.</p>
<p>In the above scenario, if a node is interrupted during the training process, it is guaranteed that the training can continue after pulling up the corresponding script of the corresponding process for the same environment variables (<code class="docutils literal notranslate"><span class="pre">MS_ENABLE_RECOVERY</span></code> and <code class="docutils literal notranslate"><span class="pre">MS_RECOVERY_PATH</span></code>), and the accuracy convergence is not affected.</p>
<ol class="arabic">
<li><p>Enable disaster recovery:</p>
<p>Enabling disaster recovery through environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                 </span><span class="c1"># Enable disaster recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/path/to/recovery/<span class="w">  </span><span class="c1"># Configure the persistence path file</span>
</pre></div>
</div>
</li>
<li><p>Configure the checkpoint save interval. The sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span>

<span class="n">ckptconfig</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt_of_rank_/&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">get_rank</span><span class="p">()),</span> <span class="n">config</span><span class="o">=</span><span class="n">ckptconfig</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>Each worker is enabled to save checkpoint and use different paths (e.g., the directory in the above sample is set using rank id to ensure that the paths are not the same) to prevent conflicts in saving checkpoints with the same name. The checkpoint is used for abnormal process recovery and normal process rollback. The rollback of training means that each Worker in the cluster is restored to the state corresponding to the latest checkpoint, while the data side is also rolled back to the corresponding step, and then training continues. The interval between checkpoint saves is configurable, which determines the granularity of disaster recovery. The smaller the interval, the smaller the number of steps rolled back to the checkpoint saved last time, but frequent checkpoint saves may also affect training efficiency. The larger the interval, the opposite effect. keep_checkpoint_max is set to at least 2 (to prevent checkpoint save failures).</p>
<blockquote>
<div><p>The running directory of sample: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_training">distributed_training</a>.</p>
</div></blockquote>
<p>The scripts involved are <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_recovery.sh</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_gpu_recovery.py</span></code>, and <code class="docutils literal notranslate"><span class="pre">resnet.py</span></code>. The script contents <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_recovery.sh</span></code> are as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster_recovery.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster_recovery.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                 </span><span class="c1"># Enable disaster recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/path/to/recovery/<span class="w">  </span><span class="c1"># Configure the persistence path file</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Start 1 Scheduler process</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># Set Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># Set the started process to the MS_SCHED role</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">             </span><span class="c1"># Set Node ID as &#39;sched&#39;</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Start 8 Worker training process in a loop</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># Set Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">            </span><span class="c1"># Set the started process to the MS_WORKER role</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>worker_<span class="nv">$i</span><span class="w">         </span><span class="c1"># Set Node ID as &#39;worker_$i&#39;</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>Before starting Worker and Scheduler, you need to add relevant environment variables settings, such as IP and Port of Scheduler, and whether the role of the current process is Worker or Scheduler.</p>
<p>Execute the following command to start a single-machine 8-card data parallel training</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_gpu_cluster_recovery.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
<p>Distributed training starts, and if an abnormal case is encountered during training, such as a process quitting abnormally, and then restarting the corresponding process, the training process can be recovered:
For example, if the Scheduler process abnormally exits during training, the following command can be executed to restart the Scheduler:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span>YOUR_DATA_PATH
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                </span><span class="c1"># Enable disaster recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/path/to/recovery/<span class="w"> </span><span class="c1"># Configure the persistence path file</span>

<span class="nb">cd</span><span class="w"> </span>./device

<span class="c1"># 启动1个Scheduler进程</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># Set the number of Worker processes in the cluster to 8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># Set the Scheduler IP address to the local loop address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># Set Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># Set the started process to the MS_SCHED role</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">             </span><span class="c1"># Set Node ID as &#39;sched&#39;</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Worker and Scheduler cluster is automatically restored.</p>
<p>Worker processes with abnormal exit are handled in a similar way (Note: Worker processes with abnormal exit need to wait 30s before pulling up to resume training. Before that, Scheduler rejects Worker with the same node id to register again in order to prevent network jitter and malicious registration).</p>
</section>
<section id="security-authentication">
<h2>Security Authentication<a class="headerlink" href="#security-authentication" title="Permalink to this headline"></a></h2>
<p>To support SSL security authentication between nodes/processes, to enable security authentication, configure <code class="docutils literal notranslate"><span class="pre">enable_ssl=True</span></code> via the Python API <code class="docutils literal notranslate"><span class="pre">mindspore.set_ps_context</span></code> (defaults to False when not passed in, indicating that SSL security authentication is not enabled). The config.json configuration file specified by config_file_path needs to add the following fields:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;server_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;server.p12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;crl_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;client_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;client.p12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;ca_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ca.crt&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cipher_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ECDHE-R SA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-DSS-AES256-GCM-SHA384:DHE-PSK-AES128-GCM-SHA256:DHE-PSK-AES256-GCM-SHA384:DHE-PSK-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-PSK-CHACHA20-POLY1305:DHE-RSA-AES128-CCM:DHE-RSA-AES256-CCM:DHE-RSA-CHACHA20-POLY1305:DHE-PSK-AES128-CCM:DHE-PSK-AES256-CCM:ECDHE-ECDSA-AES128-CCM:ECDHE-ECDSA-AES256-CCM:ECDHE-ECDSA-CHACHA20-POLY1305&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cert_expire_warning_time_in_day&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">90</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>server_cert_path: The path to the p12 file (SSL-specific certificate file) that contains the cipher text of the certificate and the secret key on the server side.</p></li>
<li><p>The file path to the revocation list (used to distinguish invalid untrusted certificates from valid trusted certificates).</p></li>
<li><p>client_cert_path: The client contains the path to the p12 file (SSL-specific certificate file) with the cipher text of the certificate and secret key.</p></li>
<li><p>ca_cert_path: The path to root certificate</p></li>
<li><p>cipher_list: Cipher suite (list of supported SSL encrypted types)</p></li>
<li><p>cert_expire_warning_time_in_da: The warning time of certificate expiration.</p></li>
</ul>
<p>The secret key in the p12 file is stored in cipher text, and the password needs to be passed in when starting. Please refer to the Python API <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">mindspore.set_ps_context</a> for the <code class="docutils literal notranslate"><span class="pre">client_password</span></code> and <code class="docutils literal notranslate"><span class="pre">server_password</span></code> fields.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="startup_method.html" class="btn btn-neutral float-left" title="Distributed Parallel Startup Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ms_operator.html" class="btn btn-neutral float-right" title="Performing Distributed Training on K8S Clusters" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>