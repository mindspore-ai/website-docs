<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Host&amp;Device Heterogeneous &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Inference" href="distributed_inference.html" />
    <link rel="prev" title="Recomputation" href="recompute.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_mode.html">Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Host&amp;Device Heterogeneous</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/host_device_training.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="hostdevice-heterogeneous">
<h1>Host&amp;Device Heterogeneous<a class="headerlink" href="#hostdevice-heterogeneous" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/host_device_training.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In deep learning, one usually has to deal with the huge model problem, in which the total size of parameters in the model is beyond the device memory capacity. To efficiently train a huge model, one solution is to employ homogeneous accelerators (<em>e.g.</em>, Ascend 910 AI Accelerator and GPU) for distributed training. When the size of a model is hundreds of GBs or several TBs, the number of required accelerators is too overwhelming for people to access, resulting in this solution inapplicable.  One alternative is Host+Device hybrid training. This solution simultaneously leveraging the huge memory in hosts and fast computation in accelerators, is a promisingly efficient method for addressing huge model problem.</p>
<p>In MindSpore, users can easily implement hybrid training by configuring trainable parameters and necessary operators to run on hosts, and other operators to run on accelerators. This tutorial introduces how to train <a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/recommend/Wide_and_Deep">Wide&amp;Deep</a> in the Host+Ascend 910 AI Accelerator mode.</p>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>Pipeline parallel and operator-level parallel are suitable for the model to have a large number of operators, and the parameters are more evenly distributed among the operators. What if the number of operators in the model is small, and the parameters are concentrated in only a few operators? Wide &amp; Deep is an example of this, as shown in the image below. The Embedding table in Wide &amp; Deep can be trained as a parameter of hundreds of GIGabytes or even a few terabytes. If it is executed on an accelerator ( device ) , the number of accelerators required is huge, and the training cost is expensive. On the other hand, if you use accelerator computing, the training acceleration obtained is limited, and it will also trigger cross-server traffic, and the end-to-end training efficiency will not be very high.</p>
<p><img alt="image" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/parallel/images/host_device_image_0_zh.png" /></p>
<p><em>Figure: Part of the structure of the Wide &amp; Deep model</em></p>
<p>A careful analysis of the special structure of the Wide &amp; Deep model can be obtained: although the Embedding table has a huge amount of parameters, it participates in very little computation, and the Embedding table and its corresponding operator, the EmbeddingLookup operator, can be placed on the Host side, by using the CPU for calculation, and the rest of the operators are placed on the accelerator side. This can take advantage of the large amount of memory on the Host side and the fast computing of the accelerator side, while taking advantage of the high bandwidth of the Host to accelerator of the same server. The following diagram shows how Wide &amp; Deep heterogeneous slicing works:</p>
<p><img alt="" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/parallel/images/host_device_image_1_zh.png" /></p>
<p><em>Figure: Wide &amp; Deep Heterogeneous Approach</em></p>
</section>
<section id="practices">
<h2>Practices<a class="headerlink" href="#practices" title="Permalink to this headline"></a></h2>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Prepare the model code. The Wide&amp;Deep code can be found at: <a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/recommend/Wide_and_Deep">https://gitee.com/mindspore/models/tree/master/official/recommend/Wide_and_Deep</a>, in which <code class="docutils literal notranslate"><span class="pre">train_and_eval_auto_parallel.py</span></code> defines the main function for model training, <code class="docutils literal notranslate"><span class="pre">src/</span></code> directory contains the model definition, data processing and configuration files, and <code class="docutils literal notranslate"><span class="pre">script/</span></code> directory contains the training scripts in different modes.</p></li>
<li><p>Prepare the dataset. Please refer the link in [1] to download the dataset, and use the script <code class="docutils literal notranslate"><span class="pre">src/preprocess_data.py</span></code> to transform dataset into MindRecord format.</p></li>
<li><p>Configure the device information. When performing distributed training in the bare-metal environment (That is, there is an Ascend 910 AI processor locally), the network information file needs to be configured. This example only employs one accelerator, thus <code class="docutils literal notranslate"><span class="pre">rank_table_1p_0.json</span></code> containing #0 accelerator is configured. MindSpore provides an automated build script for generating this configuration file and related instructions. For the detailed, see <a class="reference external" href="https://gitee.com/mindspore/models/tree/master/utils/hccl_tools">HCCL_TOOL</a>.</p></li>
</ol>
</section>
<section id="configuring-for-hybrid-training">
<h3>Configuring for Hybrid Training<a class="headerlink" href="#configuring-for-hybrid-training" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Configure the flag of hybrid training. In the file <code class="docutils literal notranslate"><span class="pre">default_config.yaml</span></code>, change the default value of <code class="docutils literal notranslate"><span class="pre">host_device_mix</span></code> to be <code class="docutils literal notranslate"><span class="pre">1</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">host_device_mix</span><span class="p">:</span> <span class="mi">1</span>
</pre></div>
</div>
</li>
<li><p>Check the deployment of necessary operators and optimizers. In class <code class="docutils literal notranslate"><span class="pre">WideDeepModel</span></code> of file <code class="docutils literal notranslate"><span class="pre">src/wide_and_deep.py</span></code>, check the execution of <code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code> is at host:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">deep_embeddinglookup</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">wide_embeddinglookup</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span>
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">TrainStepWrap(nn.Cell)</span></code> of file <code class="docutils literal notranslate"><span class="pre">src/wide_and_deep.py</span></code>, check two optimizers are also executed at host:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_w</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
<span class="bp">self</span><span class="o">.</span><span class="n">optimizer_d</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="training-the-model">
<h3>Training the Model<a class="headerlink" href="#training-the-model" title="Permalink to this headline"></a></h3>
<p>In order to save enough log information, use the command <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOG_v=1</span></code> to set the log level to INFO before executing the script, and add the <code class="docutils literal notranslate"><span class="pre">-p</span> <span class="pre">on</span></code> option when compiling MindSpore. For the details about compiling MindSpore, refer to <a class="reference external" href="https://www.mindspore.cn/install/detail/en?path=install/master/mindspore_ascend_install_source_en.md&amp;highlight=%E7%BC%96%E8%AF%91mindspore">Compiling MindSpore</a>.</p>
<p>Use the script <code class="docutils literal notranslate"><span class="pre">script/run_auto_parallel_train.sh</span></code>. Run the command <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">run_auto_parallel_train.sh</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">&lt;DATASET_PATH&gt;</span> <span class="pre">&lt;RANK_TABLE_FILE&gt;</span></code>, where the first <code class="docutils literal notranslate"><span class="pre">1</span></code> is the number of cards used in the case, the second <code class="docutils literal notranslate"><span class="pre">1</span></code> is the number of epochs, <code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code> is the path of dataset, and <code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code> is the path of the above <code class="docutils literal notranslate"><span class="pre">rank_table_1p_0.json</span></code> file.</p>
<p>The running log is in the directory of <code class="docutils literal notranslate"><span class="pre">device_0</span></code>, where <code class="docutils literal notranslate"><span class="pre">loss.log</span></code> contains every loss value of every step in the epoch. Here is an example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, wide_loss is 0.6873926, deep_loss is 0.8878349
epoch: 1 step: 2, wide_loss is 0.6442529, deep_loss is 0.8342661
epoch: 1 step: 3, wide_loss is 0.6227323, deep_loss is 0.80273706
epoch: 1 step: 4, wide_loss is 0.6107221, deep_loss is 0.7813441
epoch: 1 step: 5, wide_loss is 0.5937832, deep_loss is 0.75526017
epoch: 1 step: 6, wide_loss is 0.5875453, deep_loss is 0.74038756
epoch: 1 step: 7, wide_loss is 0.5798845, deep_loss is 0.7245408
epoch: 1 step: 8, wide_loss is 0.57553077, deep_loss is 0.7123517
epoch: 1 step: 9, wide_loss is 0.5733629, deep_loss is 0.70278376
epoch: 1 step: 10, wide_loss is 0.566089, deep_loss is 0.6884129
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">test_deep0.log</span></code> contains the runtime log.
Search <code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code> in <code class="docutils literal notranslate"><span class="pre">test_deep0.log</span></code>, the following can be found:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:34.928.275 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/network-VirtualDatasetCellTriple/_backbone-NetWithLossClass/network-WideDeepModel/EmbeddingLookup-op297 costs 3066 us.
[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:34.943.896 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/network-VirtualDatasetCellTriple/_backbone-NetWithLossClass/network-WideDeepModel/EmbeddingLookup-op298 costs 15521 us.
</pre></div>
</div>
<p>The above shows the running time of <code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code> on the host.</p>
<p>Search <code class="docutils literal notranslate"><span class="pre">FusedSparseFtrl</span></code> and <code class="docutils literal notranslate"><span class="pre">FusedSparseLazyAdam</span></code> in <code class="docutils literal notranslate"><span class="pre">test_deep0.log</span></code>, the following can be found:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:35.422.963 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/optimizer_w-FTRL/FusedSparseFtrl-op299 costs 54492 us.
[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:35.565.953 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/optimizer_d-LazyAdam/FusedSparseLazyAdam-op300 costs 142865 us.
</pre></div>
</div>
<p>The above shows the running time of two optimizers on the host.</p>
</section>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h2>
<p>[1] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He. <a class="reference external" href="https://doi.org/10.24963/ijcai.2017/239">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction.</a> IJCAI 2017.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="recompute.html" class="btn btn-neutral float-left" title="Recomputation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_inference.html" class="btn btn-neutral float-right" title="Distributed Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>