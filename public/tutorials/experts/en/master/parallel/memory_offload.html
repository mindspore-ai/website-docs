<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Heterogeneous Storage &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom Operators (Custom-based)" href="../operation/op_custom.html" />
    <link rel="prev" title="Sharding Propagation" href="sharding_propagation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_mode.html">Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pangu_alpha.html">PengCheng·PanGu Model Network Multi-dimension Hybrid Parallel Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_fusion.html">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_subgraph.html">Communication Subgraph Extraction and Reuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_graph_partition.html">Distributed Graph Partition</a></li>
<li class="toctree-l2"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="sharding_propagation.html">Sharding Propagation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Heterogeneous Storage</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed High-Level Configuration Case</a> &raquo;</li>
      <li>Heterogeneous Storage</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/memory_offload.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="heterogeneous-storage">
<h1>Heterogeneous Storage<a class="headerlink" href="#heterogeneous-storage" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/memory_offload.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In recent years Transformer-based large models have made rapid progress in various downstream tasks in nlp and vision, and often the larger the model, the higher the accuracy achieved in downstream tasks. The model size develops from hundreds of millions to hundreds of billions, however, large model training consumes a large amount of computational storage resources and the training overhead is huge.</p>
<p>Large model training is limited by the size of the video memory, and the number of model parameters that can be stored on a single card is limited. With model parallel, we can split large models into different machines, and after introducing the necessary inter-process communication, we can conduct collaborative training in clusters, where the model size is proportional to the machine size. At the same time, when the model size exceeds the memory capacity of a single machine, the overhead of inter-machine communication in model parallel will become larger, and the resource utilization will decrease significantly. How to train larger models on a single machine and avoid inter-machine communication in model parallel has become the key to improve the performance of large model training.</p>
<p>Heterogeneous storage management enables 10x to 100x storage expansion of model parameters, thus breaking the memory limitation of large model training and realizing low-cost large model training. This tutorial will explain the basic principles of heterogeneous storage management and introduce the related configuration parameters and their use. With this feature, developers can use the same hardware to train larger models.</p>
</section>
<section id="basic-principles-of-heterogeneous-storage-management">
<h2>Basic Principles of Heterogeneous Storage Management<a class="headerlink" href="#basic-principles-of-heterogeneous-storage-management" title="Permalink to this headline"></a></h2>
<p>During training, the main stored data consists of parameters and intermediate results:</p>
<ul class="simple">
<li><p>Parameters: data such as the weights of the model and the state of the optimizer, which need to be stored all the time during the training process.</p></li>
<li><p>Intermediate results: data generated by the computation in the forward, backward and optimization processes, which can be released and deleted after the corresponding computation is completed.</p></li>
</ul>
<p>Through heterogeneous storage management, parameters or intermediate results that do not need to participate in computation temporarily can be copied to the memory of Host side or even hard disk storage during the training process, and then copied and restored to the device side when the data is needed to participate in computation. By the above means, the model size that can be trained by the same hardware device can be increased.</p>
<p><img alt="image.png" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/tutorials/experts/source_zh_cn/parallel/images/memory_offload.png" /></p>
</section>
<section id="code-example">
<h2>Code Example<a class="headerlink" href="#code-example" title="Permalink to this headline"></a></h2>
<p>Taking the ResNet-50 network as an example, for the code implementation, refers to <a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/memory_offload">example</a>. The directory structure is shown below, where resnet.py is the ResNet-50 network implementation, cifa_resnet50.py is the training script, and run.sh is the execution script.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ memory_offload
       ├── resnet.py
       ├── cifa_resnet50.py
       ├── run.sh
    ...
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh<span class="w"> </span>Ascend<span class="w"> </span><span class="m">512</span><span class="w"> </span>OFF
</pre></div>
</div>
<p>When training with batch_size=512 without turning on heterogeneous storage, an ‘Out of Memory’ error occurs due to insufficient memory space:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>----------------------------------------------------
-<span class="w"> </span>Framework<span class="w"> </span>Error<span class="w"> </span>Message:
----------------------------------------------------
Out<span class="w"> </span>of<span class="w"> </span>Memory!!!<span class="w"> </span>Request<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>33100113920B,<span class="w"> </span>Memory<span class="w"> </span>Statistic:
Device<span class="w"> </span>HBM<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>32768M
MindSpore<span class="w"> </span>Used<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>30684M
MindSpore<span class="w"> </span>memory<span class="w"> </span>base<span class="w"> </span>address:<span class="w"> </span>0x124140000000
Total<span class="w"> </span>Static<span class="w"> </span>Memory<span class="w"> </span>size:<span class="w"> </span>496M
Total<span class="w"> </span>Dynamic<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>0M
Dynamic<span class="w"> </span>memory<span class="w"> </span>size<span class="w"> </span>of<span class="w"> </span>this<span class="w"> </span>graph:<span class="w"> </span>0M

Please<span class="w"> </span>try<span class="w"> </span>to<span class="w"> </span>reduce<span class="w"> </span><span class="s1">&#39;batch_size&#39;</span><span class="w"> </span>or<span class="w"> </span>check<span class="w"> </span>whether<span class="w"> </span>exists<span class="w"> </span>extra<span class="w"> </span>large<span class="w"> </span>shape.<span class="w"> </span>For<span class="w"> </span>more<span class="w"> </span>details,<span class="w"> </span>please<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span><span class="s1">&#39;Out of Memory&#39;</span><span class="w"> </span>at<span class="w"> </span>https://www.mindspore.cn<span class="w"> </span>.

----------------------------------------------------
-<span class="w"> </span>C++<span class="w"> </span>Call<span class="w"> </span>Stack:<span class="w"> </span><span class="o">(</span>For<span class="w"> </span>framework<span class="w"> </span>developers<span class="o">)</span>
----------------------------------------------------
mindspore/ccsrc/plugin/device/ascend/hal/hardware/ascend_kernel_executor.cc:252<span class="w"> </span>PreprocessBeforeRunGraph
mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_memory_adapter.cc:169<span class="w"> </span>MallocDynamicDevMem
</pre></div>
</div>
<p>After turning on heterogeneous storage, batch_size=512 is used normally during training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh<span class="w"> </span>Ascend<span class="w"> </span><span class="m">512</span><span class="w"> </span>ON
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">111</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.1563000679016113
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">112</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.1421408653259277
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">113</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.129314422607422
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">114</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.127141237258911
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">115</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.1191487312316895
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">116</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.1299633979797363
epoch:<span class="w"> </span><span class="m">1</span><span class="w"> </span>step:<span class="w"> </span><span class="m">117</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.138218402862549
</pre></div>
</div>
<p>Heterogeneous storage configuration and switch code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>

<span class="n">offload_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;auto_offload&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                  <span class="s2">&quot;offload_cpu_size&quot;</span><span class="p">:</span> <span class="s2">&quot;512GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_disk_size&quot;</span><span class="p">:</span> <span class="s2">&quot;1024GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./offload/&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;host_mem_block_size&quot;</span><span class="p">:</span><span class="s2">&quot;1GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;enable_aio&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s2">&quot;enable_pinned_mem&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">memory_offload</span><span class="o">=</span><span class="s1">&#39;ON&#39;</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="s1">&#39;30GB&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="o">=</span><span class="n">offload_config</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">offload_config</span></code> is the configuration option for heterogeneous storage, where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;:</span> <span class="pre">&quot;cpu&quot;</span></code> sets the model parameters to be stored on the cpu memory, loaded to the device side only when the data is needed to be used during training, and then unloaded to the cpu memory as soon as use is complete.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;auto_offload&quot;:</span> <span class="pre">False</span></code> sets the auto-offload policy to be turned off and the parameter data will be installed with the previous configuration option.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;offload_cpu_size&quot;:</span> <span class="pre">&quot;512GB&quot;,</span> <span class="pre">&quot;offload_disk_size&quot;:</span> <span class="pre">&quot;1024GB&quot;</span></code> sets the amount of cpu memory and disk size that can be used for offload, respectively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;offload_path&quot;:</span> <span class="pre">&quot;.</span> <span class="pre">/offload/&quot;</span></code> sets the path to the disk file to be used for the offload.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enable_pinned_mem&quot;:</span> <span class="pre">True</span></code> sets page locking to be turned on, which when turned on speeds up copying between HBM-CPU memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;host_mem_block_size&quot;:</span> <span class="pre">&quot;1GB&quot;</span></code> sets the block size of cpu page locking memory pool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enable_aio&quot;:</span> <span class="pre">True</span></code> sets to enable asynchronous IO for files, when enabled it speeds up DDR-disk-to-disk copying. (Requires compilation with -o option, and only supports Linux environments with aio installed)</p></li>
</ul>
<p>In this example, the offload_param parameter is configured as “cpu” and auto_offload is not enabled. The parameters will be stored in the cpu memory during the whole training process. When some parameters are needed to participate in the computation, the data will be copied to the device side, and after the computation is completed, it will be copied back to the cpu memory again.</p>
<section id="automatic-generation-of-offload-strategy">
<h3>Automatic Generation of offload Strategy<a class="headerlink" href="#automatic-generation-of-offload-strategy" title="Permalink to this headline"></a></h3>
<p>In addition to strictly installing user <code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;</span></code> configurations for data copying, MindSpore also supports automatic generation of heterogeneous storage strategy. MindSpore can analyze the network video memory usage information and combine it with <code class="docutils literal notranslate"><span class="pre">&quot;max_device_memory&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;offload_</span> <span class="pre">cpu_size&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;offload_disk_size&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;hbm_ratio&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cpu_ratio&quot;</span></code> configured by the user to generate a heterogeneous storage strategy, and then move the data across multiple storage media according to the established strategy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>

<span class="n">offload_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;offload_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./offload/&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;auto_offload&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_cpu_size&quot;</span><span class="p">:</span> <span class="s2">&quot;512GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_disk_size&quot;</span><span class="p">:</span> <span class="s2">&quot;1024GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;host_mem_block_size&quot;</span><span class="p">:</span><span class="s2">&quot;1GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;enable_aio&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s2">&quot;enable_pinned_mem&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">memory_offload</span><span class="o">=</span><span class="s1">&#39;ON&#39;</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="s1">&#39;30GB&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="o">=</span><span class="n">offload_config</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">&quot;auto_offload&quot;:</span> <span class="pre">True</span></code> is set, and <code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;</span></code> only affects the initial storage location of the parameter, and the framework adjusts the weights and intermediate results storage location during the computation process according to the generated strategy.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sharding_propagation.html" class="btn btn-neutral float-left" title="Sharding Propagation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../operation/op_custom.html" class="btn btn-neutral float-right" title="Custom Operators (Custom-based)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>