<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performing Distributed Training on K8S Clusters &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom Operators (Custom-based)" href="../operation/op_custom.html" />
    <link rel="prev" title="Multi-dimensional Hybrid Parallel Case Based on Double Recursive Search" href="multiple_mix.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Experimental Characteristics</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pangu_alpha.html">PengCheng·PanGu Model Network Multi-dimension Hybrid Parallel Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiple_mix.html">Multi-dimensional Hybrid Parallel Case Based on Double Recursive Search</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Performing Distributed Training on K8S Clusters</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/sdc.html">Accuracy-Sensitive Detection</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed High-Level Configuration Case</a> &raquo;</li>
      <li>Performing Distributed Training on K8S Clusters</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/ms_operator.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="performing-distributed-training-on-k8s-clusters">
<h1>Performing Distributed Training on K8S Clusters<a class="headerlink" href="#performing-distributed-training-on-k8s-clusters" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/ms_operator.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<p>MindSpore Operator is a plugin that follows Kubernetes’ Operator pattern (based on the CRD-Custom Resource Definition feature) and implements distributed training on Kubernetes. MindSpore Operator defines Scheduler, PS, worker three roles in CRD, and users can easily use MindSpore on K8S for distributed training through simple YAML file configuration. The code repository of mindSpore Operator is described in: <a class="reference external" href="https://gitee.com/mindspore/ms-operator/">ms-operator</a>.</p>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<p>There are three installation methods:</p>
<ol class="arabic">
<li><p>Install directly by using YAML</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>deploy/v1/ms-operator.yaml
</pre></div>
</div>
<p>After installation:</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">pods</span> <span class="pre">--all-namespaces</span></code> to see the namespace as the deployment task for the ms-operator-system.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">describe</span> <span class="pre">pod</span> <span class="pre">ms-operator-controller-manager-xxx-xxx</span> <span class="pre">-n</span> <span class="pre">ms-operator-system</span></code> to view pod details.</p>
</li>
<li><p>Install by using make deploy</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>make<span class="w"> </span>deploy<span class="w"> </span><span class="nv">IMG</span><span class="o">=</span>swr.cn-south-1.myhuaweicloud.com/mindspore/ms-operator:latest
</pre></div>
</div>
</li>
<li><p>Local debugging environment</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>make<span class="w"> </span>run
</pre></div>
</div>
</li>
</ol>
</section>
<section id="sample">
<h2>Sample<a class="headerlink" href="#sample" title="Permalink to this headline"></a></h2>
<p>The current ms-operator supports ordinary single worker training, single worker training in PS mode, and Scheduler and Worker startups for automatic parallelism (such as data parallelism and model parallelism).</p>
<p>There are running examples in <a class="reference external" href="https://gitee.com/mindspore/ms-operator/tree/master/config/samples">config/samples/</a>. Take the data-parallel Scheduler and Worker startup as an example, where the dataset and network scripts need to be prepared in advance:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>config/samples/ms_wide_deep_dataparallel.yaml
</pre></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">all</span> <span class="pre">-o</span> <span class="pre">wide</span></code> to see scheduler and worker launched in the cluster, as well as the services corresponding to Scheduler.</p>
</section>
<section id="development-guide">
<h2>Development Guide<a class="headerlink" href="#development-guide" title="Permalink to this headline"></a></h2>
<section id="core-code">
<h3>Core Code<a class="headerlink" href="#core-code" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">pkg/apis/v1/msjob_types.go</span></code> is the CRD definition for MSJob.</p>
<p><code class="docutils literal notranslate"><span class="pre">pkg/controllers/v1/msjob_controller.go</span></code> is the core logic of the MSJob controller.</p>
</section>
<section id="image-creation-and-uploading">
<h3>Image Creation and Uploading<a class="headerlink" href="#image-creation-and-uploading" title="Permalink to this headline"></a></h3>
<p>To modify the ms-operator code and create an upload image, please refer to the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>make<span class="w"> </span>docker-build<span class="w"> </span><span class="nv">IMG</span><span class="o">={</span>image_name<span class="o">}</span>:<span class="o">{</span>tag<span class="o">}</span>
docker<span class="w"> </span>push<span class="w"> </span><span class="o">{</span>image_name<span class="o">}</span>:<span class="o">{</span>tag<span class="o">}</span>
</pre></div>
</div>
</section>
<section id="yaml-file-configuration-instructions">
<h3>YAML File Configuration Instructions<a class="headerlink" href="#yaml-file-configuration-instructions" title="Permalink to this headline"></a></h3>
<p>Taking the data parallelization of self-developed networking as an example, the YAML configuration of MSJob is introduced, such as <code class="docutils literal notranslate"><span class="pre">runPolicy</span></code>, <code class="docutils literal notranslate"><span class="pre">successPolicy</span></code>, the number of roles, mindspore images, and file mounting, and users need to configure it according to their actual needs.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mindspore.gitee.com/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MSJob</span><span class="w">  </span><span class="c1"># ms-operator custom CRD type, MSJob</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ms-widedeep-dataparallel</span><span class="w">  </span><span class="c1"># Task name</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">runPolicy</span><span class="p">:</span><span class="w"> </span><span class="c1"># RunPolicy encapsulates various runtime strategies for distributed training jobs, such as how to clean up resources and how long the job can remain active.</span>
<span class="w">    </span><span class="nt">cleanPodPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">None</span><span class="w">   </span><span class="c1"># All/Running/None</span>
<span class="w">  </span><span class="nt">successPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">AllWorkers</span><span class="w"> </span><span class="c1"># The condition that marks MSJob as subcess, which defaults to blank, represents the use of the default rule (success after a single worker execution is completed)</span>
<span class="w">  </span><span class="nt">msReplicaSpecs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">Scheduler</span><span class="p">:</span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># The number of Scheduler</span>
<span class="w">      </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Never</span><span class="w">  </span><span class="c1"># Restart the policy Always，OnFailure，Never</span>
<span class="w">      </span><span class="nt">template</span><span class="p">:</span>
<span class="w">        </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">          </span><span class="nt">volumes</span><span class="p">:</span><span class="w"> </span><span class="c1"># File mounts, such as datasets, network scripts, and so on</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">script-data</span>
<span class="w">              </span><span class="nt">hostPath</span><span class="p">:</span>
<span class="w">                </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/absolute_path</span>
<span class="w">          </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mindspore</span><span class="w"> </span><span class="c1"># Each character must have a container with only one mindspore name, configure containerPort to adjust the default port number (2222), and you need to set the port name to msjob-port</span>
<span class="w">              </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mindspore-image-name:tag</span><span class="w"> </span><span class="c1"># mindspore image</span>
<span class="w">              </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span>
<span class="w">              </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="c1"># Execute the command after the container starts</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/bin/bash</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-c</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python -s /absolute_path/train_and_eval_distribute.py --device_target=&quot;GPU&quot; --epochs=1 --data_path=/absolute_path/criteo_mindrecord  --batch_size=16000</span>
<span class="w">              </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/absolute_path</span>
<span class="w">                  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">script-data</span>
<span class="w">              </span><span class="nt">env</span><span class="p">:</span><span class="w">  </span><span class="c1"># Configurable environment variables</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">GLOG_v</span>
<span class="w">                  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1&quot;</span>
<span class="w">    </span><span class="nt">Worker</span><span class="p">:</span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"> </span><span class="c1"># The number of Worker</span>
<span class="w">      </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Never</span>
<span class="w">      </span><span class="nt">template</span><span class="p">:</span>
<span class="w">        </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">          </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">script-data</span>
<span class="w">              </span><span class="nt">hostPath</span><span class="p">:</span>
<span class="w">                </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/absolute_path</span>
<span class="w">          </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mindspore</span>
<span class="w">              </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mindspore-image-name:tag</span><span class="w"> </span><span class="c1"># mindspore image</span>
<span class="w">              </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span>
<span class="w">              </span><span class="nt">command</span><span class="p">:</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/bin/bash</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-c</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python -s /absolute_path/train_and_eval_distribute.py --device_target=&quot;GPU&quot; --epochs=1 --data_path=/absolute_path/criteo_mindrecord --batch_size=16000</span>
<span class="w">              </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/absolute_path</span>
<span class="w">                  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">script-data</span>
<span class="w">              </span><span class="nt">env</span><span class="p">:</span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">GLOG_v</span>
<span class="w">                  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1&quot;</span>
<span class="w">              </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="c1"># Resource limit configuration</span>
<span class="w">                </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">                  </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
</section>
<section id="frequent-questions">
<h3>Frequent Questions<a class="headerlink" href="#frequent-questions" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>If you find that gcr.io/distroless/static cannot be pulled during the image construction process, see <a class="reference external" href="https://github.com/anjia0532/gcr.io_mirror/issues/169">issue</a>.</p></li>
<li><p>During the installation and deployment process, when finding that the gcr.io/kubebuilder/kube-rbac-proxy cannot be pulled, see <a class="reference external" href="https://github.com/anjia0532/gcr.io_mirror/issues/153">issue</a>.</p></li>
<li><p>When you call up tasks through k8s in the GPU and need to use NVIDIA graphics cards, you need to install k8s device plugin, nvidia-docker2 and other environments.</p></li>
<li><p>Do not use underscores in YAML file configuration items.</p></li>
<li><p>When k8s is blocked but the cause cannot be determined by the pod log, view the log of the pod creation process via <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">logs</span> <span class="pre">$(kubectl</span> <span class="pre">get</span> <span class="pre">statefulset,pods</span> <span class="pre">-o</span> <span class="pre">wide</span> <span class="pre">--all</span> <span class="pre">-namespaces|grep</span> <span class="pre">ms-operator-system|awk-F&quot;&quot;'{print$2}')</span> <span class="pre">-n</span> <span class="pre">ms-operator-system</span></code>.</p></li>
<li><p>Performing tasks through the pod, it will be executed in the root directory of the launched container, and the relevant files generated will be stored in the root directory by default. But if the mapping path is only a directory under the root directory, the generated files will not be mapped and saved to the host. It is recommended to switch the path to the specified directory before officially performing the task, so as to save the files generated during the execution of the task.</p></li>
<li><p>In the disaster recovery scenario, if bindIP failed occurs, confirm whether the persistence file generated by the last training has not been cleaned.</p></li>
<li><p>It is not recommended to redirect log files directly in YAML. If redirection is required, distinguish between redirect log file names for different pods.</p></li>
<li><p>When there are residual processes or other processes on the Device, the pod may be in Pending state due to the inability to apply for all the resources, and it is recommended that the user set a timeout strategy to avoid being blocked all the time.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="multiple_mix.html" class="btn btn-neutral float-left" title="Multi-dimensional Hybrid Parallel Case Based on Double Recursive Search" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../operation/op_custom.html" class="btn btn-neutral float-right" title="Custom Operators (Custom-based)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>