<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallelism Overview &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Parallel Startup Methods" href="startup_method.html" />
    <link rel="prev" title="Dependency Control" href="../network/dependency_control.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_mode.html">Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Parallelism Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/overview.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallelism-overview">
<h1>Distributed Parallelism Overview<a class="headerlink" href="#distributed-parallelism-overview" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/overview.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<p>In deep learning, as the size of the dataset and number of parameters grows, the time and hardware resources required for training will increase and eventually become a bottleneck that constrains training. Distributed parallel training, which can reduce the demand on hardware such as memory and computational performance, is an important optimization means to perform training. In addition, distributed parallel is important for large model training and inference, which provides powerful computational capabilities and performance advantages for handling large-scale data and complex models.</p>
<p>To implement distributed parallel training and inference, you can refer to the following guidelines:</p>
<section id="startup-methods">
<h2>Startup Methods<a class="headerlink" href="#startup-methods" title="Permalink to this headline"></a></h2>
<p>MindSpore currently supports three startup methods:</p>
<ul class="simple">
<li><p><strong>Dynamic Networking</strong>: Start via MindSpore internal dynamic grouping module, no dependency on external configurations or modules, Ascend/GPU/CPU support.</p></li>
<li><p><strong>mpirun</strong>: Start via the multi-process communication library OpenMPI, with Ascend/GPU support.</p></li>
<li><p><strong>rank table</strong>: After configuring the rank_table table, Ascend is supported by start scripts and processes corresponding to the number of cards.</p></li>
</ul>
<p>Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/startup_method.html">Startup Methods</a> section for details.</p>
</section>
<section id="parallel-modes">
<h2>Parallel Modes<a class="headerlink" href="#parallel-modes" title="Permalink to this headline"></a></h2>
<p>Currently MindSpore can take the following parallel mode, and you can choose according to your needs:</p>
<ul class="simple">
<li><p><strong>Data Parallel Mode</strong>: In data parallel mode, the dataset can be split in sample dimensions and distributed to different cards. If your dataset is large and the model parameters scale is able to operate on a single card, you can choose this parallel model. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/data_parallel.html">Data Parallel</a> tutorial for more information.</p></li>
<li><p><strong>Automatic Parallel Mode</strong>: a distributed parallel mode that combines data parallel and operator-level model parallel. It can automatically build cost models, find the parallel strategy with shorter training time, and select the appropriate parallel mode for the user. If your dataset and model parameters are large in size, and you want to configure the parallel strategy automatically, you can choose this parallel model. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/auto_parallel.html">Automatic Parallelism</a> tutorial for more information.</p></li>
<li><p><strong>Semi-Automatic Parallel Mode</strong>: Compared with automatic parallel, this mode requires the user to manually configure a slice strategy for the operators to realize parallel. If your dataset and model parameters are large, and you are familiar with the structure of the model, and know which “key operators” are prone to become computational bottlenecks to configure the appropriate slice strategy for the “key operators” to achieve better performance, you can choose this mode. Parallel mode. This mode also allows you to manually configure optimizer parallel and pipeline parallel. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/semi_auto_parallel.html">Semi-Automatic Parallel</a> tutorial for more information.</p></li>
<li><p><strong>Manual Parallel Mode</strong>: In manual parallel mode, you can manually implement parallel communication of models in distributed systems by transferring data based on communication operators such as <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>, <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, <code class="docutils literal notranslate"><span class="pre">Broadcast</span></code> and other communication operators. You can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/manual_parallel.html">Manual Parallel</a> tutorial for more information.</p></li>
<li><p><strong>Parameter Server Mode</strong>: parameter servers offer better flexibility, scalability, and node disaster tolerance than synchronized training methods. You can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/parameter_server_training.html">Parameter Server</a> mode tutorial for more information.</p></li>
</ul>
</section>
<section id="saving-and-loading-models">
<h2>Saving and Loading Models<a class="headerlink" href="#saving-and-loading-models" title="Permalink to this headline"></a></h2>
<p>Model saving can be categorized into merged and non-merged saving, which can be selected via the <code class="docutils literal notranslate"><span class="pre">integrated_save</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">mindspore.save_checkpoint</span></code> or <code class="docutils literal notranslate"><span class="pre">mindspore.train.CheckpointConfig</span></code>. Model parameters are automatically aggregated and saved to the model file in merged save mode, while each card saves slices of the parameters on their respective cards in non-merged saving mode. You can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_saving.html">Model Saving</a> tutorial for more information about model saving in each parallel mode.</p>
<p>Model loading can be categorized into full loading and slice loading. If the model file is saved with complete parameters, the model file can be loaded directly through the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> interface. If the model file is a parameter-sliced file under multi-card, we need to consider whether the distributed slice strategy or cluster size has changed after loading. If the distributed slice strategy or cluster size remains unchanged, the corresponding parameter slice file for each card can be loaded via the <code class="docutils literal notranslate"><span class="pre">load_distributed_checkpoint</span></code> interface, which can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_loading.html">model loading</a> tutorial.</p>
<p>In the case that the saved and loaded distributed slice strategy or cluster size changes, the Checkpoint file under distribution needs to be converted to adapt to the new distributed slice strategy or cluster size. You can refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_transformation.html">Model Transformation</a> for more information.</p>
</section>
<section id="fault-recovery">
<h2>Fault Recovery<a class="headerlink" href="#fault-recovery" title="Permalink to this headline"></a></h2>
<p>During the distributed parallel training process, problems such as failures of computing nodes or communication interruptions may be encountered. MindSpore provides two recovery methods to ensure the stability and continuity of training:</p>
<ul class="simple">
<li><p><strong>Recovery based on full Checkpoint</strong>：Before saving the Checkpoint file, the complete parameters of the model are aggregated by the AllGather operator, and the complete model parameter file is saved for each card, which can be loaded directly for recovery. Multiple copies improve the fault tolerance of the model, but for large models, the aggregation process leads to excessive overhead of various resources.</p></li>
<li><p><strong>Recovery of redundant information based on parameter slicing</strong>: In large model training, devices that are divided according to the dimension of data parallel have the same model parameters. According to this principle, these redundant parameter information can be utilized as a backup. When one node fails, another node utilizing the same parameters can recover the failed node. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/fault_recover.html">Failure Recovery</a> tutorial.</p></li>
</ul>
</section>
<section id="optimization-methods">
<h2>Optimization Methods<a class="headerlink" href="#optimization-methods" title="Permalink to this headline"></a></h2>
<p>If there is a requirement on performance, throughput, or scale, or if you don’t know how to choose a parallel strategy, consider the following optimization techniques:</p>
<ul class="simple">
<li><p><strong>Parallel strategy optimization</strong>：</p>
<ul>
<li><p><strong>Strategy Selection</strong>: Depending on the size of your model and the amount of data, you can refer to the <span class="xref myst">Strategy Selection</span> tutorial to select different parallel strategies to improve training efficiency and resource utilization.</p></li>
<li><p><strong>Slicing Techniques</strong>: Slicing techniques are also key to achieving efficient parallel computation. In the <span class="xref myst">Slicing Techniques</span> tutorial, you can learn how to apply a variety of slicing techniques to improve efficiency through concrete examples.</p></li>
<li><p><strong>Multi-copy</strong>: Under the existing single-copy mode, certain underlying operators cannot perform computation at the same time when communicating, which leads to resource waste. Multi-copy mode slicing the data into multiple copies in accordance with the batchsize dimension can make one copy communicate while the other copy performs computational operations, which improves the resource utilization rate. For details, please refer to the <span class="xref myst">Multi-copy</span> tutorial.</p></li>
</ul>
</li>
<li><p><strong>Memory optimization</strong>:</p>
<ul>
<li><p><strong>Recompute</strong>: Recompute saves memory space by not saving the result of the forward operators. When calculating the backward operators, you need to use the forward result before recalculating the forward operators. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/recompute.html">recompute</a> tutorial.</p></li>
<li><p><strong>Dataset Slicing</strong>: When a dataset is too large for a single piece of data, the data can be sliced for distributed training. Dataset slicing with model parallel is an effective way to reduce graphics memory usage. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dataset_slice.html">dataset slicing</a> tutorial.</p></li>
<li><p><strong>Host&amp;Device Heterogeneous</strong>: When the number of parameters exceeds the upper limit of Device memory, you can put some operators with large memory usage and small computation amount on the Host side, so that you can utilize the large memory on the Host side and the fast computation on the Device side at the same time, and improve the utilization rate of the device. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/host_device_training.html">Host&amp;Device Heterogeneous</a> tutorial.</p></li>
<li><p><strong>Heterogeneous Storage</strong>: large models are currently limited by the size of the graphics memory, making it difficult to train on a single card. In large-scale distributed cluster training, with communication becoming more and more costly, boosting the graphics memory of a single machine and reducing communication can also improve training performance. Heterogeneous storage can copy the parameters or intermediate results that are not needed temporarily to the memory or hard disk on the Host side, and then restore them to the Device side when needed. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/memory_offload.html">Heterogeneous Storage</a> tutorial.</p></li>
</ul>
</li>
<li><p><strong>Communication Optimization</strong>：</p>
<ul>
<li><p><strong>Communication fusion</strong>: communication fusion can merge the communication operators of the same source and target nodes into a single communication process, avoiding the extra overhead of multiple communications. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/comm_fusion.html">Communication Fusion</a>.</p></li>
<li><p><strong>Communication Subgraph Extraction and Reuse</strong>: By extracting communication subgraphs for communication operators and replacing the original communication operators, we can reduce the communication time consumption and also reduce the model compilation time. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/comm_subgraph.html">Communication Subgraph Extraction and Reuse</a>.</p></li>
</ul>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../network/dependency_control.html" class="btn btn-neutral float-left" title="Dependency Control" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="startup_method.html" class="btn btn-neutral float-right" title="Distributed Parallel Startup Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>