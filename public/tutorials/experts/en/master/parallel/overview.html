<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallelism Overview &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Parallel Startup Methods" href="startup_method.html" />
    <link rel="prev" title="For Experts" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Parallelism Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/overview.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallelism-overview">
<h1>Distributed Parallelism Overview<a class="headerlink" href="#distributed-parallelism-overview" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/overview.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<p>In deep learning, as the size of the dataset and number of parameters grows, the time and hardware resources required for training will increase and eventually become a bottleneck that constrains training. Distributed parallel training, which can reduce the demand on hardware such as memory and computational performance, is an important optimization means to perform training. In addition, distributed parallel is important for large model training and inference, which provides powerful computational capabilities and performance advantages for handling large-scale data and complex models.</p>
<p>To implement distributed parallel training and inference, you can refer to the following guidelines:</p>
<section id="startup-methods">
<h2>Startup Methods<a class="headerlink" href="#startup-methods" title="Permalink to this headline"></a></h2>
<p>MindSpore currently supports three startup methods:</p>
<ul class="simple">
<li><p><strong>Dynamic Networking</strong>: Start via MindSpore internal dynamic grouping module, no dependency on external configurations or modules, Ascend/GPU/CPU support.</p></li>
<li><p><strong>mpirun</strong>: Start via the multi-process communication library OpenMPI, with Ascend/GPU support.</p></li>
<li><p><strong>rank table</strong>: After configuring the rank_table table, Ascend is supported by start scripts and processes corresponding to the number of cards.</p></li>
</ul>
<p>Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/startup_method.html">Distributed Parallel Startup Methods</a> section for details.</p>
</section>
<section id="parallel-modes">
<h2>Parallel Modes<a class="headerlink" href="#parallel-modes" title="Permalink to this headline"></a></h2>
<p>Currently MindSpore can take the following parallel mode, and you can choose according to your needs:</p>
<ul class="simple">
<li><p><strong>Data Parallel Mode</strong>: In data parallel mode, the dataset can be split in sample dimensions and distributed to different cards. If your dataset is large and the model parameters scale is able to operate on a single card, you can choose this parallel model. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/data_parallel.html">Data Parallel</a> tutorial for more information.</p></li>
<li><p><strong>Automatic Parallel Mode</strong>: a distributed parallel mode that combines data parallel and operator-level model parallel. It can automatically build cost models, find the parallel strategy with shorter training time, and select the appropriate parallel mode for the user. If your dataset and model parameters are large in size, and you want to configure the parallel strategy automatically, you can choose this parallel model. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/auto_parallel.html">Automatic Parallelism</a> tutorial for more information.</p></li>
<li><p><strong>Semi-Automatic Parallel Mode</strong>: Compared with automatic parallel, this mode requires the user to manually configure a slice strategy for the operators to realize parallel. If your dataset and model parameters are large, and you are familiar with the structure of the model, and know which “key operators” are prone to become computational bottlenecks to configure the appropriate slice strategy for the “key operators” to achieve better performance, you can choose this mode. Parallel mode. This mode also allows you to manually configure optimizer parallel and pipeline parallel. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/semi_auto_parallel.html">Semi-Automatic Parallel</a> tutorial for more information.</p></li>
<li><p><strong>Manual Parallel Mode</strong>: In manual parallel mode, you can manually implement parallel communication of models in distributed systems by transferring data based on communication operators such as <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>, <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, <code class="docutils literal notranslate"><span class="pre">Broadcast</span></code> and other communication operators. You can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/manual_parallel.html">Manual Parallel</a> tutorial for more information.</p></li>
<li><p><strong>Parameter Server Mode</strong>: parameter servers offer better flexibility and scalability than synchronized training methods. You can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/parameter_server_training.html">Parameter Server</a> mode tutorial for more information.</p></li>
</ul>
</section>
<section id="saving-and-loading-models">
<h2>Saving and Loading Models<a class="headerlink" href="#saving-and-loading-models" title="Permalink to this headline"></a></h2>
<p>Model saving can be categorized into merged and non-merged saving, which can be selected via the <code class="docutils literal notranslate"><span class="pre">integrated_save</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">mindspore.save_checkpoint</span></code> or <code class="docutils literal notranslate"><span class="pre">mindspore.train.CheckpointConfig</span></code>. Model parameters are automatically aggregated and saved to the model file in merged save mode, while each card saves slices of the parameters on their respective cards in non-merged saving mode. You can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_saving.html">Model Saving</a> tutorial for more information about model saving in each parallel mode.</p>
<p>Model loading can be categorized into full loading and slice loading. If the model file is saved with complete parameters, the model file can be loaded directly through the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> interface. If the model file is a parameter-sliced file under multi-card, we need to consider whether the distributed slice strategy or cluster size has changed after loading. If the distributed slice strategy or cluster size remains unchanged, the corresponding parameter slice file for each card can be loaded via the <code class="docutils literal notranslate"><span class="pre">load_distributed_checkpoint</span></code> interface, which can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_loading.html">model loading</a> tutorial.</p>
<p>In the case that the saved and loaded distributed slice strategy or cluster size changes, the Checkpoint file under distribution needs to be converted to adapt to the new distributed slice strategy or cluster size. You can refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_transformation.html">Model Transformation</a> for more information.</p>
</section>
<section id="fault-recovery">
<h2>Fault Recovery<a class="headerlink" href="#fault-recovery" title="Permalink to this headline"></a></h2>
<p>During the distributed parallel training process, problems such as failures of computing nodes or communication interruptions may be encountered. MindSpore provides three recovery methods to ensure the stability and continuity of training:</p>
<ul class="simple">
<li><p><strong>Recovery based on full Checkpoint</strong>：Before saving the Checkpoint file, the complete parameters of the model are aggregated by the AllGather operator, and the complete model parameter file is saved for each card, which can be loaded directly for recovery. Multiple checkpoints copies improve the fault tolerance of the model, but for large models, the aggregation process leads to excessive overhead of various resources. Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/model_loading.html">Model Loading</a> tutorial for details.</p></li>
<li><p><strong>Disaster Recovery in Dynamic Cluster Scenarios</strong>: In dynamic cluster, if a process fails, the other processes will enter a waiting state, and the training task can be resumed by pulling up the fault process (only GPU hardware platforms are supported at present). Compared with other methods, this fault recovery method does not require restarting the cluster. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/disaster_recover.html">Disaster Recovery in Dynamic Cluster Scenarios</a> tutorial.</p></li>
<li><p><strong>Recovery of redundant information based on parameter slicing</strong>: In large model training, devices that are divided according to the dimension of data parallel have the same model parameters. According to this principle, these redundant parameter information can be utilized as a backup. When one node fails, another node utilizing the same parameters can recover the failed node. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/fault_recover.html">Fault Recovery Based on Redundant Information</a> tutorial.</p></li>
</ul>
</section>
<section id="optimization-methods">
<h2>Optimization Methods<a class="headerlink" href="#optimization-methods" title="Permalink to this headline"></a></h2>
<p>If there is a requirement on performance, throughput, or scale, or if you don’t know how to choose a parallel strategy, consider the following optimization techniques:</p>
<ul class="simple">
<li><p><strong>Parallel strategy optimization</strong>：</p>
<ul>
<li><p><strong>Strategy Selection</strong>: Depending on the size of your model and the amount of data, you can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/strategy_select.html">Strategy Selection</a> tutorial to select different parallel strategies to improve training efficiency and resource utilization.</p></li>
<li><p><strong>Slicing Techniques</strong>: Slicing techniques are also key to achieving efficient parallel computation. In the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/split_technique.html">Slicing Techniques</a> tutorial, you can learn how to apply a variety of slicing techniques to improve efficiency through concrete examples.</p></li>
<li><p><strong>Multi-copy Parallel</strong>: Under the existing single-copy mode, certain underlying operators cannot perform computation at the same time when communicating, which leads to resource waste. Multi-copy mode slicing the data into multiple copies in accordance with the batchsize dimension can make one copy communicate while the other copy performs computational operations, which improves the resource utilization rate. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/multiple_copy.html">Multi-copy Parallel</a> tutorial.</p></li>
</ul>
</li>
<li><p><strong>Memory optimization</strong>:</p>
<ul>
<li><p><strong>Gradient Accumulation</strong>: Gradient accumulation updates the parameters of a neural network by computing gradients over multiple MicroBatches and summing them up, then applying this accumulated gradient at once. In this way, a small number of devices can be trained on a large Batch Size, effectively minimizing memory spikes. For detailed information, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/distributed_gradient_accumulation.html">Gradient Accumulation</a> tutorial.</p></li>
<li><p><strong>Recompute</strong>: Recompute saves memory space by not saving the result of the forward operators. When calculating the backward operators, you need to use the forward result before recalculating the forward operators. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/recompute.html">recompute</a> tutorial.</p></li>
<li><p><strong>Dataset Slicing</strong>: When a dataset is too large for a single piece of data, the data can be sliced for distributed training. Dataset slicing with model parallel is an effective way to reduce graphics memory usage. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dataset_slice.html">dataset slicing</a> tutorial.</p></li>
<li><p><strong>Host&amp;Device Heterogeneous</strong>: When the number of parameters exceeds the upper limit of Device memory, you can put some operators with large memory usage and small computation amount on the Host side, so that you can utilize the large memory on the Host side and the fast computation on the Device side at the same time, and improve the utilization rate of the device. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/host_device_training.html">Host&amp;Device Heterogeneous</a> tutorial.</p></li>
<li><p><strong>Heterogeneous Storage</strong>: large models are currently limited by the size of the graphics memory, making it difficult to train on a single card. In large-scale distributed cluster training, with communication becoming more and more costly, boosting the graphics memory of a single machine and reducing communication can also improve training performance. Heterogeneous storage can copy the parameters or intermediate results that are not needed temporarily to the memory or hard disk on the Host side, and then restore them to the Device side when needed. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/memory_offload.html">Heterogeneous Storage</a> tutorial.</p></li>
</ul>
</li>
<li><p><strong>Communication Optimization</strong>：</p>
<ul>
<li><p><strong>Communication fusion</strong>: communication fusion can merge the communication operators of the same source and target nodes into a single communication process, avoiding the extra overhead of multiple communications. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/comm_fusion.html">Communication Fusion</a>.</p></li>
<li><p><strong>Communication Subgraph Extraction and Reuse</strong>: By extracting communication subgraphs for communication operators and replacing the original communication operators, we can reduce the communication time consumption and also reduce the model compilation time. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/comm_subgraph.html">Communication Subgraph Extraction and Reuse</a>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="differences-in-different-platforms">
<h2>Differences in Different Platforms<a class="headerlink" href="#differences-in-different-platforms" title="Permalink to this headline"></a></h2>
<p>In distributed training, different hardware platforms (Ascend, CPU or GPU) support different characters, and users can choose the corresponding distributed startup method, parallel mode and optimization method according to their platforms.</p>
<section id="differences-in-startup-methods">
<h3>Differences in Startup Methods<a class="headerlink" href="#differences-in-startup-methods" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Ascend supports dynamic cluster, mpirun, and rank table startup.</p></li>
<li><p>GPU supports dynamic cluster and mpirun startup.</p></li>
<li><p>CPU only supports dynamic cluster startup.</p></li>
</ul>
<p>For the detailed process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/startup_method.html">startup methods</a>.</p>
</section>
<section id="differences-in-parallel-methods">
<h3>Differences in Parallel Methods<a class="headerlink" href="#differences-in-parallel-methods" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Ascend and GPUs support all methods of parallel, including data parallel, semi-automatic parallel, automatic parallel, and more.</p></li>
<li><p>CPU only supports data parallel.</p></li>
</ul>
<p>For the detailed process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/data_parallel.html">data parallel</a>, <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/semi_auto_parallel.html">semi-automatic parallel</a>, <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/auto_parallel.html">auto-parallel</a>.</p>
</section>
<section id="differences-in-optimization-feature-support">
<h3>Differences in Optimization Feature Support<a class="headerlink" href="#differences-in-optimization-feature-support" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Ascend supports all optimization features.</p></li>
<li><p>GPU support optimization features other than communication subgraph extraction and multiplexing.</p></li>
<li><p>CPU does not support optimization features.</p></li>
</ul>
<p>For the detailed process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/optimize_technique.html">optimization methods</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="For Experts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="startup_method.html" class="btn btn-neutral float-right" title="Distributed Parallel Startup Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>