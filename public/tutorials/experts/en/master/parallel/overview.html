

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed Parallelism Overview &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Operator-level Parallelism" href="operator_parallel.html" />
    <link rel="prev" title="Fault Recovery" href="../debug/fault_recover.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/master/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_ascend.html">Semi-automatic Parallelism Integration Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Automatic Parallelism Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Distributed Parallelism Overview</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/parallel/overview.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="distributed-parallelism-overview">
<h1>Distributed Parallelism Overview<a class="headerlink" href="#distributed-parallelism-overview" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/overview.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<p>In deep learning, as the size of the dataset and number of parameters grows, the time and hardware resources required for training will increase and eventually become a bottleneck that constrains training. Distributed parallel training, which can reduce the demand on hardware such as memory and computational performance, is an important optimization means to perform training. According to the different principles and modes of parallelism, the types of mainstream parallelism are as follows:</p>
<ul class="simple">
<li><p>Data Parallel: The parallel mode of slicing the data is generally sliced according to the batch dimension, and the data is assigned to each computational unit (worker) for model computation.</p></li>
<li><p>Model Parallel: Parallel mode for slicing models. Model parallelism can be classified as: operator model parallelism, pipeline model parallelism, optimizer model parallelism.</p></li>
<li><p>Hybrid Parallel: Refers to a parallel model that covers data parallelism and model parallelism.</p></li>
</ul>
<div class="section" id="distributed-parallelism-training-mode">
<h2>Distributed Parallelism Training Mode<a class="headerlink" href="#distributed-parallelism-training-mode" title="Permalink to this headline">¶</a></h2>
<p>MindSpore currently offers the following four parallel modes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code>: Data parallel mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code>: Automatic parallel mode, a distributed parallel mode that incorporates data parallelism and operator model parallelism, can automatically build cost models, find parallel strategies with shorter training times, and select the appropriate parallel mode for the user. MindSpore currently supports automatic search for operator parallel strategy, and provides three different strategy search algorithms as follows:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>: Dynamic programming strategy search algorithm. It is able to search for the optimal strategy inscribed by the cost model, but it is time consuming in searching for parallel strategies for huge network models. Its cost model is modeled based on the memory-based computational overhead and communication overhead of the Ascend 910 chip for training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code>: Double recursive strategy search algorithm. The optimal strategy is generated instantaneously for huge networks and large-scale multi-card slicing. Its cost model based on symbolic operations can be freely adapted to different accelerator clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code>: Sharding strategy propagation algorithm。A parallel strategy is propagated from operators configured with parallel policies to operators not configured. When propagating, the algorithm tries to select the strategy that triggers the least amount of tensor redistribution communication. For the parallel strategy configuration and tensor redistribution of the operator, refer to this <a class="reference external" href="https://www.mindspore.cn/docs/en/master/design/distributed_training_design.html">design document</a>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code>: Semi-automatic parallelism mode, compared to automatic parallelism, requires the user to manually configure the shard strategy for the operator to achieve parallelism.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HYBRID_PARALLEL</span></code>: In MindSpore it specifically refers to scenarios where the user achieves hybrid parallelism by manually slicing the model.</p></li>
</ul>
</div>
<div class="section" id="reading-guide">
<h2>Reading Guide<a class="headerlink" href="#reading-guide" title="Permalink to this headline">¶</a></h2>
<p>MindSpore provides you with a series of easy-to-use parallel training components. To get a better understanding of MindSpore distributed parallel training components, we recommend that you read this tutorial in the following order.</p>
<ul class="simple">
<li><p>If your model parameter scale can be operated on a single card, you can read the Data Parallelism tutorial.</p></li>
<li><p>If your model parameter scale cannot run on a single card, you can read the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/operator_parallel.html">operator-level parallelism</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/pipeline_parallel.html">pipeline parallel</a> tutorials to learn how MindSpore provides you with model parallelism capabilities.</p></li>
<li><p>If you want to learn how to reduce the memory occupation during model parallel, you can read the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/recompute.html">recompute</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/host_device_training.html">Host&amp;Device Side Heterogeneity</a> tutorials.</p></li>
<li><p>If you want to experience MindSpore easy-to-use model parallelism interfaces, you can read the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_ascend.html">Semi-automatic Parallelism</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/parallel_training_quickstart.html">Automatic Parallelism</a> tutorials.</p></li>
<li><p>If you have an in-depth understanding of parallel training and would like to learn more about the high-level configuration and application of MindSpore distributed parallelism, please read the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/distributed_case.html">Distributed Parallelism Case</a> chapter</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="operator_parallel.html" class="btn btn-neutral float-right" title="Operator-level Parallelism" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../debug/fault_recover.html" class="btn btn-neutral float-left" title="Fault Recovery" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>