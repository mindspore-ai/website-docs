

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Automatic Parallelism Case &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Parallel Startup Methods" href="startup_method.html" />
    <link rel="prev" title="Semi-automatic Parallelism Integration Case" href="train_ascend.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/master/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_ascend.html">Semi-automatic Parallelism Integration Case</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Automatic Parallelism Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Automatic Parallelism Case</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/parallel/parallel_training_quickstart.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="automatic-parallelism-case">
<h1>Automatic Parallelism Case<a class="headerlink" href="#automatic-parallelism-case" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/parallel_training_quickstart.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This tutorial shows how to perform MindSpore distributed parallel training in a single 8-card <strong>GPU</strong> environment via <strong>OpenMPI</strong> with a simple example of a single hidden layer fully connected neural network.</p>
<p>A tutorial on distributed parallel training of ResNet networks on a GPU platform is available at <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_gpu.html">Sample Distributed Parallel Training Basics (GPU)</a>. In contrast: (1) the example uses a more complex ResNet network; (2) in addition to pull-up training by using OpenMPI, the example also introduces pull-up training by using a scripted approach.</p>
<blockquote>
<div><p>You can download the complete sample code here:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_training_quickstart">https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_training_quickstart</a></p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distributed_training_quickstart
        ├── net.py
        ├── run_with_mpi.sh
    ...
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">net.py</span></code> is the network definition script and <code class="docutils literal notranslate"><span class="pre">run_with_mpi.sh</span></code> is the execution script.</p>
<blockquote>
<div><p>In addition, tutorials for distributed parallel training on Ascend 910 platform are available in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_ascend.html">Distributed Parallel Training Example (Ascend)</a>.</p>
</div></blockquote>
</div>
<div class="section" id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h3>
<p>This sample example constructs a random set of input data and labels, with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">label_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_per_epoch</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">label_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generate</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code> is the number of steps performed per epoch for training, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">in_dim</span></code> is the input vector length, and <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> is the output vector length.</p>
</div>
<div class="section" id="network-structure">
<h3>Network Structure<a class="headerlink" href="#network-structure" title="Permalink to this headline">¶</a></h3>
<p>The network code used in this sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;define net&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">]),</span> <span class="s2">&quot;w2&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">in_dim</span></code> is the network input dimension, <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> is the output dimension, which needs to match the data dimension, and <code class="docutils literal notranslate"><span class="pre">hidden_dim</span></code> is the number of nodes in the hidden layer of the network.</p>
</div>
</div>
<div class="section" id="semi-automatic-parallel-distributed-training-via-openmpi">
<h2>Semi-automatic Parallel Distributed Training via OpenMPI<a class="headerlink" href="#semi-automatic-parallel-distributed-training-via-openmpi" title="Permalink to this headline">¶</a></h2>
<div class="section" id="openmpi-environment-configuration">
<h3>OpenMPI Environment Configuration<a class="headerlink" href="#openmpi-environment-configuration" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://www.open-mpi.org/">OpenMPI</a> is a high-performance messaging library, a multi-process communication library adopted by MindSpore. For the related environment configuration, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_ascend.html#running-the-script-through-openmpi">Running the Script through OpenMPI</a>.</p>
<blockquote>
<div><p>In addition, MindSpore also supports distributed training without relying on OpenMPI. For the details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_gpu.html#training-without-relying-on-openmpi">Training without Relying on OpenMPI</a>.</p>
</div></blockquote>
</div>
<div class="section" id="semi-automatic-parallelism">
<h3>Semi-automatic Parallelism<a class="headerlink" href="#semi-automatic-parallelism" title="Permalink to this headline">¶</a></h3>
<p>Currently MindSpore supports four parallel modes, and see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/introduction.html#distributed-parallel-training-mode-1">Distributed Parallel Training Modes</a> for details.</p>
<p>This example demonstrates fully automatic parallelism, which is achieved by configuring <code class="docutils literal notranslate"><span class="pre">parallel_mode=ms.ParallelMode.AUTO_PARALLEL</span></code> through the <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context()</span></code> interface.
There are three configurable parallel strategy search algorithms under fully automatic parallelism, see: <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/introduction.html#fully-automatic-parallelism">Fully automatic parallelism</a> for details. In this example, the <strong>sharding strategy propagation algorithm</strong> is selected, which is implemented by configuring <code class="docutils literal notranslate"><span class="pre">search_mode=&quot;sharding_propagation&quot;</span></code> through the <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context()</span></code> interface, and manually setting the <code class="docutils literal notranslate"><span class="pre">matmul</span></code> operator sharding strategy. The sharding strategy of other operators is given by the parallel strategy search algorithm automatically. The code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;define net&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">]),</span> <span class="s2">&quot;w2&quot;</span><span class="p">)</span>

        <span class="c1"># Set the sharding strategy manually for the matmul operator</span>
        <span class="c1"># where (2, 4) means that the input data of matmul operator is sliced into two parts in batch dimension and four parts in width dimension</span>
        <span class="c1"># (4, 1) indicates that the weights of the matmul operator are sliced into four parts in the HEIGHT dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>where the <code class="docutils literal notranslate"><span class="pre">shard()</span></code> method is described in detail in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/design/distributed_training_design.html#principle-of-automatic-parallelism">Principles of Automatic Parallelism</a>. The inference introduction is in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/pynative_shard_function_parallel.html">functional operator sharding</a></p>
<p>For the parallel sharding strategy set in the above example, the <code class="docutils literal notranslate"><span class="pre">matmul</span></code> operator computation process for the forward propagation process in a single-machine 8-card environment is schematically shown as follows:</p>
<p><img alt="image" src="https://gitee.com/mindspore/docs/raw/master/tutorials/experts/source_zh_cn/parallel/images/matmul_shard.png" /></p>
<p>The top half of the diagram shows the data sharding, and the bottom half shows the calculation and communication process performed by each GPU card at logical number (rank) 0-7.</p>
<div class="section" id="code-running">
<h4>Code Running<a class="headerlink" href="#code-running" title="Permalink to this headline">¶</a></h4>
<p>In this example, the loss function, optimizer and training procedure are defined similarly to single card training, with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">var_step_per_epoch</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">var_single_batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">var_in_dim</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">var_hidden_dim</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">var_out_dim</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s2">&quot;../saved_graph&quot;</span><span class="p">)</span>

<span class="c1"># Single-machine 8-card environment. Parallel mode is fully automatic parallelism, and strategy search is set to strategy propagation algorithm</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">,</span> <span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">)</span>

<span class="c1"># Initialize the communication environment and get the logical serial number of the current card, i.e. rank_id</span>
<span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>

<span class="c1"># Randomly constructed datasets</span>
<span class="n">fake_dataset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">var_single_batch_size</span><span class="p">,</span> <span class="n">var_step_per_epoch</span><span class="p">,</span> <span class="n">var_in_dim</span><span class="p">,</span> <span class="n">var_out_dim</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">fake_dataset</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>

<span class="c1"># Define the network structure</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">var_in_dim</span><span class="p">,</span> <span class="n">var_hidden_dim</span><span class="p">,</span> <span class="n">var_out_dim</span><span class="p">)</span>

<span class="c1"># Define the loss function and callback</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
<span class="n">callback</span> <span class="o">=</span> <span class="p">[</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">))]</span>

<span class="c1"># Define the optimizer</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">epoch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>

<span class="c1"># Model training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Training can be performed with <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command of OpenMPI, as specified in the script <code class="docutils literal notranslate"><span class="pre">run_with_mpi.sh</span></code>.</p>
<p>After running, the script is performed in the background and the training log is saved in the <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/device</span></code> directory, and the model of the card with the logical number <code class="docutils literal notranslate"><span class="pre">rank_id</span></code> is saved in the <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/device/{rank_id}</span></code> directory.</p>
<p>In addition, <code class="docutils literal notranslate"><span class="pre">save_graphs=2</span></code> is configured via the <code class="docutils literal notranslate"><span class="pre">ms.set_context()</span></code> interface to save the model intermediate representation <code class="docutils literal notranslate"><span class="pre">MindIR</span></code>, and the <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> of the card with the logical number <code class="docutils literal notranslate"><span class="pre">rank_id</span></code> is saved in the <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/saved_graph/{rank_id}</span></code> directory. MindSpore IR (MindIR) is a program representation between the source language and the target language during the compilation of MindSpore framework programs to facilitate program analysis and optimization by the compiler, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/design/mindir.html">MindIR</a>.</p>
</div>
<div class="section" id="verification">
<h4>Verification<a class="headerlink" href="#verification" title="Permalink to this headline">¶</a></h4>
<p>After running the <code class="docutils literal notranslate"><span class="pre">run_with_mpi.sh</span></code> script, the recorded loss should decrease, e.g.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ./device/train.log: #
...
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
...
</pre></div>
</div>
<p>You can check the configuration of the sharding strategy for each operator in <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/saved_graph/rank_x/step_parallel_begin_xxxx.ir</span></code> to see the configuration of the sharding strategy for each operator, e.g.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ./saved_graph/rank_0/step_parallel_begin_0041.ir: #
...
%3(out) = MatMul(%1, %2) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 4), (4, 1)), output_names: [output], transpose_a: false, transpose_x1: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (16, 32)&gt;, &lt;Tensor[Float32], (32, 16)&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%4(out) = ReLU(%3) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]} {in_strategy: ((2, 4))}
    : (&lt;Tensor[Float32], (16, 16)&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%5([CNode]472) = Load($(@1_construct_wrapper.337:para4_w2), %para12_u)
    : (&lt;Ref[Tensor(F32)], (16, 16), ref_key=:w2&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell)
%6(out) = MatMul(%4, %5) {instance name: matmul2} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (16, 16)&gt;, &lt;Tensor[Float32], (16, 16)&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
...
</pre></div>
</div>
<p>It can be seen that the <code class="docutils literal notranslate"><span class="pre">relu</span></code> operator corresponding to the <code class="docutils literal notranslate"><span class="pre">%4(out)</span></code> line and the <code class="docutils literal notranslate"><span class="pre">matmul2</span></code> operator corresponding to the <code class="docutils literal notranslate"><span class="pre">%6(out)</span></code> line are automatically configured with a sharding strategy.</p>
<p>Further, you can view <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/saved_graph/rank_x/18_execute_xxxx.ir</span></code> to see the actual execution of the slice operator dimension for each card, e.g.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ./saved_graph/rank_0/18_execute_0185.ir: #
...
%12(equivout) = MatMul(%10, %11) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 4), (4, 1)), output_names: [output], transpose_a: false, transpose_x1: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (8, 8)&gt;, &lt;Tensor[Float32], (8, 16)&gt;) -&gt; (&lt;Tensor[Float32], (8, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
    # In file /home/jenkins/my_dir/parallel_training_quick_start/device/./matmul.py(45)/        out = self.matmul(x, self.weight)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(114)/        out = self._backbone(data)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(376)/        loss = self.network(*inputs)/
%13(equiv[CNode]520) = AllReduce(%12) {instance name: forward_op_11795743325248501408} primitive_attrs: {group: 4-6301172352641561019, fusion: 0, op: sum, rank_list: (0, 1, 2, 3), group_ranks: 0-1-2-3, index: 0, group_rank_ids: (0, 1, 2, 3), no_eliminate: true} cnode_attrs: {comm_reuse: true}
    : (&lt;Tensor[Float32], (8, 16)&gt;) -&gt; (&lt;Tensor[Float32], (8, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%14(equiv[CNode]519) = StridedSlice(%13, (0, 0), (8, 4), (1, 1)) {instance name: redistribution_op_16390315056374637535StridedSlice} primitive_attrs: {new_axis_mask: 0, shrink_axis_mask: 0, end_mask: 0, input_names: [x, begin, end, strides], output_names: [output], keep_value_node_input: true, begin_mask: 0, ellipsis_mask: 0}
    : (&lt;Tensor[Float32], (8, 16)&gt;, &lt;Tuple[Int64*2], sequence_nodes={node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}}&gt;, &lt;Tuple[Int64*2], sequence_nodes={node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}}&gt;, &lt;Tuple[Int64*2], sequence_nodes={node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}}&gt;) -&gt; (&lt;Tensor[Float32], (8, 4)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%15(equivout) = ReLU(%14) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]} {in_strategy: ((2, 4))}
    : (&lt;Tensor[Float32], (8, 4)&gt;) -&gt; (&lt;Tensor[Float32], (8, 4)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
    # In file /home/jenkins/my_dir/parallel_training_quick_start/device/./matmul.py(46)/        out = self.relu(out)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(114)/        out = self._backbone(data)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(376)/        loss = self.network(*inputs)/
%16(equiv[CNode]472) = Load(%para4_w2, U)
    : (&lt;Ref[Tensor(F32)], (4, 16), ref_key=:w2&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (4, 16)&gt;)
    # scope: (Default/network-WithLossCell)
%17(equivout) = MatMul(%15, %16) {instance name: matmul2} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (8, 4)&gt;, &lt;Tensor[Float32], (4, 16)&gt;) -&gt; (&lt;Tensor[Float32], (8, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
    # In file /home/jenkins/my_dir/parallel_training_quick_start/device/./matmul.py(47)/        out = self.matmul2(out, self.weight2)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(114)/        out = self._backbone(data)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(376)/        loss = self.network(*inputs)/
...
</pre></div>
</div>
<p>It can be seen that the dimension of the <code class="docutils literal notranslate"><span class="pre">matmul</span></code> operator corresponding to the <code class="docutils literal notranslate"><span class="pre">%12(equivout)</span></code> line is the same as that shown in the figure.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="startup_method.html" class="btn btn-neutral float-right" title="Distributed Parallel Startup Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="train_ascend.html" class="btn btn-neutral float-left" title="Semi-automatic Parallelism Integration Case" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>