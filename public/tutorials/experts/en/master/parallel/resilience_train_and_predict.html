

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed Resilience Training and Inference &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Other Features" href="other_features.html" />
    <link rel="prev" title="Distributed Graph Partition" href="distributed_graph_partition.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/master/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Distributed Resilience Training and Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/parallel/resilience_train_and_predict.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="distributed-resilience-training-and-inference">
<h1>Distributed Resilience Training and Inference<a class="headerlink" href="#distributed-resilience-training-and-inference" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/resilience_train_and_predict.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<div class="section" id="background">
<h3>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h3>
<p>When using MindSpore for distributed training, it is often necessary to convert the distributed Checkpoint obtained from training for the next step, such as inference, fine-tuning, multi-stage training. This tutorial will introduce how to convert the Checkpoint obtained from distributed training to carry out resilient training and inference with distributed strategies and changed number of cards in the cluster.
This function only supports semi_auto_parallel/auto_parallel mode, and temporarily does not support pipeline parallel dimension conversion.</p>
</div>
<div class="section" id="usage-scenarios">
<h3>Usage Scenarios<a class="headerlink" href="#usage-scenarios" title="Permalink to this headline">¶</a></h3>
<p>If you encounter the following scenarios, you need to refer to this tutorial for resilience training and inference.</p>
<p>Scenario1: M number of cards perform training, while N number of cards perform fine-tuning training. M and N can have no multiplicative relationship.
Scenario2: The training is divided into multiple phases, each with a different cluster size.
Scenario3: M number of cards perform training, while N number of cards perform inference. M and N can have no multiplicative relationship.
Scenario4: Changes need to be made to the network’s sharding strategy.</p>
<p>Using the example of training on 8 cards and fine-tuning on 4 cards, the overall procedure is as follows:</p>
<ol class="simple">
<li><p>Execute training, configure the storage location of model parameter sharding strategy files, and automatically generate Checkpoint files and model parameter sharding strategy files.</p></li>
<li><p>Compile fine-tuned networks, configure distributed strategy file storage locations, and automatically generate model parameter sharding strategy files.</p></li>
<li><p>The user converts the saved Checkpoint file based on the strategy file involved in the training and inference.</p></li>
<li><p>After compiling the fine-tuned network, load the distributed Checkpoint file obtained by the conversion.</p></li>
<li><p>Perform fine-tuning of the network.</p></li>
</ol>
<p>Note that loading a distributed Checkpoint requires that the network be compiled before it can be loaded.</p>
<blockquote>
<div><p>For dataset download, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/fault_recover.html#preparation">Preparation</a> in the Distributed Parallel Training Transformer Model tutorial.</p>
<p>Download the complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_resilience_training">Distributed Resilience Training</a>.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="converting-distributed-checkpoint-files">
<h2>Converting Distributed Checkpoint Files<a class="headerlink" href="#converting-distributed-checkpoint-files" title="Permalink to this headline">¶</a></h2>
<div class="section" id="overall-process">
<h3>Overall Process<a class="headerlink" href="#overall-process" title="Permalink to this headline">¶</a></h3>
<p>First, perform distributed training with parallel mode set to <code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code>/<code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>, and also custom <code class="docutils literal notranslate"><span class="pre">strategy_ckpt_save_file</span></code> parameter and configure the model sharding strategy file storage path by calling the <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code> interface.
After a period of training, the callback function that stores Checkpoint is called to store the distributed Checkpoint. And then compile the network under the new number of cards/sharding strategy, generate the model sharding strategy file of the target network, and call the distributed checkpoint conversion interface for distributed checkpoint conversion.</p>
</div>
<div class="section" id="executing-the-distributed-training">
<h3>Executing the Distributed Training<a class="headerlink" href="#executing-the-distributed-training" title="Permalink to this headline">¶</a></h3>
<p>Define the network, perform distributed initialization, and get the number of devices and card numbers. For the non-pipelined parallel case, the content of the sharding strategy file is the same for each card, so just call <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context(strategy_ckpt_save_file=&quot;.</span> <span class="pre">/src_strategy.ckpt&quot;)</span></code> on card 0 to save the strategy file.</p>
<p>Add a callback function to save Checkpoint, first define configuration object <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> related to the Checkpoint storage. Note that <code class="docutils literal notranslate"><span class="pre">integrated_save</span></code> is configured to <code class="docutils literal notranslate"><span class="pre">False</span></code>, which means that aggregated saving is not performed on distributed training weights to accommodate the memory overhead under large models.
And then define the callback function <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> that saves the checkpoint. Finally, call <code class="docutils literal notranslate"><span class="pre">model.train</span></code> to perform the training.
For the basic usage of distributed training, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_ascend.html">Distributed Training Ascend</a> or <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_gpu.html">Distributed Training GPU</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">TimeMonitor</span><span class="p">,</span> <span class="n">LossMonitor</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;../src_strategy.ckpt&quot;</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">callback_size</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                  <span class="n">integrated_save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;src_checkpoint&quot;</span><span class="p">,</span>
                                <span class="n">directory</span> <span class="o">=</span> <span class="s2">&quot;../src_checkpoints/rank_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">),</span>
                                <span class="n">config</span><span class="o">=</span><span class="n">ckpt_config</span><span class="p">)</span>
<span class="n">callback</span> <span class="o">=</span> <span class="p">[</span><span class="n">TimeMonitor</span><span class="p">(</span><span class="n">callback_size</span><span class="p">),</span> <span class="n">LossMonitor</span><span class="p">(</span><span class="n">callback_size</span><span class="p">),</span> <span class="n">ckpoint_cb</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset</span></code>: MindData objects, which need to be constructed in advance to feed into <code class="docutils literal notranslate"><span class="pre">model.train</span></code>.</p></li>
</ul>
<p>The 8-card training script execution command executed in the example is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_train_8p.sh<span class="w"> </span>../output/wmt14.en_fr.txt
</pre></div>
</div>
<p>After execution, the source Checkpoint file directory and the source sharding strategy file will be generated:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>src_checkpoints/
src_strategy.ckpt
</pre></div>
</div>
</div>
<div class="section" id="converting-the-distributed-checkpoint">
<h3>Converting the Distributed Checkpoint<a class="headerlink" href="#converting-the-distributed-checkpoint" title="Permalink to this headline">¶</a></h3>
<div class="section" id="executing-compilation-on-the-target-network">
<h4>Executing Compilation on the Target Network<a class="headerlink" href="#executing-compilation-on-the-target-network" title="Permalink to this headline">¶</a></h4>
<p>Perform the distributed checkpoint conversion, which depends on the original distributed strategy file and the target distributed strategy file. When the network training under the original strategy is executed, the distributed strategy file is already stored, so the distributed strategy file under the target strategy needs to be obtained separately.
The distributed strategy file of the target strategy network can be obtained by performing compilation on the network of the target strategy. Compilation is performed on the network alone by the <code class="docutils literal notranslate"><span class="pre">model.build</span></code> interface.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;../dst_strategy.ckpt&quot;</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset</span></code>: MindData objects, which need to be constructed in advance to feed into <code class="docutils literal notranslate"><span class="pre">model.train</span></code>.</p></li>
</ul>
<p>When the target network is for inference, <code class="docutils literal notranslate"><span class="pre">model.build</span></code> is replaced with <code class="docutils literal notranslate"><span class="pre">model.infer_preict_layout</span></code> to perform compilation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;../dst_strategy.ckpt&quot;</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">infer_predict_layout</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">data_shape</span><span class="p">)))</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data_shape</span></code>: Shape of the inference data.</p></li>
</ul>
<p>The 4-card target network compilation command executed in the example is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_compile_4p.sh<span class="w"> </span>../output/wmt14.en_fr.txt
</pre></div>
</div>
<p>After execution, the target sharding strategy files will be generated:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>dst_strategy.ckpt
</pre></div>
</div>
</div>
<div class="section" id="performing-distributed-checkpoint-conversions">
<h4>Performing Distributed Checkpoint Conversions<a class="headerlink" href="#performing-distributed-checkpoint-conversions" title="Permalink to this headline">¶</a></h4>
<p>In the example, the original strategy is trained with 8-card, model parallelism with 4-card, data parallelism with 2-card. The optimizer parallelism is turned on, and the strategy file is named <code class="docutils literal notranslate"><span class="pre">src_strategy.ckpt</span></code>.
The target strategy is trained with 4-card, model parallelism with 4-card, data parallelism with 1-card. The optimizer parallelism is turned off, and the strategy file is named <code class="docutils literal notranslate"><span class="pre">dst_stategy.ckpt</span></code>.</p>
<p>Distributed Checkpoint provides two interfaces to convert Checkpoint. The first interface, <code class="docutils literal notranslate"><span class="pre">transform_checkpoints</span></code>, requires the user to place all Checkpoints in one directory, and the subdirectories must be named in the format “rank_0, rank_1, rank_2, … “.
The user calls this interface to convert the entire directory directly. This approach is easier to use, but the memory overhead required for conversion is slightly higher. The second interface, <code class="docutils literal notranslate"><span class="pre">transform_checkpoint_by_rank</span></code>, is used to get the Checkpoint of a specific rank, which has more flexibility and lower memory overhead.
It needs to be used with the <code class="docutils literal notranslate"><span class="pre">rank_list_for_transform</span></code> interface to get which original Checkpoint is needed for the target Checkpoint of this rank.</p>
<ol>
<li><p>Use interface <code class="docutils literal notranslate"><span class="pre">transform_checkpoints</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">transform_checkpoints</span><span class="p">(</span><span class="n">src_checkpoints_dir</span><span class="p">,</span> <span class="n">dst_checkpoints_dir</span><span class="p">,</span>
                         <span class="s2">&quot;transformed&quot;</span><span class="p">,</span> <span class="n">src_strategy_file</span><span class="p">,</span> <span class="n">dst_strategy_file</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>The subdirectories in src_checkpoints_dir are required to be stored in the format “rank_x/checkpoint_x.ckpt”.</p>
</div></blockquote>
<p>In the example, the script execution command for the conversion of the entire Checkpoint directory is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>transform_checkpoint_dir.py<span class="w"> </span>--src_strategy_file<span class="o">=</span>./src_strategy.ckpt<span class="w"> </span>--dst_strategy_file<span class="o">=</span>./dst_strategy.ckpt<span class="w"> </span>--src_checkpoints_dir<span class="o">=</span>./src_checkpoints<span class="w"> </span>--dst_checkpoints_dir<span class="o">=</span>./dst_checkpoints
</pre></div>
</div>
</li>
<li><p>Call <code class="docutils literal notranslate"><span class="pre">transform_checkpoint_by_rank</span></code> interface to merge the parameters of <code class="docutils literal notranslate"><span class="pre">transform_rank</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">rank_list</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">rank_list_for_transform</span><span class="p">(</span><span class="n">transform_rank</span><span class="p">,</span> <span class="n">src_strategy_file</span><span class="p">,</span> <span class="n">dst_strategy_file</span><span class="p">)</span>
<span class="n">checkpoint_file_map</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">rank_id</span> <span class="ow">in</span> <span class="n">rank_list</span><span class="p">:</span>
    <span class="n">checkpoint_file_map</span><span class="p">[</span><span class="n">rank_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">src_checkpoints_dir</span><span class="p">,</span> <span class="s2">&quot;rank_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">),</span> <span class="s2">&quot;src_checkpoint</span><span class="si">{}</span><span class="s2">.ckpt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">))</span>
<span class="n">save_checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dst_checkpoints_dir</span><span class="p">,</span> <span class="s2">&quot;rank_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">transform_rank</span><span class="p">),</span>
                                    <span class="s2">&quot;dst_checkpoint</span><span class="si">{}</span><span class="s2">.ckpt&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">transform_rank</span><span class="p">))</span>
<span class="n">ms</span><span class="o">.</span><span class="n">transform_checkpoint_by_rank</span><span class="p">(</span><span class="n">transform_rank</span><span class="p">,</span> <span class="n">checkpoint_file_map</span><span class="p">,</span> <span class="n">save_checkpoint_path</span><span class="p">,</span>
                                <span class="n">src_strategy_file</span><span class="p">,</span> <span class="n">dst_strategy_file</span><span class="p">)</span>
</pre></div>
</div>
<p>In the example, the script execution command to convert Checkpoint by rank one by one is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>transform_by_rank.sh<span class="w"> </span>./src_strategy.ckpt<span class="w"> </span>./dst_strategy.ckpt<span class="w"> </span>./src_checkpoints<span class="w"> </span>./dst_checkpoints
</pre></div>
</div>
</li>
</ol>
<p>After execution, the following directory of converted target Checkpoint files will be generated:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>dst_checkpoints/
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-the-checkpoint-files-obtained-from-conversion">
<h2>Loading the Checkpoint Files Obtained from Conversion<a class="headerlink" href="#loading-the-checkpoint-files-obtained-from-conversion" title="Permalink to this headline">¶</a></h2>
<div class="section" id="overall-process-1">
<h3>Overall Process<a class="headerlink" href="#overall-process-1" title="Permalink to this headline">¶</a></h3>
<p>Compile the network for the target strategy and call the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> interface to load the model parameter data from the converted Checkpoint file.</p>
</div>
<div class="section" id="compiling-and-executing-the-target-network">
<h3>Compiling and Executing the Target Network<a class="headerlink" href="#compiling-and-executing-the-target-network" title="Permalink to this headline">¶</a></h3>
<p>Compile the network by using the <code class="docutils literal notranslate"><span class="pre">model.build</span></code> (for training) or <code class="docutils literal notranslate"><span class="pre">model.infer_predict_layout</span></code> (for inference) interfaces. At this time, the weight Shape is sliced in the compilation process. Call the <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> interface to load the model parameter data of each card from the Checkpoint file.</p>
<p>The target network is the training scenario:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;../dst_strategy.ckpt&quot;</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt_file</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ckpt_file</span></code>: The name of the Checkpoint model parameter file to be loaded.</p></li>
</ul>
<p>The target network is the inference scenario:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;../dst_strategy.ckpt&quot;</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt_file</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">infer_predict_layout</span><span class="p">(</span><span class="n">predict_data</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">predict_data</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">predict_data</span></code>: Tensor data used to infer.</p></li>
</ul>
<p>In the example, the script execution command to load the converted Checkpoint for two-stage fine-tuning training is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_train_4p.sh<span class="w"> </span>../output/wmt14.en_fr.txt
</pre></div>
</div>
<p>After the execution is completed, the loss can be seen to decrease from 6.45.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 73, loss is 6.45995
epoch: 1 step: 73, loss is 6.13733
</pre></div>
</div>
</div>
</div>
<div class="section" id="pipeline-parallel-dimension-conversion">
<h2>Pipeline Parallel Dimension Conversion<a class="headerlink" href="#pipeline-parallel-dimension-conversion" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/pipeline_parallel.html">Pipeline Parallelism</a> is to slice the linear network to get multiple sub-networks, which are pipelined between multiple cards. Therefore the sharding strategy file stored down for each subgraph is inconsistent, and all sharding strategies are aggregated to get the complete sharding information of the network.
Therefore, for the pipelined parallel dimensions, compared to the conversion of other dimensions, it is necessary to perform an aggregated shardig strategy file operation in advance to obtain the aggregated sharding strategy file, and use this file as the strategy file for the distributed Checkpoint conversion dependency. In addition, there is no difference from the previous section <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/resilience_train_and_predict.html#converting-the-distributed-checkpoint">Sharding Strategy Conversion</a>.</p>
<p>First, execute an 8-card pipeline parallel training, where the pipeline parallel dimension is 2, the operator-level model parallel dimension is 4, and the data parallel dimension is 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">PipelineCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># micro_batch=4</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;../src_pipeline_strategys/src_strategy</span><span class="si">{}</span><span class="s2">.ckpt&quot;</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">callback_size</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                  <span class="n">integrated_save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;src_checkpoint&quot;</span><span class="p">,</span>
                                <span class="n">directory</span> <span class="o">=</span> <span class="s2">&quot;../src_checkpoints/rank_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">),</span>
                                <span class="n">config</span><span class="o">=</span><span class="n">ckpt_config</span><span class="p">)</span>
<span class="n">callback</span> <span class="o">=</span> <span class="p">[</span><span class="n">train</span><span class="o">.</span><span class="n">TimeMonitor</span><span class="p">(</span><span class="n">callback_size</span><span class="p">),</span> <span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="n">callback_size</span><span class="p">),</span> <span class="n">ckpoint_cb</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset</span></code>: MindData objects, which need to be constructed in advance to feed into <code class="docutils literal notranslate"><span class="pre">model.train</span></code>.</p></li>
</ul>
<p>The 8-card training script execution command executed in the example is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_train_8p_pipeline.sh<span class="w"> </span>../output/wmt14.en_fr.txt
</pre></div>
</div>
<p>After execution, the source Checkpoint file directory and the source sharding strategy file will be generated:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>src_checkpoints_pipeline/
src_pipeline_strategys/
</pre></div>
</div>
<p>Refer to “performing compilation on target network module” of <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/resilience_train_and_predict.html#converting-the-distributed-checkpoint">Sharding Strategy Conversion</a> section to also compile the target network to get the sharding strategy file for the target network.</p>
<p>The 4-card target network compilation command executed in the example is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_compile_4p.sh<span class="w"> </span>../output/wmt14.en_fr.txt
</pre></div>
</div>
<p>After execution, the target sharding strategy files will be generated:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>dst_strategy.ckpt
</pre></div>
</div>
<p>The next step unfolds the distributed Checkpoint dimension conversion that contains the pipeline parallel dimension. First, the <code class="docutils literal notranslate"><span class="pre">merge_pipeline_strategys</span></code> interface is used to merge the sharding strategy files obtained from pipeline training, and then the distributed checkpoint conversion is performed by using the interface <code class="docutils literal notranslate"><span class="pre">transform_checkpoints</span></code> or <code class="docutils literal notranslate"><span class="pre">transform_checkpoint_by_rank</span></code>.</p>
<p>The example gives the interface using <code class="docutils literal notranslate"><span class="pre">transform_checkpoints</span></code>. For the interface using <code class="docutils literal notranslate"><span class="pre">transform_checkpoint_by_rank</span></code> please refer to introduction in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/resilience_train_and_predict.html#converting-the-distributed-checkpoint">sharding strategy conversion</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">merge_pipeline_strategys</span><span class="p">(</span><span class="n">src_pipeline_strategys_dir</span><span class="p">,</span> <span class="n">src_strategy_file</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">transform_checkpoints</span><span class="p">(</span><span class="n">src_checkpoints_dir</span><span class="p">,</span> <span class="n">dst_checkpoints_dir</span><span class="p">,</span>
                            <span class="s2">&quot;transformed&quot;</span><span class="p">,</span> <span class="n">src_strategy_file</span><span class="p">,</span> <span class="n">dst_strategy_file</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>The subdirectories in src_checkpoints_dir are required to be stored in the format “rank_x/checkpoint_x.ckpt”.</p>
</div></blockquote>
<p>In the example, the script execution command for the entire Checkpoint directory conversion is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>transform_checkpoint_dir_pipeline.py<span class="w"> </span>--src_strategy_dir<span class="o">=</span>./src_pipeline_strategys<span class="w"> </span>--dst_strategy_file<span class="o">=</span>dst_strategy.ckpt<span class="w"> </span>--src_checkpoints_dir<span class="o">=</span>./src_checkpoints<span class="w"> </span>--dst_checkpoints_dir<span class="o">=</span>./dst_checkpoints
</pre></div>
</div>
<p>After the conversion is complete, refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/resilience_train_and_predict.html#loading-the-checkpoint-files-obtained-from-conversion">Execute Target Network Chapter</a>. Load the distributed checkpoint obtained from the conversion and execute the distributed network without the pipeline dimension.</p>
<p>In the example, the script execution command to load the converted Checkpoint for two-stage fine-tuning training is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_train_4p.sh<span class="w"> </span>../output/wmt14.en_fr.txt
</pre></div>
</div>
<p>After the execution is completed, the loss can be seen to decrease from 6.45.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 73, loss is 6.45995
epoch: 1 step: 73, loss is 6.13733
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="other_features.html" class="btn btn-neutral float-right" title="Other Features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="distributed_graph_partition.html" class="btn btn-neutral float-left" title="Distributed Graph Partition" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>