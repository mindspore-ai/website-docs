<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sharding Propagation &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Heterogeneous Storage" href="memory_offload.html" />
    <link rel="prev" title="Saving and Loading Models in Hybrid Parallel Mode" href="save_load.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_mode.html">Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pangu_alpha.html">PengCheng·PanGu Model Network Multi-dimension Hybrid Parallel Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_fusion.html">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_subgraph.html">Communication Subgraph Extraction and Reuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_graph_partition.html">Distributed Graph Partition</a></li>
<li class="toctree-l2"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Sharding Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="memory_offload.html">Heterogeneous Storage</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed High-Level Configuration Case</a> &raquo;</li>
      <li>Sharding Propagation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/sharding_propagation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="sharding-propagation">
<h1>Sharding Propagation<a class="headerlink" href="#sharding-propagation" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/sharding_propagation.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>Distributed operator, Tensor Layout, and Tensor Redistribution are fundamental concepts in op-level parallelism of MindSpore. In <a class="reference external" href="https://www.mindspore.cn/docs/en/master/design/distributed_training_design.html#semi-automatic-parallelism">here</a>, these concepts are introduced by examples. Here, we formally define them.</p>
<p>In op-level parallelism, we conduct SPMD (Single Program Multiple Data) style parallelism, that is, a single program is produced for all partitions. MindSpore transforms a stand-alone program to a parallel one. The transformation is fine-grained in the sense that each operator in the stand-alone program is substituted by (a) distributed operator(s), guaranteeing that the substitution is mathematically equivalent.</p>
<section id="distributed-operator">
<h3>Distributed Operator<a class="headerlink" href="#distributed-operator" title="Permalink to this headline"></a></h3>
<p>Distributed Operator: together, the distributed operators running on multiple devices preserve the same semantics of the stand-alone counterpart. That is, given the same input, the distributed operators’ output is the same as the stand-alone counterpart.</p>
<p>Say a matrix multiplication (MatMul) operator with two matrix X and W as input: Y = MatMul(X, W) is to be parallelized on 4 devices. If matrix X has copies on 4 devices, and W is split into 4 copies by column, one for each device, then the distributed operator corresponding to the stand-alone version of the MatMul operator is also MatMul, that is, MatMul operator is executed on each device. If X is split into 4 parts according to the column, W is cut into 4 parts by row, and each device gets a shard of X and W, then the distributed operator corresponding to the stand-alone version of the MatMul operator is MatMul-&gt;AllReduce, that is, the two operators of MatMul and AllReduce will be executed sequentially on each device to ensure mathematical equivalence.</p>
<p>Besides the SP (Single Program) part, MD (Multiple Data) part also needs to be specified. Before that, we first define the Sharding Strategy.</p>
</section>
<section id="sharding-strategy">
<h3>Sharding Strategy<a class="headerlink" href="#sharding-strategy" title="Permalink to this headline"></a></h3>
<p>Sharding Strategy: a Sharding Strategy for an operator is a two-dimensional array, specifying how many partitions to split each dimension of each input tensor for the operator.</p>
<p>From the sharding strategy, you can derive the <strong>Tensor Layout</strong> to describe how tensors are distributed across devices.</p>
</section>
<section id="tensor-layout">
<h3>Tensor Layout<a class="headerlink" href="#tensor-layout" title="Permalink to this headline"></a></h3>
<p>Tensor Layout: given a Sharding Strategy for an operator, the <strong>Tensor Layout</strong> is inferred to describe the distributions of the input tensors of the operator, which includes the <strong>Logical Device Matrix</strong> and the <strong>Tensor Map</strong>. The Logical Device Matrix is an one-dimensional array, describing how devices are arranged for the operator. The Tensor Map the dimensions of input tensors to dimensions of the device matrix, indicating that input tensors are partitioned across the Logical Device Matrix.</p>
<p>Use again the MatMul operator Y = MatMul(X, W). We configure the operator with Sharding Strategy [[2, 1], [1, 4]] and the corresponding Tensor Layout information is demonstrated in the following figure. X is partitioned into 2 parts along the row dimension, and W is partitioned into 4 parts along the column dimension (figure (b)). From the Sharding Strategy, the Logical Device Matrix and the Tensor Map are inferred, as shown in figure (c). The coordinates are also determined to describe the locations of devices in the Logical Device Matrix, based on which the distributions of tensors are determined. From the ‘2’ column in the coordinate table, Device 0—3 are assigned X<sub>0</sub>, while Device 4—7 are assigned X<sub>1</sub>. From the ‘4’ column in the coordinate table, Device 0 and Device 4 are assigned W<sub>0</sub>, Device 1 and Device 5 are assigned W<sub>1</sub>, Device 2 and Device 6 are assigned W<sub>2</sub>, and Device 3 and Device 7 are assigned W<sub>3</sub>. As a result, the local computation is determined, as shown in figure (d).</p>
<p><img alt="tensor_layout" src="../_images/tensor_layout.png" /></p>
<p>For two consecutive operators that are dependent, the Tensor Layouts defined by two operators may be inconsistent, due to either Logical Device Matrix or Tensor Map. We propose an algorithm, called <strong>Tensor Redistribution</strong>, that transforms the inconsistent Tensor Layout. We omit the algorithm here, and only give a definition.</p>
</section>
<section id="tensor-redistribution">
<h3>Tensor Redistribution<a class="headerlink" href="#tensor-redistribution" title="Permalink to this headline"></a></h3>
<p>Tensor Redistribution: given two inconsistent Tensor Layouts of a tensor, Tensor Redistribution is an algorithm that can transform from the source Tensor Layout to the target Tensor Layout, with minimum communication cost.</p>
<p>Here, the communication cost is measured by the bytes that each device transmits.</p>
<p>Say a two-operator example: Z = MatMul(X, W), O = MatMul(Z, Y). To make Tensor Redistribution effective, two operators are configured Sharding Strategies so that the Tensor Layouts of Z are inconsistent, as shown in the following figure. In figure (a), the output of the first MatMul is row partitioned, while the second MatMul requires that Z are full-sized. Therefore, an AllGather is inferred by Tensor Redistribution to perform the transformation[1]. In figure (b), the output tensor Z of the first matrix multiplication operator is row-sliced, while the second matrix multiplicator requires that the tensor Z be split by columns, so the tensor redistribution derivation needs to be inserted here to complete the conversion.</p>
<p><img alt="tensor_redistribution" src="../_images/tensor_redistribution.png" /></p>
</section>
</section>
<section id="sharding-propagation-1">
<h2>Sharding Propagation<a class="headerlink" href="#sharding-propagation-1" title="Permalink to this headline"></a></h2>
<p>Given a computation graph, <strong>Sharding Propagation</strong> is a functionality that propagates the Sharding Strategies from configured operator to the whole graph, with the goal of minimizing the communication cost in Tensor Redistribution.</p>
<p>The input of Sharding Propagation is a computation graph, in which nodes represent operators, and edges encode the data-dependency relationship of operators. From a model definition with some operators configured Sharding Strategies, Sharding Propagation executes as follows:</p>
<ol class="arabic simple">
<li><p>Generate possible Sharding Strategies for non-configured operators;</p></li>
<li><p>Generate Tensor Redistributions and the associated communication costs for each edge;</p></li>
<li><p>Start from the configured operators, and propagate the Sharding Strategies to non-configured operators using BFS, with the goal of minimizing the communication cost along each edge.</p></li>
</ol>
<p>The following figure illustrates an example process of applying Sharding Propagation. Given an computation graph with some configured strategies, it first enumerates possible strategies for non-configured operators, as shown in figure (b). Next, it enumerates possible strategies and the Tensor Redistribution costs for each edge. Demonstrated in figure (c), the strategy for an edge is defined as a pair [<em>s_strategy</em>, <em>t_strategy</em>], where <em>s_strategy</em> and <em>t_strategy</em> denote Sharding Strategy for source operator and target operator, respectively. Finally, starting from the configured operator, it determines the next operator’s Sharding Strategy, such that the communication cost in Tensor Redistribution is minimized. The propagation ends when the Sharding Strategies for all operators are settled, as shown in figure (d).</p>
<p><img alt="sharding_propagation" src="../_images/sharding_propagation.png" /></p>
</section>
<section id="how-to-use-sharding-propagation-in-mindspore">
<h2>How to use Sharding Propagation in MindSpore<a class="headerlink" href="#how-to-use-sharding-propagation-in-mindspore" title="Permalink to this headline"></a></h2>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Download the complete sample code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/sharding_propagation">https://gitee.com/mindspore/docs/tree/master/docs/sample_code/sharding_propagation</a>.</p>
</div></blockquote>
<p>The directory structure is as follows, where <code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code> is the IP configuration for Ascend devices (see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/train_ascend.html#configuring-distributed-environment-variables">here</a> for the explanation), <code class="docutils literal notranslate"><span class="pre">train.py</span></code> is the model definition, and <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the execution script.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ sharding_propagatinon
       ├── rank_table_8pcs.json
       ├── run.sh
       ├── train.py
    ...
</pre></div>
</div>
</section>
<section id="model-definition">
<h3>Model definition<a class="headerlink" href="#model-definition" title="Permalink to this headline"></a></h3>
<p>We use the FeedForward Network (<code class="docutils literal notranslate"><span class="pre">FFN</span></code>) as an example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FFN</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="configuring-the-sharding-propagation">
<h3>Configuring the Sharding Propagation<a class="headerlink" href="#configuring-the-sharding-propagation" title="Permalink to this headline"></a></h3>
<p>Annotate Sharding Strategy for a <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> operator in <code class="docutils literal notranslate"><span class="pre">FFN</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
</pre></div>
</div>
<p>Configure the search_mode as <code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code> in Auto_Parallel mode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;auto_parallel&quot;</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-model-and-checking-the-sharding-strategies">
<h3>Training the Model and Checking the Sharding Strategies<a class="headerlink" href="#training-the-model-and-checking-the-sharding-strategies" title="Permalink to this headline"></a></h3>
<p>Run the command <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">run.sh</span> <span class="pre">8</span></code>. By setting the context: <code class="docutils literal notranslate"><span class="pre">save_graphs=2</span></code>, the IR graphs in the compilation process are saved. We choose the IRs corresponding to device 0.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">step_parallel_begin_xxx.ir</span></code>, each computation operator is annotated with a Sharding Strategy:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
  %3(x) = MatMul(%1, %2) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 1), (1, 4)), output_names: [output], transpose_a: false, transpose_x1: false}
 {in_strategy: ((2, 1), (1, 4))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64, 64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %4([CNode]453) = Load($(@1_construct_wrapper.298:para4_dense1.bias), %para15_u)
      : (&lt;Ref[Tensor(F32)], (64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (64)&gt;)
  %5(x) = Add(%3, %4) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %6(x) = ReLU(%5) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
 {in_strategy: ((2, 4))}      : (&lt;Tensor[Float32], (64, 64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %7([CNode]447) = Load($(@1_construct_wrapper.298:para5_dense2.weight), %para15_u)
      : (&lt;Ref[Tensor(F32)], (64, 64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %8(x) = MatMul(%6, %7) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false}
 {in_strategy: ((2, 4), (4, 1))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64, 64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %9([CNode]449) = Load($(@1_construct_wrapper.298:para6_dense2.bias), %para15_u)
      : (&lt;Ref[Tensor(F32)], (64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (64)&gt;)
  %10(x) = Add(%8, %9) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
...
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">xx_validate_xxx.ir</span></code>, each input and output tensor in the computation operator is sliced according to the Sharding Strategy.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
  %2(equivx) = MatMul(%0, %1) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 1), (1, 4)), output_names: [output], transpose_a: false, transpose_x1: false}
 {in_strategy: ((2, 1), (1, 4))}      : (&lt;Tensor[Float32], (32, 64)&gt;, &lt;Tensor[Float32], (64, 16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
      # In file ./train.py(33)/        x = self.matmul(x, self.weight)/
  %3(equiv[CNode]453) = Load(%para4_dense1.bias, U)
      : (&lt;Ref[Tensor(F32)], (16)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16)&gt;)
  %4(equivx) = Add(%2, %3) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (32, 16)&gt;, &lt;Tensor[Float32], (16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
      # In file ./train.py(34)/        x = self.add(x, self.bias)/
  %5(equivx) = ReLU(%4) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
 {in_strategy: ((2, 4))}      : (&lt;Tensor[Float32], (32, 16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
      # In file ./train.py(48)/        x = self.relu(x)/
  %6(equiv[CNode]447) = Load(%para5_dense2.weight, U)
      : (&lt;Ref[Tensor(F32)], (16, 64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16, 64)&gt;)
  %7(equivx) = MatMul(%5, %6) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false}
 {in_strategy: ((2, 4), (4, 1))}      : (&lt;Tensor[Float32], (32, 16)&gt;, &lt;Tensor[Float32], (16, 64)&gt;) -&gt; (&lt;Tensor[Float32], (32, 64)&gt;)
      # In file ./train.py(33)/        x = self.matmul(x, self.weight)/
  %8(equiv[CNode]493) = AllReduce(%7) {instance name: forward_op_4025687080669949636} primitive_attrs: {group: 4-6301172352641561019, fusion: 0, op: sum, group_ranks: 0-1-2-3, index: 0}
      : (&lt;Tensor[Float32], (32, 64)&gt;) -&gt; (&lt;Tensor[Float32], (32, 64)&gt;)
  %9(equiv[CNode]492) = StridedSlice(%8, (0, 0), (32, 16), (1, 1)) {instance name: redistribution_op_145462406996255498StridedSlice} primitive_attrs: {new_axis_mask: 0, shrink_axis_mask: 0, end_mask: 0, input_names: [x, begin, end, strides], output_names: [output], keep_value_node_input: true, begin_mask: 0, ellipsis_mask: 0}
      : (&lt;Tensor[Float32], (32, 64)&gt;, &lt;Tuple[Int64*2]&gt;, &lt;Tuple[Int64*2]&gt;, &lt;Tuple[Int64*2]&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
  %10(equiv[CNode]449) = Load(%para6_dense2.bias, U)
      : (&lt;Ref[Tensor(F32)], (16)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16)&gt;)
  %11(equivx) = Add(%9, %10) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (32, 16)&gt;, &lt;Tensor[Float32], (16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
...
</pre></div>
</div>
</section>
</section>
<section id="empirical-principles-on-configuring-sharding-strategies">
<h2>Empirical Principles on Configuring Sharding Strategies<a class="headerlink" href="#empirical-principles-on-configuring-sharding-strategies" title="Permalink to this headline"></a></h2>
<p>Given a new model with numerous operators, from the user’s perspective, a key problem is to determine which operators should be configured, with what sharding strategies. Since the goal of Sharding Propagation is to minimize Tensor Redistribution cost, instead of finding the global minima of end-to-end step time, it is crucial to configure proper sharding strategies for “key operators”. There is no compulsory standard specifying which operators must be configured. However, based on our experience of training large models, there are indeed some principles guiding users to annotate shardings. Here, we list three principles, which may be useful for new users.</p>
<section id="configuring-parameter-involved-operators">
<h3>Configuring Parameter-involved Operators<a class="headerlink" href="#configuring-parameter-involved-operators" title="Permalink to this headline"></a></h3>
<p>The sharding strategies for parameters are important especially for large models, since parameter-induced memory consumption is the majority of total memory consumption. Therefore, parameter-involved operators usually need to explicitly configure the sharding strategies. In the following examples in the figure, Gather and MatMul involving weights are configured shardings, while other operators are not. These correspond the data-parallel Embedding layer and hybrid-parallel FeedForward Layer in <a class="reference external" href="https://gitee.com/mindspore/mindformers/tree/master/mindformers/modules">mindformers</a>, respectively.</p>
<p><img alt="sp_case1" src="../_images/sp_case1.png" /></p>
</section>
<section id="configure-dimension-manipulation-operators">
<h3>Configure dimension-manipulation operators<a class="headerlink" href="#configure-dimension-manipulation-operators" title="Permalink to this headline"></a></h3>
<p>In deep learning frameworks, operators can be broadly classified into two categories: semantically simple dimension-preserving operators and dimension-manipulation operators. Sharding Propagation could easily propagate shardings from inputs to outputs for dimension-preserving operators. However, for dimension-manipulation operators, explicit annotations should be configured to express users’ intuition, to avoid Sharding Propagation to derive non-user-desired shardings. In the following figure, ReduceMean and MatMul operators are configured shardings.</p>
<p><img alt="sp_case2" src="../_images/sp_case2.png" /></p>
</section>
<section id="configure-parallelism-changing-boundary-operators">
<h3>Configure parallelism-changing-boundary operators<a class="headerlink" href="#configure-parallelism-changing-boundary-operators" title="Permalink to this headline"></a></h3>
<p>For a model like ResNet, different parts of the model may prefer different parallelisms: front part uses data parallelism, while tail part uses model parallelism. This is achieved by annotating parallelism-changing boundary operators. In the example of following figure, the first MatMul propagates the data-parallelism sharding to the front part, while the second MatMul propagates the model-parallelism sharding to the tail part.</p>
<p><img alt="sp_case3" src="../_images/sp_case3.png" /></p>
<p>Users should not only understand the main idea of Sharding Propagation, but also have a preferred parallelism for their training models. If there is a sharding strategy inferred by Sharding Propagation that is conflict with your intuition, just add the preferred sharding to the operator. It indeed needs some trial-and-errors to acquire the satisfactory configuration.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="save_load.html" class="btn btn-neutral float-left" title="Saving and Loading Models in Hybrid Parallel Mode" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="memory_offload.html" class="btn btn-neutral float-right" title="Heterogeneous Storage" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>