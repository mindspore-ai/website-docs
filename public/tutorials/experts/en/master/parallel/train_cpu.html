<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallel Training Base Sample (CPU) &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Parallel Training Example (GPU)" href="train_gpu.html" />
    <link rel="prev" title="Distributed Parallel Training Basic Sample (Ascend)" href="train_ascend.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="train_ascend.html">Distributed Parallel Training Basic Sample (Ascend)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Parallel Training Base Sample (CPU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="train_gpu.html">Distributed Parallel Training Example (GPU)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="basic_cases.html">Distributed Basic Cases</a> &raquo;</li>
      <li>Distributed Parallel Training Base Sample (CPU)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/train_cpu.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallel-training-base-sample-cpu">
<h1>Distributed Parallel Training Base Sample (CPU)<a class="headerlink" href="#distributed-parallel-training-base-sample-cpu" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_en/parallel/train_cpu.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial focuses on how to use MindSpore for data parallel distributed training on CPU platforms to improve training efficiency.</p>
<blockquote>
<div><p>The complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/master/docs/sample_code/distributed_training_cpu">distributed_training_cpu</a></p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distributed_training_cpu
       ├── resnet.py
       ├── resnet50_distributed_training.py
       ├── run.sh
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">resnet.py</span></code> and <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training.py</span></code> are the training network definition scripts and <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the distributed training execution script.</p>
<blockquote>
<div><p>This tutorial makes use of <strong>Dynamic Cluster</strong> to launch CPU distributed job. Please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dynamic_cluster.html">this link</a> for more details about <strong>Dynamic Cluster</strong>.</p>
</div></blockquote>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<section id="downloading-the-datasets">
<h3>Downloading the Datasets<a class="headerlink" href="#downloading-the-datasets" title="Permalink to this headline"></a></h3>
<p>This sample is taken <a class="reference external" href="http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz">with CIFAR-10 dataset</a>, consisting of 10 classes of 32*32 color images. Each class contains 6000 images, with a total of 50,000 images in the training set and 10,000 images in the test set images.</p>
<p>Download and unzip the dataset locally, and unzip the folder as <code class="docutils literal notranslate"><span class="pre">cifar-10-batches-bin</span></code>.</p>
</section>
<section id="configuring-a-distributed-environment">
<h3>Configuring a Distributed Environment<a class="headerlink" href="#configuring-a-distributed-environment" title="Permalink to this headline"></a></h3>
<p>Data parallelism on CPU is mainly divided into two types of parallelism: single-machine multi-node and multi-machine multi-node (a training process can be understood as a node). Before running the training script, you need to set up the networking environment, mainly the environment variable configuration and the calling of the initialization interface in the training script.</p>
<p>The environment variable configuration is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export MS_WORKER_NUM=8                # Worker number
export MS_SCHED_HOST=127.0.0.1        # Scheduler IP address
export MS_SCHED_PORT=6667             # Scheduler port
export MS_ROLE=MS_WORKER              # The role of this node: MS_SCHED represents the scheduler, MS_WORKER represents the worker
</pre></div>
</div>
<p>where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MS_WORKER_NUM</span></code>: denotes the number of worker nodes. In a multi-machine scenario, the number of worker nodes is the sum of worker nodes per machine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MS_SCHED_HOST</span></code>: denotes the ip address of the scheduler node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MS_SCHED_PORT</span></code>: denotes the service port of the scheduler node, used to receive the ip and service port sent by worker nodes, and then distribute the collected ip and port of all worker nodes down to each worker.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MS_ROLE</span></code>: denotes the type of the node, worker (MS_WORKER) and scheduler (MS_SCHED) two types. Whether it is single-machine multi-node or multi-machine multi-node, a scheduler node needs to be configured for networking.</p></li>
</ul>
<p>The calling of the initialization interface in the training script is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_ps_context</span><span class="p">(</span><span class="n">enable_ssl</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
<p>where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ms.set_context(mode=ms.GRAPH_MODE,</span> <span class="pre">device_target=&quot;CPU&quot;)</span></code>: Specify the mode as graph mode (parallelism is not supported in PyNative mode on CPU) and the device as <code class="docutils literal notranslate"><span class="pre">CPU</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ms.set_auto_parallel_context(parallel_mode=ParallelMode.DATA_PARALLEL,</span> <span class="pre">gradients_mean=True)</span></code>: Specify the data parallelism mode. <code class="docutils literal notranslate"><span class="pre">gradients_mean=True</span></code> means averaging will be done after gradient normalization. Only summation is supported for gradient normalization on the current CPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ms.set_ps_context</span></code>: Configure secure encrypted communication and enable secure encrypted communication by <code class="docutils literal notranslate"><span class="pre">ms.set_ps_context(enable_ssl=True)</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code> to turn off secure encrypted communication.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init</span></code>: Initialize the node. The completion of initialization indicates successful network formation.</p></li>
</ul>
</section>
</section>
<section id="loading-the-dataset">
<h2>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h2>
<p>For distributed training, the dataset is imported as data in parallel. In the following, we introduce the method of importing CIFAR-10 dataset in a data parallel way, taking the CIFAR-10 dataset as an example. <code class="docutils literal notranslate"><span class="pre">data_path</span></code> is the path to the dataset, i.e. the path to the <code class="docutils literal notranslate"><span class="pre">cifar-10-batches-bin</span></code> folder. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create training dataset&quot;&quot;&quot;</span>
    <span class="n">resize_height</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># get rank_id and rank_size</span>
    <span class="n">rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
    <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="c1"># define map operations</span>
    <span class="n">random_crop_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">random_horizontal_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">))</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">normalize_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4465</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4914</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2010</span><span class="p">,</span> <span class="mf">0.1994</span><span class="p">,</span> <span class="mf">0.2023</span><span class="p">))</span>
    <span class="n">changeswap_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">c_trans</span> <span class="o">=</span> <span class="p">[</span><span class="n">random_crop_op</span><span class="p">,</span> <span class="n">random_horizontal_op</span><span class="p">]</span>
    <span class="n">c_trans</span> <span class="o">+=</span> <span class="p">[</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">rescale_op</span><span class="p">,</span> <span class="n">normalize_op</span><span class="p">,</span> <span class="n">changeswap_op</span><span class="p">]</span>

    <span class="c1"># apply map operations on images</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">)</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">c_trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">)</span>

    <span class="c1"># apply shuffle operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># apply batch operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># apply repeat operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_set</span>
</pre></div>
</div>
<p>Unlike stand-alone, the <code class="docutils literal notranslate"><span class="pre">num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id</span></code> parameters need to be passed in when constructing the Cifar10Dataset, corresponding to the number of worker nodes and logical serial number, respectively, which can be obtained through the framework interface as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_group_size</span></code>: Obtain the number of worker nodes in the cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_rank</span></code>: Obtain the logical serial number of the current worker node in the cluster.</p></li>
</ul>
<blockquote>
<div><p>When loading datasets in data parallel mode, it is recommended to specify the same dataset file for each card. If the datasets loaded for each card are different, it may affect the calculation accuracy.</p>
</div></blockquote>
</section>
<section id="defining-the-model">
<h2>Defining the Model<a class="headerlink" href="#defining-the-model" title="Permalink to this headline"></a></h2>
<p>The network definition in data parallel mode is written in the same way with the stand-alone, which can be found in <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/sample_code/distributed_training_cpu/resnet.py">ResNet Network Sample Script</a>.</p>
<p>The definitions of optimizer, loss function and training model can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/train.html">Training Model Definition</a>.</p>
<p>The reference sample of the full training script code and the training startup code is listed below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">resnet</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="k">def</span> <span class="nf">train_resnet50_with_cifar10</span><span class="p">(</span><span class="n">epoch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Start the training&quot;&quot;&quot;</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DATA_PATH&#39;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_ps_context</span><span class="p">(</span><span class="n">enable_ssl</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">init</span><span class="p">()</span>
    <span class="n">train_resnet50_with_cifar10</span><span class="p">()</span>
</pre></div>
</div>
<blockquote>
<div><p>The interfaces <code class="docutils literal notranslate"><span class="pre">create_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyExpand</span></code> in script are referenced from <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/sample_code/distributed_training_cpu/resnet50_distributed_training.py">distributed_training_cpu</a>.
The interfaces <code class="docutils literal notranslate"><span class="pre">resnet50</span></code> is referenced from <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/sample_code/distributed_training_cpu/resnet.py">ResNet network sample script</a>.</p>
</div></blockquote>
</section>
<section id="starting-the-training">
<h2>Starting the Training<a class="headerlink" href="#starting-the-training" title="Permalink to this headline"></a></h2>
<p>Distributed training is performed on a CPU platform with a single-machine 8-node as an example. Start the training with the following shell script, and command <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">run.sh</span> <span class="pre">/dataset/cifar-10-batches-bin</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># run data parallel training on CPU</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script with dataset path, such as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">set</span><span class="w"> </span>-e
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8117</span>

<span class="c1"># Launch 1 scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
python3<span class="w"> </span>resnet50_distributed_training.py<span class="w"> </span>&gt;scheduler.txt<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;scheduler start success!&quot;</span>

<span class="c1"># Launch 8 workers.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">MS_WORKER_NUM</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span>python3<span class="w"> </span>resnet50_distributed_training.py<span class="w"> </span>&gt;worker_<span class="nv">$i</span>.txt<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;worker </span><span class="si">${</span><span class="nv">i</span><span class="si">}</span><span class="s2"> start success with pid </span><span class="si">${</span><span class="p">!</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training.py</span></code> is the defined training script.</p>
<p>For a multi-machine, multi-node scenario, the corresponding worker node needs to be started on each machine to participate in the training in this way, but there is only one scheduler node, which only needs to be started on one of the machines (i.e. MS_SCHED_HOST).</p>
<blockquote>
<div><p>The defined value of MS_WORKER_NUM indicates that the corresponding number of worker nodes need to be started to participate in the training, otherwise the networking is not successful.</p>
<p>Although training scripts are also started for scheduler nodes, the scheduler is mainly used for networking and does not participate in training.</p>
</div></blockquote>
<p>After a period of training, open the worker_0 log and the training information is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 2.4686084
epoch: 1 step: 2, loss is 2.3278534
epoch: 1 step: 3, loss is 2.4246798
epoch: 1 step: 4, loss is 2.4920032
epoch: 1 step: 5, loss is 2.4324203
epoch: 1 step: 6, loss is 2.432581
epoch: 1 step: 7, loss is 2.319618
epoch: 1 step: 8, loss is 2.439193
epoch: 1 step: 9, loss is 2.2922952
</pre></div>
</div>
</section>
<section id="security-authentication">
<h2>Security Authentication<a class="headerlink" href="#security-authentication" title="Permalink to this headline"></a></h2>
<p>For CPU security authentication, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/parallel/dynamic_cluster.html#security-authentication">Dynamic Cluster Security Authentication</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="train_ascend.html" class="btn btn-neutral float-left" title="Distributed Parallel Training Basic Sample (Ascend)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train_gpu.html" class="btn btn-neutral float-right" title="Distributed Parallel Training Example (GPU)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>