<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Functional Operator Sharding &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performing Distributed Training on K8S Clusters" href="ms_operator.html" />
    <link rel="prev" title="Dataset Slicing" href="dataset_slice.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.11/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="other_features.html">Other Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="sharding_propagation.html">Sharding Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_fusion.html">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_subgraph.html">Communication Subgraph Extraction and Reuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Functional Operator Sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="ms_operator.html">Performing Distributed Training on K8S Clusters</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="other_features.html">Other Features</a> &raquo;</li>
      <li>Functional Operator Sharding</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/pynative_shard_function_parallel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="functional-operator-sharding">
<h1>Functional Operator Sharding<a class="headerlink" href="#functional-operator-sharding" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.11/tutorials/experts/source_en/parallel/pynative_shard_function_parallel.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.11/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Dynamic diagram supports richer syntax and are more flexible to use, but currently MindSpore’s dynamic diagram mode does not support the various features of automatic parallelism. Drawing on the design concept of Jax’s pmap, we design the shard function to support specifying a certain part to be executed in graph mode and performing various parallel operations in dynamic graph mode.</p>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>In MindSpore dynamic graph mode, you can specify a segment to be compiled and executed in the graph mode by using the <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code> decorator. During forward execution, the executed operators and subgraphs will be recorded, and after the forward execution, the whole graph obtained will be automatically differentiated to obtain the reverse graph, as shown in the following diagram:</p>
<p><img alt="structure image" src="../_images/pynative_jit.png" /></p>
<p><em>Figure 1: Schematic diagram of the &#64;jit decorator implementation</em></p>
<p>The Shard function follows this pattern, except that it can perform operator-level model parallelism in the session where the graph pattern is compiled and executed.</p>
</section>
<section id="operation-practices">
<h2>Operation Practices<a class="headerlink" href="#operation-practices" title="Permalink to this headline"></a></h2>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>You can download the full sample code here:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.11/docs/sample_code/pynative_shard_function_parallel">https://gitee.com/mindspore/docs/tree/r1.11/docs/sample_code/pynative_shard_function_parallel</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─shard_function_parallel
        ├── rank_table_8pcs.json
        ├── run_shard_function_example.sh
        └── shard_function_example.py
</pre></div>
</div>
<p>The role of each of these files is as follows:</p>
<ul class="simple">
<li><p>shard_function_example.py: The shard function sample code describes how to use the shard function to specify the part of the code to perform parallel execution.</p></li>
<li><p>rank_table_8pcs.json: 8-card configuration file for RANK_TABLE_FILE.</p></li>
<li><p>run_shard_function_example.sh: Start script for shard function example.</p></li>
</ul>
</section>
<section id="interface-introduction">
<h3>Interface Introduction<a class="headerlink" href="#interface-introduction" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_strategy</span><span class="p">,</span> <span class="n">out_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parameter_plan</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">shard_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_strategy</span><span class="p">,</span> <span class="n">out_strategy</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">in_strategy(tuple)</span></code>: Specify the shard strategy of the input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. Each element is a tuple indicating the shard strategy corresponding to the input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. The length of each tuple should be equal to the dimension of the corresponding <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, indicating how each dimension is sliced, and <code class="docutils literal notranslate"><span class="pre">None</span></code> can be passed in. The corresponding shard strategy will be automatically derived and generated.</p>
<p><code class="docutils literal notranslate"><span class="pre">out_strategy(None,</span> <span class="pre">tuple)</span></code>: Specify the shard strategy for output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. The usage is the same as <code class="docutils literal notranslate"><span class="pre">in_strategy</span></code>, and the default value is None. This shard strategy is not enabled yet, will open later. In the deep learning model, the output strategy is replaced with data parallelism (False) and repeated computation (True), according to the value of full_batch.</p>
<p><code class="docutils literal notranslate"><span class="pre">parameter_plan(None,</span> <span class="pre">dict)</span></code>: Specify the shard strategy for each parameter. When passed into the dictionary, the key is the parameter name of str type, and the value is a 1-dimensional integer tuple indicating the corresponding shard strategy. The setting of this parameter will be skipped if the parameter name is wrong or the corresponding parameter has already set the shard strategy. Default: None, which means no setting.</p>
<p><code class="docutils literal notranslate"><span class="pre">device(string)</span></code>: Specify the device to execute on. The optional range is <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPU</span></code> and <code class="docutils literal notranslate"><span class="pre">CPU</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, currently not enabled, will open later.</p>
<p><code class="docutils literal notranslate"><span class="pre">level(int)</span></code>: Specify the search strategy for all operators. The shard strategy for the input and output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is specified by the user, and the shard strategy for the rest of the operators will be obtained by the framework search. This parameter specifies the objective function when searching, and the optional range is 0, 1, 2, which represents maximizing the computational communication ratio, minimizing memory consumption, and maximizing operation speed, respectively. Default: 0, currently not enabled, will open later.</p>
</section>
<section id="importing-relevant-packages-and-setting-execution-mode">
<h3>Importing Relevant Packages and Setting Execution Mode<a class="headerlink" href="#importing-relevant-packages-and-setting-execution-mode" title="Permalink to this headline"></a></h3>
<p>As mentioned earlier, the shard function executes a part of the dynamic graph schema in parallel with the operator-level model in graph mode, so during using the shard function you need to set the mode to PyNative.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>


<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span>
                             <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>The current functional operator slicing is only supported when the parallel mode is “auto_parallel” and the policy search algorithm is “sharding_propagation”.</p>
</div></blockquote>
</section>
<section id="specifying-output-scheduling">
<h3>Specifying Output Scheduling<a class="headerlink" href="#specifying-output-scheduling" title="Permalink to this headline"></a></h3>
<p>Support specifying output scheduling as data parallelism and double counting and control through the <code class="docutils literal notranslate"><span class="pre">dataset_strategy</span></code> or <code class="docutils literal notranslate"><span class="pre">full_batch</span></code> attribute in auto_parallel_context, which is set as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set output scheduling via dataset_strategy, which is recommended</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span>  <span class="c1"># The dataset is not sliced and the output tensor of the shard is not sliced; (default configuration)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">)</span>  <span class="c1"># The dataset is sliced in data parallelism and the output tensor of the shard is also sliced in data parallelism</span>

<span class="c1"># Set output scheduling via full_batch, and this property will be deprecated soon</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>   <span class="c1"># The dataset is not sliced and the output tensor of the shard is not sliced; (default configuration)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># The dataset is sliced in data parallelism and the output tensor of the shard is also sliced in data parallelism</span>
</pre></div>
</div>
</section>
<section id="cell-uses-shard-function">
<h3>Cell Uses Shard Function<a class="headerlink" href="#cell-uses-shard-function" title="Permalink to this headline"></a></h3>
<p>There are currently two ways to use the shard function. The following network is an example of how to use the shard function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># two dimensional input x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block3</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># All three blocks are executed as PyNative mode.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<ul>
<li><p>Self-call via member method <code class="docutils literal notranslate"><span class="pre">shard</span></code> of Cell</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net1</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># slice input along the second axis and make output as data-parallel layout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),),</span>
                          <span class="n">parameter_plan</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;self.block1.dense2.weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)})</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># block1 is executed as GRAPH.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block2 and block3 are executed as PyNative mode.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</li>
<li><p>When using the functional interface <code class="docutils literal notranslate"><span class="pre">mindspore.shard</span></code>, since the return value of the <code class="docutils literal notranslate"><span class="pre">shard</span></code> function is a function, you cannot assign an instantiated class to the return value of <code class="docutils literal notranslate"><span class="pre">shard</span></code> when using the functional interface, because MindSpore does not support assigning class instances to other types</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NetError</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),),</span>
                                <span class="n">parameter_plan</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;self.block1.dense2.weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)})</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>An error may be reported after execution:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: For &#39;Cell&#39;, the type of block1 should be cell, but got function.
</pre></div>
</div>
<p>The correct use is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net2</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># set the return function of shard a different name with the Cell instance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1_graph</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),),</span>
                                      <span class="n">parameter_plan</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;self.block1.dense2.weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># block1 is executed as GRAPH with input sliced along the first dimension</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1_graph</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block2 is executed as GRAPH as well.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block3 is executed as PyNative mode.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="function-uses-shard-function">
<h3>function Uses Shard Function<a class="headerlink" href="#function-uses-shard-function" title="Permalink to this headline"></a></h3>
<ul>
<li><p>function can using <code class="docutils literal notranslate"><span class="pre">mindspore.shard</span></code> for shard function. Taking the matmul+bias_add+relu function as an example, the use is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span> <span class="c1"># Here is an example where the dataset is unsliced and the output tensor of the shard is unsliced</span>

<span class="k">def</span> <span class="nf">dense_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Specify the shard strategy for x as (4, 2) and shard strategy of weight and bias as None via in_strategy, indicating automatic derivation generation.</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">dense_relu</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;result.shape:&#39;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>It is noted that the initialization of parameters depends on the Cell parameter management, and when the fn type passed into the shard is function, its definition should not contain parameters (e.g. Conv2D and Dense).</p>
</div></blockquote>
</li>
</ul>
</section>
<section id="running-the-code">
<h3>Running the Code<a class="headerlink" href="#running-the-code" title="Permalink to this headline"></a></h3>
<p>Currently MindSpore can pull up distributed parallel tasks by both multi-process start and mpirun.</p>
<section id="starting-via-multi-process">
<h4>Starting via Multi-process<a class="headerlink" href="#starting-via-multi-process" title="Permalink to this headline"></a></h4>
<p>When executed on Ascend and there is no sub-Group communication, distributed parallelism can be initiated by means of multi-process.</p>
<blockquote>
<div><p>Model parallelism generates sub-Group communication when the number of parts that an object is cut is smaller than the number of cards or with at least two dimensions cut.</p>
<p>That is, when started by this method, the communication generated by the model parallelism inside <code class="docutils literal notranslate"><span class="pre">shard</span></code> can only occur inside <code class="docutils literal notranslate"><span class="pre">world</span> <span class="pre">group</span></code>, so the specified shard strategy can currently only support slicing one dimension.</p>
</div></blockquote>
<p>The above code needs to be configured with distributed variables before it can run. The Ascend environment should configure with RANK_TABLE_FILE, RANK_ID and DEVICE_ID. Please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.11/parallel/train_ascend.html#configuring-distributed-environment-variables">here</a> for the configuration process.</p>
<p>Environment variables related to Ascend distributed are:</p>
<ul class="simple">
<li><p>RANK_TABLE_FILE: The path to the network information file. The rank_table_file file can be generated by using hccl_tools.py in the models code bin, which can be obtained from <a class="reference external" href="https://gitee.com/mindspore/models/tree/r1.11/utils/hccl_tools">here</a>.</p></li>
<li><p>DEVICE_ID: The actual serial number of the current card on the machine.</p></li>
<li><p>RANK_ID: The logical serial number of the current card.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">set</span><span class="w"> </span>-e
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_shard_function_example.sh RANK_SIZE RANK_TABLE_FILE&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_fusion_example.sh 8&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;This example is expected to run on the Ascend environment.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="nv">$#</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">]</span>
<span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: bash run_shard_function_example.sh RANK_SIZE RANK_TABLE_FILE&quot;</span>
<span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="nv">$2</span>
test_dist_8pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">RANK_TABLE_FILE</span><span class="si">}</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>
<span class="o">}</span>
test_dist_<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span>pcs

<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span>
<span class="k">do</span>
<span class="w">    </span>rm<span class="w"> </span>-rf<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>mkdir<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>cp<span class="w"> </span>./shard_function_example.py<span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device </span><span class="nv">$i</span><span class="s2">&quot;</span>
<span class="w">    </span>env<span class="w"> </span>&gt;<span class="w"> </span>env<span class="nv">$i</span>.log
<span class="w">    </span>python<span class="w"> </span>./shard_function_example.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="nv">$i</span><span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>../
<span class="k">done</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;The program launch succeed, the log is under device0/train.log0.&quot;</span>
</pre></div>
</div>
<p>After configuring RANK_TABLE_FILE in the current directory, the following command requires the user to have 8 Ascend 910 devices. Run the command as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_shard_function_example.sh<span class="w"> </span><span class="m">8</span><span class="w"> </span>rank_table_8pcs.json
</pre></div>
</div>
<p>During execution, the framework automatically performs operator-level model parallelism for the input function of <code class="docutils literal notranslate"><span class="pre">shard</span></code>, and the parallel policy of each operator is obtained by the framework search. The whole process is not perceived by the user. The graph can be saved as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">step_parallel_end.ir</span></code>, you can see the specific parallel strategy for each operator.</p>
</section>
<section id="starting-via-mpirun">
<h4>Starting via mpirun<a class="headerlink" href="#starting-via-mpirun" title="Permalink to this headline"></a></h4>
<p>On Ascend and GPU, distributed parallelism can be started by means of mpirun. <strong>This start method supports the creation of sub-Group communication</strong>. Run the command as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_NUM</span><span class="si">}</span><span class="w"> </span>--allow-run-as-root<span class="w"> </span>python<span class="w"> </span><span class="si">${</span><span class="nv">PYTHON_SCRIPT_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>Taking the sample code as an example to start 8-card, the corresponding command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>--allow-run-as-root<span class="w"> </span>python<span class="w"> </span>shard_function_example.py
</pre></div>
</div>
<blockquote>
<div><p>It should be noted that when starting with mpirun on Ascend and a large number of sub-Group, the error of failure to create a communication domain is reported, as shown in the error message “Ascend collective Error: “HcclCommInitRootInfo failed. | Error Number 2”. You can reduce the <code class="docutils literal notranslate"><span class="pre">max_device_memory</span></code> in <code class="docutils literal notranslate"><span class="pre">context</span></code> to reserve enough memory for hccl to create communication domains.</p>
</div></blockquote>
</section>
</section>
</section>
<section id="usage-restrictions">
<h2>Usage Restrictions<a class="headerlink" href="#usage-restrictions" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The execution mode should be set to <code class="docutils literal notranslate"><span class="pre">PYNATIVE_MODE</span></code>, the parallelism configuration to <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code>, and the <code class="docutils literal notranslate"><span class="pre">search_mode</span></code> to <code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code>.</p></li>
<li><p>Support using nested <code class="docutils literal notranslate"><span class="pre">vmap</span></code>. When using, <code class="docutils literal notranslate"><span class="pre">shard</span></code> must  be outside, <code class="docutils literal notranslate"><span class="pre">vmap</span></code> be inside.</p></li>
<li><p>Not support using nested <code class="docutils literal notranslate"><span class="pre">shard</span></code>.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dataset_slice.html" class="btn btn-neutral float-left" title="Dataset Slicing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ms_operator.html" class="btn btn-neutral float-right" title="Performing Distributed Training on K8S Clusters" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>