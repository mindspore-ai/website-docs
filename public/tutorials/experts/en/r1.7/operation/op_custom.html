<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Operators (Custom based) &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference Model Overview" href="../infer/inference.html" />
    <link rel="prev" title="Custom Operators (Ascend)" href="op_ascend.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Tensor Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Operator Execution</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="op_classification.html">Operators Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_overload.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_ascend.html">Custom Operators (Ascend)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Operators (Custom based)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-usage">Basic Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-hybrid-type">Defining Custom Operator of hybrid Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-tbe-type">Defining Custom Operator of tbe Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-aot-type">Defining Custom Operator of aot Type</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#a-gpu-example">A GPU Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#a-cpu-example">A CPU Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-pyfunc-type">Defining Custom Operator of pyfunc Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-julia-type">Defining Custom Operator of julia Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-akg-type">Defining Custom Operator of akg Type</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#registering-the-operator-information">Registering the Operator Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-bprop-function-for-operators">Defining the bprop Function for Operators</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-hybrid-developer-guide">MindSpore Hybrid Developer Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#variables">Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#expressions">Expressions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loop">Loop</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute">Attribute</a></li>
<li class="toctree-l4"><a class="reference internal" href="#keywords">Keywords</a></li>
<li class="toctree-l4"><a class="reference internal" href="#frequent-error-messages-and-error-attributions">Frequent Error Messages and Error Attributions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/mindir.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/custom_debug.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/train_ascend.html">Parallel Distributed Training Example (Ascend)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/train_gpu.html">Distributed Parallel Training Example (GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Custom Operators (Custom based)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/operation/op_custom.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="custom-operators-custom-based">
<h1>Custom Operators (Custom based)<a class="headerlink" href="#custom-operators-custom-based" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.7/tutorials/experts/source_en/operation/op_custom.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When built-in operators cannot meet requirements during network development, you can call the Python API <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive defined in MindSpore to quickly create different types of custom operators for use.</p>
<p>Traditional methods to add a custom operator need three steps: defining the operator primitive, implementing the operator, and registering the operator information.</p>
<p>The related concepts are as follows:</p>
<ul class="simple">
<li><p>Operator primitive: defines the frontend API prototype of an operator on the network. It is the basic unit for forming a network model and includes the operator name, attribute (optional), input and output names, output shape inference method, and output data type inference method.</p></li>
<li><p>Operator implementation: defines a Python function(Ascend custom operators) or a C++ class(GPU and CPU custom operators), which describes the implementation of the internal computation logic of an operator.</p></li>
<li><p>Operator information: describes basic information about an operator, such as the operator name, supported input and output data types, supported input and output data formats, and attributes. It is the basis for the backend to select and map operators.</p></li>
</ul>
<p>Compared with traditional custom operator creating methods, creating custom operators based on <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive has several advantages:</p>
<ul class="simple">
<li><p>Different custom operators use the same <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive, there is no need to define a primitive for every operator. The above three parts of work can be implemented in a network script in a unified way and used as part of the network expression, there is no need to modify and recompile the source codes of MindSpore.</p></li>
<li><p>It unifies the interface and usage for different kinds of custom operators, which is convenient for network developers to flexibly choose which kind of custom operator to use according to their needs.</p></li>
<li><p>Supports defining custom operators with hybrid expression, which can be used across platforms.</p></li>
</ul>
</section>
<section id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this headline"></a></h2>
<p>The supported custom operator defining methods based on the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive include: hybrid, tbe, aot, pyfunc, julia, and akg.</p>
<p>The difference between these operator defining methods are as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p>Defining Methods</p></th>
<th class="text-center head"><p>Development Language</p></th>
<th class="text-center head"><p>Compilation Method</p></th>
<th class="head"><p>Supported Platforms</p></th>
<th class="head"><p>Recommended Scenarios</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>hybrid</p></td>
<td class="text-center"><p>MindSpore HYBRID DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
<td><p>Ascend/GPU platform general scenarios and proof of concept</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>tbe</p></td>
<td class="text-center"><p>TBE DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
<td><p>Ascend AICORE platform scenarios</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>aot</p></td>
<td class="text-center"><p>C/C++/CUDA</p></td>
<td class="text-center"><p>AOT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>high-performance scenarios / use third-party operators scenarios</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>pyfunc</p></td>
<td class="text-center"><p>Python</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>Fast algorithm verification, need to interact with Python and other scenarios</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>julia</p></td>
<td class="text-center"><p>Julia</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>Science compute scenarios / use Julia scenarios</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>akg</p></td>
<td class="text-center"><p>MindSpore AKG DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
<td><p>Ascend/GPU platform general scenarios</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The full name of DSL is Domain Specific Language.</p></li>
<li><p>AOT(Ahead Of Time) compiling means the operator implementation needs to be compiled into a dynamic library in advance and then automatically called by the framework when the network is running. JIT(Just In Time) compiling does not need to compile the operator implementation in advance, the operator implementation will be directly called by the framework during network compilation or runtime.</p></li>
</ul>
</div></blockquote>
<p>Different custom operator defining methods use different development languages to implement the operator, but the development process is the same, including operator implementation, operator output shape, data type inference, and operator information registration (optional). You can choose which one to use based on needs. The defining methods of these custom operators will be introduced here, and examples are provided for each method.</p>
<blockquote>
<div><p>More examples can be found in the MindSpore source code <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>.</p>
</div></blockquote>
<section id="defining-custom-operator-of-hybrid-type">
<h3>Defining Custom Operator of hybrid Type<a class="headerlink" href="#defining-custom-operator-of-hybrid-type" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">hybrid</span></code> is the default <code class="docutils literal notranslate"><span class="pre">func_type</span></code> of <code class="docutils literal notranslate"><span class="pre">Custom</span></code>.  By defining the custom operation with hybrid type, the user can use Python-like grammar to describe the logic of operation computation and focus on the algorithm itself as the details of framework-related operation engineering are blocked from the user.</p>
<p>The internal computation logic of the custom operator of type <code class="docutils literal notranslate"><span class="pre">hybrid</span></code> is described by <a class="reference internal" href="#mindspore-hybrid-developer-guide"><span class="std std-doc">MindSpore Hybrid DSL</span></a>. The function written by MindSpore Hybrid DSL can be parsed and compiled by the kernel compiler <a class="reference external" href="https://gitee.com/mindspore/akg">AKG</a> to generate high-performance operators in a JIT way and then be used in training and inference workload of AI models. Meanwhile, such functions can be used as <code class="docutils literal notranslate"><span class="pre">numpy</span></code> functions, so that users can easily tune the algorithm as well as switch to <a class="reference internal" href="#defining-custom-operator-of-pyfunc-type"><span class="std std-doc">custom operators of pyfunc type</span></a>. In this way, users will achieve the goal of using custom operations in multiply platforms and multiple scenarios in the same definition of the custom operator.</p>
<p>The following example test_custom_hybrid.py shows how to write a custom operator of the hybrid type. The operator computes the sum of two tensors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">ms_hybrid</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># the function written by MindSpore Hybrid DSL</span>
<span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># define the custom operator using the default func_type hybrid</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hybrid</span></code> is the default <code class="docutils literal notranslate"><span class="pre">func_type</span></code> of <code class="docutils literal notranslate"><span class="pre">Custom</span></code>.</p></li>
<li><p>The input of custom operators with hybrid type must be a function with decorator <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/ops/mindspore.ops.ms_hybrid.html"><code class="docutils literal notranslate"><span class="pre">&#64;ms_hybrid</span></code></a>.</p></li>
<li><p>Users can use the automatic shape/dtype inference functionality of the custom operators with hybrid type, while they can still handwrite shape/dtype functions.</p></li>
</ul>
<p>Execute the example file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_hybrid.py
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-tbe-type">
<h3>Defining Custom Operator of tbe Type<a class="headerlink" href="#defining-custom-operator-of-tbe-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of tbe type uses the TBE(Tensor Boost Engine) operator DSL to describe the internal calculation logic of the operator. You can refer to the <a class="reference external" href="https://support.huaweicloud.com/odevg-A800_3000_3010/atlaste_10_0063.html">TBE document</a> for the implementation details.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>Operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes test_custom_tbe.py as an example to introduce how to define a custom operator of tbe type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_tbe.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">CustomRegOp</span><span class="p">,</span> <span class="n">custom_info_register</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, and operator information registration</span>
<span class="nd">@custom_info_register</span><span class="p">(</span><span class="n">CustomRegOp</span><span class="p">()</span> \
                      <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">target</span><span class="p">(</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">get_op_info</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">kernel_name</span><span class="o">=</span><span class="s2">&quot;add&quot;</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">te.lang.cce</span>
    <span class="kn">from</span> <span class="nn">te</span> <span class="kn">import</span> <span class="n">tvm</span>
    <span class="n">data0</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data0&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">data1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data1&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">vadd</span><span class="p">(</span><span class="n">data0</span><span class="p">,</span> <span class="n">data1</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">cce</span><span class="p">():</span>
        <span class="n">sch</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">auto_schedule</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;print_ir&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">kernel_name</span><span class="p">,</span> <span class="s2">&quot;tensor_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">data0</span><span class="p">,</span> <span class="n">data1</span><span class="p">,</span> <span class="n">res</span><span class="p">]}</span>
    <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">cce_build_code</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of tbe type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;tbe&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">CustomRegOp</span></code> to create the operator information and use <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator to register it.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_tbe.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-aot-type">
<h3>Defining Custom Operator of aot Type<a class="headerlink" href="#defining-custom-operator-of-aot-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of aot type adopts the AOT compilation method, which requires network developers to hand-write the source code file of the operator implementation based on a specific interface and compiles the source code file into a dynamic library in advance, and then the framework will automatically call and run the function defined in the dynamic library. In terms of the development language of the operator implementation, the GPU platform supports CUDA, and the CPU platform supports C and C++. The interface specification of the operator implementation in the source file is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">func_name</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">);</span>
</pre></div>
</div>
<p>where the function name <code class="docutils literal notranslate"><span class="pre">func_name</span></code> can be replaced with any valid function name. The return value is of type int, and 0 means normal exit, non-zero means an exception occurs. The meaning of the parameter list is as follows:</p>
<ul class="simple">
<li><p>nparam (int): The number of inputs and outputs. For example, if an operator has 2 inputs and 1 output, then the value of nparam is 3.</p></li>
<li><p>params (void **): An array of pointers, with each pointer pointing to the input or output data. For example, if an operator has 2 inputs and 1 output, then params[0] points to the first input data, params[1] points to the second input data, params[2] points to the output data.</p></li>
<li><p>ndims (int *): An array of integers, each integer represents the dimensions of the shape of input or output. For example, if params[i] is a tensor with shape [1024, 1024], then ndims[i] is 2.</p></li>
<li><p>shapes (int64_t **): An array of shapes, each element in array represents for the shape of input or output. For example, if params[i] is a tensor with shape [1024, 1024], then shapes[i][0] is 1024, shapes[i][1] is 1024.</p></li>
<li><p>dtypes (const char **): Array of data types, each element in array represents for the data type of input or output. The value of data type can be “float32”, “float16”, “float”, “float64”, “int”, “int8”, “int16”, “int32”, “int64”, “uint”, “uint8”, “uint16”, “uint32”, “uint64”, “bool”.</p></li>
<li><p>stream (void *): Stream pointer, only used in Cuda file.</p></li>
<li><p>extra (void *): Used for further extension.</p></li>
</ul>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator only supports some specific input and output data types, then the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>The following examples introduce the development process of aot type custom operator on GPU platform and CPU platform, where the custom operator implements the function of adding two input tensors.</p>
<section id="a-gpu-example">
<h4>A GPU Example<a class="headerlink" href="#a-gpu-example" title="Permalink to this headline"></a></h4>
<p>Use the CUDA language to write the source file add.cu for the operator implementation:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#define THREADS 1024</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">CustomAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">THREADS</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">CustomAdd</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span>
<span class="w">                         </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">custream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">nparam</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ndims</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">THREADS</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;float32&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">CustomAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">THREADS</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">custream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input1</span><span class="p">),</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input2</span><span class="p">),</span>
<span class="w">                                                   </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Compile add.cu into a dynamic library add.so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nvcc<span class="w"> </span>--shared<span class="w"> </span>-Xcompiler<span class="w"> </span>-fPIC<span class="w"> </span>-o<span class="w"> </span>add.so<span class="w"> </span>add.cu
</pre></div>
</div>
<p>Write the test case test_custom_aot.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of aot type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.so:CustomAdd&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>In this example, you need to place test_custom_aot.py and add.so in the same directory. If add.so is in another directory, you need to replace the value of the first parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive with the absolute path of add.so.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_aot.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="a-cpu-example">
<h4>A CPU Example<a class="headerlink" href="#a-cpu-example" title="Permalink to this headline"></a></h4>
<p>Use C/C++ language to write the source file add.cc for the operator implementation:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
<span class="k">using</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span><span class="p">));</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">CustomAdd</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">nparam</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;float32&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Compile add.cc into a dynamic library add.so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>g++<span class="w"> </span>--shared<span class="w"> </span>-fPIC<span class="w"> </span>-o<span class="w"> </span>add.so<span class="w"> </span>add.cc
</pre></div>
</div>
<p>Write the test case test_custom_aot.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of aot type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.so:CustomAdd&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>In this example, you need to place test_custom_aot.py and add.so in the same directory. If add.so is in another directory, you need to replace the value of the first parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive with the absolute path of add.so.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_aot.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
</section>
<section id="defining-custom-operator-of-pyfunc-type">
<h3>Defining Custom Operator of pyfunc Type<a class="headerlink" href="#defining-custom-operator-of-pyfunc-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of pyfunc type uses native Python syntax to define the operator implementation, which describes the internal calculation logic of the operator. The framework will automatically call this function during the network runtime.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator only supports some specific input and output data types, then the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes test_custom_pyfunc.py as an example to introduce how to define a custom operator of pyfunc type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_pyfunc.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of pyfunc type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;pyfunc&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_pyfunc.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-julia-type">
<h3>Defining Custom Operator of julia Type<a class="headerlink" href="#defining-custom-operator-of-julia-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of julia type uses Julia to describe the internal calculation logic of the operator. The framework will automatically call this function during the network runtime.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator has attributes or only supports specific input and output data types or data formats, the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>. If the operator information is not registered, then the operator information will be derived from the inputs of the current operator during the operator selection process.</p>
<p>Takes the function of adding two input tensors as an example to introduce how to define a custom operator of julia type.</p>
<p>Firstly, users should write a Julia function into a Julia file. Here is an example of add.jl:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># add.jl</span>
<span class="k">module</span><span class="w"> </span><span class="n">Add</span>
<span class="c"># inputs: x, y, output: z, output should use .= to inplace assign</span>
<span class="k">function</span><span class="w"> </span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span>
<span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Secondly, use the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> operator with julia func type in the script to call Julia function, here is an example of test_custom_julia.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.jl:Add:add&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;julia&quot;</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_julia.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>Matters need attention:</p>
<ol class="arabic">
<li><p>User should use Julia version &gt;= 1.6.0,</p></li>
<li><p>User should add <code class="docutils literal notranslate"><span class="pre">julia/lib</span></code> into <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>, consider julia-1.6.5:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># download julia-1.6.5</span>
wget<span class="w"> </span>https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.5-linux-x86_64.tar.gz
<span class="c1"># extract file</span>
tar<span class="w"> </span>xvf<span class="w"> </span>julia-1.6.5-linux-x86_64.tar.gz
<span class="c1"># if $JULIA_DIR not exist</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$PWD</span>/julia-1.6.5/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="c1"># else</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$JULIA_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Custom</span></code> operator’s first arg <code class="docutils literal notranslate"><span class="pre">func</span></code> should keep format like <code class="docutils literal notranslate"><span class="pre">file_name:module_name:func_name</span></code>, <code class="docutils literal notranslate"><span class="pre">file_name</span></code> should include path, suggest using absolute path.</p></li>
<li><p>Julia file should include <code class="docutils literal notranslate"><span class="pre">module</span></code>, <code class="docutils literal notranslate"><span class="pre">module</span></code> include <code class="docutils literal notranslate"><span class="pre">function</span></code>, both ends with <code class="docutils literal notranslate"><span class="pre">end</span></code>.</p></li>
<li><p>The Julia function called by kernel should keep inputs and outputs order same with kernel.</p></li>
<li><p>The Julia function called by kernel should use <code class="docutils literal notranslate"><span class="pre">.=</span></code> to write function result into output memory.</p></li>
<li><p>User should make sure Julia code is runnable.</p></li>
<li><p>User should make sure Julia third-party package exists when using it. Install package when not exist: <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pkg;</span> <span class="pre">pkg.add(&quot;somepkg&quot;)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">julia</span> <span class="pre">array</span></code> is <code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">major</span></code>, and <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">array</span></code> is <code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">major</span></code>, User should consider this when computing an un-elementwise function. Users can use the functions to transform layout between <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">array</span></code> and <code class="docutils literal notranslate"><span class="pre">julia</span> <span class="pre">array</span></code> as below:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">permutedims</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">reverse</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span><span class="w"> </span><span class="n">length</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">1</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">function</span><span class="w"> </span><span class="n">change_output_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">reshape</span><span class="p">(</span><span class="n">permutedims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">end</span>
</pre></div>
</div>
<p>An example of MatMul:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># julia array is column-major, numpy aray is row-major</span>
<span class="c"># user should change julia or numpy&#39;s layout to keep same behavior</span>
<span class="cm">#= EXAMPLE</span>
<span class="cm">A[2,3]               B[3,4]               C[2,4]</span>
<span class="cm">NUMPY:</span>
<span class="cm">[[1, 2, 3]       [[1, 2, 3, 4]         [[38, 44, 50,  56]</span>
<span class="cm"> [4, 5, 6]]       [5, 6, 7, 8]          [83, 98, 113,128]]</span>
<span class="cm">                  [9,10,11,12]]</span>
<span class="cm">JULIA:</span>
<span class="cm">change_input_to_row_major:</span>
<span class="cm">1.inputs read numpy data from memory:</span>
<span class="cm">[[1, 3, 5]       [[1, 4, 7,10]</span>
<span class="cm"> [2, 4, 6]]       [2, 5, 8,11]</span>
<span class="cm">                  [3, 6, 9,12]]</span>
<span class="cm">2.inputs after reshape(reverse(shape)):</span>
<span class="cm">[[1, 4]          [[1, 5, 9]</span>
<span class="cm"> [2, 5]           [2, 6,10]</span>
<span class="cm"> [3, 6]]          [3, 7,11]</span>
<span class="cm">                  [4, 8,12]]</span>
<span class="cm">3.inputs after transpose/permutedims:</span>
<span class="cm">[[1, 2, 3]       [[1, 2, 3, 4]         [[38, 44, 50,  56]</span>
<span class="cm"> [4, 5, 6]]       [5, 6, 7, 8]          [83, 98, 113,128]]</span>
<span class="cm">                  [9,10,11,12]]</span>
<span class="cm">change_output_to_row_major:</span>
<span class="cm">1.output after transpose/permutedims:</span>
<span class="cm">                                       [[38, 83]</span>
<span class="cm">                                        [44, 98]</span>
<span class="cm">                                        [50,113]</span>
<span class="cm">                                        [56,128]</span>
<span class="cm">2.output after reshape:</span>
<span class="cm">                                       [[38, 50, 83, 113]</span>
<span class="cm">                                        [44, 56, 98, 128]]</span>
<span class="cm">3.output read numpy data from memory:</span>
<span class="cm">                                       [[38, 44, 50,  56]</span>
<span class="cm">                                        [83, 98,113, 128]]</span>
<span class="cm">=#</span>
<span class="k">function</span><span class="w"> </span><span class="n">foo!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">gemm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">change_output_to_row_major</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="defining-custom-operator-of-akg-type">
<h3>Defining Custom Operator of akg Type<a class="headerlink" href="#defining-custom-operator-of-akg-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of akg type uses the <a class="reference external" href="https://gitee.com/mindspore/akg">MindSpore AKG</a> operator DSL to describe the internal calculation logic of the operator. MindSpore AKG is an operator development and compilation framework based on TVM(Tensor Virtual Machine) and Polyhedral technology, it supports multiple types of operator DSL, such as Hybrid, IR builder and TVM compute.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator has attributes or only supports specific input and output data types or data formats, the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>. If the operator information is not registered, then the operator information will be derived from the inputs of the current operator during the operator selection process.</p>
<p>Takes test_custom_akg.py as an example of how to define a custom operator of akg type, where the operator computes the sum of two tensors.</p>
<p>Here is the content of test_custom_akg.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, Hybrid DSL</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of akg type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context.set_context(device_target=&quot;GPU&quot;)</span></code> indicates that the operator runs on the GPU platform. To run on the Ascend platform, please compile an Ascend version of MindSpore and set the value of device_target to “Ascend”.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_akg.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="registering-the-operator-information">
<h3>Registering the Operator Information<a class="headerlink" href="#registering-the-operator-information" title="Permalink to this headline"></a></h3>
<p>The operator information describes the supported inputs and outputs data type, the supported inputs and outputs format, attributes, and target(platform information) of the operator implementation. It is used to select and map operators later. The operator information can be defined by using the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/ops/mindspore.ops.CustomRegOp.html#mindspore-ops-customregop">CustomRegOp</a> API, then you can use the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/ops/mindspore.ops.custom_info_register.html#mindspore-ops-custom-info-register">custom_info_register</a> decorator or just pass it to the <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter of <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive to bind the information to the operator implementation. The operator information will be registered to the operator information library on the MindSpore C++ side at last. The <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter takes higher priority than the <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator.</p>
<p>The target value in operator information can be “Ascend”, “GPU” or “CPU”. Which describes the operator information on a specific target. For the same operator implementation, it may have different supported data types on different targets, so you can use the target value in operator information to differ this. The operator information on a specific target will be registered only once.</p>
<blockquote>
<div><ul class="simple">
<li><p>The numbers and sequences of the input and output information defined in the operator information must be the same as those in the parameters of the operator implementation.</p></li>
<li><p>For the custom operator of akg type, if the operator has attributes, you need to register operator information, The attribute name in the operator information must be consistent with the attribute name used in the operator implementation. For the custom operator of tbe type, you need to register operator information. For the custom operator of aot type, since the operator implementation needs to be compiled into a dynamic library in advance, the decorator will not work, and the operator information can only be passed in through the <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter.</p></li>
<li><p>If the custom operator only supports a specific input and output data type or data format, the operator information needs to be registered so that the data type and data format can be checked when the operator is selected in the backend. For the case where the operator information is not provided, the information will be derived from the inputs of the current operator.</p></li>
</ul>
</div></blockquote>
</section>
<section id="defining-the-bprop-function-for-operators">
<h3>Defining the bprop Function for Operators<a class="headerlink" href="#defining-the-bprop-function-for-operators" title="Permalink to this headline"></a></h3>
<p>If an operator needs to support automatic differentiation, the backpropagation(bprop) function needs to be defined first and then passed to the <code class="docutils literal notranslate"><span class="pre">bprop</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In the bprop function, you need to describe the backward computation logic that uses the forward input, forward output, and output gradients to obtain the input gradients. The backward computation logic can be composed of built-in operators or custom backward operators.</p>
<p>Note the following points when defining the bprop function:</p>
<ul class="simple">
<li><p>The input parameter sequence of the bprop function is the forward input, forward output, and output gradients. For a multi-output operator, the forward output and output gradients are provided in the form of tuples.</p></li>
<li><p>The return value of the bprop function is tuples consisting of input gradients. The sequence of elements in a tuple is the same as that of the forward input parameters. Even if there is only one input gradient, the return value must be a tuple.</p></li>
</ul>
<p>Take test_grad.py as an example to show the usage of the backpropagation function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Forward computation of custom operator</span>
<span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="c1"># Backward computation of custom operator</span>
<span class="k">def</span> <span class="nf">square_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dout</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">dx</span>

<span class="c1"># Backpropagation function</span>
<span class="k">def</span> <span class="nf">bprop</span><span class="p">():</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">square_grad</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">custom_bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">custom_bprop</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Define a custom operator of akg type and provide a backpropagation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">bprop</span><span class="o">=</span><span class="n">bprop</span><span class="p">(),</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">Net</span><span class="p">())(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sens</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>The backpropagation function uses a custom operator of akg type, and the operator definition and use need to be separated, that is, the custom operator is defined outside the <code class="docutils literal notranslate"><span class="pre">custom_bprop</span></code> function and used inside the <code class="docutils literal notranslate"><span class="pre">custom_bprop</span></code> function.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_grad.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ 2.  8. 18.]
</pre></div>
</div>
<blockquote>
<div><p>More examples can be found in the MindSpore source code <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>.</p>
</div></blockquote>
</section>
<section id="mindspore-hybrid-developer-guide">
<h3>MindSpore Hybrid Developer Guide<a class="headerlink" href="#mindspore-hybrid-developer-guide" title="Permalink to this headline"></a></h3>
<p>MindSpore Hybrid DSL writes Python-like codes, such as function definitions, indents, and comments. With the decorator <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/ops/mindspore.ops.ms_hybrid.html"><code class="docutils literal notranslate"><span class="pre">&#64;ms_hybrid</span></code></a>, functions written by MindSpore Hybrid DSL can be used as a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> function, as well as used in the custom operators of the hybrid type.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">ms_hybrid</span>

<span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">outer_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">allocate</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">i2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">d</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span>
                <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">sin</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i2</span><span class="p">,</span> <span class="n">i1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="n">np_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">np_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">outer_product</span><span class="p">(</span><span class="n">np_x</span><span class="p">,</span> <span class="n">np_y</span><span class="p">))</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np_x</span><span class="p">)</span>
<span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np_y</span><span class="p">)</span>

<span class="n">test_op_akg</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">outer_product</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">test_op_akg</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>The detailed developer guide of MindSpore Hybrid DSL is as follows.</p>
<section id="variables">
<h4>Variables<a class="headerlink" href="#variables" title="Permalink to this headline"></a></h4>
<p>Variables MindSpore Hybrid DSL includes Tensor and Scalar.</p>
<p>Tensor variables, besides those in the inputs of the function, must be declared with <code class="docutils literal notranslate"><span class="pre">shape</span></code>和 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> before use.</p>
<ul class="simple">
<li><p>declare a output tensor by <code class="docutils literal notranslate"><span class="pre">output_tensor</span></code>, such as <code class="docutils literal notranslate"><span class="pre">output_tensor(shape,</span> <span class="pre">dtype)</span></code>.</p></li>
<li><p>declare an intermediate tensor by <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, such as <code class="docutils literal notranslate"><span class="pre">allocate(shape,</span> <span class="pre">dtype)</span></code>.</p></li>
</ul>
<p>Example of Tensor allocation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># We can use a and b directly as they are inputs of the function</span>

    <span class="c1"># d is a tensor with dtype fp16 and shape (2,), and will be used as an intermediate tensor</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">allocate</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>
    <span class="c1"># c is a tensor with same dtype and shape as a, and will be used as a output tensor</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># assign value to c by d</span>
    <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># c as output</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Scalar variables will regard its first assignment as the declaration. The assignment can be either a number or an expression. The place of the first assignment of a scalar variable defines its scope, such as inside a certain level of for loop. Using the variable outside its scope will lead to error.</p>
<p>Example of using Scalar variable:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="c1"># i loop</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="c1"># j loop</span>
            <span class="c1"># assign a number to Scalar d</span>
            <span class="n">d</span> <span class="o">=</span> <span class="mf">2.0</span>
            <span class="c1"># assign an expression to Scalar e</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="c1"># use scalars</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">e</span>

    <span class="c1"># Wrong: c[0, 0] = d</span>
    <span class="c1"># Can&#39;t use Scalar d outside its scope (j loop)</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Unlike native Python language, once a variable is defined, we can’t change its <code class="docutils literal notranslate"><span class="pre">shape</span></code>和 <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</section>
<section id="expressions">
<h4>Expressions<a class="headerlink" href="#expressions" title="Permalink to this headline"></a></h4>
<p>MindSpore Hybrid DSL supports basic math operators, including <code class="docutils literal notranslate"><span class="pre">+,</span> <span class="pre">-,</span> <span class="pre">*,</span> <span class="pre">/</span></code>, as well as self-assign operators, including <code class="docutils literal notranslate"><span class="pre">=,</span> <span class="pre">+=,</span> <span class="pre">-=,</span> <span class="pre">*=,</span> <span class="pre">/=</span></code>.
Users can write codes like writing Python expressions.</p>
<p><strong>All the expressions must be based on scalars. Computation for the tensors must include all indices, such as <code class="docutils literal notranslate"><span class="pre">C[i,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">A[i,</span> <span class="pre">j]</span> <span class="pre">+</span> <span class="pre">B[i,</span> <span class="pre">j]</span></code>. Currently, tensorized codes such as <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">A</span> <span class="pre">+</span> <span class="pre">B</span></code> are not supported.</strong></p>
<p>When writing assignment expressions, users must take care of the dtype of the expression and make them consistent on both sides of the equality. Otherwise, the error might be thrown on the stage of <strong>operator compilation</strong>. Any integer numbers in the expression will be treated as int32, while float numbers will be treated as float32. There is no implicit dtype casting in MindSpore Hybrid DSL, and all dtype casting must be written with dtype names as casting functions, including:</p>
<ul class="simple">
<li><p>int32</p></li>
<li><p>float16</p></li>
<li><p>float32</p></li>
<li><p>(only on gpu)int8, int16, int64, float64</p></li>
</ul>
<p>Example of dtype casting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="c1"># Wrong: c[0] = 0.1 c&#39;s dtype is fp16, while 0.1&#39;s dtype is fp32</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">float16</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># float16(0.1) cast the number 0.1 to dtype fp16</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">float16</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># float16(a[0, 0])cast the number 0.1 to dtype fp16</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
</section>
<section id="loop">
<h4>Loop<a class="headerlink" href="#loop" title="Permalink to this headline"></a></h4>
<p>Currently, only the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is supported. <code class="docutils literal notranslate"><span class="pre">while</span></code>, <code class="docutils literal notranslate"><span class="pre">break</span></code>, and <code class="docutils literal notranslate"><span class="pre">continue</span></code> are illegal in MindSpore Hybrid DSL.</p>
<p>Loops are the same as those in Python. <code class="docutils literal notranslate"><span class="pre">range</span></code> and <code class="docutils literal notranslate"><span class="pre">grid</span></code> are supported to express extents of loops. <code class="docutils literal notranslate"><span class="pre">range</span></code> is for one-dimensional loops and accept a number as the upper bound of the loop, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
                <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>The iteration space of the above loops is <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">i</span> <span class="pre">&lt;</span> <span class="pre">3,</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">j</span> <span class="pre">&lt;</span> <span class="pre">4,</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">k</span> <span class="pre">&lt;</span> <span class="pre">5</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">grid</span></code> is for multi-dimensional loops and accepts <code class="docutils literal notranslate"><span class="pre">tuple</span></code> as its input. For example, the above code can be also written as follows in <code class="docutils literal notranslate"><span class="pre">grid</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>Right now <code class="docutils literal notranslate"><span class="pre">arg</span></code> is equivalent to a three dimensional index <code class="docutils literal notranslate"><span class="pre">(i,j,k)</span></code>, with upper bound 4, 5, 6 respectively. We also have access to each element in <code class="docutils literal notranslate"><span class="pre">arg</span></code>, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>Then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>.</p>
</section>
<section id="attribute">
<h4>Attribute<a class="headerlink" href="#attribute" title="Permalink to this headline"></a></h4>
<p>Current we support only tensor’s <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">dtype</span></code> attributes, such as <code class="docutils literal notranslate"><span class="pre">a.shape</span></code>, <code class="docutils literal notranslate"><span class="pre">c.dtype</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">shape</span></code> attribute of a Tensor variable is a <code class="docutils literal notranslate"><span class="pre">tuple</span></code>. We have access to its element with a <strong>fixed</strong> index, such as <code class="docutils literal notranslate"><span class="pre">a.shape[0]</span></code>.</p>
<p>Once <code class="docutils literal notranslate"><span class="pre">grid</span></code> accepts one tensor’s <code class="docutils literal notranslate"><span class="pre">shape</span></code> attribute as its input, then the dimension of the loops is the same as the dimension of the tensor. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_hybrid</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>If a is a two dimensional tensor, then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>, while if a is a three dimensional tensor, then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>.</p>
</section>
<section id="keywords">
<h4>Keywords<a class="headerlink" href="#keywords" title="Permalink to this headline"></a></h4>
<p>Currently, we support keywords including:</p>
<ul class="simple">
<li><p>Math keywords(all platform): <code class="docutils literal notranslate"><span class="pre">log</span></code>, <code class="docutils literal notranslate"><span class="pre">exp</span></code>, <code class="docutils literal notranslate"><span class="pre">sqrt</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, <code class="docutils literal notranslate"><span class="pre">power</span></code>, <code class="docutils literal notranslate"><span class="pre">floor</span></code></p></li>
<li><p>Allocate keywords: <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, <code class="docutils literal notranslate"><span class="pre">output_tensor</span></code></p></li>
<li><p>Datatype keywords: <code class="docutils literal notranslate"><span class="pre">int32</span></code>, <code class="docutils literal notranslate"><span class="pre">float16</span></code>, <code class="docutils literal notranslate"><span class="pre">float32</span></code>, <code class="docutils literal notranslate"><span class="pre">float64</span></code></p></li>
<li><p>For keywords: <code class="docutils literal notranslate"><span class="pre">for</span></code>, <code class="docutils literal notranslate"><span class="pre">range</span></code>, <code class="docutils literal notranslate"><span class="pre">grid</span></code></p></li>
<li><p>In current version, some GPU platform only keywords:</p>
<ul>
<li><p>Math keywords: <code class="docutils literal notranslate"><span class="pre">rsqrt</span></code>, <code class="docutils literal notranslate"><span class="pre">erf</span></code>, <code class="docutils literal notranslate"><span class="pre">isnan</span></code>, <code class="docutils literal notranslate"><span class="pre">sin</span></code>, <code class="docutils literal notranslate"><span class="pre">cos</span></code>, <code class="docutils literal notranslate"><span class="pre">isinf</span></code>, <code class="docutils literal notranslate"><span class="pre">isfinite</span></code>, <code class="docutils literal notranslate"><span class="pre">atan</span></code>, <code class="docutils literal notranslate"><span class="pre">atan2</span></code>, <code class="docutils literal notranslate"><span class="pre">expm1</span></code>, <code class="docutils literal notranslate"><span class="pre">floor</span></code>, <code class="docutils literal notranslate"><span class="pre">ceil</span></code>, <code class="docutils literal notranslate"><span class="pre">trunc</span></code>, <code class="docutils literal notranslate"><span class="pre">round</span></code>, <code class="docutils literal notranslate"><span class="pre">ceil_div</span></code></p></li>
<li><p>Datatype keywords: <code class="docutils literal notranslate"><span class="pre">int8</span></code>, <code class="docutils literal notranslate"><span class="pre">int16</span></code>, <code class="docutils literal notranslate"><span class="pre">int64</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="frequent-error-messages-and-error-attributions">
<h4>Frequent Error Messages and Error Attributions<a class="headerlink" href="#frequent-error-messages-and-error-attributions" title="Permalink to this headline"></a></h4>
<p>To help users effectively develop and locate bugs, MindSpore Hybrid DSL provides the following error messages, including:</p>
<ul class="simple">
<li><p>TypeError: there are Python keywords such as <code class="docutils literal notranslate"><span class="pre">while</span></code>, <code class="docutils literal notranslate"><span class="pre">break</span></code> and <code class="docutils literal notranslate"><span class="pre">continue</span></code> which are not supported by MindSpore Hybrid DSL.</p></li>
<li><p>ValueError:</p>
<ul>
<li><p>there are built-in function names which are not in the above support list;</p></li>
<li><p>in the DSL, it trys to get an attribute of a tensor, but the attribute name is neither <code class="docutils literal notranslate"><span class="pre">shape</span></code> nor <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p></li>
</ul>
</li>
<li><p>Other frequent error message:</p>
<ul>
<li><p>“SyntaxError”: DSL does not conform to the Python syntax(not the syntax defined by MindSpore Hybrid DSL), and is reported by the Python interpreter itself;</p></li>
<li><p>“ValueError: Compile error” and “The pointer[kernel_mod] is null”: the kernel compiler fails in compiling the DSL. Check error messages from AKG for further information;</p></li>
<li><p>“Launch graph failed”: the compiled kernel fails in running. Check the error message from the hardware. For example, when the kernel fails in Ascend, there will be an “Ascend error occurred” message and corresponding hareware error messages.</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="op_ascend.html" class="btn btn-neutral float-left" title="Custom Operators (Ascend)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../infer/inference.html" class="btn btn-neutral float-right" title="Inference Model Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>