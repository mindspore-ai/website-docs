

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Inference Using the MindIR Model on Ascend 310 AI Processors &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference on the Ascend 310 AI Processor" href="ascend_310_air.html" />
    <link rel="prev" title="Inference on the Ascend 910 AI processor" href="ascend_910_mindir.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_gpu_mindir.html">Inference on a GPU/CPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Inference Using the MindIR Model on Ascend 310 AI Processors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preparing-the-development-environment">Preparing the Development Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exporting-the-mindir-model">Exporting the MindIR Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference-directory-structure">Inference Directory Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference-code">Inference Code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#infer-model-with-defining-preprocess-manually-main-cc">Infer model with defining preprocess manually: main.cc</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#data-preprocessing-by-cpu-operators">Data-preprocessing by CPU operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-pre-processing-by-ascend-310-operators">Data pre-processing by Ascend 310 operators</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#infer-model-without-defining-preprocess-main-hide-preprocess-cc">Infer model without defining preprocess: main_hide_preprocess.cc</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#introducing-building-script">Introducing Building Script</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-inference-code">Building Inference Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performing-inference-and-viewing-the-result">Performing Inference and Viewing the Result</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Inference Using the MindIR Model on Ascend 310 AI Processors</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/infer/ascend_310_mindir.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="inference-using-the-mindir-model-on-ascend-310-ai-processors">
<h1>Inference Using the MindIR Model on Ascend 310 AI Processors<a class="headerlink" href="#inference-using-the-mindir-model-on-ascend-310-ai-processors" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8//tutorials/experts/source_en/infer/ascend_310_mindir.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Ascend 310 is a highly efficient and integrated AI processor oriented to edge scenarios. This tutorial describes how to use MindSpore to perform inference on the Ascend 310 based on the MindIR model file. The process is as follows:</p>
<ol class="simple">
<li><p>Export the MindIR model file. The ResNet-50 model is used as an example.</p></li>
<li><p>Build the inference code to generate an executable file.</p></li>
<li><p>Load the saved MindIR model, perform inference, and view the result.</p></li>
</ol>
<blockquote>
<div><p>You can obtain the complete executable sample code at <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample</a>.</p>
</div></blockquote>
</div>
<div class="section" id="preparing-the-development-environment">
<h2>Preparing the Development Environment<a class="headerlink" href="#preparing-the-development-environment" title="Permalink to this headline">¶</a></h2>
<p>Refer to <a class="reference external" href="https://www.mindspore.cn/install/en">Installation Guide</a> to install Ascend environment and MindSpore.</p>
</div>
<div class="section" id="exporting-the-mindir-model">
<h2>Exporting the MindIR Model<a class="headerlink" href="#exporting-the-mindir-model" title="Permalink to this headline">¶</a></h2>
<p>Train the target network on the CPU/GPU/Ascend 910 AI Processor, save it as a checkpoint file, and export the model file in MindIR format through the network and checkpoint file. For details about the export process, see <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.8/advanced/train/save.html#export-mindir-model">Export MindIR Model</a>.</p>
<blockquote>
<div><p>The <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/sample_resources/ascend310_resnet50_preprocess_sample/resnet50_imagenet.mindir">resnet50_imagenet.mindir</a> is a sample MindIR file exported using the ResNet-50 model, whose BatchSize is 1. We also provide a ResNet-50 MindIR with data preprocess <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/sample_resources/ascend310_resnet50_preprocess_sample/resnet50_imagenet_preprocess.mindir">resnet50_imagenet_preprocess.mindir</a>.</p>
</div></blockquote>
</div>
<div class="section" id="inference-directory-structure">
<h2>Inference Directory Structure<a class="headerlink" href="#inference-directory-structure" title="Permalink to this headline">¶</a></h2>
<p>Create a directory to store the inference code project, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/ascend310_resnet50_preprocess_sample</span></code>. The directory code can be obtained from the <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample">official website</a>. The <code class="docutils literal notranslate"><span class="pre">model</span></code> directory stores the exported <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model files and the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory stores the images to be classified. The directory structure of the inference code project is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ascend310_resnet50_preprocess_sample
    ├── CMakeLists.txt                           // Build script
    ├── README.md                                // Usage description
    ├── main.cc                                  // Main function, infer with defining preprocess manually
    ├── main_hide_preprocess.cc                  // Main function2, infer without defining preprocess
    ├── model
    │   ├── resnet50_imagenet.mindir             // MindIR model file
    │   └── resnet50_imagenet_preprocess.mindir  // MindIR model file with data preprocess
    └── test_data
        ├── ILSVRC2012_val_00002138.JPEG          // Input sample image 1
        ├── ILSVRC2012_val_00003014.JPEG          // Input sample image 2
        ├── ...                                   // Input sample image n
</pre></div>
</div>
</div>
<div class="section" id="inference-code">
<h2>Inference Code<a class="headerlink" href="#inference-code" title="Permalink to this headline">¶</a></h2>
<div class="section" id="infer-model-with-defining-preprocess-manually-main-cc">
<h3>Infer model with defining preprocess manually: main.cc<a class="headerlink" href="#infer-model-with-defining-preprocess-manually-main-cc" title="Permalink to this headline">¶</a></h3>
<div class="section" id="data-preprocessing-by-cpu-operators">
<h4>Data-preprocessing by CPU operators<a class="headerlink" href="#data-preprocessing-by-cpu-operators" title="Permalink to this headline">¶</a></h4>
<p>Inference sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample/main.cc">https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample/main.cc</a> .</p>
<p>Using namespace of <code class="docutils literal notranslate"><span class="pre">mindspore</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore::dataset</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">ms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="p">;</span><span class="w"></span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">ds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="o">::</span><span class="nn">dataset</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Set global context, and device target is <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">310</span></code> and device id is <code class="docutils literal notranslate"><span class="pre">0</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend310_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Ascend310DeviceInfo</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="n">ascend310_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend310_info</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Load MindIR file:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load MindIR model</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span><span class="w"></span>
<span class="n">ms</span><span class="o">::</span><span class="n">Status</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">resnet_file</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Build model with graph object</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">resnet50</span><span class="p">;</span><span class="w"></span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Get information of this model:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<p>Load image file:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Readfile is a function to read images</span>
<span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="nf">ReadFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">file</span><span class="p">);</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">image_file</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Image preprocess (CPU operators):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create the CPU operator provided by MindData to get the function object</span>

<span class="c1">// Decode the input to RGB format</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">decode</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Decode</span><span class="p">());</span><span class="w"></span>
<span class="c1">// Resize the image to the given size</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Resize</span><span class="p">({</span><span class="mi">256</span><span class="p">}));</span><span class="w"></span>
<span class="c1">// Normalize the input</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">normalize</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Normalize</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="mf">0.485</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.456</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.406</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mf">0.229</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.224</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.225</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">}));</span><span class="w"></span>
<span class="c1">// Crop the input image at the center</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">center_crop</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">CenterCrop</span><span class="p">({</span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">}));</span><span class="w"></span>
<span class="c1">// shape (H, W, C) to shape (C, H, W)</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hwc2chw</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">HWC2CHW</span><span class="p">());</span><span class="w"></span>

<span class="c1">// // Define a MindData preprocessor</span>
<span class="n">ds</span><span class="o">::</span><span class="n">Execute</span><span class="w"> </span><span class="nf">preprocessor</span><span class="p">({</span><span class="n">decode</span><span class="p">,</span><span class="w"> </span><span class="n">resize</span><span class="p">,</span><span class="w"> </span><span class="n">normalize</span><span class="p">,</span><span class="w"> </span><span class="n">center_crop</span><span class="p">,</span><span class="w"> </span><span class="n">hwc2chw</span><span class="p">});</span><span class="w"></span>

<span class="c1">// Call the function object to get the processed image</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">preprocessor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">image</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Execute the model:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create outputs vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span><span class="w"></span>
<span class="c1">// Create inputs vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span><span class="w"></span>
<span class="w">                    </span><span class="n">image</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">image</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span><span class="w"></span>
<span class="c1">// Call the Predict function of Model for inference</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Print the result:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Output the maximum probability to the screen</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Image: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">image_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; infer result: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">GetMax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
</div>
<div class="section" id="data-pre-processing-by-ascend-310-operators">
<h4>Data pre-processing by Ascend 310 operators<a class="headerlink" href="#data-pre-processing-by-ascend-310-operators" title="Permalink to this headline">¶</a></h4>
<p>Dvpp module is a hardware decoder embedded in Ascend 310 AI chip which has a better performance on image processing compared with CPU operators. Several transforms applied on JPEG format image are supported.</p>
<p>Using namespace of <code class="docutils literal notranslate"><span class="pre">mindspore</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore::dataset</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">ms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="p">;</span><span class="w"></span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">ds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="o">::</span><span class="nn">dataset</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Set global context, and device target is <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">310</span></code> and device id is <code class="docutils literal notranslate"><span class="pre">0</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend310_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Ascend310DeviceInfo</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="n">ascend310_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend310_info</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Load image file:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Readfile is a function to read images</span>
<span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="nf">ReadFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">file</span><span class="p">);</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">image_file</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Image preprocess (Ascend 310 operators):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create the CPU operator provided by MindData to get the function object</span>

<span class="c1">// Decode the input to YUV420 format</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">decode</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Decode</span><span class="p">());</span><span class="w"></span>
<span class="c1">// Resize the image to the given size</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Resize</span><span class="p">({</span><span class="mi">256</span><span class="p">}));</span><span class="w"></span>
<span class="c1">// Normalize the input</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">normalize</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">Normalize</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="mf">0.485</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.456</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.406</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mf">0.229</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.224</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">,</span><span class="w"> </span><span class="mf">0.225</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">255</span><span class="p">}));</span><span class="w"></span>
<span class="c1">// Crop the input image at the center</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">TensorTransform</span><span class="o">&gt;</span><span class="w"> </span><span class="n">center_crop</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">ds</span><span class="o">::</span><span class="n">vision</span><span class="o">::</span><span class="n">CenterCrop</span><span class="p">({</span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">}));</span><span class="w"></span>
</pre></div>
</div>
<p>Image preprocess (Ascend 310 operators, 130% performance increasing compared to CPU operators).</p>
<p>Explicitly specify the computing hardware as Ascend 310.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Define a MindData preprocessor, set deviceType = kAscend310, device id = 0</span>
<span class="n">ds</span><span class="o">::</span><span class="n">Execute</span><span class="w"> </span><span class="nf">preprocessor</span><span class="p">({</span><span class="n">decode</span><span class="p">,</span><span class="w"> </span><span class="n">resize</span><span class="p">,</span><span class="w"> </span><span class="n">center_crop</span><span class="p">,</span><span class="w"> </span><span class="n">normalize</span><span class="p">},</span><span class="w"> </span><span class="n">MapTargetDevice</span><span class="o">::</span><span class="n">kAscend310</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>

<span class="c1">// Call the function object to get the processed image</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">preprocessor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">image</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Load MindIR file: Ascend 310 operators must bind with Aipp module, insert Aipp module for model graph compiling.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load MindIR model</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span><span class="w"></span>
<span class="n">ms</span><span class="o">::</span><span class="n">Status</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">resnet_file</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Build model with graph object</span>
<span class="n">ascend310_info</span><span class="o">-&gt;</span><span class="n">SetInsertOpConfigPath</span><span class="p">(</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">AippCfgGenerator</span><span class="p">());</span><span class="w"></span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">resnet50</span><span class="p">;</span><span class="w"></span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Get input information of this model:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<p>Execute inference:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create outputs vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span><span class="w"></span>
<span class="c1">// Create inputs vector</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span><span class="w"></span>
<span class="w">                    </span><span class="n">image</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">image</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span><span class="w"></span>
<span class="c1">// Call the Predict function of Model for inference</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Print the result:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Output the maximum probability to the screen</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Image: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">image_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; infer result: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">GetMax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="infer-model-without-defining-preprocess-main-hide-preprocess-cc">
<h3>Infer model without defining preprocess: main_hide_preprocess.cc<a class="headerlink" href="#infer-model-without-defining-preprocess-main-hide-preprocess-cc" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Note: Only supports CV models currently.</p>
</div></blockquote>
<p>Inference sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample/main_hide_preprocess.cc">https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample/main_hide_preprocess.cc</a> .</p>
<p>Using namespace of <code class="docutils literal notranslate"><span class="pre">mindspore</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore::dataset</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">ms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="p">;</span><span class="w"></span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">ds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nn">mindspore</span><span class="o">::</span><span class="nn">dataset</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Set global context, and device target is <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">310</span></code> and device id is <code class="docutils literal notranslate"><span class="pre">0</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend310_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">Ascend310DeviceInfo</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="n">ascend310_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend310_info</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Load MindIR file: When there is a definition of data preprocessing in MindIR, it is loaded automatically.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load MindIR model</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span><span class="w"></span>
<span class="n">ms</span><span class="o">::</span><span class="n">Status</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">resnet_file</span><span class="p">,</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Build model with graph object</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">resnet50</span><span class="p">;</span><span class="w"></span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Get information of this model and check if model has preprocess:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">resnet50</span><span class="p">.</span><span class="n">HasPreprocess</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;data preprocess not exists in MindIR&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Read image and execute data preprocessing and model inference:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="n">t1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">::</span><span class="n">CreateTensorFromFile</span><span class="p">(</span><span class="n">image_file</span><span class="p">);</span><span class="w"></span>
<span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{{</span><span class="o">*</span><span class="n">t1</span><span class="p">}};</span><span class="w"></span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span><span class="w"></span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resnet50</span><span class="p">.</span><span class="n">PredictWithPreprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="p">.</span><span class="n">IsError</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;ERROR: PredictWithPreprocess failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Print the result:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Obtain the maximum probability of the inference result</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Image: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">image_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; infer result: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">GetMax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span><span class="w"></span>

<span class="c1">// Note that the pointer resource t1 needs to be released at the end</span>
<span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">::</span><span class="n">DestroyTensorPtr</span><span class="p">(</span><span class="n">t1</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="introducing-building-script">
<h2>Introducing Building Script<a class="headerlink" href="#introducing-building-script" title="Permalink to this headline">¶</a></h2>
<p>The building script is used to building applications and the sample is from: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample/CMakeLists.txt">https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/ascend310_resnet50_preprocess_sample/CMakeLists.txt</a>.</p>
<p>Add head files to gcc search path to the compiler:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">option</span><span class="p">(</span><span class="s">MINDSPORE_PATH</span> <span class="s2">&quot;mindspore install path&quot;</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/include</span><span class="p">)</span>
</pre></div>
</div>
<p>Find the shared libraries in MindSpore:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">find_library</span><span class="p">(</span><span class="s">MS_LIB</span> <span class="s">libmindspore.so</span> <span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>
<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span> <span class="s">MD_LIB</span> <span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/_c_dataengine*</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the source files to generate the target executable file, and link the MindSpore libraries for the executable file:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">resnet50_sample</span> <span class="s">main.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">resnet50_sample</span> <span class="o">${</span><span class="nv">MS_LIB</span><span class="o">}</span> <span class="o">${</span><span class="nv">MD_LIB</span><span class="o">}</span><span class="p">)</span>

<span class="nb">add_executable</span><span class="p">(</span><span class="s">resnet50_hide_preprocess</span> <span class="s">main_hide_preprocess.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">resnet50_hide_preprocess</span> <span class="o">${</span><span class="nv">MS_LIB</span><span class="o">}</span> <span class="o">${</span><span class="nv">MD_LIB</span><span class="o">}</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="building-inference-code">
<h2>Building Inference Code<a class="headerlink" href="#building-inference-code" title="Permalink to this headline">¶</a></h2>
<p>Go to the project directory <code class="docutils literal notranslate"><span class="pre">ascend310_resnet50_preprocess_sample</span></code> and set the following environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># control log level. 0-DEBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.</span>
<span class="nb">export</span> <span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>

<span class="c1"># Conda environmental options</span>
<span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend <span class="c1"># the root directory of run package</span>

<span class="c1"># lib libraries that the run package depends on</span>
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe/op_tiling:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>

<span class="c1"># lib libraries that the mindspore depends on, modify &quot;pip3&quot; according to the actual situation</span>
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="sb">`</span>pip3 show mindspore-ascend <span class="p">|</span> grep Location <span class="p">|</span> awk <span class="s1">&#39;{print $2&quot;/mindspore/lib&quot;}&#39;</span> <span class="p">|</span> xargs realpath<span class="sb">`</span>:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
<span class="c1"># if MindSpore is installed by binary, run &quot;export LD_LIBRARY_PATH=path-to-your-custom-dir:${LD_LIBRARY_PATH}&quot;</span>

<span class="c1"># Environment variables that must be configured</span>
<span class="nb">export</span> <span class="nv">TBE_IMPL_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe            <span class="c1"># TBE operator implementation tool path</span>
<span class="nb">export</span> <span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/opp                                       <span class="c1"># OPP path</span>
<span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/ascend-toolkit/latest/compiler/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span>                  <span class="c1"># TBE operator compilation tool path</span>
<span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">TBE_IMPL_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>                                                       <span class="c1"># Python library that TBE implementation depends on</span>
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">cmake</span></code> command, and modify <code class="docutils literal notranslate"><span class="pre">pip3</span></code> according to the actual situation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake . -DMINDSPORE_PATH<span class="o">=</span><span class="sb">`</span>pip3 show mindspore-ascend <span class="p">|</span> grep Location <span class="p">|</span> awk <span class="s1">&#39;{print $2&quot;/mindspore&quot;}&#39;</span> <span class="p">|</span> xargs realpath<span class="sb">`</span>
<span class="c1"># if MindSpore is installed by binary, run &quot;cmake . -DMINDSPORE_PATH=path-to-your-custom-dir&quot;</span>
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">make</span></code> command for building.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
<p>After building, the executable file is generated in <code class="docutils literal notranslate"><span class="pre">ascend310_resnet50_preprocess_sample</span></code>.</p>
</div>
<div class="section" id="performing-inference-and-viewing-the-result">
<h2>Performing Inference and Viewing the Result<a class="headerlink" href="#performing-inference-and-viewing-the-result" title="Permalink to this headline">¶</a></h2>
<p>Log in to the Ascend 310 server, and create the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory for storing the MindIR file <code class="docutils literal notranslate"><span class="pre">resnet50_imagenet.mindir</span></code>, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/ascend310_resnet50_preprocess_sample/model</span></code>.
Create the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory to store images, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/ascend310_resnet50_preprocess_sample/test_data</span></code>.
Then, perform the inference.</p>
<p>If your MindIR file does not contain preprocess information, you can execute the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./resnet50_sample
</pre></div>
</div>
<p>Inference is performed on all images stored in the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory. For example, if there are 9 images whose label is 0 in the <a class="reference external" href="http://image-net.org/download-images">ImageNet2012</a> validation set, the inference result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Image: ./test_data/ILSVRC2012_val_00002138.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00003014.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00006697.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00007197.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009111.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009191.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009346.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009379.JPEG infer result: 0
Image: ./test_data/ILSVRC2012_val_00009396.JPEG infer result: 0
</pre></div>
</div>
<p>If you export the preprocess information simultaneously when you export a MindIR file, you can execute the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./resnet50_hide_preprocess
</pre></div>
</div>
<p>The model will load the image file inside the <code class="docutils literal notranslate"><span class="pre">test_data</span></code> directory (for example: ILSVRC2012_val_00002138.JPEG, configable in main_hide_preprocess.cc) and start prediction, then you get the inference result as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Image: ./test_data/ILSVRC2012_val_00002138.JPEG infer result: 0
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="ascend_310_air.html" class="btn btn-neutral float-right" title="Inference on the Ascend 310 AI Processor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="ascend_910_mindir.html" class="btn btn-neutral float-left" title="Inference on the Ascend 910 AI processor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>