

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Inference on a GPU &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference on the Ascend 910 AI processor" href="ascend_910_mindir.html" />
    <link rel="prev" title="Inference Model Overview" href="inference.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference Model Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Inference on a GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#use-c++-interface-to-load-a-mindir-file-for-inferencing">Use C++ Interface to Load a MindIR File for Inferencing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#inference-directory-structure">Inference Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-code">Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#introducing-building-script">Introducing Building Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-inference-code">Building Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-inference-and-viewing-the-result">Performing Inference and Viewing the Result</a></li>
<li class="toctree-l3"><a class="reference internal" href="#notices">Notices</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inference-by-using-an-onnx-file">Inference by Using an ONNX File</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Inference on a GPU</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/infer/cpu_gpu_mindir.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="inference-on-a-gpu">
<h1>Inference on a GPU<a class="headerlink" href="#inference-on-a-gpu" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/experts/source_en/infer/cpu_gpu_mindir.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<section id="use-c++-interface-to-load-a-mindir-file-for-inferencing">
<h2>Use C++ Interface to Load a MindIR File for Inferencing<a class="headerlink" href="#use-c++-interface-to-load-a-mindir-file-for-inferencing" title="Permalink to this headline"></a></h2>
<p>Users can create C++ applications to call MindSpore’s C++ interface to infer the MindIR model.</p>
<section id="inference-directory-structure">
<h3>Inference Directory Structure<a class="headerlink" href="#inference-directory-structure" title="Permalink to this headline"></a></h3>
<p>Create a directory to store the inference code project, for example, <code class="docutils literal notranslate"><span class="pre">/home/mindspore_sample/gpu_resnet50_inference_sample</span></code>. You can download the <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/gpu_resnet50_inference_sample">sample code</a> from the official website. The <code class="docutils literal notranslate"><span class="pre">model</span></code> directory is used to store the exported <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/sample_resources/ascend310_resnet50_preprocess_sample/resnet50_imagenet.mindir">model file</a>. The directory structure of the inference code project is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─gpu_resnet50_inference_sample
    ├── build.sh                          // Build script
    ├── CMakeLists.txt                    // CMake script
    ├── README.md                         // Usage description
    ├── src
    │   └── main.cc                       // Main function
    └── model
        └── resnet50_imagenet.mindir      // MindIR model file
</pre></div>
</div>
</section>
<section id="inference-code">
<h3>Inference Code<a class="headerlink" href="#inference-code" title="Permalink to this headline"></a></h3>
<p>Inference sample code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/gpu_resnet50_inference_sample/src/main.cc">https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/gpu_resnet50_inference_sample/src/main.cc</a> .</p>
<p>Using namespace of <code class="docutils literal notranslate"><span class="pre">mindspore</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Serialization</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Status</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="p">;</span>
</pre></div>
</div>
<p>Initialize the environment, specify the hardware platform used for inference, and set DeviceID and precision.</p>
<p>Set the hardware to GPU, set DeviceID to 0 and inference precision Mode to FP16. The code example is as follows:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetPrecisionMode</span><span class="p">(</span><span class="s">&quot;fp16&quot;</span><span class="p">);</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>Load the model file.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load the MindIR model.</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">mindir_path</span><span class="p">,</span><span class="w"> </span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>
<span class="c1">// Build a model using a graph.</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
</pre></div>
</div>
<p>Obtain the input information required by the model.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>Construct network inputs.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">dummy_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="kt">float</span><span class="p">[</span><span class="mi">1</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="mi">224</span><span class="p">];</span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span>
<span class="w">                      </span><span class="n">dummy_data</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</pre></div>
</div>
<p>Start inference.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create an output vector.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="c1">// Create an input vector.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span>
<span class="w">                    </span><span class="n">image</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">image</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span>
<span class="c1">// Call the Predict function of the model for inference.</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="introducing-building-script">
<h3>Introducing Building Script<a class="headerlink" href="#introducing-building-script" title="Permalink to this headline"></a></h3>
<p>Add the header file search path for the compiler:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">option</span><span class="p">(</span><span class="s">MINDSPORE_PATH</span><span class="w"> </span><span class="s2">&quot;mindspore install path&quot;</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/include</span><span class="p">)</span>
</pre></div>
</div>
<p>Search for the required dynamic library in MindSpore.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">find_library</span><span class="p">(</span><span class="s">MS_LIB</span><span class="w"> </span><span class="s">libmindspore.so</span><span class="w"> </span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the specified source file to generate the target executable file and link the target file to the MindSpore library.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">main</span><span class="w"> </span><span class="s">src/main.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">main</span><span class="w"> </span><span class="o">${</span><span class="nv">MS_LIB</span><span class="o">}</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>For details, see
<a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/gpu_resnet50_inference_sample/CMakeLists.txt">https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/gpu_resnet50_inference_sample/CMakeLists.txt</a></p>
</div></blockquote>
</section>
<section id="building-inference-code">
<h3>Building Inference Code<a class="headerlink" href="#building-inference-code" title="Permalink to this headline"></a></h3>
<p>Next compile the inference code, and go to the project directory <code class="docutils literal notranslate"><span class="pre">gpu_resnet50_inference_sample</span></code>:</p>
<p>According to the actual situation, the <code class="docutils literal notranslate"><span class="pre">pip3</span></code> in the build.sh can be modified, and the <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">build.sh</span></code> command can be compiled after the modification is completed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<p>After building, the executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file is generated in <code class="docutils literal notranslate"><span class="pre">gpu_resnet50_inference_sample/out</span></code>.</p>
</section>
<section id="performing-inference-and-viewing-the-result">
<h3>Performing Inference and Viewing the Result<a class="headerlink" href="#performing-inference-and-viewing-the-result" title="Permalink to this headline"></a></h3>
<p>After completing the preceding operations, you can learn how to perform inference.</p>
<p>Log in to the GPU environment, and create the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory to store the <code class="docutils literal notranslate"><span class="pre">resnet50_imagenet.mindir</span></code> file, for example, <code class="docutils literal notranslate"><span class="pre">/home/mindspore_sample/gpu_resnet50_inference_sample/model</span></code>.</p>
<p>Set the environment variable base on the actual situation, where the TensorRT is an optional configuration item. It is recommended to add <code class="docutils literal notranslate"><span class="pre">TensorRT</span></code> path to <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> to improve mode inference performance.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>/home/miniconda3/lib/libpython37m.so
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/TensorRT-7.2.2.3/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>Then, perform inference.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>out/
./main<span class="w"> </span>../model/resnet50_imagenet.mindir<span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
<p>In the current test script, we printed the inference delay and average delay for each step:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Start to load model..
Load model successuflly
Start to warmup..
Warmup finished
Start to infer..
step 0 cost 1.54004ms
step 1 cost 1.5271ms
... ...
step 998 cost 1.30688ms
step 999 cost 1.30493ms
infer finished.
=================Average inference time: 1.35195 ms
</pre></div>
</div>
</section>
<section id="notices">
<h3>Notices<a class="headerlink" href="#notices" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>During the training process, some networks set operator precision to FP16 artificially. For example, the <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/nlp/bert/src/bert_model.py">Bert mode</a> set the Dense and LayerNorm to FP16:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BertOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
                 <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertOutput</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Set the nn.Dense to fp16.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                              <span class="n">weight_init</span><span class="o">=</span><span class="n">TruncatedNormal</span><span class="p">(</span><span class="n">initializer_range</span><span class="p">))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="n">dropout_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="c1"># Set the nn.LayerNorm to fp16.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">((</span><span class="n">out_channels</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="o">...</span> <span class="o">...</span>
</pre></div>
</div>
<p>It is recommended that export the MindIR model with fp32 precision mode before deploying inference. If you want to further improve the inference performance, the inference precision can be set to FP16 through <code class="docutils literal notranslate"><span class="pre">mindspore::GPUDeviceInfo::SetPrecisionMode</span> <span class="pre">(&quot;fp16&quot;)</span></code>,and the framework automatically selects the operator inference with the better performance.</p>
<ul class="simple">
<li><p>Some inference scripts may introduce some unique network structures in the training process. For example, the model requires the image label, which are transmitted to the network output directly. It is suggested to delete this part of operators and then export MindIR model to improve inference performance.</p></li>
</ul>
</section>
</section>
<section id="inference-by-using-an-onnx-file">
<h2>Inference by Using an ONNX File<a class="headerlink" href="#inference-by-using-an-onnx-file" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Generate a model in ONNX format on the training platform. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.8/advanced/train/save.html#export-onnx-model">Export ONNX Model</a>.</p></li>
<li><p>Perform inference on a GPU by referring to the runtime or SDK document. For example, use TensorRT to perform inference on the Nvidia GPU. For details, see <a class="reference external" href="https://github.com/onnx/onnx-tensorrt">TensorRT backend for ONNX</a>.</p></li>
</ol>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="inference.html" class="btn btn-neutral float-left" title="Inference Model Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ascend_910_mindir.html" class="btn btn-neutral float-right" title="Inference on the Ascend 910 AI processor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>