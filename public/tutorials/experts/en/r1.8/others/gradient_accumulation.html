

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gradient Accumulation Algorithm &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Adaptive Gradient Summation Algorithm" href="adaptive_summation.html" />
    <link rel="prev" title="Enabling Mixed Precision" href="mixed_precision.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradient Accumulation Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-accumulation-principle">Gradient Accumulation Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-accumulation-implement">Gradient Accumulation Implement</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#standalone-mode">Standalone Mode</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#importing-library-files">Importing Library Files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loading-the-dataset">Loading the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-training-process">Defining the Training Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-training-model">Defining the Training Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-and-saving-the-model">Training and Saving the Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#experiment-result">Experiment Result</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#boost-model">Boost Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#importing-library-files-1">Importing Library Files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loading-the-dataset-1">Loading the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-network-1">Defining the Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-training-model-1">Defining the Training Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-the-model-and-make-inference">Training the Model and Make Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#experiment-result-1">Experiment Result</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Gradient Accumulation Algorithm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/others/gradient_accumulation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="gradient-accumulation-algorithm">
<h1>Gradient Accumulation Algorithm<a class="headerlink" href="#gradient-accumulation-algorithm" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/experts/source_en/others/gradient_accumulation.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial introduces the training algorithm of gradient accumulation, the purpose of which is to solve the OOM (Out Of Memory) problem that the Batch size is too large to train the neural network or the network model is too large to load due to insufficient memory.</p>
</section>
<section id="gradient-accumulation-principle">
<h2>Gradient Accumulation Principle<a class="headerlink" href="#gradient-accumulation-principle" title="Permalink to this headline"></a></h2>
<p>Gradient accumulation is a way of training a neural network in which data samples are split into several small Batches by Batch and then calculated sequentially.</p>
<p>Before we discuss the gradient accumulation further, check the calculation process of the neural network.</p>
<p>Deep learning models are made up of many interconnected neural network units, and in all neural network layers, sample data propagates continuously forward. After passing through all the layers, the network model outputs the predicted values of the samples, and then calculates the loss values (errors) for each sample through the loss function. The neural network calculates the gradient of the loss value relative to the model parameters by backpropagation. Finally, the gradient information is used to update the parameters in the network model.</p>
<p>The optimizer is a mathematical formula used to update the weight parameters of the network model. Take a simple stochastic gradient descent (SGD) algorithm as an example.</p>
<p>Assuming the Loss Function function formula is:</p>
<div class="math notranslate nohighlight">
\[Loss(\theta)=\frac{1}{2}\left(h(x^{k})-y^{k}\right)^{2}\]</div>
<p>When building a model, the optimizer is used to calculate the algorithm that minimizes losses. Here the SGD algorithm uses the Loss function to update the weight parameter formula as follows:</p>
<div class="math notranslate nohighlight">
\[\theta{i}=\theta_{i-1}-lr * grad_{i}\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is the trainable parameter (weight or error) in the network model. lr is the learning rate, and <span class="math notranslate nohighlight">\(grad_{i}\)</span> is the loss relative to network model parameter.</p>
<p>Gradient accumulation only calculates the neural network model, does not update the parameters of the network model in time, and accumulates the gradient information when calculation, and finally uses the accumulated gradient to update the parameters.</p>
<div class="math notranslate nohighlight">
\[accumulated=\sum_{i=0}^{N} grad_{i}\]</div>
<p>When the model variables are not updated, the original data Batch is actually divided into several Mini-Batches, and the samples used in each step are actually smaller data sets.</p>
<p>The variables are not updated within N steps, so that all Mini-Batches use the same model variables to calculate the gradient, to ensure that the same gradient and weight information is calculated, which is equivalent to using the original Batch size without splitting.</p>
<div class="math notranslate nohighlight">
\[\theta{i}=\theta_{i-1}-lr * \sum_{i=0}^{N} grad_{i}\]</div>
<p>Eventually accumulating the gradient in the previous step yields the sum of the gradients of the same size as using the global Batche size.</p>
<p><img alt="" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/experts/source_zh_cn/others/images/GradientAccumulation1.png" /></p>
<p>In the actual project, there are two points to pay attention to on the tuning parameters and algorithms:</p>
<ol class="arabic simple">
<li><p><strong>learning rate</strong>: Under certain conditions, the larger the Batch size, the better the training effect. The gradient accumulation simulates the effect of the increase of the Batch size. If the accumulation steps is 4, the Batch size is increased by 4 times. According to experience, the learning rate needs to be appropriately amplified when using gradient accumulation.</p></li>
<li><p><strong>Batch Norm</strong>: Batch size simulation amplification effect is performed when the accumulation steps are 4. Compared with the real Batch size, the distribution of the data is not exactly the same, and the mean and variance calculated by BN of 4 times Batch size is not the same as the actual data mean and variance, so some implementations will use Group Norm instead of Batch Norm.</p></li>
</ol>
</section>
<section id="gradient-accumulation-implement">
<h2>Gradient Accumulation Implement<a class="headerlink" href="#gradient-accumulation-implement" title="Permalink to this headline"></a></h2>
<p>The following tutorial content will introduce the implementation of gradient accumulation training in standalone mode and Boost mode.</p>
<blockquote>
<div><p>Note: <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> and <code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code> don’t support the training way of gradient accumulation.</p>
</div></blockquote>
<section id="standalone-mode">
<h3>Standalone Mode<a class="headerlink" href="#standalone-mode" title="Permalink to this headline"></a></h3>
<p>In standalone mode, the training process consists of three parts: forward and backward training, parameter update, and accumulated gradient clearance.</p>
<p>MNIST is used as an example dataset. To customize a simple model to implement gradient accumulation, perform the following steps:</p>
<blockquote>
<div><p>Download the main training sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/gradient_accumulation/train.py">train.py</a>.</p>
</div></blockquote>
<p>Since you need to use the LeNet network in the models repository, please execute the following command to pull the code of the models repository:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>git clone https://gitee.com/mindspore/models.git
</pre></div>
</div>
<p>If the models repository is not in the system path, it needs to be in <code class="docutils literal notranslate"><span class="pre">train.py</span></code> add the following two pieces of code at the beginning of the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">path</span> <span class="n">to</span> <span class="n">models</span> <span class="n">repository</span><span class="p">)</span>
</pre></div>
</div>
<section id="importing-library-files">
<h4>Importing Library Files<a class="headerlink" href="#importing-library-files" title="Permalink to this headline"></a></h4>
<p>The following are the required public modules and MindSpore modules and library files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterable</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">models.official.cv.lenet.src.dataset</span> <span class="kn">import</span> <span class="n">create_dataset</span>
<span class="kn">from</span> <span class="nn">models.official.cv.lenet.src.lenet</span> <span class="kn">import</span> <span class="n">LeNet5</span>
</pre></div>
</div>
</section>
<section id="loading-the-dataset">
<h4>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h4>
<p>Use the <code class="docutils literal notranslate"><span class="pre">MnistDataset</span></code> API provided by <code class="docutils literal notranslate"><span class="pre">dataset</span></code> of MindSpore to load the MNIST dataset. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/lenet/src/dataset.py">dataset.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory of models.</p>
</section>
<section id="defining-the-network">
<h4>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h4>
<p>LeNet is used as an example network. You can also use other networks, such as ResNet-50 and BERT. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/lenet/src/lenet.py">lenet.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory of models.</p>
</section>
<section id="defining-the-training-process">
<h4>Defining the Training Process<a class="headerlink" href="#defining-the-training-process" title="Permalink to this headline"></a></h4>
<p>The training process consists of three parts: forward and backward training, parameter update, and accumulated gradient clearance.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TrainForwardBackward</span></code> calculates the loss and gradient, and uses grad_sum to implement gradient accumulation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainOptim</span></code> updates parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainClear</span></code> clears the gradient accumulation variable grad_sum.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_sum_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;grad_sum_op&quot;</span><span class="p">)</span>
<span class="n">_clear_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;clear_op&quot;</span><span class="p">)</span>


<span class="nd">@_sum_op</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_cumulative_grad</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply grad sum to cumulative gradient.&quot;&quot;&quot;</span>
    <span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AssignAdd</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="nd">@_clear_op</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_clear_grad_sum</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">zero</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply zero to clear grad_sum.&quot;&quot;&quot;</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">success</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">success</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">zero</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">success</span>


<span class="k">class</span> <span class="nc">TrainForwardBackward</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainForwardBackward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">add_flags</span><span class="p">(</span><span class="n">defer_inline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">ParameterTuple</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="n">sens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">ops</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_sum_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">TrainOptim</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOptim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TrainClear</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">,</span> <span class="n">zeros</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainClear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span> <span class="o">=</span> <span class="n">zeros</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_clear_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">success</span>
</pre></div>
</div>
</section>
<section id="defining-the-training-model">
<h4>Defining the Training Model<a class="headerlink" href="#defining-the-training-model" title="Permalink to this headline"></a></h4>
<p>Each mini-batch computes the loss and gradient through forward and backward training, and uses mini_steps to control the accumulated times before each parameter update. After the number of accumulation times is reached, the parameter is updated and the accumulated gradient variable is cleared.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientAccumulation</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;grad_sum&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_zeros</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_forward_backward_network</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_optim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_optim</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_clear</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_clear</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_transform_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform callback to a list.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">callbacks</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_train_forward_backward_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build forward and backward network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">)</span>
        <span class="n">loss_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainForwardBackward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">,</span> <span class="n">loss_scale</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_build_train_optim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build optimizer network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainOptim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_build_train_clear</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build clear network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainClear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zeros</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">train_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">mini_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Training process. The data would be passed to network directly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataset_helper</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">DatasetHelper</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epoch_num</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
            <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">next_element</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_helper</span><span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span><span class="p">(</span><span class="o">*</span><span class="n">next_element</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">mini_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch:&quot;</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;step:&quot;</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;loss is &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_optim</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_clear</span><span class="p">()</span>

            <span class="n">train_dataset</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span><span class="p">,</span> <span class="s2">&quot;gradient_accumulation.ckpt&quot;</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-and-saving-the-model">
<h4>Training and Saving the Model<a class="headerlink" href="#training-and-saving-the-model" title="Permalink to this headline"></a></h4>
<p>Call the network, optimizer, and loss function, and then customize the <code class="docutils literal notranslate"><span class="pre">train_process</span></code> API of <code class="docutils literal notranslate"><span class="pre">GradientAccumulation</span></code> to train the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;MindSpore Grad Cumulative Example&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device_target&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;device where the code will be implemented (default: GPU)&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_path&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;./Data&quot;</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;path where the dataset is saved&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
    <span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GradientAccumulation</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Training ==============&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train_process</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">mini_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="experiment-result">
<h4>Experiment Result<a class="headerlink" href="#experiment-result" title="Permalink to this headline"></a></h4>
<p>After 10 epochs, the accuracy on the test set is about 96.31%.</p>
<p><strong>Start training:</strong></p>
<ol class="arabic">
<li><p>Run the training code and view the running result.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>train.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data
</pre></div>
</div>
<p>The output is as follows. You can see that the loss value decreases with the training.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 27 loss is  0.3660637
epoch: 1 step: 28 loss is  0.25238192
...
epoch: 3 step: 2 loss is  0.12296932
epoch: 3 step: 3 loss is  0.15799297
...
epoch: 10 step: 448 loss is  0.06443884
epoch: 10 step: 449 loss is  0.0067842817
</pre></div>
</div>
</li>
<li><p>Check the saved checkpoint files.</p>
<p>The checkpoint file <code class="docutils literal notranslate"><span class="pre">gradient_accumulation.ckpt</span></code>, that is, the model file, is saved during training.</p>
</li>
</ol>
<p><strong>Validate the model:</strong></p>
<p>Through the <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/lenet/eval.py">eval.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory in ModelZoo, use the saved CheckPoint file, load the verification dataset, and verify it.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>eval.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data<span class="w"> </span>--ckpt_path<span class="o">=</span>./gradient_accumulation.ckpt<span class="w"> </span>--device_target<span class="o">=</span>GPU
</pre></div>
</div>
<p>The output is as follows. The accuracy of the validation dataset is about 96.31%, which is the same as the result when the value of batch_size is 32.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============== Starting Testing ==============
============== {&#39;Accuracy&#39;: 0.9631730769230769} ==============
</pre></div>
</div>
</section>
</section>
<section id="boost-model">
<h3>Boost Model<a class="headerlink" href="#boost-model" title="Permalink to this headline"></a></h3>
<p>In Boost mode, as long as you simply call Boost’s gradient accumulation interface, you can realize the gradient accumulation function. MNIST is also used as a demonstration dataset to show how to call the Boost interface to implement the gradient accumulation function.</p>
<blockquote>
<div><p>You can download the main tranining example code here: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/gradient_accumulation/train_and_eval_boost.py">train_and_eval_boost.py</a>.</p>
</div></blockquote>
<section id="importing-library-files-1">
<h4>Importing Library Files<a class="headerlink" href="#importing-library-files-1" title="Permalink to this headline"></a></h4>
<p>The following are the required public modules and MindSpore modules and library files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">WithLossCell</span><span class="p">,</span> <span class="n">TrainOneStepCell</span><span class="p">,</span> <span class="n">Accuracy</span>
<span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">GradientAccumulation</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="kn">from</span> <span class="nn">models.official.cv.lenet.src.dataset</span> <span class="kn">import</span> <span class="n">create_dataset</span>
<span class="kn">from</span> <span class="nn">models.official.cv.lenet.src.lenet</span> <span class="kn">import</span> <span class="n">LeNet5</span>
</pre></div>
</div>
</section>
<section id="loading-the-dataset-1">
<h4>Loading the Dataset<a class="headerlink" href="#loading-the-dataset-1" title="Permalink to this headline"></a></h4>
<p>Use the <code class="docutils literal notranslate"><span class="pre">MnistDataset</span></code> API provided by <code class="docutils literal notranslate"><span class="pre">dataset</span></code> of MindSpore to load the MNIST dataset. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/lenet/src/dataset.py">dataset.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory of models.</p>
</section>
<section id="defining-the-network-1">
<h4>Defining the Network<a class="headerlink" href="#defining-the-network-1" title="Permalink to this headline"></a></h4>
<p>LeNet is used as an example network. You can also use other networks, such as ResNet-50 and BERT. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/lenet/src/lenet.py">lenet.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory of models.</p>
</section>
<section id="defining-the-training-model-1">
<h4>Defining the Training Model<a class="headerlink" href="#defining-the-training-model-1" title="Permalink to this headline"></a></h4>
<p>We can call <code class="docutils literal notranslate"><span class="pre">GradientAccumulation</span></code> under MindSpore Boost to enable gradient accumulation, controlling the number of accumulations before each parameter update by max_accumulation_step.</p>
<p>Parameter updates and zeroing of the accumulated gradient variables after the number of accumulations is reached, only the interface needs to be called based on the <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> definition <code class="docutils literal notranslate"><span class="pre">TrainGradAccumulationStepsCell</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TrainGradAccumulationStepsCell</span><span class="p">(</span><span class="n">TrainOneStepCell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;construct train accu step cell&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_accumulation_step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainGradAccumulationStepsCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span> <span class="o">=</span> <span class="n">max_accumulation_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span> <span class="o">=</span> <span class="n">GradientAccumulation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="training-the-model-and-make-inference">
<h4>Training the Model and Make Inference<a class="headerlink" href="#training-the-model-and-make-inference" title="Permalink to this headline"></a></h4>
<p>After the network is defined, it can be trained. After the training, the ckpt file saved during the training process is loaded for inference, and the accuracy of the model can be obtained.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;MindSpore Grad Cumulative Example&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device_target&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;device where the code will be implemented (default: Ascend)&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_path&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;./Data&quot;</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;path where the dataset is saved&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
    <span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">time_cb</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">ds_train</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">())</span>

    <span class="n">train_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">)</span>
    <span class="n">train_net</span> <span class="o">=</span> <span class="n">TrainGradAccumulationStepsCell</span><span class="p">(</span><span class="n">train_net</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">train_net</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Training ==============&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">()])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Testing ==============&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>
    <span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ds_eval</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please check dataset size &gt; 0 and batch_size &lt;= dataset size&quot;</span><span class="p">)</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== </span><span class="si">{}</span><span class="s2"> ==============&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="experiment-result-1">
<h4>Experiment Result<a class="headerlink" href="#experiment-result-1" title="Permalink to this headline"></a></h4>
<p>After 10 epochs, the accuracy on the test set is about 98.30%.</p>
<ol class="arabic">
<li><p>Run the training and inference code and view the results.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>train_and_eval_boost.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data
</pre></div>
</div>
<p>The output is as follows, and you can see that the loss value gradually decreases with the training:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1875 loss is  0.1889342879
...
epoch: 5 step: 1875 loss is  0.11749879342
...
epoch: 10 step: 1875 loss is  0.00029468764328
</pre></div>
</div>
</li>
<li><p>Looking at the inference precision, the code saves the checkpoint to the current directory, and then loads the checkpoint inference.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============== Starting Testing ==============
============== {&#39;Accuracy&#39;: 0.983072916666} ==============
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>[1] Hermans, Joeri R., Gerasimos Spanakis, and Rico Möckel. “Accumulated gradient normalization.” Asian Conference on Machine Learning. PMLR, 2017.</p></li>
<li><p>[2] Lin, Yujun, et al. “Deep gradient compression: Reducing the communication bandwidth for distributed training.” arXiv preprint arXiv:1712.01887 (2017).</p></li>
<li><p>[3] <a class="reference external" href="https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce">how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes</a></p></li>
<li><p>[4] <a class="reference external" href="https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa">what-is-gradient-accumulation-in-deep-learning</a></p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mixed_precision.html" class="btn btn-neutral float-left" title="Enabling Mixed Precision" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="adaptive_summation.html" class="btn btn-neutral float-right" title="Adaptive Gradient Summation Algorithm" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>