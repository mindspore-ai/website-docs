

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Other Features &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Environment Variables" href="../env/env_var_list.html" />
    <link rel="prev" title="Multi Dimensional" href="multi_dimensional.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Other Features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#sharding-propagation">Sharding Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parameter-server-training">Parameter Server Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#communication-operator-fusion">Communication Operator Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-splitting">Dataset Splitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#functional-operator-splitting">Functional Operator Splitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performing-distributed-training-on-k8s-clusters">Performing Distributed Training on K8S Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#description-of-the-interface-related-to-the-feature">Description of the Interface Related to the Feature</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Other Features</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/other_features.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="other-features">
<h1>Other Features<a class="headerlink" href="#other-features" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/experts/source_en/parallel/other_features.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<section id="sharding-propagation">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/sharding_propagation.html">Sharding Propagation</a><a class="headerlink" href="#sharding-propagation" title="Permalink to this headline"></a></h2>
<p>In operator-level parallelism, the user is required to configure a slicing strategy for each operator in the forward network (if not configured, the data-parallel policy is used by default). The slicing strategy propagation feature can configure only a few operators to automatically generate a feasible sharding strategy for operators without a sharding strategy, and achieve the effect of minimizing communication overhead.</p>
</section>
<section id="parameter-server-training">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/parameter_server_training.html">Parameter Server Training</a><a class="headerlink" href="#parameter-server-training" title="Permalink to this headline"></a></h2>
<p>Parameter Server is a widely used architecture in distributed training, which has better flexibility, scalability, and node disaster tolerance than the AllReduce training method of data parallel synchronization. The parameter server supports both synchronous SGD (Stochastic Gradient Descent) and asynchronous SGD training algorithms. In terms of scalability, the calculation of the model and the update of the model are deployed in the worker and server processes respectively, so that the resources of the worker and server can be scaled horizontally independently (adding or removing the worker and server resources). In addition, in the environment of large-scale data centers, computing equipment, networks and storage often have various failures that lead to some node abnormalities, and under the architecture of parameter servers, such failures can be easily handled without affecting the tasks in training.</p>
</section>
<section id="communication-operator-fusion">
<h2>Communication Operator Fusion<a class="headerlink" href="#communication-operator-fusion" title="Permalink to this headline"></a></h2>
<p>In the distributed training scenario, cross-device or even cross-node data transmission is a bottleneck that restricts scalability and computing power utilization. Communication operator fusion is an important method to improve the utilization of network resources and accelerate the efficiency of data transmission, which packages the communication operators of the same source node and the destination node and executes them at the same time to avoid the additional overhead caused by multiple single operator execution.</p>
</section>
<section id="dataset-splitting">
<h2>Dataset Splitting<a class="headerlink" href="#dataset-splitting" title="Permalink to this headline"></a></h2>
<p>When doing distributed training, you need to import the training dataset to each device. There are two common ways to import: 1) Import in parallel with the data, that is, the data is split into match dimensions, and each device is imported as part; 2) Import full amount of data per device. In addition, when some dimensions of the data are particularly large (such as the H/W dimension of the remote sensing picture may be particularly large), even if the sample size is small, the picture needs to be split, that is, the data is split in the H/W dimension, and each device reads a part of the picture. This special performance supports splitting datasets into specific dimensions to meet training requirements in the field of large-format image processing.</p>
</section>
<section id="functional-operator-splitting">
<h2>Functional Operator Splitting<a class="headerlink" href="#functional-operator-splitting" title="Permalink to this headline"></a></h2>
<p>In dynamic graph mode, you specify that a part of the network structure executes in graph mode and performs various parallel operations.</p>
</section>
<section id="performing-distributed-training-on-k8s-clusters">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/ms_operator.html">Performing Distributed Training on K8S Clusters</a><a class="headerlink" href="#performing-distributed-training-on-k8s-clusters" title="Permalink to this headline"></a></h2>
<p>MindSpore Operator is a plugin that follows Kubernetes’ Operator pattern (based on the CRD-Custom Resource Definition feature) and implements distributed training on Kubernetes. MindSpore Operator defines Scheduler, PS, worker three roles in CRD, and users can easily use MindSpore on K8S for distributed training through simple YAML file configuration. The code repository of mindSpore Operator is described in: <a class="reference external" href="https://gitee.com/mindspore/ms-operator/">ms-operator</a>.</p>
</section>
<section id="description-of-the-interface-related-to-the-feature">
<h2>Description of the Interface Related to the Feature<a class="headerlink" href="#description-of-the-interface-related-to-the-feature" title="Permalink to this headline"></a></h2>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Feature category</p></th>
<th class="head"><p>Feature interface</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>operator parallel</p></td>
<td><p>shard(in_strategy=None, out_strategy=None)<br />In Primitive class</p></td>
<td><p>Set the sharding strategy of the input and output tensors of the operator (where the sharding strategy of the output tensor only supports some operators, such as Gauther and MatMul.)</p></td>
<td><p>Reduce the memory capacity of a single device by slicing the tensor involved in each operator in the network model to complete the large model training/inference. Or use cluster resources to perform distributed computing to reduce the overall execution time.</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>add_prim_attr(name, value)<br />In Primitive class</p></td>
<td><p>Gather Operator:<br />add_prim_attr(“manual_split”, config): Configure a non-uniform sharding strategy for its first input, where config type is tuple, which describes how the first parameter, dimension 0, is split. For example , ( 10 , 20 , 30 , 4 ) means that the 0th dimension of the first input of the operator is tangent into 4 parts , and the shape size of each part is 10 , 20 , 30 , 4, respectively.</p></td>
<td><p>In the recommended field, there is a scene where each column of the dataset corresponds to a subtable. In this scenario, using this configuration can reduce traffic and improve overall performance.</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
<td><p>EmbeddingLookUp Operator:<br />add_prim_attr(“primitive_target”, “CPU”): Configure it to execute on the CPU for heterogeneous scenarios.</p></td>
<td><p>In the recommended field, there is a particularly large scene of the Embedding Table, in order to save device memory, you can use this configuration to put EmbeddingLookUp on the CPU to execute to complete the training of the recommended large model.</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>set_auto_parallel_context(enable_alltoall=bool_value)</p></td>
<td><p>Indicate whether the AllToAll communication operator is allowed to be generated when communicating, and its value is the bool type, which defaults to False.</p></td>
<td><p>AllToAll communication can reduce the amount of communication data and improve communication efficiency, but it requires environmental support.</p></td>
</tr>
<tr class="row-even"><td><p>Pipeline parallel</p></td>
<td><p>set_auto_parallel_context(pipeline_stages=stage_num)</p></td>
<td><p>Set the number of pipes in pipeline parallelism, the value of which is a positive integer, and the value range is [1, number of devices].</p></td>
<td><p>Specify the number of stages, limiting the communication domain of the collection communication to the stage, and the point-to-point communication between the stages.</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>pipeline_stage(value)<br />In Cell class</p></td>
<td><p>Set which stage the Cell executes in.</p></td>
<td><p>Set which stage the Cell executes in.</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>PipelineCell(network, micro_size)</p></td>
<td><p>Specify the number of MicroSizes for the training network, where the network is the network to be trained and the micro_size is a positive integer.</p></td>
<td><p>Specify micro_size can reduce the idle wait time between stages and improve the overall efficiency of pipeline parallel.</p></td>
</tr>
<tr class="row-odd"><td><p>Optimizer parallel</p></td>
<td><p>set_auto_parallel_context(enable_parallel_optimizer=bool_value)</p></td>
<td><p>Indicate whether optimizer parallelism is enabled. Its value is bool type, and the default is False.</p></td>
<td><p>Optimizer parallel saves static memory overhead, but increases communication overhead.</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>set_auto_parallel_context(parallel_optimizer_config=config)</p></td>
<td><p>This configuration takes effect only after optimizer parallel is turned on. The config is a dict and supports two key values: <br />gradient_accumulation_shard(bool): If True, the cumulative gradient variable will be sharded on the data parallelism, defaulting to False.<br />parallel_optimizer_threshold(int): This value represents the optimizer sharding threshold in KB (default value is 64KB). When the parameter size does not exceed this value, it will not be split.</p></td>
<td><p>gradient_accumulation_shard true saves a portion of the parameter size of static memory, but increases communication overhead. <br />Optimizer sharding thresholds allow smaller shape parameters to be not optimized for splitting to save communication resources.</p></td>
</tr>
<tr class="row-odd"><td><p>Recompute</p></td>
<td><p>recompute(mode=True)<br />In primitive class</p></td>
<td><p>Used to specify whether the operator needs to be recalculated, and its value is bool type, which defaults to True and means that the operator recalculation is enabled.</p></td>
<td><p>After enabling operator recalculation, you can reduce the peak of dynamic memory, but increase the overall computation amount.</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>recompute(**kwargs)<br />In Cell class</p></td>
<td><p>When this interface is called, the operator in this Cell is recalculated.<br />The input parameter has two bool class options:<br />mp_comm_recompute: Whether to enable model parallel communication operator recalculation, and the default is True.<br />parallel_optimizer_comm_recompute: Whether to enable optimizer parallel communication operator recompute, and the default is False.</p></td>
<td><p>Enable Cell recompute and configure whether the model parallel communication operator and the optimizer parallel communication operator are recomputed. When the communication operator is recomputed, it consumes communication resources but reduces the peak of dynamic memory.</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="multi_dimensional.html" class="btn btn-neutral float-left" title="Multi Dimensional" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../env/env_var_list.html" class="btn btn-neutral float-right" title="Environment Variables" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>