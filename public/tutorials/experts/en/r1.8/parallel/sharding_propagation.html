<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sharding Propagation &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Sharding Propagation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/sharding_propagation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="sharding-propagation">
<h1>Sharding Propagation<a class="headerlink" href="#sharding-propagation" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/experts/source_en/parallel/sharding_propagation.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png" /></a></p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>Distributed operator, Tensor Layout, and Tensor Redistribution are fundamental concepts in op-level parallelism of MindSpore. In <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/design/distributed_training_design.html#automatic-parallelism">here</a>, these concepts are introduced by examples. Here, we formally define them.</p>
<p>In op-level parallelism, we conduct SPMD (Single Program Multiple Data) style parallelism, that is, a single program is produced for all partitions. MindSpore transforms a stand-alone program to a parallel one. The transformation is fine-grained in the sense that each operator in the stand-alone program is substituted by (a) distributed operator(s), guaranteeing that the substitution is mathematically equivalent.</p>
<section id="distributed-operator">
<h3>Distributed Operator<a class="headerlink" href="#distributed-operator" title="Permalink to this headline"></a></h3>
<p>Distributed Operator: together, the distributed operators running on multiple devices preserve the same semantics of the stand-alone counterpart. That is, given the same input, the distributed operators’ output is the same as the stand-alone counterpart.</p>
<p>Say a matrix multiplication (MatMul) operator with two matrix X and W as input: Y = MatMul(X, W) is to be parallelized on 4 devices. If matrix X has copies on 4 devices, and W is split into 4 copies by column, one for each device, then the distributed operator corresponding to the stand-alone version of the MatMul operator is also MatMul, that is, MatMul operator is executed on each device. If X is split into 4 parts according to the column, W is cut into 4 parts by row, and each device gets a shard of X and W, then the distributed operator corresponding to the stand-alone version of the MatMul operator is MatMul-&gt;AllReduce, that is, the two operators of MatMul and AllReduce will be executed sequentially on each device to ensure mathematical equivalence.</p>
<p>Besides the SP (Single Program) part, MD (Multiple Data) part also needs to be specified. Before that, we first define the Sharding Strategy.</p>
</section>
<section id="sharding-strategy">
<h3>Sharding Strategy<a class="headerlink" href="#sharding-strategy" title="Permalink to this headline"></a></h3>
<p>Sharding Strategy: a Sharding Strategy for an operator is a two-dimensional array, specifying how many partitions to split each dimension of each input tensor for the operator.</p>
<p>From the sharding strategy, you can derive the <strong>Tensor Layout</strong> to describe how tensors are distributed across devices.</p>
</section>
<section id="tensor-layout">
<h3>Tensor Layout<a class="headerlink" href="#tensor-layout" title="Permalink to this headline"></a></h3>
<p>Tensor Layout: given a Sharding Strategy for an operator, the <strong>Tensor Layout</strong> is inferred to describe the distributions of the input tensors of the operator, which includes the <strong>Logical Device Matrix</strong> and the <strong>Tensor Map</strong>. The Logical Device Matrix is an one-dimensional array, describing how devices are arranged for the operator. The Tensor Map the dimensions of input tensors to dimensions of the device matrix, indicating that input tensors are partitioned across the Logical Device Matrix.</p>
<p>Use again the MatMul operator Y = MatMul(X, W). We configure the operator with Sharding Strategy [[2, 1], [1, 4]] and the corresponding Tensor Layout information is demonstrated in the following figure. X is partitioned into 2 parts along the row dimension, and W is partitioned into 4 parts along the column dimension (figure (b)). From the Sharding Strategy, the Logical Device Matrix and the Tensor Map are inferred, as shown in figure (c). The coordinates are also determined to describe the locations of devices in the Logical Device Matrix, based on which the distributions of tensors are determined. From the ‘2’ column in the coordinate table, Device 0—3 are assigned X<sub>0</sub>, while Device 4—7 are assigned X<sub>1</sub>. From the ‘4’ column in the coordinate table, Device 0 and Device 4 are assigned W<sub>0</sub>, Device 1 and Device 5 are assigned W<sub>1</sub>, Device 2 and Device 6 are assigned W<sub>2</sub>, and Device 3 and Device 7 are assigned W<sub>3</sub>. As a result, the local computation is determined, as shown in figure (d).</p>
<p><img alt="tensor_layout" src="../_images/tensor_layout.png" /></p>
<p>For two consecutive operators that are dependent, the Tensor Layouts defined by two operators may be inconsistent, due to either Logical Device Matrix or Tensor Map. We propose an algorithm, called <strong>Tensor Redistribution</strong>, that transforms the inconsistent Tensor Layout. We omit the algorithm here, and only give a definition.</p>
</section>
<section id="tensor-redistribution">
<h3>Tensor Redistribution<a class="headerlink" href="#tensor-redistribution" title="Permalink to this headline"></a></h3>
<p>Tensor Redistribution: given two inconsistent Tensor Layouts of a tensor, Tensor Redistribution is an algorithm that can transform from the source Tensor Layout to the target Tensor Layout, with minimum communication cost.</p>
<p>Here, the communication cost is measured by the bytes that each device transmits.</p>
<p>Say a two-operator example: Z = MatMul(X, W), O = MatMul(Z, Y). To make Tensor Redistribution effective, two operators are configured Sharding Strategies so that the Tensor Layouts of Z are inconsistent, as shown in the following figure. In figure (a), the output of the first MatMul is row partitioned, while the second MatMul requires that Z are full-sized. Therefore, an AllGather is inferred by Tensor Redistribution to perform the transformation[1]. In figure (b), the output tensor Z of the first matrix multiplication operator is row-sliced, while the second matrix multiplicator requires that the tensor Z be split by columns, so the tensor redistribution derivation needs to be inserted here to complete the conversion.</p>
<p><img alt="tensor_redistribution" src="../_images/tensor_redistribution.png" /></p>
</section>
</section>
<section id="sharding-propagation-1">
<h2>Sharding Propagation<a class="headerlink" href="#sharding-propagation-1" title="Permalink to this headline"></a></h2>
<p>Given a computation graph, <strong>Sharding Propagation</strong> is a functionality that propagates the Sharding Strategies from configured operator to the whole graph, with the goal of minimizing the communication cost in Tensor Redistribution.</p>
<p>The input of Sharding Propagation is a computation graph, in which nodes represent operators, and edges encode the data-dependency relationship of operators. From a model definition with some operators configured Sharding Strategies, Sharding Propagation executes as follows:</p>
<ol class="arabic simple">
<li><p>Generate possible Sharding Strategies for non-configured operators;</p></li>
<li><p>Generate Tensor Redistributions and the associated communication costs for each edge;</p></li>
<li><p>Start from the configured operators, and propagate the Sharding Strategies to non-configured operators using BFS, with the goal of minimizing the communication cost along each edge.</p></li>
</ol>
<p>The following figure illustrates an example process of applying Sharding Propagation. Given an computation graph with some configured strategies, it first enumerates possible strategies for non-configured operators, as shown in figure (b). Next, it enumerates possible strategies and the Tensor Redistribution costs for each edge. Demonstrated in figure (c), the strategy for an edge is defined as a pair [<em>s_strategy</em>, <em>t_strategy</em>], where <em>s_strategy</em> and <em>t_strategy</em> denote Sharding Strategy for source operator and target operator, respectively. Finally, starting from the configured operator, it determines the next operator’s Sharding Strategy, such that the communication cost in Tensor Redistribution is minimized. The propagation ends when the Sharding Strategies for all operators are settled, as shown in figure (d).</p>
<p><img alt="sharding_propagation" src="../_images/sharding_propagation.png" /></p>
</section>
<section id="how-to-use-sharding-propagation-in-mindspore">
<h2>How to use Sharding Propagation in MindSpore<a class="headerlink" href="#how-to-use-sharding-propagation-in-mindspore" title="Permalink to this headline"></a></h2>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Download the complete sample code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/sharding_propagation">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/sharding_propagation</a>.</p>
</div></blockquote>
<p>The directory structure is as follows, where <code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code> is the IP configuration for Ascend devices (see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_ascend.html#configuring-distributed-environment-variables">here</a> for the explanation), <code class="docutils literal notranslate"><span class="pre">train.py</span></code> is the model definition, and <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the execution script.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─sharding_propagatinon
    │      rank_table_8pcs.json
    │      run.sh
    │      train.py
    ...
</pre></div>
</div>
</section>
<section id="model-definition">
<h3>Model definition<a class="headerlink" href="#model-definition" title="Permalink to this headline"></a></h3>
<p>We use the FeedForward Network (<code class="docutils literal notranslate"><span class="pre">FFN</span></code>) as an example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FFN</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="configuring-the-sharding-propagation">
<h3>Configuring the Sharding Propagation<a class="headerlink" href="#configuring-the-sharding-propagation" title="Permalink to this headline"></a></h3>
<p>Annotate Sharding Strategy for a <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> operator in <code class="docutils literal notranslate"><span class="pre">FFN</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
</pre></div>
</div>
<p>Configure the search_mode as <code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code> in Auto_Parallel mode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;auto_parallel&quot;</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-model-and-checking-the-sharding-strategies">
<h3>Training the model and checking the Sharding Strategies<a class="headerlink" href="#training-the-model-and-checking-the-sharding-strategies" title="Permalink to this headline"></a></h3>
<p>Run the command <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">run.sh</span> <span class="pre">8</span></code>. By setting the context: <code class="docutils literal notranslate"><span class="pre">save_graphs=True</span></code>, the IR graphs in the compilation process are saved. We choose the IRs corresponding to device 0.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">step_parallel_begin_xxx.ir</span></code>, each computation operator is annotated with a Sharding Strategy:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
  %3(x) = MatMul(%1, %2) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 1), (1, 4)), output_names: [output], transpose_a: false, transpose_x1: false}
 {in_strategy: ((2, 1), (1, 4))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64, 64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %4([CNode]453) = Load($(@1_construct_wrapper.298:para4_dense1.bias), %para15_u)
      : (&lt;Ref[Tensor(F32)], (64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (64)&gt;)
  %5(x) = Add(%3, %4) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %6(x) = ReLU(%5) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
 {in_strategy: ((2, 4))}      : (&lt;Tensor[Float32], (64, 64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %7([CNode]447) = Load($(@1_construct_wrapper.298:para5_dense2.weight), %para15_u)
      : (&lt;Ref[Tensor(F32)], (64, 64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %8(x) = MatMul(%6, %7) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false}
 {in_strategy: ((2, 4), (4, 1))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64, 64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
  %9([CNode]449) = Load($(@1_construct_wrapper.298:para6_dense2.bias), %para15_u)
      : (&lt;Ref[Tensor(F32)], (64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (64)&gt;)
  %10(x) = Add(%8, %9) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (64, 64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tensor[Float32], (64, 64)&gt;)
...
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">xx_validate_xxx.ir</span></code>, each input and output tensor in the computation operator is sliced according to the Sharding Strategy.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>…
  %2(equivx) = MatMul(%0, %1) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 1), (1, 4)), output_names: [output], transpose_a: false, transpose_x1: false}
 {in_strategy: ((2, 1), (1, 4))}      : (&lt;Tensor[Float32], (32, 64)&gt;, &lt;Tensor[Float32], (64, 16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
      # In file ./train.py(33)/        x = self.matmul(x, self.weight)/
  %3(equiv[CNode]453) = Load(%para4_dense1.bias, U)
      : (&lt;Ref[Tensor(F32)], (16)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16)&gt;)
  %4(equivx) = Add(%2, %3) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (32, 16)&gt;, &lt;Tensor[Float32], (16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
      # In file ./train.py(34)/        x = self.add(x, self.bias)/
  %5(equivx) = ReLU(%4) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]}
 {in_strategy: ((2, 4))}      : (&lt;Tensor[Float32], (32, 16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
      # In file ./train.py(48)/        x = self.relu(x)/
  %6(equiv[CNode]447) = Load(%para5_dense2.weight, U)
      : (&lt;Ref[Tensor(F32)], (16, 64)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16, 64)&gt;)
  %7(equivx) = MatMul(%5, %6) {instance name: matmul} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false}
 {in_strategy: ((2, 4), (4, 1))}      : (&lt;Tensor[Float32], (32, 16)&gt;, &lt;Tensor[Float32], (16, 64)&gt;) -&gt; (&lt;Tensor[Float32], (32, 64)&gt;)
      # In file ./train.py(33)/        x = self.matmul(x, self.weight)/
  %8(equiv[CNode]493) = AllReduce(%7) {instance name: forward_op_4025687080669949636} primitive_attrs: {group: 4-6301172352641561019, fusion: 0, op: sum, group_ranks: 0-1-2-3, index: 0}
      : (&lt;Tensor[Float32], (32, 64)&gt;) -&gt; (&lt;Tensor[Float32], (32, 64)&gt;)
  %9(equiv[CNode]492) = StridedSlice(%8, (0, 0), (32, 16), (1, 1)) {instance name: redistribution_op_145462406996255498StridedSlice} primitive_attrs: {new_axis_mask: 0, shrink_axis_mask: 0, end_mask: 0, input_names: [x, begin, end, strides], output_names: [output], keep_value_node_input: true, begin_mask: 0, ellipsis_mask: 0}
      : (&lt;Tensor[Float32], (32, 64)&gt;, &lt;Tuple[Int64*2]&gt;, &lt;Tuple[Int64*2]&gt;, &lt;Tuple[Int64*2]&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
  %10(equiv[CNode]449) = Load(%para6_dense2.bias, U)
      : (&lt;Ref[Tensor(F32)], (16)&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16)&gt;)
  %11(equivx) = Add(%9, %10) {instance name: add} primitive_attrs: {output_names: [output], input_names: [x, y]}
 {in_strategy: ((2, 4), (4))}      : (&lt;Tensor[Float32], (32, 16)&gt;, &lt;Tensor[Float32], (16)&gt;) -&gt; (&lt;Tensor[Float32], (32, 16)&gt;)
…
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>