<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallel Training Example (Ascend) &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Parallel Training Example (GPU)" href="train_gpu.html" />
    <link rel="prev" title="Distributed Case" href="distributed_case.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed Case</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Parallel Training Example (Ascend)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparations">Preparations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-dataset">Downloading the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-distributed-environment-variables">Configuring Distributed Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calling-the-collective-communication-library">Calling the Collective Communication Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset-in-data-parallel-mode">Loading the Dataset in Data Parallel Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network">Defining the Network</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hybrid-parallel-mode">Hybrid Parallel Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#semi-auto-parallel-mode">Semi Auto Parallel Mode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-loss-function-and-optimizer">Defining the Loss Function and Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-loss-function">Defining the Loss Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-optimizer">Defining the Optimizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-script">Running the Script</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-host-training">Single-host Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-host-training">Multi-host Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-script-through-openmpi">Running the Script through OpenMPI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-host-training-1">Single-host Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-host-training-1">Multi-host Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#non-sink-mode-training">Non-sink Mode Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#saving-and-loading-distributed-training-model-parameters">Saving and Loading Distributed Training Model Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#auto-parallel-mode">Auto Parallel Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-parallel-mode">Data Parallel Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#semi-auto-parallel-mode-1">Semi Auto Parallel Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hybrid-parallel-mode-1">Hybrid Parallel Mode</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="train_gpu.html">Distributed Parallel Training Example (GPU)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed Case</a> &raquo;</li>
      <li>Distributed Parallel Training Example (Ascend)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/train_ascend.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallel-training-example-ascend">
<h1>Distributed Parallel Training Example (Ascend)<a class="headerlink" href="#distributed-parallel-training-example-ascend" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/experts/source_en/parallel/train_ascend.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes how to train the ResNet-50 network in data parallel and automatic parallel modes on MindSpore based on the Ascend 910 AI processor.</p>
<blockquote>
<div><p>Download address of the complete sample code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training</a></p>
</div></blockquote>
<p>The directory structure is as follow:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distributed_training
        ├── cell_wrapper.py
        ├── rank_table_16pcs.json
        ├── rank_table_2pcs.json
        ├── rank_table_8pcs.json
        ├── resnet50_distributed_training_dataset_slice.py
        ├── resnet50_distributed_training_gpu.py
        ├── resnet50_distributed_training_pipeline.py
        ├── resnet50_distributed_training.py
        ├── resnet.py
        ├── run_cluster.sh
        ├── run_dataset_slice.sh
        ├── run_gpu.sh
        ├── run_pipeline.sh
        └── run.sh
    ...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">rank_table_16pcs.json</span></code>, <code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code> and <code class="docutils literal notranslate"><span class="pre">rank_table_2pcs.json</span></code> are the networking information files. <code class="docutils literal notranslate"><span class="pre">resnet.py</span></code>,<code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training.py</span></code> , <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_gpu.py</span></code> and <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_grad_accu.py</span></code> are the network structure files. <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> , <code class="docutils literal notranslate"><span class="pre">run_gpu.sh</span></code>, <code class="docutils literal notranslate"><span class="pre">run_grad_accu.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">run_cluster.sh</span></code> are the execution scripts.</p>
<p>Besides, we describe the usages of hybrid parallel and semi-auto parallel modes in the sections <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_ascend.html#defining-the-network">Defining the Network</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_ascend.html#saving-and-loading-distributed-training-model-parameters">Distributed Training Model Parameters Saving and Loading</a>.</p>
</section>
<section id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Permalink to this headline"></a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h3>
<p>This sample uses the <code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset, which consists of color images of 32 x 32 pixels in 10 classes, with 6000 images per class. There are 50,000 images in the training set and 10,000 images in the test set.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset download address: <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz">https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a>. If the download is unsuccessful, try copying the link address and download it.</p>
</div></blockquote>
<p>Download the dataset and decompress it to a local path. The folder generated after the decompression is <code class="docutils literal notranslate"><span class="pre">cifar-10-batches-bin</span></code>.</p>
</section>
<section id="configuring-distributed-environment-variables">
<h3>Configuring Distributed Environment Variables<a class="headerlink" href="#configuring-distributed-environment-variables" title="Permalink to this headline"></a></h3>
<p>When distributed training is performed in bare-metal environment (compared with the cloud environment where the Ascend 910 AI processor is deployed on the local host), you need to configure the network information file for the current multi-device environment. If the HUAWEI CLOUD environment is used, skip this section because the cloud service has been configured. If you intend to launch script with OpenMPI, you can skip this section.</p>
<p>The following uses the Ascend 910 AI processor as an example. The JSON configuration file for an environment with eight devices is as follows. In this example, the configuration file is named as <code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code>. For details about how to configure the 2-device environment, see the <code class="docutils literal notranslate"><span class="pre">rank_table_2pcs.json</span></code> file in the sample code.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;server_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;10.*.*.*&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">}],</span>
<span class="w">             </span><span class="nt">&quot;host_nic_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;reserve&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;status&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;completed&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The following parameters need to be modified based on the actual training environment:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">server_count</span></code>: number of hosts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">server_id</span></code>: IP address of the local host.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_id</span></code>: physical sequence number of a device, that is, the actual sequence number of the device on the corresponding host.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_ip</span></code>: IP address of the integrated NIC. You can run the <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/etc/hccn.conf</span></code> command on the current host. The key value of <code class="docutils literal notranslate"><span class="pre">address_x</span></code> is the IP address of the NIC.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_id</span></code>: logical sequence number of a device, which starts from 0.</p></li>
</ul>
</section>
<section id="calling-the-collective-communication-library">
<h3>Calling the Collective Communication Library<a class="headerlink" href="#calling-the-collective-communication-library" title="Permalink to this headline"></a></h3>
<p>The Huawei Collective Communication Library (HCCL) is used for the communication of MindSpore parallel distributed training and can be found in the Ascend 310 AI processor software package. In addition, <code class="docutils literal notranslate"><span class="pre">mindspore.communication.management</span></code> encapsulates the collective communication API provided by the HCCL to help users configure distributed information.</p>
<blockquote>
<div><p>HCCL implements multi-device multi-node communication based on the Ascend AI processor. The common restrictions on using the distributed service are as follows. For details, see the HCCL documentation.</p>
<ul class="simple">
<li><p>In a single-node system, a cluster of 1, 2, 4, or 8 devices is supported. In a multi-node system, a cluster of 8 x N devices is supported.</p></li>
<li><p>Each host has four devices numbered 0 to 3 and four devices numbered 4 to 7 deployed on two different networks. During training of 2 or 4 devices, the devices must be connected and clusters cannot be created across networks.</p></li>
<li><p>When we create a multi-node system, all nodes should use one same switch.</p></li>
<li><p>The server hardware architecture and operating system require the symmetrical multi-processing (SMP) mode.</p></li>
<li><p>Currently only supports global single group communication in PyNative mode.</p></li>
</ul>
</div></blockquote>
<p>The sample code for calling the HCCL is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DEVICE_ID&quot;</span><span class="p">]))</span>
    <span class="n">init</span><span class="p">()</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>In the preceding code:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=GRAPH_MODE</span></code>: sets the running mode to graph mode for distributed training. (The PyNative mode only support data parallel running.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_id</span></code>: physical sequence number of a device, that is, the actual sequence number of the device on the corresponding host.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init</span></code>: enables HCCL communication and completes the distributed training initialization.</p></li>
</ul>
</section>
</section>
<section id="loading-the-dataset-in-data-parallel-mode">
<h2>Loading the Dataset in Data Parallel Mode<a class="headerlink" href="#loading-the-dataset-in-data-parallel-mode" title="Permalink to this headline"></a></h2>
<p>During distributed training, data is imported in data parallel mode. The following takes the CIFAR-10 dataset as an example to describe how to import the CIFAR-10 dataset in data parallel mode. <code class="docutils literal notranslate"><span class="pre">data_path</span></code> indicates the dataset path, which is also the path of the <code class="docutils literal notranslate"><span class="pre">cifar-10-batches-bin</span></code> folder.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">rank_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">rank_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">resize_height</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># get rank_id and rank_size</span>
    <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="c1"># define map operations</span>
    <span class="n">random_crop_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">random_horizontal_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">))</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">normalize_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4465</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4914</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2010</span><span class="p">,</span> <span class="mf">0.1994</span><span class="p">,</span> <span class="mf">0.2023</span><span class="p">))</span>
    <span class="n">changeswap_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">c_trans</span> <span class="o">=</span> <span class="p">[</span><span class="n">random_crop_op</span><span class="p">,</span> <span class="n">random_horizontal_op</span><span class="p">]</span>
    <span class="n">c_trans</span> <span class="o">+=</span> <span class="p">[</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">rescale_op</span><span class="p">,</span> <span class="n">normalize_op</span><span class="p">,</span> <span class="n">changeswap_op</span><span class="p">]</span>

    <span class="c1"># apply map operations on images</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">)</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">c_trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">)</span>

    <span class="c1"># apply shuffle operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># apply batch operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># apply repeat operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_set</span>
</pre></div>
</div>
<p>Different from the single-node system, the multi-node system needs to transfer the <code class="docutils literal notranslate"><span class="pre">num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id</span></code> parameters to the dataset API. The two parameters correspond to the number of devices and logical sequence numbers of devices, respectively. You are advised to obtain the parameters through the HCCL API.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_rank</span></code>: obtains the ID of the current device in the cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_group_size</span></code>: obtains the number of devices.</p></li>
</ul>
<blockquote>
<div><p>Under data parallel mode, it is recommended to load the same dataset file for each device, or it may cause accuracy problems.</p>
</div></blockquote>
</section>
<section id="defining-the-network">
<h2>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h2>
<p>In data parallel and automatic parallel modes, the network definition method is the same as that in a single-node system. The reference code of ResNet is as follows: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/resnet/resnet.py">ResNet network sample script</a></p>
<p>In this section we focus on how to define a network in hybrid parallel or semi-auto parallel mode.</p>
<section id="hybrid-parallel-mode">
<h3>Hybrid Parallel Mode<a class="headerlink" href="#hybrid-parallel-mode" title="Permalink to this headline"></a></h3>
<p>Hybrid parallel mode adds the setting <code class="docutils literal notranslate"><span class="pre">layerwise_parallel</span></code> for <code class="docutils literal notranslate"><span class="pre">parameter</span></code> based on the data parallel mode. The <code class="docutils literal notranslate"><span class="pre">parameter</span></code> with the setting would be saved and computed in slice tensor and would not apply gradients aggregation. In this mode, MindSpore would not infer computation and communication for parallel operators automatically. To ensure the consistency of calculation logic, users are required to manually infer extra operations and insert them to networks. Therefore, this parallel mode is suitable for the users with deep understanding of parallel theory.</p>
<p>In the following example, specify the <code class="docutils literal notranslate"><span class="pre">self.weight</span></code> as the <code class="docutils literal notranslate"><span class="pre">layerwise_parallel</span></code>, that is, the <code class="docutils literal notranslate"><span class="pre">self.weight</span></code> and the output of <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> are sliced on the second dimension. At this time, perform ReduceSum on the second dimension would only get one sliced result. <code class="docutils literal notranslate"><span class="pre">AllReduce.Sum</span></code> is required here to accumulate the results among all devices. More information about the parallel theory please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/design/distributed_training_design.html">design document</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">HybridParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HybridParallelNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># initialize the weight which is sliced at the second dimension</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">),</span> <span class="n">layerwise_parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="semi-auto-parallel-mode">
<h3>Semi Auto Parallel Mode<a class="headerlink" href="#semi-auto-parallel-mode" title="Permalink to this headline"></a></h3>
<p>Compared with the auto parallel mode, semi auto parallel mode supports manual configuration on shard strategies for network tuning. The definition of shard strategies could be referred by this <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/design/distributed_training_design.html">design document</a>.</p>
<p>In the above example <code class="docutils literal notranslate"><span class="pre">HybridParallelNet</span></code>, the script in semi auto parallel mode is as follows. The shard stratege of <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> is <code class="docutils literal notranslate"><span class="pre">((1,</span> <span class="pre">1),</span> <span class="pre">(1,</span> <span class="pre">2))</span></code>, which means <code class="docutils literal notranslate"><span class="pre">self.weight</span></code> is sliced at the second dimension.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">SemiAutoParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SemiAutoParallelNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># initialize full tensor weight</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="c1"># set shard strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<blockquote>
<div><ul class="simple">
<li><p>In the semi auto parallel mode, the operators that are not assigned with any shard strategies would be executed in data parallel.</p></li>
<li><p>The auto parallel mode not only supports the parallel strategy that can automatically acquire efficient operators by strategy searching algorithms, this mode also enables users to manually assign specific parallel strategies.</p></li>
<li><p>If a parameter is used by multiple operators, each operator’s shard strategy for this parameter needs to be consistent, otherwise an error will be reported.</p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="defining-the-loss-function-and-optimizer">
<h2>Defining the Loss Function and Optimizer<a class="headerlink" href="#defining-the-loss-function-and-optimizer" title="Permalink to this headline"></a></h2>
<section id="defining-the-loss-function">
<h3>Defining the Loss Function<a class="headerlink" href="#defining-the-loss-function" title="Permalink to this headline"></a></h3>
<p>Automatic parallelism splits models using the operator granularity and obtains the optimal parallel strategy through algorithm search. Therefore, to achieve a better parallel training effect, you are advised to use small operators to implement the loss function.</p>
<p>In the loss function, the <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code> is expanded into multiple small operators for implementation according to a mathematical formula. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftmaxCrossEntropyExpand</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Div</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">logit_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">logit_max</span><span class="p">))</span>
        <span class="n">exp_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">softmax_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">exp</span><span class="p">,</span> <span class="n">exp_sum</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logit</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">softmax_result_log</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax_result</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum_cross_entropy</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">softmax_result_log</span><span class="p">,</span> <span class="n">label</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">scalar_to_array</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="defining-the-optimizer">
<h3>Defining the Optimizer<a class="headerlink" href="#defining-the-optimizer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Momentum</span></code> optimizer is used as the parameter update tool. The definition is the same as that in the single-node system. For details, see the implementation in the sample code.</p>
</section>
</section>
<section id="training-the-network">
<h2>Training the Network<a class="headerlink" href="#training-the-network" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code> is an API for users to set parallel training parameters and must be called before the initialization of networks. The related parameters are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code>: parallel distributed mode. The default value is <code class="docutils literal notranslate"><span class="pre">ParallelMode.STAND_ALONE</span></code>. The other options are <code class="docutils literal notranslate"><span class="pre">ParallelMode.DATA_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">ParallelMode.AUTO_PARALLEL</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parameter_broadcast</span></code>: the data parallel weights on the first device would be broadcast to other devices. The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gradients_mean</span></code>: During backward computation, the framework collects gradients of parameters in data parallel mode across multiple hosts, obtains the global gradient value, and transfers the global gradient value to the optimizer for update. The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>, which indicates that the <code class="docutils literal notranslate"><span class="pre">AllReduce.Sum</span></code> operation is applied. The value <code class="docutils literal notranslate"><span class="pre">True</span></code> indicates that the <code class="docutils literal notranslate"><span class="pre">AllReduce.Mean</span></code> operation is applied.</p></li>
<li><p>You are advised to set <code class="docutils literal notranslate"><span class="pre">device_num</span></code> and <code class="docutils literal notranslate"><span class="pre">global_rank</span></code> to their default values. The framework calls the HCCL API to obtain the values.</p></li>
</ul>
<blockquote>
<div><p>For more information about distributed parallelism configuration items, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/introduction.html">Distributed Parallel Overview</a>.</p>
</div></blockquote>
<p>If multiple network cases exist in the script, call <code class="docutils literal notranslate"><span class="pre">reset_auto_parallel_context</span></code> to restore all parameters to default values before executing the next case.</p>
<p>In the following sample code, the automatic parallel mode is specified. To switch to the data parallel mode, you only need to change <code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code> to <code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code>.</p>
<blockquote>
<div><p>The pynative mode currently supports data parallelism, and the usage is consistent with the data parallelism in graph mode. You only need to change ‘mode’ to ‘ PYNATIVE_MODE ‘.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Momentum</span>
<span class="kn">from</span> <span class="nn">resnet</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">))</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span> <span class="c1"># set device_id</span>

<span class="k">def</span> <span class="nf">test_train_cifar</span><span class="p">(</span><span class="n">epoch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>In the preceding code:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset_sink_mode=True</span></code>: uses the dataset sink mode. That is, the training computing is sunk to the hardware platform for execution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code>: returns the loss value through the callback function to monitor the loss function.</p></li>
</ul>
</section>
<section id="running-the-script">
<h2>Running the Script<a class="headerlink" href="#running-the-script" title="Permalink to this headline"></a></h2>
<section id="single-host-training">
<h3>Single-host Training<a class="headerlink" href="#single-host-training" title="Permalink to this headline"></a></h3>
<p>After the script required for training is edited, run the corresponding command to call the script.</p>
<p>Currently, MindSpore distributed execution uses the single-device single-process running mode. That is, one process runs on each device, and the number of total processes is the same as the number of devices that are being used. For device 0, the corresponding process is executed in the foreground. For other devices, the corresponding processes are executed in the background. You need to create a directory for each process to store log information and operator compilation information. The following takes the distributed training script for eight devices as an example to describe how to run the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run.sh DATA_PATH RANK_SIZE&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run.sh /path/dataset 8&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="nv">$2</span>

<span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>

test_dist_8pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_8pcs.json
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>
<span class="o">}</span>

test_dist_2pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_2pcs.json
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">2</span>
<span class="o">}</span>

test_dist_<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span>pcs

<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">1</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span>
<span class="k">do</span>
<span class="w">    </span>rm<span class="w"> </span>-rf<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>mkdir<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>cp<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device </span><span class="nv">$i</span><span class="s2">&quot;</span>
<span class="w">    </span>env<span class="w"> </span>&gt;<span class="w"> </span>env<span class="nv">$i</span>.log
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="nv">$i</span><span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>../
<span class="k">done</span>
rm<span class="w"> </span>-rf<span class="w"> </span>device0
mkdir<span class="w"> </span>device0
cp<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device0
<span class="nb">cd</span><span class="w"> </span>./device0
<span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="m">0</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device 0&quot;</span>
env<span class="w"> </span>&gt;<span class="w"> </span>env0.log
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>&gt;<span class="w"> </span>train.log0<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-eq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;training success&quot;</span>
<span class="k">else</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;training failed&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">2</span>
<span class="k">fi</span>
<span class="nb">cd</span><span class="w"> </span>../
</pre></div>
</div>
<p>The variables <code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">RANK_SIZE</span></code> need to be transferred to the script, which indicate the absolute path of the dataset and the number of devices, respectively.</p>
<p>The distributed related environment variables are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>: path for storing the network information file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code>: actual sequence number of the current device on the corresponding host.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_ID</span></code>: logical sequence number of the current device.</p></li>
</ul>
<p>For details about other environment variables, see configuration items in the <a class="reference external" href="https://www.mindspore.cn/install">installation guide</a>.</p>
<p>The running time is about 5 minutes, which is mainly occupied by operator compilation. The actual training time is within 20 seconds. You can use <code class="docutils literal notranslate"><span class="pre">ps</span> <span class="pre">-ef</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">pytest</span></code> to monitor task processes.</p>
<p>Log files are saved in the <code class="docutils literal notranslate"><span class="pre">device0</span></code>,<code class="docutils literal notranslate"><span class="pre">device1</span></code>… directory. The <code class="docutils literal notranslate"><span class="pre">env.log</span></code> file records environment variable information. The <code class="docutils literal notranslate"><span class="pre">train.log</span></code> file records the loss function information. The following is an example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 156, loss is 2.0084016
epoch: 2 step: 156, loss is 1.6407638
epoch: 3 step: 156, loss is 1.6164391
epoch: 4 step: 156, loss is 1.6838071
epoch: 5 step: 156, loss is 1.6320667
epoch: 6 step: 156, loss is 1.3098773
epoch: 7 step: 156, loss is 1.3515002
epoch: 8 step: 156, loss is 1.2943741
epoch: 9 step: 156, loss is 1.2316195
epoch: 10 step: 156, loss is 1.1533381
</pre></div>
</div>
</section>
<section id="multi-host-training">
<h3>Multi-host Training<a class="headerlink" href="#multi-host-training" title="Permalink to this headline"></a></h3>
<p>The previous chapters introduced the distributed training of MindSpore, which is based on the Ascend environment of a single host with multiple devices. Using multiple hosts for distributed training can greatly improve the training speed.
In the Ascend environment, the communication between NPU units across hosts is the same as the communication between each NPU unit in a single host. It is still communicated through HCCL. The difference is that the NPU units in a single host are naturally interoperable, while cross-host communication needs to be guaranteed that the networks of the two hosts are interoperable.</p>
<p>Execute the following command on server 1 to configure the target connect IP as the <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">ip</span></code> on the server 2. For example, configure the target IP of device 0 of server 1 as the IP of device 0 of server 2. Configuration command requires the <code class="docutils literal notranslate"><span class="pre">hccn_tool</span></code> tool. <a class="reference external" href="https://support.huawei.com/enterprise/en/ascend-computing/a300t-9000-pid-250702906?category=developer-documents">HCCL tool</a> comes with the CANN package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">0</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.92.131
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.93.131
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">2</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.94.131
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">3</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.95.131
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">4</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.92.141
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">5</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.93.141
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">6</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.94.141
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">7</span><span class="w"> </span>-netdetect<span class="w"> </span>-s<span class="w"> </span>address<span class="w"> </span><span class="m">192</span>.98.95.141
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">-i</span> <span class="pre">0</span></code> specifies the device ID. <code class="docutils literal notranslate"><span class="pre">-netdetect</span></code> specifies the IP attribute of the network detection. <code class="docutils literal notranslate"><span class="pre">-s</span> <span class="pre">address</span></code> means to set the property to an IP address. <code class="docutils literal notranslate"><span class="pre">192.98.92.131</span></code> represents the ip address of device 0 on the server 2. Interface commands can be found <a class="reference external" href="https://support.huawei.com/enterprise/en/doc/EDOC1100207443/efde9769/sets-the-ip-address-of-the-network-detection-object">here</a>.</p>
<p>After executing the above command on server 1, run the following command to start the detection of the network link status. The corresponding command can be found <a class="reference external" href="https://support.huawei.com/enterprise/en/doc/EDOC1100207443/f6c5a628/obtains-the-status-of-a-link">here</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">0</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">2</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">3</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">4</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">5</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">6</span><span class="w"> </span>-net_health<span class="w"> </span>-g
hccn_tool<span class="w"> </span>-i<span class="w"> </span><span class="m">7</span><span class="w"> </span>-net_health<span class="w"> </span>-g
</pre></div>
</div>
<p>If the connection is normal, the corresponding output is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>net<span class="w"> </span>health<span class="w"> </span>status:<span class="w"> </span>Success
</pre></div>
</div>
<p>If the connection fails, the corresponding output is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>net<span class="w"> </span>health<span class="w"> </span>status:<span class="w"> </span>Fault
</pre></div>
</div>
<p>After confirming that the network of the NPU unit between the hosts is connected, configure the json configuration file of multiple hosts. This tutorial takes the configuration file of 16 devices as an example. The detailed configuration file description can refer to the introduction of the single-host multi-device part of this tutorial. It should be noted that in the json file configuration of multiple hosts, the order of rank_id is required to be consistent with the lexicographic order of server_id.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;server_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;10.*.*.*&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">}],</span>
<span class="w">             </span><span class="nt">&quot;host_nic_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;reserve&quot;</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;server_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;10.*.*.*&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.8&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;8&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.8&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;9&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.8&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;10&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.8&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;11&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.9&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;12&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.9&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;13&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.9&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;14&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.9&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;15&quot;</span><span class="p">}],</span>
<span class="w">            </span><span class="nt">&quot;host_nic_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;reserve&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;status&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;completed&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>After preparing the configuration file, you can organize distributed multi-host training scripts. Taking 2 hosts with 16 devices as an example, the scripts written on the two hosts are similar to the running scripts of a single host with multiple devices. The difference is that different rank_id variables are specified.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_cluster.sh DATA_PATH RANK_TABLE_FILE RANK_SIZE RANK_START&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_cluster.sh /path/dataset /path/rank_table.json 16 0&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;The time interval between multiple hosts to execute the script should not exceed 120s&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>

<span class="nv">execute_path</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>
<span class="nv">script_self</span><span class="o">=</span><span class="k">$(</span>readlink<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$0</span><span class="s2">&quot;</span><span class="k">)</span>
<span class="nv">self_path</span><span class="o">=</span><span class="k">$(</span>dirname<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">script_self</span><span class="si">}</span><span class="s2">&quot;</span><span class="k">)</span>
<span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="nv">$2</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="nv">$3</span>
<span class="nv">RANK_START</span><span class="o">=</span><span class="nv">$4</span>
<span class="nv">DEVICE_START</span><span class="o">=</span><span class="m">0</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="o">=</span><span class="m">7</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span>$<span class="o">[</span>i+RANK_START<span class="o">]</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span>$<span class="o">[</span>i+DEVICE_START<span class="o">]</span>
<span class="w">  </span>rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/device_<span class="nv">$RANK_ID</span>
<span class="w">  </span>mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/device_<span class="nv">$RANK_ID</span>
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/device_<span class="nv">$RANK_ID</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
<span class="w">  </span>pytest<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/resnet50_distributed_training.py<span class="w"> </span>&gt;train<span class="nv">$RANK_ID</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>When executing, the two hosts execute the following commands respectively, among which rank_table.json is configured according to the 16-device distributed json file reference configuration shown in this chapter.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># server0</span>
bash<span class="w"> </span>run_cluster.sh<span class="w"> </span>/path/dataset<span class="w"> </span>/path/rank_table.json<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="m">0</span>
<span class="c1"># server1</span>
bash<span class="w"> </span>run_cluster.sh<span class="w"> </span>/path/dataset<span class="w"> </span>/path/rank_table.json<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
</section>
</section>
<section id="running-the-script-through-openmpi">
<h2>Running the Script through OpenMPI<a class="headerlink" href="#running-the-script-through-openmpi" title="Permalink to this headline"></a></h2>
<p>Currently MindSpore also supports <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>of OpenMPI for distributed training on Ascend hardware platform without environment variable <code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>.</p>
<section id="single-host-training-1">
<h3>Single-host Training<a class="headerlink" href="#single-host-training-1" title="Permalink to this headline"></a></h3>
<p>Take the distributed training script for eight devices <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/distributed_training/run_with_mpi.sh">run_with_mpi.sh</a> for an example, the script will run in the background. The log file is saved in the device directory, the log for different device will be saved in <code class="docutils literal notranslate"><span class="pre">log_output/1/</span></code> directory.</p>
<blockquote>
<div><p>If the script is executed by the root user, the <code class="docutils literal notranslate"><span class="pre">--allow-run-as-root</span></code> parameter must be added to <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.</p>
<p>if user want to forbid OpenMPI to abort all processes as one or more processes return a non-zero status，the MCA parameter can be set as <code class="docutils literal notranslate"><span class="pre">-mca</span> <span class="pre">orte_abort_on_non_zero_status</span> <span class="pre">0</span></code>.</p>
<p>OpenMPI will bind processes to specified core by default. you can set <code class="docutils literal notranslate"><span class="pre">-bind-to</span> <span class="pre">none</span></code> to unbind when performing mutli-threads application.</p>
<p>OpenMPI will set several environment variable with prefix <code class="docutils literal notranslate"><span class="pre">OPMI_</span></code> as running. Please don’t overwrite them in script.</p>
<p>Refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_gpu.html">GPU Distributed Parallel Training Example</a>or OpenMPI document for detailed information.</p>
</div></blockquote>
</section>
<section id="multi-host-training-1">
<h3>Multi-host Training<a class="headerlink" href="#multi-host-training-1" title="Permalink to this headline"></a></h3>
<p>Before running multi-host training, you need to ensure that you have the same openMPI, Python, and MindSpore versions and install path on each node. you also need to configure password-free login between nodes. Please refers to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_gpu.html">GPU Configuring Distributed Environment</a>.</p>
<p>OpenMPI multi-host training generally adopts the way of configuring hostfile, adding <code class="docutils literal notranslate"><span class="pre">--hostfile</span> <span class="pre">filepath</span></code> to the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command line argument. The format of each line of the hostfile file is <code class="docutils literal notranslate"><span class="pre">[hostname]</span> <span class="pre">slots=[slotnum]</span></code>. The hostname can be ip or hostname, and slotnum represents the number of child processes started by the machine.</p>
</section>
<section id="non-sink-mode-training">
<h3>Non-sink Mode Training<a class="headerlink" href="#non-sink-mode-training" title="Permalink to this headline"></a></h3>
<p>In graph mode, you can specify to train the model in a non-sink mode by setting the environment variable <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/env/env_var_list.html">GRAPH_OP_RUN</a>=1. In this case, you need to set environment variable <code class="docutils literal notranslate"><span class="pre">HCCL_WHITELIST_DISABLE=1</span></code> and train model with OpenMPI <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.</p>
</section>
</section>
<section id="saving-and-loading-distributed-training-model-parameters">
<h2>Saving and Loading Distributed Training Model Parameters<a class="headerlink" href="#saving-and-loading-distributed-training-model-parameters" title="Permalink to this headline"></a></h2>
<p>In MindSpore, four distributed parallel training modes are supported, namely Auto Parallel, Data Parallel, Semi Auto Parallel, and Hybrid Parallel. The below content introduced how to save and load models under the four distributed parallel training modes respectively. Before saving model parameters for distributed training, it is necessary to configure distributed environment variables and collective communication library in accordance with this tutorial.</p>
<section id="auto-parallel-mode">
<h3>Auto Parallel Mode<a class="headerlink" href="#auto-parallel-mode" title="Permalink to this headline"></a></h3>
<p>It is convenient to save and load the model parameters in auto parallel mode. Just add configuration <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code> and <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> to <code class="docutils literal notranslate"><span class="pre">test_train_cifar</span></code> method in the training network steps of this tutorial, and the model parameters can be saved. It should be noted that in parallel mode, you need to specify a different checkpoint save path for the scripts running on each device to prevent conflicts when reading and writing files, The code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">def</span> <span class="nf">test_train_cifar</span><span class="p">(</span><span class="n">epoch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">()</span>
    <span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;auto_parallel&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_rank</span><span class="p">())</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">ckpt_config</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">,</span> <span class="n">ckpt_callback</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>After saving the checkpoint file, users can easily load model parameters for reasoning or retraining. For example, the following code can be used for retraining:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># The parameter for load_checkpoint is a .ckpt file which has been successfully saved</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">pretrain_ckpt_path</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>For checkpoint configuration policy and saving method, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/save_load.html">Saving and Loading Model Parameters</a>.</p>
<p>By default, sliced parameters would be merged before saving automatocally. However, considering large-scaled networks, a large size checkpoint file will be difficult to be transferred and loaded. So every device can save sliced parameters separately by setting <code class="docutils literal notranslate"><span class="pre">integrated_save</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> in <code class="docutils literal notranslate"><span class="pre">CheckpointConfig</span></code>. If the shard strategies of retraining or inference are different with that of training, the special loading way is needed.</p>
<p>In retraining with multiple devices scenarios, users can infer shard strategy of retraining with <code class="docutils literal notranslate"><span class="pre">model.infer_train_layout</span></code> (only dataset sink mode is supported). The shard strategy will be used as <code class="docutils literal notranslate"><span class="pre">predict_strategy</span></code> for <code class="docutils literal notranslate"><span class="pre">load_distributed_checkpoint</span></code> function, which restores sliced parameters from <code class="docutils literal notranslate"><span class="pre">strategy_ckpt_load_file</span></code> (training strategy) to <code class="docutils literal notranslate"><span class="pre">predict_strategy</span></code> (retraining strategy) and load them into <code class="docutils literal notranslate"><span class="pre">model.train_network</span></code>. If there is only one device in retraining, <code class="docutils literal notranslate"><span class="pre">predict_strategy</span></code> could be <code class="docutils literal notranslate"><span class="pre">None</span></code>. The code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="s1">&#39;semi_auto_parallel&#39;</span><span class="p">,</span> <span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="s1">&#39;./train_strategy.ckpt&#39;</span><span class="p">)</span>
<span class="c1"># create model and dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_custom_dataset</span><span class="p">()</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">resnet</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
<span class="c1"># infer train strategy</span>
<span class="n">layout_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">infer_train_layout</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="c1"># load into `model.train_network` net</span>
<span class="n">ckpt_file_list</span> <span class="o">=</span> <span class="n">create_ckpt_file_list</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_distributed_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">train_network</span><span class="p">,</span> <span class="n">ckpt_file_list</span><span class="p">,</span> <span class="n">layout_dict</span><span class="p">)</span>
<span class="c1"># training the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>Distributed inference could be referred to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/distributed_inference.html">Distributed inference</a>.</p>
</div></blockquote>
</section>
<section id="data-parallel-mode">
<h3>Data Parallel Mode<a class="headerlink" href="#data-parallel-mode" title="Permalink to this headline"></a></h3>
<p>In data parallel mode, checkpoint is used in the same way as in auto parallel mode. You just need to change:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>Under data parallel mode, we recommend to load the same checkpoint for each device to avoid accuracy problems. <code class="docutils literal notranslate"><span class="pre">parameter_broadcast</span></code> could also be used for sharing the values of parameters among devices.</p>
</div></blockquote>
</section>
<section id="semi-auto-parallel-mode-1">
<h3>Semi Auto Parallel Mode<a class="headerlink" href="#semi-auto-parallel-mode-1" title="Permalink to this headline"></a></h3>
<p>In semi auto parallel mode, checkpoint is used in the same way as in auto parallel mode and data parallel mode. The difference is in the definition of a network and the definition of network model, you can refer to defining the network <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_ascend.html#semi-auto-parallel-mode">Semi Auto Parallel Mode</a> in this tutorial.</p>
<p>To save the model, you can use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">SemiAutoParallelNet</span><span class="p">()</span>
<span class="o">...</span>
<span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">()</span>
<span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;semi_auto_parallel&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">ckpt_config</span><span class="p">)</span>
</pre></div>
</div>
<p>To load the model, you can use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">SemiAutoParallelNet</span><span class="p">()</span>
<span class="c1"># The parameter for load_checkpoint is a .ckpt file which has been successfully saved</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">pretrain_ckpt_path</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>For the three parallel training modes described above, the checkpoint file is saved in a complete way on each device. Users also can save only the checkpoint file of this device on each device, take Semi Auto parallel Mode as an example for explanation.</p>
<p>Only by changing the code that sets the checkpoint saving policy, the checkpoint file of each device can be saved by itself. The specific changes are as follows:</p>
<p>Change the checkpoint configuration policy from:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># config checkpoint</span>
<span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># config checkpoint</span>
<span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">integrated_save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>It should be noted that if users choose this checkpoint saving policy, users need to save and load the segmented checkpoint for subsequent reasoning or retraining. Specific usage can refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/save_load.html#integrating-the-saved-checkpoint-files">Integrating the Saved Checkpoint Files</a>.</p>
</section>
<section id="hybrid-parallel-mode-1">
<h3>Hybrid Parallel Mode<a class="headerlink" href="#hybrid-parallel-mode-1" title="Permalink to this headline"></a></h3>
<p>For model parameter saving and loading in Hybrid Parallel Mode, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/save_load.html">Saving and Loading Model Parameters in the Hybrid Parallel Scenario</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed_case.html" class="btn btn-neutral float-left" title="Distributed Case" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train_gpu.html" class="btn btn-neutral float-right" title="Distributed Parallel Training Example (GPU)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>