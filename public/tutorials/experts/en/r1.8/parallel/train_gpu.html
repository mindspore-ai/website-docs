<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallel Training Example (GPU) &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Inference" href="distributed_inference.html" />
    <link rel="prev" title="Distributed Parallel Training Example (Ascend)" href="train_ascend.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiler optimization for optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing <strong>bprop</strong> Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.8/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed Case</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="train_ascend.html">Distributed Parallel Training Example (Ascend)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Parallel Training Example (GPU)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-distributed-environment">Configuring Distributed Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calling-the-collective-communication-library">Calling the Collective Communication Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-dataset">Downloading the Dataset</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset-in-the-data-parallel-mode">Loading the Dataset in the Data Parallel Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-loss-function-and-optimizer">Defining the Loss Function and Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-loss-function">Defining the Loss Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-optimizer">Defining the Optimizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-script">Running the Script</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-host-training">Single-host Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-host-training">Multi-host Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#saving-and-loading-the-distributed-training-model-parameter">Saving and Loading the Distributed Training Model Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-without-relying-on-openmpi">Training without Relying on OpenMPI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#running-the-script-1">Running the Script</a></li>
<li class="toctree-l4"><a class="reference internal" href="#security-authentication">Security Authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="#disaster-tolerance-recovery">Disaster Tolerance Recovery</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-ms-operators-for-distributed-training-in-k8s-clusters">Using ms-operators for Distributed Training in K8s Clusters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed Case</a> &raquo;</li>
      <li>Distributed Parallel Training Example (GPU)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/train_gpu.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallel-training-example-gpu">
<h1>Distributed Parallel Training Example (GPU)<a class="headerlink" href="#distributed-parallel-training-example-gpu" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/tutorials/experts/source_en/parallel/train_gpu.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes how to train a ResNet-50 network by using a CIFAR-10 dataset on a GPU processor hardware platform through MindSpore and data parallelism and automatic parallelism mode.</p>
<blockquote>
<div><p>You can download the complete sample code here:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training</a></p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distributed_training
    │      rank_table_16pcs.json
    │      rank_table_8pcs.json
    │      rank_table_2pcs.json
    │      cell_wrapper.py
    │      model_accu.py
    │      resnet.py
    │      resnet50_distributed_training.py
    │      resnet50_distributed_training_gpu.py
    │      resnet50_distributed_training_grad_accu.py
    │      run.sh
    │      run_gpu.sh
    │      run_grad_accu.sh
    │      run_cluster.sh
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">resnet.py</span></code> and <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_gpu.py</span></code> are the scripts that define the structure of the network. <code class="docutils literal notranslate"><span class="pre">run_gpu.sh</span></code> is the execution script and the remaining files are Ascend 910.</p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<p>In order to ensure the normal progress of the distributed training, we need to configure and initially test the distributed environment first. After completion, prepare the CIFAR-10 dataset.</p>
<section id="configuring-distributed-environment">
<h3>Configuring Distributed Environment<a class="headerlink" href="#configuring-distributed-environment" title="Permalink to this headline"></a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">OpenMPI-4.0.3</span></code>: multi-process communication library used by MindSpore.</p>
<p>Download the OpenMPI-4.0.3 source code package <code class="docutils literal notranslate"><span class="pre">openmpi-4.0.3.tar.gz</span></code> from <a class="reference external" href="https://www.open-mpi.org/software/ompi/v4.0/">https://www.open-mpi.org/software/ompi/v4.0/</a>.</p>
<p>For details about how to install OpenMPI, see the official tutorial: <a class="reference external" href="https://www.open-mpi.org/faq/?category=building#easy-build">https://www.open-mpi.org/faq/?category=building#easy-build</a>.</p>
</li>
<li><p>Password-free login between hosts (required for multi-host training). If multiple hosts are involved in the training, you need to configure password-free login between them. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Ensure that the same user is used to log in to each host. (The root user is not recommended.)</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">ssh-keygen</span> <span class="pre">-t</span> <span class="pre">rsa</span> <span class="pre">-P</span> <span class="pre">&quot;&quot;</span></code> command to generate a key.</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">ssh-copy-id</span> <span class="pre">DEVICE-IP</span></code> command to set the IP address of the host that requires password-free login.</p></li>
<li><p>Run the<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">DEVICE-IP</span></code> command. If you can log in without entering the password, the configuration is successful.</p></li>
<li><p>Run the preceding command on all hosts to ensure that every two hosts can communicate with each other.</p></li>
</ol>
</li>
</ul>
</section>
<section id="calling-the-collective-communication-library">
<h3>Calling the Collective Communication Library<a class="headerlink" href="#calling-the-collective-communication-library" title="Permalink to this headline"></a></h3>
<p>On the GPU hardware platform, communication in MindSpore distributed parallel training uses NVIDIA’s collective communication library <code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Collective</span> <span class="pre">Communication</span> <span class="pre">Library</span></code> (NCCL for short).</p>
<blockquote>
<div><p>On the GPU platform, MindSpore does not support the following operations:</p>
<p><code class="docutils literal notranslate"><span class="pre">get_local_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">get_local_size</span></code>, <code class="docutils literal notranslate"><span class="pre">get_world_rank_from_group_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">get_group_rank_from_world_rank</span></code> and <code class="docutils literal notranslate"><span class="pre">create_group</span></code></p>
</div></blockquote>
<p>The sample code for calling the HCCL is as follows and sets the file name as nccl_allgather.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># nccl_allgather.py</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allgather</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllGather</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">allgather</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
    <span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">value</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>In the preceding information,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=ms.GRAPH_MODE</span></code>: sets the running mode to graph mode for distributed training. (The PyNative mode only supports data parallel running.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_target=&quot;GPU&quot;</span></code>: specifies device as GPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init(&quot;nccl&quot;)</span></code>: enables NCCL communication and completes the distributed training initialization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_rank()</span></code>: obtains the rank number of  the current process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ops.AllGather</span></code>: invokes the NCCL’s AllGather communication operation on the GPU, the meaning of which and more can be found in Distributed Set Communication Primitives.</p></li>
</ul>
<p>On GPU hardware platform, MindSpore uses mpirun of OpenMPI to initiate processes, usually one computing device for each process.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span>DEVICE_NUM<span class="w"> </span>python<span class="w"> </span>nccl_allgather.py
</pre></div>
</div>
<p>Where the DEVICE_NUM is the number of GPUs of the machine. Taking DEVICE_NUM=4 as an example, the expected output is:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[0.],
 [1.],
 [2.],
 [3.]]
</pre></div>
</div>
<p>The output log can be found in <code class="docutils literal notranslate"><span class="pre">log/1/rank.0</span></code> after the program is executed. If the above output is obtained, it means that OpenMPI and NCCL are working normally, and the process is starting normally.</p>
</section>
<section id="downloading-the-dataset">
<h3>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h3>
<p>This example uses the <code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset, which consists of 10 classes of 32*32 color pictures, each containing 6,000 pictures, for a total of 60,000 pictures. The training set has a total of 50,000 pictures, and the test set has a total of 10,000 pictures.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset download link: <a class="reference external" href="http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz">http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz</a>. If the download is unsuccessful, please try copying the link address and download it.</p>
</div></blockquote>
<p>The Linux machine can use the following command to download to the current path of the terminal and extract the dataset, and the folder of the extracted data is <code class="docutils literal notranslate"><span class="pre">cifar-10-batches-bin</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz
tar<span class="w"> </span>-zxvf<span class="w"> </span>cifar-10-binary.tar.gz
</pre></div>
</div>
</section>
</section>
<section id="loading-the-dataset-in-the-data-parallel-mode">
<h2>Loading the Dataset in the Data Parallel Mode<a class="headerlink" href="#loading-the-dataset-in-the-data-parallel-mode" title="Permalink to this headline"></a></h2>
<p>During distributed training, data is imported in the data parallel mode. Taking the CIFAR-10 dataset as an example, we introduce the method of importing the CIFAR-10 dataset in parallel. <code class="docutils literal notranslate"><span class="pre">data_path</span></code> refers to the path of the dataset, that is, the path of the <code class="docutils literal notranslate"><span class="pre">cifar-10-batches-bin</span></code> folder.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>


<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">rank_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">rank_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">resize_height</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># get rank_id and rank_size</span>
    <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="c1"># define map operations</span>
    <span class="n">random_crop_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">random_horizontal_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">))</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">normalize_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4465</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4914</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2010</span><span class="p">,</span> <span class="mf">0.1994</span><span class="p">,</span> <span class="mf">0.2023</span><span class="p">))</span>
    <span class="n">changeswap_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">c_trans</span> <span class="o">=</span> <span class="p">[</span><span class="n">random_crop_op</span><span class="p">,</span> <span class="n">random_horizontal_op</span><span class="p">]</span>
    <span class="n">c_trans</span> <span class="o">+=</span> <span class="p">[</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">rescale_op</span><span class="p">,</span> <span class="n">normalize_op</span><span class="p">,</span> <span class="n">changeswap_op</span><span class="p">]</span>

    <span class="c1"># apply map operations on images</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">)</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">c_trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">)</span>

    <span class="c1"># apply shuffle operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># apply batch operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># apply repeat operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_set</span>
</pre></div>
</div>
<p>Unlike stand-alone machines, the <code class="docutils literal notranslate"><span class="pre">num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id</span></code> parameters need to be passed in on the dataset interface, which correspond to the number of cards and logical ordinal numbers, respectively, and it is recommended to obtain them through the NCCL interface:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_rank</span></code>: obtains the ID of the current device in the cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_group_size</span></code>: obtains the number of the clusters.</p></li>
</ul>
<blockquote>
<div><p>When loading datasets in a data-parallel scenario, it is recommended that you specify the same dataset file for each card. If the datasets loaded by each card are different, the calculation accuracy may be affected.</p>
</div></blockquote>
</section>
<section id="defining-the-network">
<h2>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h2>
<p>On the GPU hardware platform, the network definition is the same as that for the Ascend 910 AI processor.</p>
<p>In the <strong>Data Parallelism</strong> and <strong>Auto Parallelism</strong> modes, the network is defined in the same way as the stand-alone writing, see <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/resnet/resnet.py">ResNet Network Sample Script</a>.</p>
<blockquote>
<div><ul class="simple">
<li><p>In semi-automatic parallel mode, operators without a policy configured default to data parallelism.</p></li>
<li><p>The automatic parallel mode supports automatically obtaining efficient operator parallel policies through the policy search algorithm, and also allows users to manually configure specific parallel policies for operators.</p></li>
<li><p>If a <code class="docutils literal notranslate"><span class="pre">parameter</span></code> is used by more than one operator, the segment policy of each operator for that <code class="docutils literal notranslate"><span class="pre">parameter</span></code> needs to be consistent, otherwise an error will be reported.</p></li>
</ul>
</div></blockquote>
</section>
<section id="defining-the-loss-function-and-optimizer">
<h2>Defining the Loss Function and Optimizer<a class="headerlink" href="#defining-the-loss-function-and-optimizer" title="Permalink to this headline"></a></h2>
<p>Consistent with the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_ascend.html">Distributed Parallel Training Basics Sample</a> on Ascend.</p>
<section id="defining-the-loss-function">
<h3>Defining the Loss Function<a class="headerlink" href="#defining-the-loss-function" title="Permalink to this headline"></a></h3>
<p>Automatic parallelism takes the operator as the granularity segment model, and the optimal parallelism policy is obtained through algorithm search. Unlike stand-alone training, in order to have a better parallel training effect, the loss function is recommended to use the MindSpore operator to achieve it, rather than directly using the encapsulated loss function class.</p>
<p>In the Loss section, we take the form of <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>, that is, according to the mathematical formula, expand it into multiple MindSpore operators for implementation, the sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>


<span class="k">class</span> <span class="nc">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftmaxCrossEntropyExpand</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_cross_entropy</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">logit_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">exp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">logit_max</span><span class="p">))</span>
        <span class="n">exp_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">softmax_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">exp</span><span class="p">,</span> <span class="n">exp_sum</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logit</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">softmax_result_log</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax_result</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum_cross_entropy</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">softmax_result_log</span><span class="p">,</span> <span class="n">label</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul2</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">scalar_to_array</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="defining-the-optimizer">
<h3>Defining the Optimizer<a class="headerlink" href="#defining-the-optimizer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Momentum</span></code> optimizer is used as a parameter update tool. The definition here is consistent with that of the stand-alone machine, and it is no longer expanded, which can be referred to the implementation in the sample code.</p>
</section>
</section>
<section id="training-the-network">
<h2>Training the Network<a class="headerlink" href="#training-the-network" title="Permalink to this headline"></a></h2>
<p>Before training, we need to configure some automatic parallelism parameters. <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code> is the interface for configuring the parallel training mode and must be called before the network is initialized. Common parameters include:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code>: distributed parallel mode. The default is stand-alone mode <code class="docutils literal notranslate"><span class="pre">ParallelMode.STAND_ALONE</span></code>. In this example, you can select data parallelism <code class="docutils literal notranslate"><span class="pre">ParallelMode.DATA_PARALLEL</span></code> and automatic parallel <code class="docutils literal notranslate"><span class="pre">ParallelMode.AUTO_PARALLEL</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parameter_broadcast</span></code>: before the start of training, the parameter weights of data parallelism on card 0 are automatically broadcast to other cards, and the default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gradients_mean</span></code>: in reverse calculation, the framework will scatter the data parallel parameters in the gradient values of multiple machines for collection, obtain the global gradient values, and then pass them into the optimizer for update. The default value is <code class="docutils literal notranslate"><span class="pre">False</span></code>, which is set to True for the <code class="docutils literal notranslate"><span class="pre">allreduce_mean</span></code> operation and False for the <code class="docutils literal notranslate"><span class="pre">allreduce_sum</span></code> action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_num</span></code>和<code class="docutils literal notranslate"><span class="pre">global_rank</span></code>: it is recommended to use the default value, and the NCCL interface will be called within the framework to get it.</p></li>
</ul>
<p>If there are multiple network use cases in the script, call <code class="docutils literal notranslate"><span class="pre">reset_auto_parallel_context</span></code> to restore all parameters to the default values before executing the next use case.</p>
<p>In the example below, we specify the parallel mode as automatic parallelism, and the user needs to switch to data parallel mode by simply changing <code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code> to <code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code>.</p>
<blockquote>
<div><p>The pynative mode currently supports data parallelism, and the usage is consistent with the data parallelism in graph mode. You only need to change ‘mode’ to ‘ PYNATIVE_MODE ‘.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Momentum</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">resnet</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">test_train_cifar</span><span class="p">(</span><span class="n">epoch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Where,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset_sink_mode=True</span></code>: indicates a sinking pattern that takes a dataset, that is, the computation of the training is performed in a hardware platform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code>: returns the loss value via a callback function for monitoring loss functions.</p></li>
</ul>
</section>
<section id="running-the-script">
<h2>Running the Script<a class="headerlink" href="#running-the-script" title="Permalink to this headline"></a></h2>
<p>On GPU hardware platform, MindSpore uses <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>of OpenMPI for distributed training. After completing the definition of the model, loss function, and optimizer, we have completed the configuration of the model parallelism strategy, and then execute the running script directly.</p>
<section id="single-host-training">
<h3>Single-host Training<a class="headerlink" href="#single-host-training" title="Permalink to this headline"></a></h3>
<p>The following takes the distributed training script for eight devices as an example to describe how to run the script:</p>
<blockquote>
<div><p>Obtain the running script of the example from:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/distributed_training/run_gpu.sh">https://gitee.com/mindspore/docs/blob/r1.8/docs/sample_code/distributed_training/run_gpu.sh</a></p>
<p>If the script is executed by the root user, the <code class="docutils literal notranslate"><span class="pre">--allow-run-as-root</span></code> parameter must be added to <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>The script will run in the bachground. The log file is saved in the device directory, we will run 10 epochs and each epochs contain 234 steps, and the loss result is saved in train.log. The output loss values of the grep command are as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
</pre></div>
</div>
</section>
<section id="multi-host-training">
<h3>Multi-host Training<a class="headerlink" href="#multi-host-training" title="Permalink to this headline"></a></h3>
<p>Before running multi-host training, you need to ensure that you have the same openMPI, NCCL, Python, and MindSpore versions on each node.</p>
<section id="mpirun-h">
<h4>mpirun -H<a class="headerlink" href="#mpirun-h" title="Permalink to this headline"></a></h4>
<p>If multiple hosts are involved in the training, you need to set the multi-host configuration in the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command. You can use the <code class="docutils literal notranslate"><span class="pre">-H</span></code> option in the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command. For example,</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mpirun -n 16 -H DEVICE1_IP:8,DEVICE2_IP:8 python hello.py
</pre></div>
</div>
<p>indicates that eight processes are started on the hosts whose IP addresses are DEVICE1_IP and DEVICE2_IP, respectively.</p>
</section>
<section id="mpirun--hostfile">
<h4>mpirun –hostfile<a class="headerlink" href="#mpirun--hostfile" title="Permalink to this headline"></a></h4>
<p>Multi-host execution of GPU can also be performed by constructing hostfile files. For ease of debugging, it is recommended to use this method to execute multi-host scripts. This is then executed in the form of <code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">--hostfile</span> <span class="pre">$HOST_FILE</span></code>. Below we give a detailed Multi-host configuration in the hostfile boot method.</p>
<p>Each line in the hostfile is in the format of <code class="docutils literal notranslate"><span class="pre">[hostname]</span> <span class="pre">slots=[slotnum]</span></code>, where hostname can be an IP address or a host name. It should be noted that the usernames on different machines need to be the same, but hostnames cannot be the same. As follows, it means that there are 8 cards on DEVICE1 and 8 cards on machines with IP 192.168.0.1:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DEVICE1 slots=8
DEVICE2 slots=8
</pre></div>
</div>
<p>The following is the execution script of the 16-device two-host cluster. The variables <code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">HOSTFILE</span></code> need to be transferred, indicating the dataset path and hostfile path. We need to set the btl parameter of mca in mpi to specify the network card that communicates with mpi, otherwise it may fail to initialize when calling the mpi interface. The btl parameter specifies the TCP protocol between nodes and the loop within the nodes for communication. The IP address of the network card through which the specified nodes communication of  btl_tcp_if_include needs to be in the given subnet. For details about more mpirun options, see the OpenMPI official website.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">HOSTFILE</span><span class="o">=</span><span class="nv">$2</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>--hostfile<span class="w"> </span><span class="nv">$HOSTFILE</span><span class="w"> </span>-x<span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$DATA_PATH</span><span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Considering that some environment variables on different machines may be different, we take the form of mpirun to start a <code class="docutils literal notranslate"><span class="pre">mpirun_gpu_cluster.sh</span></code> and specify the required environment variables in the script file on different machines. Here we have configured the <code class="docutils literal notranslate"><span class="pre">NCCL_SOCKET_IFNAME</span></code> to specify the NIC when the NCCL communicates.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># mpirun_gpu_clusher.sh</span>
<span class="c1"># Here you can set different environment variables on each machine, such as the name of the network card below</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span><span class="s2">&quot;en5&quot;</span><span class="w"> </span><span class="c1"># The name of the network card that needs to communicate between nodes may be inconsistent on different machines, and use ifconfig to view.</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="saving-and-loading-the-distributed-training-model-parameter">
<h2>Saving and Loading the Distributed Training Model Parameter<a class="headerlink" href="#saving-and-loading-the-distributed-training-model-parameter" title="Permalink to this headline"></a></h2>
<p>When performing distributed training on a GPU, the method of saving and loading the model parameters is the same as that on Ascend, which can be referred to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/train_ascend.html#saving-and-loading-distributed-training-model-parameters">Distributed Training Model Parameters Saving and Loading</a>.</p>
</section>
<section id="training-without-relying-on-openmpi">
<h2>Training without Relying on OpenMPI<a class="headerlink" href="#training-without-relying-on-openmpi" title="Permalink to this headline"></a></h2>
<p>Due to training safety and reliability requirements, MindSpore GPUs also support <strong>distributed training without relying on OpenMPI</strong>.</p>
<p>OpenMPI plays the role of synchronizing data and inter-process networking on the Host side in distributed training scenarios. MindSpore replaces openMPI capabilities by <strong>reusing the Parameter Server mode training architecture</strong>.</p>
<p>Refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/parameter_server_training.html">Parameter Server Mode</a> training tutorial to start multiple MindSpore training processes as <code class="docutils literal notranslate"><span class="pre">Workers</span></code>, and start an additional <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> with minor modifications to the script. You can perform <strong>distributed training without relying on OpenMPI</strong>.</p>
<p>Before executing the Worker script, you need to export environment variables, such as <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/parameter_server_training.html#environment-variable-setting">Environment Variable Settings</a>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export MS_SERVER_NUM=0                # Server number
export MS_WORKER_NUM=8                # Worker number
export MS_SCHED_HOST=127.0.0.1        # Scheduler IP address
export MS_SCHED_PORT=6667             # Scheduler port
export MS_ROLE=MS_WORKER              # The role of this process: MS_SCHED represents the scheduler, MS_WORKER represents the worker, MS_PSERVER represents the Server
</pre></div>
</div>
<blockquote>
<div><p>In this mode, it is not recommended to start a process that MS_PSERVER role because this role has no effect in data parallel training.</p>
</div></blockquote>
<section id="running-the-script-1">
<h3>Running the Script<a class="headerlink" href="#running-the-script-1" title="Permalink to this headline"></a></h3>
<p>On GPU hardware platform, the following shows how to run a distributed training script by using 8 cards as an example:</p>
<blockquote>
<div><p>You can find the running directory of the sample here:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training</a>.</p>
</div></blockquote>
<p>Compared with openMPI mode startup, this mode requires calling the <code class="docutils literal notranslate"><span class="pre">set_ps_context</span></code> interface in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/parallel/parameter_server_training.html">Parameter Server mode</a>. This mission of MindSpore uses the PS mode training architecture:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_ps_context</span><span class="p">(</span><span class="n">config_file_path</span><span class="o">=</span><span class="s2">&quot;/path/to/config_file.json&quot;</span><span class="p">,</span> <span class="n">enable_ssl</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">client_password</span><span class="o">=</span><span class="s2">&quot;123456&quot;</span><span class="p">,</span> <span class="n">server_password</span><span class="o">=</span><span class="s2">&quot;123456&quot;</span><span class="p">)</span>
    <span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=ms.GRAPH_MODE</span></code>: uses the distributed training, which requires specifying the run mode as graph mode. (The PyNative mode only supports data parallel running.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init(&quot;nccl&quot;)</span></code>: enables NCCL communication and completes distributed training initialization.</p></li>
<li><p>By default, the secure encrypted channel is closed, and the secure encrypted channel needs to be configured correctly through the <code class="docutils literal notranslate"><span class="pre">set_ps_context</span></code> or the secure encrypted channel must be closed before init (“nccl”) can be called, otherwise the initialization of the networking will fail.</p></li>
</ul>
<p>To use a secure encrypted tunnel, set the configuration of <code class="docutils literal notranslate"><span class="pre">set_ps_context(config_file_path=&quot;/path/to/config_file.json&quot;,</span> <span class="pre">enable_ssl=True,</span> <span class="pre">client_password=&quot;123456&quot;,</span> <span class="pre">server_password=&quot;123456&quot;)</span></code>. For detailed parameter configurations, refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">mindspore.set_ps_context</a>, and <a class="reference internal" href="#security-authentication"><span class="std std-doc">Safety Certification</span></a> section.</p>
<p>The script content <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster.sh</span></code> is as follows, before starting the Worker and Scheduler, you need to add the relevant environment variable settings:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Launch 8 workers.</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>

<span class="c1"># Launch 1 scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Execute the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster.sh<span class="w"> </span>DATA_PATH
</pre></div>
</div>
<p>that is, perform 8 card distributed training inside the single machine. If you want to perform cross-machine training, you need to split the script, such as performing 2-host 8-card training, and each machine performs the start of 4Worker:</p>
<p>The script <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_1.sh</span></code> starts 1 <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> and <code class="docutils literal notranslate"><span class="pre">Worker1</span></code> to <code class="docutils literal notranslate"><span class="pre">Worker4</span></code> on machine 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Launch 1-4 workers.</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">4</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>

<span class="c1"># Launch 1 scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Script <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_2.sh</span></code> starts <code class="docutils literal notranslate"><span class="pre">Worker5</span></code> to <code class="docutils literal notranslate"><span class="pre">Worker8</span></code> on machine 2 (no longer needs to execute Scheduler):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Launch 5-8 workers.</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">4</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>Execute on the two hosts separately:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster_1.sh<span class="w"> </span>DATA_PATH
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster_2.sh<span class="w"> </span>DATA_PATH
</pre></div>
</div>
<p>that is, perform 2-host and 8-card distributed training tasks.</p>
<p>If you want to start data parallel mode training, you need to change the <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code> in the script <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_gpu.py</span></code> to <code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The script will run in the background, and the log file will be saved to the current directory. A total of 10 epochs are run, each of which has 234 steps, and the results of the Loss part are saved in the worker_*.log. After the loss value grep is out, the example is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
</pre></div>
</div>
</section>
<section id="security-authentication">
<h3>Security Authentication<a class="headerlink" href="#security-authentication" title="Permalink to this headline"></a></h3>
<p>To support SSL security authentication between nodes/processes, to enable security authentication, configure <code class="docutils literal notranslate"><span class="pre">enable_ssl=True</span></code> through python API <code class="docutils literal notranslate"><span class="pre">mindspore.set_ps_context</span></code> (false by default when not passed in, indicating that SSL security authentication is not enabled), the config.json configuration file specified config_file_path  needs to add the following fields:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;server_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;server.p12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;crl_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;client_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;client.p12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;ca_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ca.crt&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cipher_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ECDHE-R SA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-DSS-AES256-GCM-SHA384:DHE-PSK-AES128-GCM-SHA256:DHE-PSK-AES256-GCM-SHA384:DHE-PSK-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-PSK-CHACHA20-POLY1305:DHE-RSA-AES128-CCM:DHE-RSA-AES256-CCM:DHE-RSA-CHACHA20-POLY1305:DHE-PSK-AES128-CCM:DHE-PSK-AES256-CCM:ECDHE-ECDSA-AES128-CCM:ECDHE-ECDSA-AES256-CCM:ECDHE-ECDSA-CHACHA20-POLY1305&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cert_expire_warning_time_in_day&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">90</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>server_cert_path: the server contains the path to the p12 file (SSL private certificate file) containing the ciphertext of the certificate and key on the server side.</p></li>
<li><p>crl_path: the file path of the revocation list (that distinguishes between invalid and untrusted certificates and valid trusted certificates).</p></li>
<li><p>client_cert_path: the client contains the path to the p12 file (SSL private certificate file) containing the ciphertext of the certificate and key on the server side.</p></li>
<li><p>ca_cert_path: the root certification path.</p></li>
<li><p>cipher_list: Cipher suites (list of supported SSL encryption types).</p></li>
<li><p>cert_expire_warning_time_in_day: the alarm time when the certificate expires.</p></li>
</ul>
<p>The key in the p12 file is stored in ciphertext, and the password needs to be passed in at startup. For specific parameters, please refer to <code class="docutils literal notranslate"><span class="pre">client_password</span></code> and <code class="docutils literal notranslate"><span class="pre">server_password</span></code> fields in the Python API <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">mindspore.set_ps_context</a>.</p>
</section>
<section id="disaster-tolerance-recovery">
<h3>Disaster Tolerance Recovery<a class="headerlink" href="#disaster-tolerance-recovery" title="Permalink to this headline"></a></h3>
<p>Model training has high requirements for the reliability and serviceability of the distributed training architecture, and MindSpore supports data parallel disaster recovery. There are abnormal process exits in the Multicard data parallel training scenario cluster (multiple Workers and 1 Scheduler), and after being pulled up, the training task can continue to perform normally;</p>
<p>Scene constraints:</p>
<p>In the graph mode, the <code class="docutils literal notranslate"><span class="pre">MindData</span></code> is used for data sinking mode training, and the data parallel mode is turned on. The above non-<code class="docutils literal notranslate"><span class="pre">OpenMPI</span></code> method is used to pull up the worker process.</p>
<p>In the above scenario, if there are nodes hanging up during the training process, it is guaranteed that under the same environment variables (<code class="docutils literal notranslate"><span class="pre">MS_ENABLE_RECOVERY</span></code> and <code class="docutils literal notranslate"><span class="pre">MS_RECOVERY_PATH</span></code>). The training can continue after re-pulling the script corresponding to the corresponding process, and does not affect the precision convergence.</p>
<ol class="arabic simple">
<li><p>Start Disaster Tolerance:</p></li>
</ol>
<p>Enable disaster tolerance with environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">             </span><span class="c1"># Enable disaster tolerance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>“/xxx/xxx”<span class="w">      </span><span class="c1"># Configure the persistence path folder, and the Worker and Scheduler processes perform the necessary persistence during execution, such as recovering the node information for networking and training the intermediate state of the service</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Configure the checkpoint save interval, for example:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">ckptconfig</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt_of_rank_/&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">get_rank</span><span class="p">()),</span> <span class="n">config</span><span class="o">=</span><span class="n">ckptconfig</span><span class="p">)</span>
</pre></div>
</div>
<p>Each Worker turns on save checkpoint and uses a different path (as in the example above, the directory setting uses the rank id to ensure that the paths are not the same) to prevent checkpoint save conflicts of the same name. checkpoint is used for abnormal process recovery and normal process rollback. Training rollback means that each worker in the cluster is restored to the state corresponding to the latest checkpoint, and the data side also falls back to the corresponding step, and then continues training. The interval between saving checkpoints is configurable, which determines the granularity of disaster recovery. The smaller the interval, the smaller the number of steps that are reverted to the last save checkpoint, but the frequent saving of checkpoints may also affect the training efficiency, and the larger the interval, the opposite effect. keep_checkpoint_max set to at least 2 (to prevent checkpoint save failure).</p>
<blockquote>
<div><p>The running directory of the sample:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training">https://gitee.com/mindspore/docs/tree/r1.8/docs/sample_code/distributed_training</a>.</p>
</div></blockquote>
<p>The scripts involved are <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_recovery.sh</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_gpu_recovery.py</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet.py</span></code>. The script content <code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_recovery.sh</span></code> is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster_recovery.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster_recovery.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">      </span><span class="c1"># Enable recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/XXX/XXX<span class="w"> </span><span class="c1"># Set recovery path</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># Launch 1 scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">               </span><span class="c1"># The node id for Scheduler</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Launch 8 workers.</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>worker_<span class="nv">$i</span><span class="w">           </span><span class="c1"># The node id for Workers</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>Before starting Worker and Scheduler, you need to add the relevant environment variable settings, such as IP and Port of Scheduler, and whether the role of the current process is Worker or Scheduler.</p>
<p>Execute the following command to start a single-host 8-card data parallel training</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_gpu_cluster_recovery.sh<span class="w"> </span>YOUR_DATA_PATH<span class="s2">&quot;</span>
</pre></div>
</div>
<p>Distributed training starts, if an exception is encountered during the training process, such as an abnormal process exit, and then restart the corresponding process, the training process can be resumed:</p>
<p>For example, if the Scheduler process exits abnormally during training, you can execute the following command to restart Scheduler:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span>YOUR_DATA_PATH
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">           </span><span class="c1"># Enable recovery</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/XXX/XXX<span class="w">      </span><span class="c1"># Set recovery path</span>

<span class="nb">cd</span><span class="w"> </span>./device

<span class="c1"># Launch 1 scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>XXX.XXX.XXX.XXX<span class="w">  </span><span class="c1"># Scheduler IP address</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span>XXXX<span class="w">             </span><span class="c1"># Scheduler port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">               </span><span class="c1"># The node id for Scheduler</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Worker and Scheduler’s networking is automatically restored.</p>
<p>The exception exit of the Worker process is handled in a similar way (Note: the Worker process has an abnormal exit, and it needs to wait for 30s to pull up before resuming training. Before that, Scheduler refuses to register the worker with the same node id again in order to prevent network jitter and malicious registration).</p>
</section>
</section>
<section id="using-ms-operators-for-distributed-training-in-k8s-clusters">
<h2>Using ms-operators for Distributed Training in K8s Clusters<a class="headerlink" href="#using-ms-operators-for-distributed-training-in-k8s-clusters" title="Permalink to this headline"></a></h2>
<p>MindSpore Operator is a plugin for MindSpore to conduct distributed training on Kubernetes. The CRD (Custom Resource Definition) defines three roles of Scheduler, PS, and Worker, and users only need to configure the yaml file to easily implement distributed training.</p>
<p>The current ms-operator supports ordinary single Worker training, single Worker training in PS mode, and Scheduler and Worker startups for automatic parallelism (such as data parallelism and model parallelism). For detailed procedures, see <a class="reference external" href="https://gitee.com/mindspore/ms-operator">ms-operator</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="train_ascend.html" class="btn btn-neutral float-left" title="Distributed Parallel Training Example (Ascend)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_inference.html" class="btn btn-neutral float-right" title="Distributed Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>