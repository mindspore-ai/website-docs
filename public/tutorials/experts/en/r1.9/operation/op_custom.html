<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Operators (Custom-based) &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference Model Overview" href="../infer/inference.html" />
    <link rel="prev" title="Dimension Reduction Training Algorithm" href="../others/dimention_reduce_training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/eager.html">Lightweight Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/custom_cell_reverse.html">Customizing Reverse Propagation Function of Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/ms_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../others/mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../others/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Operators (Custom-based)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-usage">Basic Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-hybrid-type">Defining Custom Operator of Hybrid Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-tbe-type">Defining Custom Operator of tbe Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-aicpu-type">Defining Custom Operator of aicpu Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-aot-type">Defining Custom Operator of aot Type</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#a-gpu-example">A GPU Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#a-cpu-example">A CPU Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-pyfunc-type">Defining Custom Operator of pyfunc Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-julia-type">Defining Custom Operator of julia Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-custom-operator-of-akg-type">Defining Custom Operator of akg Type</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#registering-the-operator-information">Registering the Operator Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-bprop-function-for-operators">Defining the bprop Function for Operators</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-hybrid-syntax-specification">MindSpore Hybrid Syntax Specification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#variables">Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#expressions">Expressions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loop">Loop</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scheduling-keywords">Scheduling keywords</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute">Attribute</a></li>
<li class="toctree-l4"><a class="reference internal" href="#keywords">Keywords</a></li>
<li class="toctree-l4"><a class="reference internal" href="#frequent-error-messages-and-error-attributions">Frequent Error Messages and Error Attributions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/cpu_gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.9/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Custom Operators (Custom-based)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/operation/op_custom.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="custom-operators-custom-based">
<h1>Custom Operators (Custom-based)<a class="headerlink" href="#custom-operators-custom-based" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.9/tutorials/experts/source_en/operation/op_custom.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When built-in operators cannot meet requirements during network development, you can call the Python API <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive defined in MindSpore to quickly create different types of custom operators for use.</p>
<p>Traditional methods to add a custom operator need three steps: registering the operator primitive, implementing the operator, and registering the operator information.</p>
<p>The related concepts are as follows:</p>
<ul class="simple">
<li><p>Operator primitive: defines the frontend API prototype of an operator on the network. It is the basic unit for forming a network model and includes the operator name, attribute (optional), input and output names, output shape inference method, and output data type inference method.</p></li>
<li><p>Operator implementation: defines a Python function (Ascend custom operators) or a C++ class (GPU and CPU custom operators), which describes the implementation of the internal computation logic of an operator.</p></li>
<li><p>Operator information: describes basic information about an operator, such as the operator name, supported input and output data types, supported input and output data formats, and attributes. It is the basis for the backend to select and map operators.</p></li>
</ul>
<p>Compared with traditional custom operator creating methods, creating custom operators based on <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive has several advantages:</p>
<ul class="simple">
<li><p>Different custom operators use the same <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive, there is no need to define a primitive for every operator. The above three parts of work can be implemented in a network script in a unified way and used as part of the network expression, there is no need to modify and recompile the source codes of MindSpore.</p></li>
<li><p>It unifies the interface and usage for different kinds of custom operators, which is convenient for network developers to flexibly choose which kind of custom operator to use according to their needs.</p></li>
<li><p>Supports defining custom operators with hybrid expression, which can be used across platforms.</p></li>
</ul>
</section>
<section id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this headline"></a></h2>
<p>The operator development methods supported by custom operator based on the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive include: hybrid, tbe, aot, pyfunc, julia, and akg.</p>
<p>The difference between these operator development methods are as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p>Defining Methods</p></th>
<th class="text-center head"><p>Development Language</p></th>
<th class="text-center head"><p>Compilation Method</p></th>
<th class="head"><p>Supported Platforms</p></th>
<th class="head"><p>Recommended Scenarios</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>hybrid</p></td>
<td class="text-center"><p>MindSpore HYBRID DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>Ascend/GPU platform common development and rapid validation</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>tbe</p></td>
<td class="text-center"><p>TBE DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
<td><p>Ascend AICORE custom the operator scenarios</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>aicpu</p></td>
<td class="text-center"><p>C/C++</p></td>
<td class="text-center"><p>AOT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
<td><p>Ascend AICORE custom the operator scenarios</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>aot</p></td>
<td class="text-center"><p>C/C++/CUDA</p></td>
<td class="text-center"><p>AOT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>high-performance scenarios / use third-party operators scenarios</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>pyfunc</p></td>
<td class="text-center"><p>Python</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>Fast algorithm verification, need to interact with Python and other scenarios</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>julia</p></td>
<td class="text-center"><p>Julia</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>Science compute scenarios / use Julia scenarios</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>akg</p></td>
<td class="text-center"><p>MindSpore AKG DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
<td><p>Ascend/GPU platform general scenarios</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The full name of DSL is Domain Specific Language.</p></li>
<li><p>AOT(Ahead Of Time) compiling means the operator implementation needs to be compiled into a dynamic library in advance and then automatically called by the framework when the network is running. JIT(Just In Time) compiling does not need to compile the operator implementation in advance, the operator implementation will be directly called by the framework during network compilation or runtime.</p></li>
</ul>
</div></blockquote>
<p>Different custom operator defining methods use different development languages to implement the operator, but the development process is the same, including operator implementation, operator output shape, data type inference, and operator information registration (optional). You can choose which one to use based on needs. The defining methods of these custom operators will be introduced here, and examples are provided for each method.</p>
<blockquote>
<div><p>More examples can be found in the MindSpore source code <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.9/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>.</p>
</div></blockquote>
<section id="defining-custom-operator-of-hybrid-type">
<h3>Defining Custom Operator of Hybrid Type<a class="headerlink" href="#defining-custom-operator-of-hybrid-type" title="Permalink to this headline"></a></h3>
<p>A custom operator of Hybrid type is the default defined type of a custom operator. By using a custom operator of the Hybrid type, users can describe the operator calculation logic in Python-like syntax without paying attention to the engineering details defined by the operator for the MindSpore framework, allowing the user to focus on the algorithm itself.</p>
<p>Custom operators of Hybrid type use <a class="reference internal" href="#mindspore-hybrid-syntax-specification"><span class="std std-doc">MindSpore Hybrid DSL</span></a> to describe the implementation of the calculation logic inside the operator. Functions defined with MindSpore Hybrid DSL can be parsed by the <a class="reference external" href="https://gitee.com/mindspore/akg">AKG Operator Compiler</a> for JIT compilation to generate efficient operators for use in training reasoning for large-scale models.</p>
<p>Custom operators of Hybrid type use <a class="reference internal" href="#mindspore-hybrid-syntax-specification"><span class="std std-doc">MindSpore Hybrid DSL</span></a> to describe the implementation of the calculation logic inside the operator. Functions defined with MindSpore Hybrid DSL can be parsed by the <a class="reference external" href="https://gitee.com/mindspore/akg">AKG Operator Compiler</a> for JIT compilation to generate efficient operators for use in training reasoning for large-scale models. At the same time, the function defined by MindSpore Hybrid DSL can be called directly as a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> function, which is convenient for users to debug and flexibly switch to <a class="reference internal" href="#defining-custom-operator-of-pyfunc-type"><span class="std std-doc">pyfunc type custom operator</span></a>, so that develop once, custom operator expressions are reused for multiple modes, multiple platforms and multiple scenes.</p>
<p>The following example (test_custom_hybrid.py) shows how to write a custom operator of the hybrid type. The operator computes the sum of two tensors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">ms_kernel</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, Hybrid DSL</span>
<span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define custom operators of the hybrid type (Custom&#39;s default mode)</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case,</p>
<ul class="simple">
<li><p>The Hybrid type is the default type for Custom.</p></li>
<li><p>The input of custom operators with Hybrid type must be a function with <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/api_python/ops/mindspore.ops.ms_kernel.html"><code class="docutils literal notranslate"><span class="pre">&#64;ms_kernel</span></code></a>.</p></li>
<li><p>When defining a custom operator for the Hybrid type, you can use the built-in automatic shape/dtype derivation function, or you can manually enter the shape/dtype deduction function.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_hybrid.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-tbe-type">
<h3>Defining Custom Operator of tbe Type<a class="headerlink" href="#defining-custom-operator-of-tbe-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of tbe type uses the TBE(Tensor Boost Engine) operator DSL to describe the internal calculation logic of the operator. You can refer to the <a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/operatordev/tbedevg/tbedevg_000003.html">TBE document</a> for the implementation details.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>Operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes test_custom_tbe.py as an example to introduce how to define a custom operator of tbe type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_tbe.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">CustomRegOp</span><span class="p">,</span> <span class="n">custom_info_register</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, and operator information registration</span>
<span class="nd">@custom_info_register</span><span class="p">(</span><span class="n">CustomRegOp</span><span class="p">()</span> \
                      <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">target</span><span class="p">(</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">get_op_info</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">kernel_name</span><span class="o">=</span><span class="s2">&quot;add&quot;</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">te.lang.cce</span>
    <span class="kn">from</span> <span class="nn">te</span> <span class="kn">import</span> <span class="n">tvm</span>
    <span class="n">data0</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data0&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">data1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data1&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">vadd</span><span class="p">(</span><span class="n">data0</span><span class="p">,</span> <span class="n">data1</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">cce</span><span class="p">():</span>
        <span class="n">sch</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">auto_schedule</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;print_ir&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">kernel_name</span><span class="p">,</span> <span class="s2">&quot;tensor_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">data0</span><span class="p">,</span> <span class="n">data1</span><span class="p">,</span> <span class="n">res</span><span class="p">]}</span>
    <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">cce_build_code</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of tbe type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;tbe&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">CustomRegOp</span></code> to create the operator information and use <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator to register it.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_tbe.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-aicpu-type">
<h3>Defining Custom Operator of aicpu Type<a class="headerlink" href="#defining-custom-operator-of-aicpu-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of the aicpu type adopts the AOT compilation method, which requires the operator developer to implement the corresponding source code file of the function based on the specific interface provided, and compiles the source code file into a dynamic link library in advance. The framework will find the corresponding dynamic link library and load the operator according to the name of the dynamic link library configured by the developer in the operator properties. Reference for specific operator implementation <a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/operatordev/aicpudevg/aicpudevg_000026.html">CANN AICPU Custom Operator Development</a>.</p>
<p>Operator output shape and data type inference can be implemented by defining Python functions that describe the derivation logic of operator output shape and data type.</p>
<p>This type of custom operator needs to register operator information, operator information generation method, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>, a custom operator of type aicpu, you need to specify the attributes of <code class="docutils literal notranslate"><span class="pre">attr(&quot;cust_aicpu&quot;,</span> <span class="pre">&quot;required&quot;,</span> <span class="pre">&quot;str&quot;,</span> <span class="pre">&quot;mindspore_aicpu_kernels&quot;)</span></code> for MindSpore to find the dynamic link library corresponding to the operator implementation.</p>
<blockquote>
<div><ul class="simple">
<li><p>It should be noted that the dynamic link library compiled after the development of a custom operator of aicpu type needs to be stored in the lib directory of MindSpore. For example, If MindSpore is installed in the virtual environment <code class="docutils literal notranslate"><span class="pre">/home/conda/envs/aicpu/lib/python3.7/site-packages/mindspore</span></code>, the aicpu so file needs to be placed in <code class="docutils literal notranslate"><span class="pre">/home/conda/envs/aicpu/lib/python3.7/site-packages/mindspore/lib/</span></code> directory.</p></li>
<li><p>The value of “cust_aicpu” is a string, which is represented by the <code class="docutils literal notranslate"><span class="pre">lib</span></code> prefix and the <code class="docutils literal notranslate"><span class="pre">.so</span></code> suffix removed from the name of the operator dynamic link library. If the name of <code class="docutils literal notranslate"><span class="pre">libmindspore_aicpu_kernels.so</span></code> is removed, it can be set to <code class="docutils literal notranslate"><span class="pre">mindspore_aicpu_kernels</span></code>.</p></li>
</ul>
</div></blockquote>
<p>The following takes test_dropout_aicpu.py as an example to introduce the development process of custom operators of type aicpu, in which the custom operator implements the function of dropout, and the compiled operator dynamic link library, we named it libmindspore_aicpu_kernels.so, and have put the dynamic link library under the lib of the mindspore root directory.</p>
<p>The contents of test_dropout_aicpu.py are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">CustomRegOp</span><span class="p">,</span> <span class="n">custom_info_register</span><span class="p">,</span> <span class="n">DataType</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, registering operator information</span>
<span class="n">dropout2d_op_info</span> <span class="o">=</span> <span class="n">CustomRegOp</span><span class="p">(</span><span class="s2">&quot;Dropout2D&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">fusion_type</span><span class="p">(</span><span class="s2">&quot;OPAQUE&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">,</span> <span class="s2">&quot;float&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="s2">&quot;cust_aicpu&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">,</span> <span class="s2">&quot;str&quot;</span><span class="p">,</span> <span class="s2">&quot;mindspore_aicpu_kernels&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">I8_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">I8_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">I16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">I16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">I32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">I32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">I64_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">I64_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">U8_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">U8_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">U16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">U16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">U32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">U32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">U64_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">U64_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F64_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F64_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">BOOL_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">target</span><span class="p">(</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">get_op_info</span><span class="p">()</span>

<span class="nd">@custom_info_register</span><span class="p">(</span><span class="n">dropout2d_op_info</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dropout2d_aicpu</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dropout2D AiCPU register&quot;&quot;&quot;</span>
    <span class="k">return</span>

<span class="c1"># Define a custom operator network</span>
<span class="k">class</span> <span class="nc">NetDropout2D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NetDropout2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">dropout2d_aicpu</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">cust_attr</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> \
                              <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">cust_attr</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">bool_</span><span class="p">),</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aicpu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">keep_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cust_aicpu_so_path</span> <span class="o">=</span> <span class="s2">&quot;mindspore_aicpu_kernels&quot;</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">,</span>  <span class="bp">self</span><span class="o">.</span><span class="n">cust_aicpu_so_path</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Defines a custom operator of type aicpu</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dropout2d_nn</span> <span class="o">=</span> <span class="n">NetDropout2D</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">dropout2d_nn</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output: &quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mask: &quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, there are the following points to explain:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive can be specified in a variety of ways, either given a type or set with a Python lambda function. In this example, the lambda function indicates that the two shapes of the output are the same as the input, the data type of the first output and the information of the input tensor are the same, and the data type of the second output is the bool type.</p></li>
<li><p>Operator information is generated via <code class="docutils literal notranslate"><span class="pre">CustomRegOp</span></code> and operator information is registered with the <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator.</p></li>
</ul>
<p>Executing cash:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_dropout_aicpu.py
</pre></div>
</div>
<p>The execution result is as follows (due to the random nature of the dropout operator, there is a difference in the result of multiple runs):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>output : [[[[2.  2.  2.] [2.  2.  2.]]]]
mask: [[[[True  True  True]  [True  True  True]]]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-aot-type">
<h3>Defining Custom Operator of aot Type<a class="headerlink" href="#defining-custom-operator-of-aot-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of aot type adopts the AOT compilation method, which requires network developers to hand-write the source code file of the operator implementation based on a specific interface and compiles the source code file into a dynamic library in advance, and then the framework will automatically call and run the function defined in the dynamic library. In terms of the development language of the operator implementation, the GPU platform supports CUDA, and the CPU platform supports C and C++. The interface specification of the operator implementation in the source file is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">func_name</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">);</span>
</pre></div>
</div>
<p>where the function name <code class="docutils literal notranslate"><span class="pre">func_name</span></code> can be replaced with any valid function name. The return value is of type int. 0 means normal exit, and non-zero means an exception occurs. The meaning of the parameter list is as follows:</p>
<ul class="simple">
<li><p>nparam (int): The number of inputs and outputs. For example, if an operator has 2 inputs and 1 output, then the value of nparam is 3.</p></li>
<li><p>params (void **): An array of pointers, with each pointer pointing to the input or output data. For example, if an operator has 2 inputs and 1 output, then params[0] points to the first input data, params[1] points to the second input data, params[2] points to the output data.</p></li>
<li><p>ndims (int *): An array of integers, each integer represents the dimensions of the shape of input or output. For example, if params[i] is a tensor with shape [1024, 1024], then ndims[i] is 2.</p></li>
<li><p>shapes (int64_t **): An array of shapes, each element in array represents for the shape of input or output. For example, if params[i] is a tensor with shape [1024, 1024], then shapes[i][0] is 1024, shapes[i][1] is 1024.</p></li>
<li><p>dtypes (const char **): Array of data types, each element in array represents for the data type of input or output. The value of data type can be “float32”, “float16”, “float”, “float64”, “int”, “int8”, “int16”, “int32”, “int64”, “uint”, “uint8”, “uint16”, “uint32”, “uint64”, “bool”.</p></li>
<li><p>stream (void *): Stream pointer, only used in Cuda file.</p></li>
<li><p>extra (void *): Used for further extension.</p></li>
</ul>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator only supports some specific input and output data types, then the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>The following examples introduce the development process of aot type custom operator on GPU platform and CPU platform, where the custom operator implements the function of adding two input tensors.</p>
<section id="a-gpu-example">
<h4>A GPU Example<a class="headerlink" href="#a-gpu-example" title="Permalink to this headline"></a></h4>
<p>Use the CUDA language to write the source file add.cu for the operator implementation:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#define THREADS 1024</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">CustomAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">THREADS</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">CustomAdd</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span>
<span class="w">                         </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">custream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">nparam</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ndims</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">THREADS</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;float32&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">CustomAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">THREADS</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">custream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input1</span><span class="p">),</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input2</span><span class="p">),</span>
<span class="w">                                                   </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Compile add.cu into a dynamic library add.so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nvcc<span class="w"> </span>--shared<span class="w"> </span>-Xcompiler<span class="w"> </span>-fPIC<span class="w"> </span>-o<span class="w"> </span>add.so<span class="w"> </span>add.cu
</pre></div>
</div>
<p>Write the test case test_custom_aot.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of aot type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.so:CustomAdd&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>In this example, you need to place test_custom_aot.py and add.so in the same directory. If add.so is in another directory, you need to replace the value of the first parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive with the absolute path of add.so.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_aot.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="a-cpu-example">
<h4>A CPU Example<a class="headerlink" href="#a-cpu-example" title="Permalink to this headline"></a></h4>
<p>Use C/C++ language to write the source file add.cc for the operator implementation:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
<span class="k">using</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span><span class="p">));</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">CustomAdd</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">nparam</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;float32&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Compile add.cc into a dynamic library add.so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>g++<span class="w"> </span>--shared<span class="w"> </span>-fPIC<span class="w"> </span>-o<span class="w"> </span>add.so<span class="w"> </span>add.cc
</pre></div>
</div>
<p>Write the test case test_custom_aot.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of aot type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.so:CustomAdd&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>In this example, you need to place test_custom_aot.py and add.so in the same directory. If add.so is in another directory, you need to replace the value of the first parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive with the absolute path of add.so.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_aot.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
</section>
<section id="defining-custom-operator-of-pyfunc-type">
<h3>Defining Custom Operator of pyfunc Type<a class="headerlink" href="#defining-custom-operator-of-pyfunc-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of pyfunc type uses native Python syntax to define the operator implementation, which describes the internal calculation logic of the operator. The framework will automatically call this function during the network runtime.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator only supports some specific input and output data types, then the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes test_custom_pyfunc.py as an example to introduce how to define a custom operator of pyfunc type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_pyfunc.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of pyfunc type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;pyfunc&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_pyfunc.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-julia-type">
<h3>Defining Custom Operator of julia Type<a class="headerlink" href="#defining-custom-operator-of-julia-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of julia type uses Julia to describe the internal calculation logic of the operator. The framework will automatically call this function during the network runtime.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic of the operator output shape and the data type.</p>
<p>If the custom operator only supports specific input and output data types, you need to define the operator information. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes the function of adding two input tensors as an example to introduce how to define a custom operator of julia type.</p>
<p>Firstly, users need to implement Julia functions via separate files, such as (add.jl):</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># add.jl</span>
<span class="k">module</span><span class="w"> </span><span class="n">Add</span>
<span class="c"># inputs: x, y, output: z, output should use .= to inplace assign</span>
<span class="k">function</span><span class="w"> </span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span>
<span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Secondly, refer to the Julia function written above in a custom operator in the network script, taking test_custom_julia.py as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.jl:Add:add&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;julia&quot;</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_julia.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>Matters need attention:</p>
<ol class="arabic">
<li><p>User should make sure to download the correct version of Julia, that is, version &gt;= 1.6.0.</p></li>
<li><p>User is required to set <code class="docutils literal notranslate"><span class="pre">julia/lib</span></code> to <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>, because the Julia C API called at runtime is obtained from <code class="docutils literal notranslate"><span class="pre">libjulia.so</span></code>, taking julia-1.6.5 as an example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># download julia-1.6.5</span>
wget<span class="w"> </span>https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.5-linux-x86_64.tar.gz
<span class="c1"># extract file</span>
tar<span class="w"> </span>xvf<span class="w"> </span>julia-1.6.5-linux-x86_64.tar.gz
<span class="c1"># if $JULIA_DIR not exist</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$PWD</span>/julia-1.6.5/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="c1"># else</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$JULIA_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Custom</span></code> operator’s first arg <code class="docutils literal notranslate"><span class="pre">func</span></code> should keep format like <code class="docutils literal notranslate"><span class="pre">file_name:module_name:func_name</span></code>, <code class="docutils literal notranslate"><span class="pre">file_name</span></code> should include path, suggest using absolute path.</p></li>
<li><p>Julia file should include <code class="docutils literal notranslate"><span class="pre">module</span></code>, <code class="docutils literal notranslate"><span class="pre">module</span></code> include <code class="docutils literal notranslate"><span class="pre">function</span></code>, both ends with <code class="docutils literal notranslate"><span class="pre">end</span></code>.</p></li>
<li><p>The input and output order of the Julia function needs to be consistent with the input and output order of the operator.</p></li>
<li><p>The final output of the Julia function, i.e. assignment of kernel output, needs to use <code class="docutils literal notranslate"><span class="pre">.=</span></code>, otherwise the result cannot be written to memory.</p></li>
<li><p>Julia code supports <a class="reference external" href="https://docs.julialang.org/en/v1/">Julia</a>’s common syntax, users need to ensure that the syntax is correct and the function can be executed correctly.</p></li>
<li><p>Users who want to use Julia’s third-party software packages in Julia files need to download the corresponding software to ensure that they can call it correctly, which can be called through <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pkg;</span> <span class="pre">pkg.add</span> <span class="pre">(&quot;somepkg&quot;)</span></code> to install.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">julia</span> <span class="pre">array</span></code> is <code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">major</span></code> arranged in memory, while <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">array</span></code> is <code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">major</span></code>. If Julia and numpy are compared, non-elemwise calculations need to consider memory arrangement. In the Julia function, the conversion of <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">array</span></code> and <code class="docutils literal notranslate"><span class="pre">julia</span> <span class="pre">array</span></code> can be performed by following the following code example:An example of MatMul:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="n">permutedims</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">reverse</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span><span class="w"> </span><span class="n">length</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">1</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">function</span><span class="w"> </span><span class="n">change_output_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="n">reshape</span><span class="p">(</span><span class="n">permutedims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">end</span>
</pre></div>
</div>
</li>
</ol>
<p>Taking matrix multiplication as an example:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># julia array is column-major, numpy aray is row-major</span>
<span class="c"># user should change julia or numpy&#39;s layout to keep same behavior</span>
<span class="cm">#= EXAMPLE</span>
<span class="cm">A[2,3]               B[3,4]               C[2,4]</span>
<span class="cm">NUMPY:</span>
<span class="cm">[[1, 2, 3]       [[1, 2, 3, 4]         [[38, 44, 50,  56]</span>
<span class="cm"> [4, 5, 6]]       [5, 6, 7, 8]          [83, 98, 113,128]]</span>
<span class="cm">                  [9,10,11,12]]</span>
<span class="cm">JULIA:</span>
<span class="cm">change_input_to_row_major:</span>
<span class="cm">1.inputs read numpy data from memory:</span>
<span class="cm">[[1, 3, 5]       [[1, 4, 7,10]</span>
<span class="cm"> [2, 4, 6]]       [2, 5, 8,11]</span>
<span class="cm">                  [3, 6, 9,12]]</span>
<span class="cm">2.inputs after reshape(reverse(shape)):</span>
<span class="cm">[[1, 4]          [[1, 5, 9]</span>
<span class="cm"> [2, 5]           [2, 6,10]</span>
<span class="cm"> [3, 6]]          [3, 7,11]</span>
<span class="cm">                  [4, 8,12]]</span>
<span class="cm">3.inputs after transpose/permutedims:</span>
<span class="cm">[[1, 2, 3]       [[1, 2, 3, 4]         [[38, 44, 50,  56]</span>
<span class="cm"> [4, 5, 6]]       [5, 6, 7, 8]          [83, 98, 113,128]]</span>
<span class="cm">                  [9,10,11,12]]</span>
<span class="cm">change_output_to_row_major:</span>
<span class="cm">1.output after transpose/permutedims:</span>
<span class="cm">                                       [[38, 83]</span>
<span class="cm">                                        [44, 98]</span>
<span class="cm">                                        [50,113]</span>
<span class="cm">                                        [56,128]</span>
<span class="cm">2.output after reshape:</span>
<span class="cm">                                       [[38, 50, 83, 113]</span>
<span class="cm">                                        [44, 56, 98, 128]]</span>
<span class="cm">3.output read numpy data from memory:</span>
<span class="cm">                                       [[38, 44, 50,  56]</span>
<span class="cm">                                        [83, 98,113, 128]]</span>
<span class="cm">=#</span>
<span class="k">function</span><span class="w"> </span><span class="n">foo!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">gemm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">change_output_to_row_major</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-akg-type">
<h3>Defining Custom Operator of akg Type<a class="headerlink" href="#defining-custom-operator-of-akg-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of akg type uses the <a class="reference external" href="https://gitee.com/mindspore/akg">MindSpore AKG</a> operator DSL to describe the internal calculation logic of the operator. MindSpore AKG is an operator development and compilation framework based on TVM(Tensor Virtual Machine) and Polyhedral technology, and it supports multiple types of operator DSL, such as Hybrid, IR builder and TVM compute.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic of operator output shape and data type.</p>
<p>If the operator contains attributes or only supports specific input and output data types or data formats, operator information needs to be registered, and for how to generate operator information, see <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>. If the operator information is not registered, when operator selection and mapping are made in the backend, the operator information is derived from the input of the current operator.</p>
<p>Takes test_custom_akg.py as an example of how to define a custom operator of akg type, where the operator computes the sum of two tensors.</p>
<p>Here is the content of test_custom_akg.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, Hybrid DSL</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of akg type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">set_context(device_target=&quot;GPU&quot;)</span></code> indicates that the operator runs on the GPU platform. To run on the Ascend platform, please compile an Ascend version of MindSpore and set the value of device_target to “Ascend”.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_akg.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="registering-the-operator-information">
<h3>Registering the Operator Information<a class="headerlink" href="#registering-the-operator-information" title="Permalink to this headline"></a></h3>
<p>The operator information describes the supported inputs and outputs data type, the supported inputs and outputs format, attributes, and target (platform information) of the operator implementation. It is used to select and map operators by the backend. The operator information can be defined by using the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/api_python/ops/mindspore.ops.CustomRegOp.html#mindspore-ops-customregop">CustomRegOp</a> API, then you can use the <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/api_python/ops/mindspore.ops.custom_info_register.html#mindspore-ops-custom-info-register">custom_info_register</a> decorator or just pass it to the <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter of <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive to bind the information to the operator implementation. The operator information will be registered to the operator information library on the MindSpore C++ side at last. The <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter takes higher priority than the <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator.</p>
<p>The target value in operator information can be “Ascend”, “GPU” or “CPU”, which describes the operator information on a specific target. For the same operator implementation, it may have different supported data types on different targets, so you can use the target value in operator information to differ this. The operator information on a specific target will be registered only once.</p>
<blockquote>
<div><ul class="simple">
<li><p>The numbers and sequences of the input and output information defined in the operator information must be the same as those in the parameters of the operator implementation.</p></li>
<li><p>For the custom operator of akg type, if the operator has attributes, you need to register operator information, The attribute name in the operator information must be consistent with the attribute name used in the operator implementation. For the custom operator of tbe type, you need to register operator information. For the custom operator of aot type, since the operator implementation needs to be compiled into a dynamic library in advance, the decorator will not work, and the operator information can only be passed in through the <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter.</p></li>
<li><p>If the custom operator only supports a specific input and output data type or data format, the operator information needs to be registered so that the data type and data format can be checked when the operator is selected in the backend. For the case where the operator information is not provided, the information will be derived from the inputs of the current operator.</p></li>
</ul>
</div></blockquote>
</section>
<section id="defining-the-bprop-function-for-operators">
<h3>Defining the bprop Function for Operators<a class="headerlink" href="#defining-the-bprop-function-for-operators" title="Permalink to this headline"></a></h3>
<p>If an operator needs to support automatic differentiation, the backpropagation(bprop) function needs to be defined first and then passed to the <code class="docutils literal notranslate"><span class="pre">bprop</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In the bprop function, you need to describe the backward computation logic that uses the forward input, forward output, and output gradients to obtain the input gradients. The backward computation logic can be composed of built-in operators or custom backward operators.</p>
<p>Note the following points when defining the bprop function for operators:</p>
<ul class="simple">
<li><p>The input parameter sequence of the bprop function is the forward input, forward output, and output gradients. For a multi-output operator, the forward output and output gradients are provided in the form of tuples.</p></li>
<li><p>The return value of the bprop function is tuples consisting of input gradients. The sequence of elements in a tuple is the same as that of the forward input parameters. Even if there is only one input gradient, the return value must be a tuple.</p></li>
</ul>
<p>Take test_grad.py as an example to show the usage of the backpropagation function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Forward computation of custom operator</span>
<span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="c1"># Backward computation of custom operator</span>
<span class="k">def</span> <span class="nf">square_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dout</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">dx</span>

<span class="c1"># Backpropagation function</span>
<span class="k">def</span> <span class="nf">bprop</span><span class="p">():</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">square_grad</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">custom_bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">custom_bprop</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Define a custom operator of akg type and provide a backpropagation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">bprop</span><span class="o">=</span><span class="n">bprop</span><span class="p">(),</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">Net</span><span class="p">())(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sens</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>The backpropagation function uses a custom operator of akg type, and the operator definition and use need to be separated, that is, the custom operator is defined outside the <code class="docutils literal notranslate"><span class="pre">custom_bprop</span></code> function and used inside the <code class="docutils literal notranslate"><span class="pre">custom_bprop</span></code> function.</p></li>
</ul>
<p>Execute case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_grad.py
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ 2.  8. 18.]
</pre></div>
</div>
<blockquote>
<div><p>More examples can be found in the MindSpore source code <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.9/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>.</p>
</div></blockquote>
</section>
<section id="mindspore-hybrid-syntax-specification">
<h3>MindSpore Hybrid Syntax Specification<a class="headerlink" href="#mindspore-hybrid-syntax-specification" title="Permalink to this headline"></a></h3>
<p>The syntax of MindSpore Hybrid DSL is similar to Python syntax, such as function definitions, indentation, and annotations. Functions written in MindSpore Hybrid DSL can be used as ordinary <code class="docutils literal notranslate"><span class="pre">numpy</span></code> functions after adding ms_kernel decorators, or they can be custom operators used for Custom.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">ms_kernel</span>

<span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">outer_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">allocate</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">i2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">d</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span>
                <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">sin</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i2</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i2</span><span class="p">,</span> <span class="n">i1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="n">np_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">np_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">outer_product</span><span class="p">(</span><span class="n">np_x</span><span class="p">,</span> <span class="n">np_y</span><span class="p">))</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_x</span><span class="p">)</span>
<span class="n">input_y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_y</span><span class="p">)</span>

<span class="n">test_op_akg</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">outer_product</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">test_op_akg</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>The detailed syntax specification of MindSpore Hybrid DSL is as follows.</p>
<section id="variables">
<h4>Variables<a class="headerlink" href="#variables" title="Permalink to this headline"></a></h4>
<p>Variables in MindSpore Hybrid DSL includes two formats Tensor and Scalar.</p>
<p>Tensor variables, besides those in the inputs of the function, must be declared with <code class="docutils literal notranslate"><span class="pre">shape</span></code>和 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> before use.</p>
<ul class="simple">
<li><p>declare a output tensor by <code class="docutils literal notranslate"><span class="pre">output_tensor</span></code>, such as <code class="docutils literal notranslate"><span class="pre">output_tensor(shape,</span> <span class="pre">dtype)</span></code>.</p></li>
<li><p>declare an intermediate tensor by <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, such as <code class="docutils literal notranslate"><span class="pre">allocate(shape,</span> <span class="pre">dtype)</span></code>.</p></li>
</ul>
<p>Example of Tensor allocation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># We can use a and b directly as input tensors</span>

    <span class="c1"># d is a tensor with dtype fp16 and shape (2,), and will be used as an intermediate variable in the following code</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">allocate</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>
    <span class="c1"># c is a tensor with same dtype and shape as a, and will be used as a output function in the following code</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># assign value to c by d as the intermediate variable</span>
    <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># c as output</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Scalar variables will regard its first assignment as the declaration. The assignment can be either a number or an expression. The place of the first assignment of a scalar variable defines its scope, such as inside a certain level of for loop. Using the variable outside its scope will lead to error.</p>
<p>Example of using Scalar variable:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="c1"># i loop</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="c1"># j loop</span>
            <span class="c1"># assign a number to Scalar d</span>
            <span class="n">d</span> <span class="o">=</span> <span class="mf">2.0</span>
            <span class="c1"># assign an expression to Scalar e</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="c1"># use scalars</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">e</span>

    <span class="c1"># Wrong: c[0, 0] = d</span>
    <span class="c1"># Can&#39;t use Scalar d outside its scope (j loop)</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
<p>Unlike native Python language, once a variable is defined, we can’t change its <code class="docutils literal notranslate"><span class="pre">shape</span></code>和 <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</section>
<section id="expressions">
<h4>Expressions<a class="headerlink" href="#expressions" title="Permalink to this headline"></a></h4>
<p>MindSpore Hybrid DSL supports basic math operators, including <code class="docutils literal notranslate"><span class="pre">+,</span> <span class="pre">-,</span> <span class="pre">*,</span> <span class="pre">/</span></code>, as well as self-assign operators, including <code class="docutils literal notranslate"><span class="pre">=,</span> <span class="pre">+=,</span> <span class="pre">-=,</span> <span class="pre">*=,</span> <span class="pre">/=</span></code>.
Users can write codes like writing Python expressions.</p>
<p><strong>All the expressions must be based on scalars. Computation for the tensors must include all indices, such as <code class="docutils literal notranslate"><span class="pre">C[i,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">A[i,</span> <span class="pre">j]</span> <span class="pre">+</span> <span class="pre">B[i,</span> <span class="pre">j]</span></code>. Currently, tensorized codes such as <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">A</span> <span class="pre">+</span> <span class="pre">B</span></code> are not supported.</strong></p>
<p>When writing assignment expressions, users must take care of the dtype of the expression and make them consistent on both sides of the equality. Otherwise, the error might be thrown on the stage of <strong>operator compilation</strong>. Any integer numbers in the expression will be treated as int32, while float numbers will be treated as float32. There is no implicit dtype casting in MindSpore Hybrid DSL, and all dtype casting must be written with dtype names as casting functions, including:</p>
<ul class="simple">
<li><p>int32</p></li>
<li><p>float16</p></li>
<li><p>float32</p></li>
<li><p>(only on gpu backend) int8, int16, int64, float64</p></li>
</ul>
<p>Example of dtype casting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="c1"># Wrong: c[0] = 0.1 c&#39;s dtype is fp16, while 0.1&#39;s dtype is fp32</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">float16</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># float16(0.1) cast the number 0.1 to dtype fp16</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">float16</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># float16(a[0, 0]) cast the number 0.1 to dtype fp16</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
</section>
<section id="loop">
<h4>Loop<a class="headerlink" href="#loop" title="Permalink to this headline"></a></h4>
<p>Currently, only the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is supported. <code class="docutils literal notranslate"><span class="pre">while</span></code>, <code class="docutils literal notranslate"><span class="pre">break</span></code>, and <code class="docutils literal notranslate"><span class="pre">continue</span></code> are illegal in MindSpore Hybrid DSL.</p>
<p>Loops are the same as those in Python. <code class="docutils literal notranslate"><span class="pre">range</span></code> and <code class="docutils literal notranslate"><span class="pre">grid</span></code> are supported to express extents of loops. <code class="docutils literal notranslate"><span class="pre">range</span></code> is for one-dimensional loops and accept a number as the upper bound of the loop, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
                <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>The iteration space of the above loops is <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">i</span> <span class="pre">&lt;</span> <span class="pre">3,</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">j</span> <span class="pre">&lt;</span> <span class="pre">4,</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">k</span> <span class="pre">&lt;</span> <span class="pre">5</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">grid</span></code> is for multi-dimensional loops and accepts <code class="docutils literal notranslate"><span class="pre">tuple</span></code> as its input. For example, the above code can be also written as follows in <code class="docutils literal notranslate"><span class="pre">grid</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>Right now <code class="docutils literal notranslate"><span class="pre">arg</span></code> is equivalent to a three dimensional index <code class="docutils literal notranslate"><span class="pre">(i,j,k)</span></code>, with upper bound 4, 5, 6 respectively. We also have access to each element in <code class="docutils literal notranslate"><span class="pre">arg</span></code>, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>Then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>.</p>
</section>
<section id="scheduling-keywords">
<h4>Scheduling keywords<a class="headerlink" href="#scheduling-keywords" title="Permalink to this headline"></a></h4>
<p>From version 1.8, MindSpore Hybrid DSL provides scheduling keywords to describe the type of loops. On the Ascend backend, scheduling keywords will help the new DSA polyhedron scheduler generate codes. The scheduling keywords include <code class="docutils literal notranslate"><span class="pre">serial</span></code>, <code class="docutils literal notranslate"><span class="pre">vectorize</span></code>, <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, and <code class="docutils literal notranslate"><span class="pre">reduce</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">serial</span></code> indicates that the scheduler should keep the order of the loop and not apply loop transformations on such loops. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">serial_test</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">serial</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">serial</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">b</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">serial</span></code> indicates that there are dependence relations on <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code>. <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code> should be in ascending order during the scheduling.</p>
<p><code class="docutils literal notranslate"><span class="pre">vectorize</span></code> is usually used in the innermost loop, indicating the chance of generation vector instructions. For example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">vector_test</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">vectorize</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">vectorize</span></code> indicates that the innermost <code class="docutils literal notranslate"><span class="pre">j</span></code> loop conducts the same computation at each iteration and that the computation can be accelerated via vector instructions.</p>
<p><code class="docutils literal notranslate"><span class="pre">parallel</span></code> is usually used in the outermost loop, indicating the chance of parallelization. For example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">parallel_test</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parallel</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">parallel</span></code> indicates that there is no dependency between each iteration of the <code class="docutils literal notranslate"><span class="pre">i</span></code> loop and that the computation can be accelerated via parallelization.</p>
<p><code class="docutils literal notranslate"><span class="pre">reduce</span></code> indicates that the loop is a reduction axis. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reduce_test</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">((</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">reduce</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">reduce</span></code> indicates that <code class="docutils literal notranslate"><span class="pre">k</span></code> is a reduction axis.</p>
<p>Notice that：</p>
<ul class="simple">
<li><p>The scheduling keywords will only influence the scheduling on the Ascend backend. On the CPU or GPU backend, the above scheduling keywords will be treated as the usual <code class="docutils literal notranslate"><span class="pre">for</span></code> keyword.</p></li>
<li><p>The scheduling keywords only provide hints to the scheduler. When the hints from the scheduling keywords contradict the analysis and validation result from the scheduler, the above scheduling keywords will be treated as the usual <code class="docutils literal notranslate"><span class="pre">for</span></code> keyword.</p></li>
</ul>
</section>
<section id="attribute">
<h4>Attribute<a class="headerlink" href="#attribute" title="Permalink to this headline"></a></h4>
<p>Currently we support only tensor’s <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">dtype</span></code> attributes, such as <code class="docutils literal notranslate"><span class="pre">a.shape</span></code>, and <code class="docutils literal notranslate"><span class="pre">c.dtype</span></code>.</p>
<p>The shape attribute of a Tensor is a <code class="docutils literal notranslate"><span class="pre">tuple</span></code>. We have access to its element with a <strong>fixed</strong> index, such as <code class="docutils literal notranslate"><span class="pre">a.shape[0]</span></code>.</p>
<p>Once <code class="docutils literal notranslate"><span class="pre">grid</span></code> accepts one Tensor’s <code class="docutils literal notranslate"><span class="pre">shape</span></code> attribute as its input, then the dimension of the loops is the same as the dimension of the Tensor. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@ms_kernel</span>
<span class="k">def</span> <span class="nf">kernel_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;float16&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span>  <span class="n">c</span>
</pre></div>
</div>
<p>If a is a two dimensional tensor, then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>, while if a is a three dimensional tensor, then the expression inside loops is equivalent to <code class="docutils literal notranslate"><span class="pre">out[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">=</span> <span class="pre">a[i,</span> <span class="pre">j,</span> <span class="pre">k]</span> <span class="pre">+</span> <span class="pre">b[i]</span></code>.</p>
</section>
<section id="keywords">
<h4>Keywords<a class="headerlink" href="#keywords" title="Permalink to this headline"></a></h4>
<p>Currently, we support keywords including:</p>
<ul class="simple">
<li><p>Math keywords (all platform): <code class="docutils literal notranslate"><span class="pre">log</span></code>, <code class="docutils literal notranslate"><span class="pre">exp</span></code>, <code class="docutils literal notranslate"><span class="pre">sqrt</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, <code class="docutils literal notranslate"><span class="pre">power</span></code>, <code class="docutils literal notranslate"><span class="pre">floor</span></code></p></li>
<li><p>Memory allocation: <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, <code class="docutils literal notranslate"><span class="pre">output_tensor</span></code></p></li>
<li><p>Datatype keywords: <code class="docutils literal notranslate"><span class="pre">int32</span></code>, <code class="docutils literal notranslate"><span class="pre">float16</span></code>, <code class="docutils literal notranslate"><span class="pre">float32</span></code>, <code class="docutils literal notranslate"><span class="pre">float64</span></code></p></li>
<li><p>For keywords: <code class="docutils literal notranslate"><span class="pre">for</span></code>, <code class="docutils literal notranslate"><span class="pre">range</span></code>, <code class="docutils literal notranslate"><span class="pre">grid</span></code></p></li>
<li><p>Scheduling keywords: <code class="docutils literal notranslate"><span class="pre">serial</span></code>, <code class="docutils literal notranslate"><span class="pre">vec</span></code>, <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, <code class="docutils literal notranslate"><span class="pre">reduce</span></code></p></li>
<li><p>In current version, advanced keywords are provided for the CPU/GPU backend:</p>
<ul>
<li><p>Math keywords: <code class="docutils literal notranslate"><span class="pre">rsqrt</span></code>, <code class="docutils literal notranslate"><span class="pre">erf</span></code>, <code class="docutils literal notranslate"><span class="pre">isnan</span></code>, <code class="docutils literal notranslate"><span class="pre">sin</span></code>, <code class="docutils literal notranslate"><span class="pre">cos</span></code>, <code class="docutils literal notranslate"><span class="pre">isinf</span></code>, <code class="docutils literal notranslate"><span class="pre">isfinite</span></code>, <code class="docutils literal notranslate"><span class="pre">atan</span></code>, <code class="docutils literal notranslate"><span class="pre">atan2</span></code> (only on GPU), <code class="docutils literal notranslate"><span class="pre">expm1</span></code> (only on GPU), <code class="docutils literal notranslate"><span class="pre">floor</span></code>, <code class="docutils literal notranslate"><span class="pre">ceil</span></code>, <code class="docutils literal notranslate"><span class="pre">trunc</span></code>, <code class="docutils literal notranslate"><span class="pre">round</span></code>, <code class="docutils literal notranslate"><span class="pre">ceil_div</span></code></p></li>
<li><p>Datatype keywords: <code class="docutils literal notranslate"><span class="pre">int8</span></code>, <code class="docutils literal notranslate"><span class="pre">int16</span></code>, <code class="docutils literal notranslate"><span class="pre">int64</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="frequent-error-messages-and-error-attributions">
<h4>Frequent Error Messages and Error Attributions<a class="headerlink" href="#frequent-error-messages-and-error-attributions" title="Permalink to this headline"></a></h4>
<p>To help users effectively develop and locate bugs, MindSpore Hybrid DSL provides the following error messages, including:</p>
<ul class="simple">
<li><p>TypeError: There are Python keywords such as <code class="docutils literal notranslate"><span class="pre">while</span></code>, <code class="docutils literal notranslate"><span class="pre">break</span></code> and <code class="docutils literal notranslate"><span class="pre">continue</span></code> which are not supported by MindSpore Hybrid DSL.</p></li>
<li><p>ValueError:</p>
<ul>
<li><p>There are built-in function names which are not in the above support list;</p></li>
<li><p>Take properties that are not <code class="docutils literal notranslate"><span class="pre">shape</span></code> or <code class="docutils literal notranslate"><span class="pre">dtype</span></code> for tensors.</p></li>
</ul>
</li>
<li><p>Other frequent error message:</p>
<ul>
<li><p>“SyntaxError”: DSL does not conform to the Python syntax(not the syntax defined by MindSpore Hybrid DSL), and is reported by the Python interpreter itself;</p></li>
<li><p>“ValueError: Compile error” and “The pointer[kernel_mod] is null”: the kernel compiler fails in compiling the DSL. Check error messages from AKG for further information;</p></li>
<li><p>“Launch graph failed”: The compiled kernel fails in running. Check the error message from the hardware. For example, when the kernel fails in Ascend, there will be an “Ascend error occurred” message and corresponding hareware error messages.</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../others/dimention_reduce_training.html" class="btn btn-neutral float-left" title="Dimension Reduction Training Algorithm" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../infer/inference.html" class="btn btn-neutral float-right" title="Inference Model Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>