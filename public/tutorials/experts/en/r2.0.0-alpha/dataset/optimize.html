<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimizing the Data Processing &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Process Control Statements" href="../network/control_flow.html" />
    <link rel="prev" title="Single-Node Data Cache" href="cache.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizing the Data Processing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#downloading-the-dataset">Downloading the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizing-the-data-loading-performance">Optimizing the Data Loading Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizing-the-shuffle-performance">Optimizing the Shuffle Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizing-the-data-augmentation-performance">Optimizing the Data Augmentation Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizing-the-operating-system-performance">Optimizing the Operating System Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-autotune-for-dataset-pipeline">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#enabling-heterogeneous-acceleration-for-data">Enabling Heterogeneous Acceleration for Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-optimization-solution-summary">Performance Optimization Solution Summary</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multi-thread-optimization-solution">Multi-thread Optimization Solution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-process-optimization-solution">Multi-process Optimization Solution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compose-optimization-solution">Compose Optimization Solution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operation-fusion-optimization-solution">Operation Fusion Optimization Solution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operating-system-optimization-solution">Operating System Optimization Solution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Optimizing the Data Processing</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/dataset/optimize.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="optimizing-the-data-processing">
<h1>Optimizing the Data Processing<a class="headerlink" href="#optimizing-the-data-processing" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.0.0-alpha/tutorials/experts/en/dataset/mindspore_optimize.ipynb"><img alt="Download Notebook" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_notebook_en.png" /></a> <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/tutorials/experts/source_en/dataset/optimize.ipynb"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png" /></a></p>
<p>Data is the most important part of the whole deep learning, because the quality of the data determines the upper limit of the final result, and the quality of the model is only to infinitely approach this upper limit, so high-quality data input will play a positive role in the entire deep neural network. The data in the entire process of data processing and data augmentation is like water through the pipeline, continuous flows to the training system, as shown in the figure:</p>
<p><img alt="pipeline" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/tutorials/experts/source_en/dataset/images/pipeline.png" /></p>
<p>MindSpore provides data processing and data augmentation functions for users. In the pipeline process, if each step can be properly used, the data performance will be greatly improved.</p>
<p>This section describes how to optimize performance during data loading, data processing, and data augmentation based on the CIFAR-10 dataset.</p>
<p>In addition, the storage, architecture and computing resources of the operating system will influence the performance of data processing to a certain extent.</p>
<section id="downloading-the-dataset">
<h2>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h2>
<p>Run the following command to obtain the dataset.</p>
<p>Download the CIFAR-10 binary format dataset and extract the dataset file to the <code class="docutils literal notranslate"><span class="pre">./datasets/</span></code> directory, which is used when the data is loaded.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">download</span> <span class="kn">import</span> <span class="n">download</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz&quot;</span>
<span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s2">&quot;./datasets&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;tar.gz&quot;</span><span class="p">)</span>  <span class="c1"># Download CIFAR-10 dataset</span>

<span class="n">test_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/test&quot;</span>
<span class="n">train_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="s2">&quot;test_batch.bin&quot;</span><span class="p">)):</span>
    <span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="p">,</span> <span class="n">test_path</span><span class="p">)</span>
<span class="p">[</span><span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/&quot;</span><span class="o">+</span><span class="n">i</span><span class="p">,</span> <span class="n">train_path</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/&quot;</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">i</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.html&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">i</span><span class="p">))]</span>
</pre></div>
</div>
</div>
<p>The directory structure of the decompressed dataset file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>./datasets/cifar-10-batches-bin
├── readme.html
├── test
│   └── test_batch.bin
└── train
    ├── batches.meta.txt
    ├── data_batch_1.bin
    ├── data_batch_2.bin
    ├── data_batch_3.bin
    ├── data_batch_4.bin
    └── data_batch_5.bin
</pre></div>
</div>
</section>
<section id="optimizing-the-data-loading-performance">
<h2>Optimizing the Data Loading Performance<a class="headerlink" href="#optimizing-the-data-loading-performance" title="Permalink to this headline"></a></h2>
<p>MindSpore supports loading common datasets in fields such as computer vision, natural language processing, datasets in specific formats, and user-defined datasets. The underlying implementation of different dataset loading interfaces is different, and the performance is also different, as follows:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 32%" />
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Common Dataset</p></th>
<th class="head"><p>User-defined Dataset</p></th>
<th class="head"><p>MindRecord Dataset</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Underlying implementation</p></td>
<td><p>C++</p></td>
<td><p>Python</p></td>
<td><p>C++</p></td>
</tr>
<tr class="row-odd"><td><p>Performance</p></td>
<td><p>High</p></td>
<td><p>Medium</p></td>
<td><p>High</p></td>
</tr>
</tbody>
</table>
<p>Performance Optimization Solution</p>
<p><img alt="data-loading-performance-scheme" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/tutorials/experts/source_en/dataset/images/data_loading_performance_scheme.png" /></p>
<p>Suggestions on data loading performance optimization are as follows:</p>
<ul class="simple">
<li><p>For commonly used datasets that have already provided loading interfaces, it is preferential to use the dataset loading interface provided by MindSpore to load, which can obtain better loading performance. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.dataset.html">Built-in Loading Operations</a>, if the performance cannot meet the requirements, use the multi-thread concurrency solution. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-thread-optimization-solution">Multi-thread Optimization
Solution</a>.</p></li>
<li><p>For a dataset format that is not supported, it is recommended to convert the dataset to the MindRecord data format before loading it using the <code class="docutils literal notranslate"><span class="pre">MindDataset</span></code> class (Please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/dataset/mindspore.dataset.MindDataset.html">API</a> for detailed use). For detailed contents, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r2.0.0-alpha/advanced/dataset/record.html">Converting Dataset to MindRecord</a>. If the performance
cannot meet the requirements, use the multi-thread concurrency solution, for details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-thread-optimization-solution">Multi-thread Optimization Solution</a>.</p></li>
<li><p>For dataset formats that are not supported, the user-defined <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> class is preferred for implementing fast algorithm verification (Please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/dataset/mindspore.dataset.GeneratorDataset.html">API</a> for detailed use). If the performance cannot meet the requirements, the multi-process concurrency solution can be used. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-process-optimization-solution">Multi-process Optimization
Solution</a>.</p></li>
</ul>
<p>Based on the preceding suggestions of data loading performance optimization, this experience uses the built-in load operation <code class="docutils literal notranslate"><span class="pre">Cifar10Dataset</span></code> class (Please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/dataset/mindspore.dataset.Cifar10Dataset.html">API</a> for detailed use), the <code class="docutils literal notranslate"><span class="pre">MindDataset</span></code> class after data conversion, and uses the <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> class to load data. The sample code is displayed as follows:</p>
<ol class="arabic simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">Cifar10Dataset</span></code> class of built-in operations to load the CIFAR-10 dataset in binary format. The multi-thread optimization solution is used for data loading. Four threads are enabled to concurrently complete the task. Finally, a dictionary iterator is created for the data and a data record is read through the iterator.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="n">cifar10_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>

<span class="c1"># create Cifar10Dataset for reading data</span>
<span class="n">cifar10_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">cifar10_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># create a dictionary iterator and read a data record through the iterator</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">cifar10_dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;image&#39;: Tensor(shape=[32, 32, 3], dtype=UInt8, value=
[[[209, 206, 192],
  [211, 209, 201],
  [221, 217, 213],
  ...
  [172, 175, 194],
  [169, 173, 190],
  [115, 121, 145]],
 [[226, 230, 211],
  [227, 229, 218],
  [230, 232, 221],
  ...
  [153, 153, 171],
  [156, 156, 173],
  [106, 111, 129]],
 [[214, 226, 203],
  [214, 222, 204],
  [217, 227, 206],
  ...
  [167, 166, 176],
  [147, 147, 156],
  [ 78,  84,  96]],
 ...
 [[ 40,  69,  61],
  [ 37,  63,  57],
  [ 43,  68,  66],
  ...
  [ 55,  70,  69],
  [ 40,  54,  51],
  [ 27,  44,  36]],
 [[ 33,  61,  50],
  [ 37,  65,  56],
  [ 54,  72,  74],
  ...
  [ 47,  60,  56],
  [ 58,  66,  64],
  [ 36,  50,  46]],
 [[ 29,  41,  37],
  [ 38,  60,  59],
  [ 51,  76,  81],
  ...
  [ 32,  51,  43],
  [ 47,  61,  54],
  [ 56,  67,  66]]]), &#39;label&#39;: Tensor(shape=[], dtype=UInt32, value= 5)}
</pre></div></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">Cifar10ToMR</span></code> class to convert the CIFAR-10 dataset into the MindSpore data format. In this example, the CIFAR-10 dataset in Python file format is used. Then use the <code class="docutils literal notranslate"><span class="pre">MindDataset</span></code> class to load the dataset in the MindSpore data format. The multi-thread optimization solution is used for data loading. Four threads are enabled to concurrently complete the task. Finally, a dictionary iterator is created for data and a data record is read through the iterator.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.mindrecord</span> <span class="kn">import</span> <span class="n">Cifar10ToMR</span>

<span class="n">trans_path</span> <span class="o">=</span> <span class="s2">&quot;./transform/&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">trans_path</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">trans_path</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;rm -f </span><span class="si">{}</span><span class="s2">cifar10*&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">trans_path</span><span class="p">))</span>

<span class="c1"># download CIFAR-10 python</span>
<span class="n">py_url</span> <span class="o">=</span> <span class="s2">&quot;https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-python.tar.gz&quot;</span>
<span class="n">download</span><span class="p">(</span><span class="n">py_url</span><span class="p">,</span> <span class="s2">&quot;./datasets&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;tar.gz&quot;</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">cifar10_path</span> <span class="o">=</span> <span class="s1">&#39;./datasets/cifar-10-batches-py&#39;</span>
<span class="n">cifar10_mindrecord_path</span> <span class="o">=</span> <span class="s1">&#39;./transform/cifar10.record&#39;</span>

<span class="n">cifar10_transformer</span> <span class="o">=</span> <span class="n">Cifar10ToMR</span><span class="p">(</span><span class="n">cifar10_path</span><span class="p">,</span> <span class="n">cifar10_mindrecord_path</span><span class="p">)</span>
<span class="c1"># execute transformation from CIFAR-10 to MindRecord</span>
<span class="n">cifar10_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>

<span class="c1"># create MindDataset for reading data</span>
<span class="n">cifar10_mind_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MindDataset</span><span class="p">(</span><span class="n">dataset_files</span><span class="o">=</span><span class="n">cifar10_mindrecord_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># create a dictionary iterator and read a data record through the iterator</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">cifar10_mind_dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;data&#39;: Tensor(shape=[1283], dtype=UInt8, value= [255, 216, 255, 224,   0,  16,  74,  70,  73,  70,   0,   1,   1,   0,   0,   1,   0,   1,   0,   0, 255, 219,   0,  67,
 107, 249,  17,  58, 213, 185, 117, 181, 143, 255, 217]), &#39;id&#39;: Tensor(shape=[], dtype=Int64, value= 32476), &#39;label&#39;: Tensor(shape=[], dtype=Int64, value= 9)}
</pre></div></div>
</div>
<ol class="arabic simple" start="3">
<li><p>The <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> class is used to load the user-defined dataset, and the multi-process optimization solution is used. Four processes are enabled to concurrently complete the task. Finally, a dictionary iterator is created for the data, and a data record is read through the iterator.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">generator_func</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">]),)</span>

<span class="c1"># create a GeneratorDataset object for reading data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">generator_func</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># create a dictionary iterator and read a data record through the iterator</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;data&#39;: Tensor(shape=[1], dtype=Int64, value= [0])}
</pre></div></div>
</div>
</section>
<section id="optimizing-the-shuffle-performance">
<h2>Optimizing the Shuffle Performance<a class="headerlink" href="#optimizing-the-shuffle-performance" title="Permalink to this headline"></a></h2>
<p>The shuffle operation is used to shuffle ordered datasets or repeated datasets. MindSpore provides the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> function for users. A larger value of <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code> indicates a higher shuffling degree, consuming more time and computing resources. This API allows users to shuffle the data at any time during the entire pipeline process. For the detailed contents, refer to <a class="reference external" href="https://mindspore.cn/tutorials/zh-CN/r2.0.0-alpha/beginner/dataset.html#shuffle">shuffle processing</a>. Because the
underlying implementation methods are different, the performance of this method is not as good as that of setting the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> parameter to directly shuffle data by referring to the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.dataset.html">Built-in Loading Operations</a>.</p>
<p>Performance Optimization Solution</p>
<p><img alt="shuffle-performance-scheme" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/tutorials/experts/source_en/dataset/images/shuffle_performance_scheme.png" /></p>
<p>Suggestions on shuffle performance optimization are as follows:</p>
<ul class="simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> parameter of built-in loading operations to shuffle data.</p></li>
<li><p>If the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> function is used and the performance still cannot meet the requirements, adjust the value of the <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code> parameter to improve the performance.</p></li>
</ul>
<p>Based on the preceding shuffle performance optimization suggestions, the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">Cifar10Dataset</span></code> class of built-in loading operations and the <code class="docutils literal notranslate"><span class="pre">Shuffle</span></code> function are used to shuffle data. The sample code is displayed as follows:</p>
<ol class="arabic simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">Cifar10Dataset</span></code> class of built-in operations to load the CIFAR-10 dataset. In this example, the CIFAR-10 dataset in binary format is used, and the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> parameter is set to True to perform data shuffle. Finally, a dictionary iterator is created for the data and a data record is read through the iterator.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cifar10_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>

<span class="c1"># create Cifar10Dataset for reading data</span>
<span class="n">cifar10_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">cifar10_path</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># create a dictionary iterator and read a data record through the iterator</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">cifar10_dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;image&#39;: Tensor(shape=[32, 32, 3], dtype=UInt8, value=
[[[119, 193, 196],
  [121, 192, 204],
  [123, 193, 209],
  ...
  [110, 168, 177],
  [109, 167, 176],
  [110, 168, 178]],
 [[110, 188, 199],
  [109, 185, 202],
  [111, 186, 204],
  ...
  [107, 173, 179],
  [107, 173, 179],
  [109, 175, 182]],
 [[110, 186, 200],
  [108, 183, 199],
  [110, 184, 199],
  ...
  [115, 183, 189],
  [117, 185, 190],
  [117, 185, 191]],
 ...
 [[210, 253, 250],
  [212, 251, 250],
  [214, 250, 249],
  ...
  [194, 247, 247],
  [190, 246, 245],
  [184, 245, 244]],
 [[215, 253, 251],
  [218, 252, 250],
  [220, 251, 249],
  ...
  [200, 248, 248],
  [195, 247, 245],
  [189, 245, 244]],
 [[216, 253, 253],
  [222, 251, 250],
  [225, 250, 249],
  ...
  [204, 249, 248],
  [200, 246, 244],
  [196, 245, 244]]]), &#39;label&#39;: Tensor(shape=[], dtype=UInt32, value= 0)}
</pre></div></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> function to shuffle data. Set <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code> to 3 and use the <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> class to generate data.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]),)</span>

<span class="n">ds1</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">generator_func</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;before shuffle:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">ds1</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="n">ds2</span> <span class="o">=</span> <span class="n">ds1</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;after shuffle:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">ds2</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
before shuffle:
[0 1 2 3 4]
[1 2 3 4 5]
[2 3 4 5 6]
[3 4 5 6 7]
[4 5 6 7 8]
after shuffle:
[2 3 4 5 6]
[0 1 2 3 4]
[1 2 3 4 5]
[4 5 6 7 8]
[3 4 5 6 7]
</pre></div></div>
</div>
</section>
<section id="optimizing-the-data-augmentation-performance">
<h2>Optimizing the Data Augmentation Performance<a class="headerlink" href="#optimizing-the-data-augmentation-performance" title="Permalink to this headline"></a></h2>
<p>During image classification training, especially when the dataset is small, users can use data augmentation to preprocess images to enrich the dataset. MindSpore provides multiple data augmentation methods, including:</p>
<ul class="simple">
<li><p>Use data augmentation operations implemented in C++ (mainly based on OpenCV).</p></li>
<li><p>Use data augmentation operations implemented in Python (mainly based on Pillow).</p></li>
<li><p>Users can define Python functions as needed to perform data augmentation.</p></li>
</ul>
<p>Please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/augment.html">Data Augmentation</a>. The performance varies according to the underlying implementation methods. This is shown below:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 21%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Programming Language</p></th>
<th class="head"><p>Third Party Library</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>C++</p></td>
<td><p>OpenCV</p></td>
<td><p>Implemented in C++ code which has higher performance</p></td>
</tr>
<tr class="row-odd"><td><p>Python</p></td>
<td><p>Pillow</p></td>
<td><p>Implemented in Python code which is more flexible</p></td>
</tr>
</tbody>
</table>
<p>Performance Optimization Solution</p>
<p><img alt="data-enhancement-performance-scheme" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/tutorials/experts/source_en/dataset/images/data_enhancement_performance_scheme.png" /></p>
<p>Suggestions on data augmentation performance optimization are as follows:</p>
<ul class="simple">
<li><p>The C++ implemented operations are preferentially used to perform data augmentation for its higher performance. If the performance cannot meet the requirements, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-thread-optimization-solution">Multi-thread Optimization Solution</a>, <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#compose-optimization-solution">Compose Optimization Solution</a>, or <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#operation-fusion-optimization-solution">Operation Fusion
Optimization Solution</a>.</p></li>
<li><p>If the Python implemented operations are used to perform data augmentation and the performance still cannot meet the requirements, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-thread-optimization-solution">Multi-thread Optimization Solution</a>, <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-process-optimization-solution">Multi-process Optimization Solution</a>, <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#compose-optimization-solution">Compose Optimization
Solution</a>, or <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#operation-fusion-optimization-solution">Operation Fusion Optimization Solution</a>.</p></li>
<li><p>If the user-defined Python functions are used to perform data augmentation and the performance still cannot meet the requirements, use the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-thread-optimization-solution">Multi-thread Optimization Solution</a> or <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/optimize.html#multi-process-optimization-solution">Multi-process Optimization Solution</a>. If the performance still cannot be
improved, in this case, optimize the user-defined Python code.</p></li>
</ul>
<p>MindSpore also supports users to use the data augmentation operations implemented in C++ and Python at the same time, but due to the different underlying implementations of the two, excessive mixing will increase resource overhead and reduce processing performance. It is recommended that users should use the operations implemented in the same language, or use one of them first, then use the other. Please do not switch frequently between the data augmentation operations of two different
implementation languages.</p>
<p>Based on the preceding suggestions of data augmentation performance optimization, the C++ implemented operations and user-defined Python functions are used to perform data augmentation. The code is displayed as follows:</p>
<ol class="arabic simple">
<li><p>The C++ implemented operations are used to perform data augmentation. During data augmentation, the multi-thread optimization solution is used. Four threads are enabled to concurrently complete the task. The operation fusion optimization solution is used and the <code class="docutils literal notranslate"><span class="pre">RandomResizedCrop</span></code> fusion class is used to replace the <code class="docutils literal notranslate"><span class="pre">RandomResize</span></code> and <code class="docutils literal notranslate"><span class="pre">RandomCrop</span></code> classes.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cifar10_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>

<span class="c1"># create Cifar10Dataset for reading data</span>
<span class="n">cifar10_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">cifar10_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">transforms</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">((</span><span class="mi">800</span><span class="p">,</span> <span class="mi">800</span><span class="p">))</span>
<span class="c1"># apply the transform to the dataset through dataset.map()</span>
<span class="n">cifar10_dataset</span> <span class="o">=</span> <span class="n">cifar10_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">transforms</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">cifar10_dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/dataset_optimize_23_0.png" src="../_images/dataset_optimize_23_0.png" />
</div>
</div>
<ol class="arabic simple" start="2">
<li><p>A user-defined Python function is used to perform data augmentation. During data augmentation, the multi-process optimization solution is used, and four processes are enabled to concurrently complete the task.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]),)</span>

<span class="n">ds3</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">generator_func</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;before map:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">ds3</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">ds4</span> <span class="o">=</span> <span class="n">ds3</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">python_multiprocessing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;after map:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">ds4</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
before map:
[0 1 2 3 4]
[1 2 3 4 5]
[2 3 4 5 6]
[3 4 5 6 7]
[4 5 6 7 8]
after map:
[ 0  1  4  9 16]
[ 1  4  9 16 25]
[ 4  9 16 25 36]
[ 9 16 25 36 49]
[16 25 36 49 64]
</pre></div></div>
</div>
</section>
<section id="optimizing-the-operating-system-performance">
<h2>Optimizing the Operating System Performance<a class="headerlink" href="#optimizing-the-operating-system-performance" title="Permalink to this headline"></a></h2>
<p>Data processing is performed on the Host. Therefore, configurations of the running environment may affect the processing performance. Major factors include storage, NUMA architecture, and CPU (computing resources).</p>
<ol class="arabic simple">
<li><p>Storage</p></li>
</ol>
<p>The data loading process involves frequent disk operations, and the performance of disk reading and writing directly affects the speed of data loading. Solid State Drive (SSD) is recommended for storing large datasets when the dataset is large. SSDs generally have higher read and write speeds than ordinary disks, reducing the impact of I/O operations on data processing performance.</p>
<p>In general, the loaded data will be cached into the operating system’s page cache, which reduces the overhead of subsequent reads to a certain extent and accelerates the data loading speed of subsequent Epochs. Users can also manually cache the augmented data through the single-node caching technology provided by MindSpore, avoiding duplicate data loading and data augmentation.</p>
<ol class="arabic simple" start="2">
<li><p>NUMA architecture</p></li>
</ol>
<p>NUMA, Non-Uniform Memory Access, is a memory architecture that was born to solve the scalability problem in the traditional symmetric multiprocessor (SMP) architecture. In traditional architectures, multiple processors share a memory bus, which is prone to problems such as insufficient bandwidth and memory conflicts.</p>
<p>In the NUMA architecture, processors and memory are divided into groups, each called a node, each node has a separate integrated memory controller (IMC) bus for intra-node communication, and different nodes communicate with each other through a fast path interconnect (QPI). For a node, memory within the same node is called local memory, while memory in other nodes is called external memory. The delay in accessing local memory will be less than the delay in accessing external memory.</p>
<p>During data processing, you can reduce the latency of memory access by binding the process to the node. In general, we can use the following command to bind the process to the node node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>numactl<span class="w"> </span>--cpubind<span class="o">=</span><span class="m">0</span><span class="w"> </span>--membind<span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>CPU (computing resource)</p></li>
</ol>
<p>Although the data processing speed can be accelerated through multi-threaded parallel technology, there is actually no guarantee that CPU computing resources will be fully utilized. If you can artificially complete the configuration of computing resources in advance, it will be able to improve the utilization of CPU computing resources to a certain extent.</p>
<ul class="simple">
<li><p>Resource allocation</p></li>
</ul>
<p>In distributed training, multiple training processes are run on one device. These training processes allocate and compete for computing resources based on the policy of the operating system. When there is a large number of processes, data processing performance may deteriorate due to resource contention. In some cases, users need to manually allocate resources to avoid resource contention.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>numactl<span class="w"> </span>--cpubind<span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>train.py
</pre></div>
</div>
<ul class="simple">
<li><p>CPU frequency</p></li>
</ul>
<p>For energy efficiency reasons, the operating system adjusts the CPU operating frequency as needed, but lower power consumption means that computing performance is degraded and data processing is slowed down. In order to get the most out of the CPU’s maximum computing power, you need to manually set the CPU’s operating frequency. If it is found that the CPU operation mode of the operating system is balanced mode or energy-saving mode, you can improve the performance of data processing by
adjusting it to performance mode.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cpupower<span class="w"> </span>frequency-set<span class="w"> </span>-g<span class="w"> </span>performance
</pre></div>
</div>
</section>
<section id="dataset-autotune-for-dataset-pipeline">
<h2>Dataset AutoTune for Dataset Pipeline<a class="headerlink" href="#dataset-autotune-for-dataset-pipeline" title="Permalink to this headline"></a></h2>
<p>MindSpore provides a tool named Dataset AutoTune for optimizing dataset.The Dataset AutoTune can automatically tune Dataset pipelines to improve performance. The detailed usage please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a>.</p>
</section>
<section id="enabling-heterogeneous-acceleration-for-data">
<h2>Enabling Heterogeneous Acceleration for Data<a class="headerlink" href="#enabling-heterogeneous-acceleration-for-data" title="Permalink to this headline"></a></h2>
<p>MindSpore provides a computing load balancing technology which can distribute the MindSpore Tensor computing to different heterogeneous hardware. On one hand, it balances the computing overhead between different hardware, on the other, it uses the advantages of heterogeneous hardware to accelerate the computing. The detailed usage please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/dataset/dataset_offload.html">Enabling Heterogeneous Acceleration for Data</a>.</p>
</section>
<section id="performance-optimization-solution-summary">
<h2>Performance Optimization Solution Summary<a class="headerlink" href="#performance-optimization-solution-summary" title="Permalink to this headline"></a></h2>
<section id="multi-thread-optimization-solution">
<h3>Multi-thread Optimization Solution<a class="headerlink" href="#multi-thread-optimization-solution" title="Permalink to this headline"></a></h3>
<p>During the data pipeline process, the number of threads for related operations can be set to improve the concurrency and performance. If the user does not manually specify the num_parallel_workers parameter, each data processing operation will use 8 sub-threads for concurrent processing by default. For example:</p>
<ul class="simple">
<li><p>During data loading, the <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter in the built-in data loading class is used to set the number of threads.</p></li>
<li><p>During data augmentation, the <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">map</span></code> function is used to set the number of threads.</p></li>
<li><p>During batch processing, the <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">batch</span></code> function is used to set the number of threads.</p></li>
</ul>
<p>For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.dataset.html">Built-in Loading Operations</a>. When using MindSpore for standalone or distributed training, the setting of the num_parallel_workers parameter should follow the following principles:</p>
<ul class="simple">
<li><p>The summary of the num_parallel_workers parameter set for each data loading and processing operation should not be greater than the maximum number of CPU cores of the machine, otherwise it will cause resource competition between each operation.</p></li>
<li><p>Before setting the num_parallel_workers parameter, it is recommended to use MindSpore’s Profiler (performance analysis) tool to analyze the performance of each operation in the training, and allocate more resources to the operation with pool performance, that is, set a large num_parallel_workers to balance the throughput between various operations and avoid unnecessary waiting.</p></li>
<li><p>In a standalone training scenario, increasing the num_parallel_workers parameter can often directly improve processing performance, but in a distributed scenario, due to increased CPU competition, blindly increasing num_parallel_workers may lead to performance degradation. You need to try to use a compromise value.</p></li>
</ul>
</section>
<section id="multi-process-optimization-solution">
<h3>Multi-process Optimization Solution<a class="headerlink" href="#multi-process-optimization-solution" title="Permalink to this headline"></a></h3>
<p>During data processing, operations implemented by Python support the multi-process mode. For example:</p>
<ul class="simple">
<li><p>By default, the <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> class is in multi-process mode. The <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter indicates the number of enabled processes. The default value is 1. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/dataset/mindspore.dataset.GeneratorDataset.html">GeneratorDataset</a>.</p></li>
<li><p>If the user-defined Python functions or the Python implemennted operations are used to perform data augmentation and the <code class="docutils literal notranslate"><span class="pre">python_multiprocessing</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">map</span></code> function is set to True, the <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter indicates the number of processes and the default value of the <code class="docutils literal notranslate"><span class="pre">python_multiprocessing</span></code> parameter is False. In this case, the <code class="docutils literal notranslate"><span class="pre">num_parallel_workers</span></code> parameter indicates the number of threads. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.dataset.html">Built-in Loading
Operations</a>.</p></li>
</ul>
</section>
<section id="compose-optimization-solution">
<h3>Compose Optimization Solution<a class="headerlink" href="#compose-optimization-solution" title="Permalink to this headline"></a></h3>
<p>Map operation can receive the Tensor operation list and apply all these operations based on a specific sequence. Compared with the Map operation used by each Tensor operation, such “Fat Map operation” can achieve better performance, as shown in the following figure:</p>
<p><img alt="compose" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/tutorials/experts/source_en/dataset/images/compose.png" /></p>
</section>
<section id="operation-fusion-optimization-solution">
<h3>Operation Fusion Optimization Solution<a class="headerlink" href="#operation-fusion-optimization-solution" title="Permalink to this headline"></a></h3>
<p>Some fusion operations are provided to aggregate the functions of two or more operations into one operation. You can configure the environment variable <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OPTIMIZE=true</span></code> to make it effective. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.dataset.transforms.html#module-mindspore.dataset.vision">Augmentation Operations</a>. Compared with the pipelines of their components, such fusion operations provide better performance. As shown in the figure:</p>
<p><img alt="operation-fusion" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/tutorials/experts/source_en/dataset/images/operation_fusion.png" /></p>
</section>
<section id="operating-system-optimization-solution">
<h3>Operating System Optimization Solution<a class="headerlink" href="#operating-system-optimization-solution" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Use Solid State Drives to store the data.</p></li>
<li><p>Bind the process to a NUMA node.</p>
<p>In the multi card training scenario, each training process can be bound to different NUMA nodes by configuring environment variables <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">DATASET_ENABLE_NUMA=True</span></code> to ensure more stable data processing of different training processes.</p>
</li>
<li><p>Manually allocate more computing resources.</p></li>
<li><p>Set a higher CPU frequency.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cache.html" class="btn btn-neutral float-left" title="Single-Node Data Cache" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../network/control_flow.html" class="btn btn-neutral float-right" title="Process Control Statements" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>