

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to Second-order Optimizer THOR &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Applying Second-Order Optimization Practices on the ResNet-50 Network" href="resnet50.html" />
    <link rel="prev" title="Second-order Optimization" href="../thor.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../thor.html">Second-order Optimization</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Introduction to Second-order Optimizer THOR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction-to-background-of-the-optimizers">Introduction to Background of the Optimizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#first-order-optimizers">First-order Optimizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#second-order-optimizers">Second-order Optimizers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#introduction-to-thor">Introduction to THOR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reducing-the-frequency-of-second-order-information-matrix-updates">Reducing the Frequency of Second-order Information Matrix Updates</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hardware-awareness-matrix-slicing">Hardware Awareness Matrix Slicing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="resnet50.html">Applying Second-Order Optimization Practices on the ResNet-50 Network</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/other_features.html">Other Features</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../thor.html">Second-order Optimization</a> &raquo;</li>
        
      <li>Introduction to Second-order Optimizer THOR</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/optimize/thor/intro.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="introduction-to-second-order-optimizer-thor">
<h1>Introduction to Second-order Optimizer THOR<a class="headerlink" href="#introduction-to-second-order-optimizer-thor" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/tutorials/experts/source_en/optimize/thor/intro.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<p>The deep learning training process can be viewed as a loss function loss value decreasing process, and the right optimizer can make deep learning training time significantly reduced. Optimizers can be divided into first-order optimizers and second-order optimizers. The industry is still the mainstream use of the first-order optimizers, while the second-order optimizers is widely used because the single-step training time is too long. In recent years, there have been theoretical breakthroughs in applying the second-order optimization to deep learning training, and good results have been achieved.</p>
<p>This article will introduce the background of the optimizers, and second-order optimizer THOR self-developed by the MindSpore team.</p>
<div class="section" id="introduction-to-background-of-the-optimizers">
<h2>Introduction to Background of the Optimizers<a class="headerlink" href="#introduction-to-background-of-the-optimizers" title="Permalink to this headline">¶</a></h2>
<p>Suppose the training sample data set: <span class="math notranslate nohighlight">\(D = {(x_1,y_1),... ,(x_i,y_i),... ,(x_N,y_N)},x_i \in X,y_i\in Y\)</span>, the deep neural network model with parameter θ formulation is: <span class="math notranslate nohighlight">\(\hat{y} = f(x;\theta),x\in{X}\)</span>, the loss function defined between the model output and the true label y is: <span class="math notranslate nohighlight">\(L(y,\hat y),y \in Y\)</span>, the process of network parameter learning is the minimization the loss function: <span class="math notranslate nohighlight">\(\min\limits_{\theta}L(y,\hat{y})\)</span>. Given the dataset, model, and loss function, the deep learning training problem boils down to the optimization problem. The deep neural network training optimization problem has a huge parameter scale and requires a large amount of computation, making it difficult to compute an analytic solution. Therefore, the process is often compared to descending a mountain. As shown in Figure 1, how can a person find the fastest path down a mountain with limited sight distance while standing at the top?</p>
<p><img alt="The process of deeplearning training" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/optimize/thor/images/deeplearning_train_process.png" /></p>
<p><em>Figure 1 Simulation of Deep Learning Training Process</em></p>
<p>The optimizer is doing this, and the optimization algorithms in the industry can be divided into first-order optimization algorithms and second-order optimization algorithms. The following is a brief description of optimizers in the industry.</p>
<div class="section" id="first-order-optimizers">
<h3>First-order Optimizers<a class="headerlink" href="#first-order-optimizers" title="Permalink to this headline">¶</a></h3>
<p>Gradient Descent (GD) is the most classic first-order optimization algorithm in machine learning and the most commonly used optimization algorithm among many machine learning algorithms. The following rule is used for updating parameters in common first-order optimization algorithms (e.g., SGD algorithm): <span class="math notranslate nohighlight">\(\theta = \theta - \eta \nabla L_\theta\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter to be updated, <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate, and <span class="math notranslate nohighlight">\(\nabla L_\theta\)</span> is the gradient of the loss function with respect to the parameter.</p>
<p>But the mainstream stochastic gradient descent methods have the following problems: Too small a learning rate will cause the network to converge too slowly, too high a learning rate may affect convergence and cause the loss function to fluctuate on the minimum or even diverge, which is more sensitive to the parameters, and it is easy to converge to the local optimum and difficult to jump out of the saddle point.</p>
<p>Therefore, many improved algorithms for stochastic gradient descent methods have been proposed in the industry, such as Momentum, Nesterov, AdaGrad, RMSprop, Adadelta, and Adam. These improved optimization algorithms can adaptively update the step size by using the historical information of the stochastic gradient, making them easier to tune the reference and convenient to use.</p>
</div>
<div class="section" id="second-order-optimizers">
<h3>Second-order Optimizers<a class="headerlink" href="#second-order-optimizers" title="Permalink to this headline">¶</a></h3>
<p>The second-order optimization algorithm uses the second-order derivative of the objective function for curvature correction to accelerate the first-order gradient descent. Compared with the first-order optimizer, its convergence is faster, highly approximate the optimal value. Geometrically the descent path is more consistent with the real optimal descent path.</p>
<p>For example, the Newton method of second-order optimization algorithms is to fit a local surface at your current location with a quadratic surface, while the gradient descent method uses a plane to fit the current local surface. Usually, the quadratic surface will be better fitted than the plane, so the descent path chosen by the Newton method will be more consistent with the true optimal descent path. As shown in Figure 2, the left descent path indicates the descent curve of Newton method, and the right indicates the descent curve of the first-order gradient. The second-order algorithm can go to the destination faster than the first-order algorithm, thus accelerating the convergence.</p>
<p><img alt="The different process of deeplearning training" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/optimize/thor/images/different_train_process.png" /></p>
<p><em>Figure 2 Descent Path of Different Optimizers</em></p>
<p>Mathematically, in contrast to the first-order optimization algorithm, the second-order optimization algorithm starts by multiplying <span class="math notranslate nohighlight">\(\nabla L_{\theta}\)</span> with a matrix <span class="math notranslate nohighlight">\(G^{-1}\)</span> to produce the following update rule: <span class="math notranslate nohighlight">\(\theta = \theta - \eta G^{-1}\nabla L_{\theta}\)</span>, where G is the second-order information matrix. The definition of G in different second-order optimization algorithms is different. The common second-order optimization algorithms are Newton method, natural gradient method, etc., which correspond to the second-order information matrix G as Hessian matrix, Fisher matrix, respectively.</p>
<p>Newton method has a very good local convergence property. When the function L satisfies <span class="math notranslate nohighlight">\(\nabla L_{\theta^{*}}=0,\nabla^{2} L_{\theta^{*}}\)</span> is a positive definite matrix at the optimal value point <span class="math notranslate nohighlight">\(\theta^{*}\)</span> point, and when the Hessian matrix is Lipschitz continuous near the extreme value point, the Newton method converges quadratically to the optimal value point. The Hessian matrix is a square matrix consisting of all second-order partial derivatives of a multivariate real-valued function. The Hessian matrix can be expressed as <span class="math notranslate nohighlight">\(H_{ij} = \frac{\partial^2L}{\partial \theta_i \partial \theta_j}\)</span>, where L is the loss function and <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter to be updated.</p>
<p>In SGD, Euclidean distance is used for both parameter space and function space metrics, but Euclidean distance cannot be used as an accurate distance metric for function space in some cases. For example, in neural networks, the change in the objective function due to the parameters is a probabilistic change, which does not fit in the Euclidean space metric, and it is not a reasonable characterization of probabilistic property changes. KL scatter is a reasonable measure of the distance between distributions. When using KL divergence as a reasonable measure of the distance between distributions. In this case, the gradient used in the parameter update is the natural gradient. The Fisher matrix in the natural gradient method can be expressed as: <span class="math notranslate nohighlight">\(F=\mathrm{E}[\frac{\partial \mathrm {log} p(y|x,\theta)}{\partial \theta}{\frac{\partial \mathrm {log} p(y|x,\theta)}{\ partial \theta}}^T]\)</span>, where P(y|x,θ) is the predictive distribution of the network model, p(y|x,θ) is its probability density, and θ is the parameter needed for the network model.</p>
<p>Although the second-order optimization algorithm converges quickly, the time complexity of computing the inverse of the second-order matrix is <span class="math notranslate nohighlight">\(\mathrm O(n^3)\)</span>. When the number of model parameters is <span class="math notranslate nohighlight">\(\n_\theta\)</span>, the size of the corresponding second-order information matrix is <span class="math notranslate nohighlight">\(\n_\theta \times n_\theta\)</span>. In deep learning models, <span class="math notranslate nohighlight">\(n_\theta\)</span> is often in the order of millions, and at this time the inverse of the second-order information matrix cannot be computed. Therefore, how to reduce the computational complexity of the inverse of the second-order information matrix becomes a key issue. Next, we introduce the second-order optimizer in deep learning.</p>
</div>
</div>
<div class="section" id="introduction-to-thor">
<h2>Introduction to THOR<a class="headerlink" href="#introduction-to-thor" title="Permalink to this headline">¶</a></h2>
<p>The second-order optimization algorithms currently available in the industry are computationally intensive and have no obvious advantages over first-order or are used in the simple scenarios. MindSpore proposes a self-developed algorithm <a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/16867">THOR (Trace-based Hardware-driven layer-ORiented Natural Gradient Descent Computation)</a> accepted by AAAI. THOR has significant gains in several scenarios, such as convergence speed in both BERT and ResNet50. THOR has made two main innovation points:</p>
<div class="section" id="reducing-the-frequency-of-second-order-information-matrix-updates">
<h3>Reducing the Frequency of Second-order Information Matrix Updates<a class="headerlink" href="#reducing-the-frequency-of-second-order-information-matrix-updates" title="Permalink to this headline">¶</a></h3>
<p>By experimentally observing that the F-parameter (Frobenius norm) of the Fisher matrix changes drastically in the early stage and gradually becomes stable in the later stage, it is assumed that <span class="math notranslate nohighlight">\(\Big\{{F^k}\Big\}^{n}_{k=1}\)</span> is a Markov process that converges to a steady-state distribution π, where <span class="math notranslate nohighlight">\(F^k\)</span> represents the Fisher matrix at the kth iteration. Therefore, gradually increasing the update interval of the Fisher matrix during the training process can reduce the training time without affecting the convergence speed. For example, in ResNet50, the number of update interval steps gets larger and larger as the training proceeds, to the point where only one update of the second-order information matrix is required per epoch in the later stages.</p>
<p>Inspired by KFAC, THOR decouples Fisher matrices by layer to reduce matrix complexity, performs experiments for each layer of Fisher matrices separately. It can be found that some layers of Fisher matrices converge to steady state faster, so the update frequency of each layer is adjusted more fine-grained on a uniform update interval. THOR uses the trace of the matrix as a judgment condition. When the change of the trace is greater than a certain threshold, the second-order information matrix of the layer is updated, otherwise the second-order information matrix of the previous iteration is used, and a stop update mechanism is introduced. Stop updating the second-order information matrix of the layer when the amount of change in the trace is less than a certain threshold. The specific update formula is as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
update\ F^{k}_{i} , \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\ \ if \ \Delta^{k} \in (\omega_{1},+\infty)\\
do\ not\ update\ F^{k}_{i}\ and\ set \  F^{k}_{i}=F^{k-1}_{i}, \ \quad\qquad\qquad\qquad\quad if \ \Delta^{k} \in [\omega_{2},\omega_{1}]\\
stop\ update\ F^{k}_{i}\ and\ set \  F^{k+t}_{t}\equiv F^{k-1}_{i}\ for\ all\ t=1,2,...\quad if \ \Delta^{k} \in [0,\omega_{2})
\end{cases}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\Delta^k=\frac{||tr(F^k_i+\lambda I)|-|tr(F^{k-1}_i+\lambda I)||}{|tr(F^{k-1}_i+\lambda I)|}\]</div>
</div>
<div class="section" id="hardware-awareness-matrix-slicing">
<h3>Hardware Awareness Matrix Slicing<a class="headerlink" href="#hardware-awareness-matrix-slicing" title="Permalink to this headline">¶</a></h3>
<p>THOR further assumes that the input and output blocks in each network layer are also independent of each other, based on the decoupling of Fisher matrices by layer. For example, the input and output of each layer of the network is sliced into n blocks, which are independent of each other, and the second-order information matrix is further sliced according to this assumption, thus improving the computational efficiency. THOR combines matrix information loss data and matrix performance data to determine the matrix tiling dimension, thus greatly improving the Fisher matrix inversion time.</p>
<p>So how can we determine the matrix tiling dimension? The specific method is:</p>
<ol>
<li><p>Determine the matrix slice dimension based on the layer with the largest dimension in the Fisher matrix. Taking ResNet50 as an example, the maximum dimension in the network layer is 2048, and the matrix slice dimensions are determined as [1,16,32,64,128,256,512,1024,2048].</p></li>
<li><p>Based on the determined matrix dimensions, the matrix loss under each dimension is calculated according to the spectral norm by the following equation:</p>
<div class="math notranslate nohighlight">
\[L=1-\sqrt{\frac{\lambda_{max}\ \ (\hat{A}\hat{A}^T)}{\lambda_{max}\ \ (AA^T)}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_{max}(X)\)</span> denotes the maximum feature value of the matrix <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(A\)</span> denotes the original unpartitioned matrix, and <span class="math notranslate nohighlight">\(\hat A\)</span> denotes the partitioned matrix. Then the number of matrices with losses less than 1% in that dimension is counted, and finally the normalized matrix loss information is obtained by dividing by the total number of matrices.</p>
</li>
<li><p>According to the determined matrix dimensions, the matrix inversion time under each dimension is calculated, and then the normalized performance data under each dimension is obtained by the formula <span class="math notranslate nohighlight">\(normalized_n = \frac{p_1}{p_n}\)</span>, where <span class="math notranslate nohighlight">\(p_1\)</span> denotes the performance data of the matrix with the smallest dimension and <span class="math notranslate nohighlight">\(p_n\)</span> denotes the performance data under the nth dimension.</p></li>
<li><p>Based on the annotated matrix loss information and the normalized performance data graph, taking the ResNet50 as example, shown in Figure 3, the falling curve in the figure is the performance curve, and the rising curve indicates the matrix loss curve. The intersection point in the figure is 106, which is closest to 128, and finally the matrix slice dimension is determined to be 128.</p></li>
</ol>
<p><img alt="The split dimension of matrix" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/optimize/thor/images/split_dimension.png" /></p>
<p><em>Figure 3 Schematic Diagram of Slice Dimension Determination</em></p>
</div>
<div class="section" id="results">
<h3>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h3>
<p>Figure 4 shows the training line graph of THOR on ResNet50+ImageNet with a batchsize of 256 on first and second order, where train loss denotes training error, test accuracy denotes testing accuracy, epoch denotes the number of iterations, and wall-clock time denotes the time. The faster falling curve and the faster rising curve are the curves of this algorithm, and the other curve with more obvious gap is the training curve of momentum.</p>
<p><img alt="The result of ResNet50" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/optimize/thor/images/thor_in_resnet.png" /></p>
<p><em>Figure 4 Results of THOR on ResNet50</em></p>
<p>THOR, THOR_stop, and THOR_NT in Figure 4 indicate (<span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>)=(0.01,0), (<span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>)=(0.01,0.001), and (<span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>)=(0,0), respectively. From the figure, we can see that the number of iterations required for THOR convergence is about half of the first order, and the time of the single-step is not significantly different from that of the first order. Compared to the first-order algorithm that takes 117 min, the second-order optimizer speeds up the device-to-device time by about 40%.</p>
<p>THOR also tested the convergence results of ResNet50+ImageNet at different batchsize, and the results are shown in Figure 5 below, where Hardware denotes the hardware platform, Software is the used deep learning framework, Batch size is the number of images per training, Optimizer denotes the used optimizer, Time refers to the overall training time, and Accuracy is the final convergence accuracy. When the batchsize is 8192 and 256 Ascend 910 blocks are used, it only takes 2.7 minutes for the accuracy to converge to 75.9%.</p>
<p><img alt="The large batchsize result of ResNet50" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/optimize/thor/images/thor_largebs_in_resnet.png" /></p>
<p><em>Figure 5 Results of THOR on ResNet50 at Large Batchsize</em></p>
<p>In BERT+WIkipedia, THOR also has a good performance effect. Taking the MLPerf as a standard, the accuracy reaches to 71.2%, and an end-to-end improvement of 30% is implemented compared to the first order. The results are shown in Figure 6. The horizontal coordinate in the figure indicates the training time, and the vertical coordinate indicates the test accuracy. The curve that rises faster is the training curve of THOR, and the other one is the training curve of Lamb.</p>
<p><img alt="The result of BERT" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/optimize/thor/images/thor_in_bert.png" /></p>
<p><em>Figure 6 Results of THOR on BERT</em></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="resnet50.html" class="btn btn-neutral float-right" title="Applying Second-Order Optimization Practices on the ResNet-50 Network" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../thor.html" class="btn btn-neutral float-left" title="Second-order Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>