

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed Training Communication Fusion &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dataset Slicing" href="dataset_slice.html" />
    <link rel="prev" title="Parameter Server Mode" href="parameter_server_training.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="other_features.html">Other Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="sharding_propagation.html">Sharding Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2"><a class="reference internal" href="pynative_shard_function_parallel.html">Functional Operator Sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="ms_operator.html">Performing Distributed Training on K8S Clusters</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="other_features.html">Other Features</a> &raquo;</li>
      <li>Distributed Training Communication Fusion</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/comm_fusion.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-training-communication-fusion">
<h1>Distributed Training Communication Fusion<a class="headerlink" href="#distributed-training-communication-fusion" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/tutorials/experts/source_en/parallel/comm_fusion.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In distributed parallel training scenarios to train large-scale parameter models (e.g., GPT-3, Pangu-<span class="math notranslate nohighlight">\(\alpha\)</span>), data transmission of cross-device or even cross-node is a bottleneck that limits scalability as well as operator power utilization [1]. Communication fusion is an important method to improve network resource utilization and accelerate data transmission efficiency by encapsulating the communication operator of the same source and destination nodes for simultaneous execution to avoid the extra overhead caused by multiple single operator executions.</p>
<p>MindSpore supports the fusion of three common communication operators (<code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>, <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> and <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>) in distributed training, and provides a simple and easy-to-use interface for user configuration. The communication fusion plays an important role in the long and steady training mission support.</p>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>This section firstly introduces the relationship between computation and communication in distributed training with the example of data parallelism, and secondly introduces the necessity of communication fusion in distributed training scenarios.</p>
<section id="computation-and-communication-in-distributed-training">
<h3>Computation and Communication in Distributed Training<a class="headerlink" href="#computation-and-communication-in-distributed-training" title="Permalink to this headline"></a></h3>
<p>The whole process of distributed training can be roughly divided into two processes: local model computation and cross-device network data interaction.</p>
<p>The following is an example of data parallelism [2] to introduce the overall training process. For other parallel approaches, such as model parallelism [3], pipeline parallelism [4], please refer to related papers.</p>
<p>As shown in the figure below, each node backs up the complete neural network model and uses the local dataset partition to train a mini-batch for forward and backward computation. The gradient obtained from the backward computation is synchronized across the nodes, and the training of the next mini-batch continues after synchronization, and so on, until the accuracy/loss reaches a threshold, or a certain number of epochs are trained. It can be seen that computation and communication alternate in the distributed training process. Work has been done on how to do pipelining of interdependent computation and transmission to reduce the percentage of cross-node data synchronization in the overall training duration [5-6], which will not be repeated here.</p>
<p><img alt="image" src="https://gitee.com/mindspore/docs/raw/r2.0.0-alpha/tutorials/experts/source_zh_cn/parallel/images/data_parallel.png" /></p>
</section>
<section id="the-necessity-of-communication-fusion">
<h3>The Necessity of Communication Fusion<a class="headerlink" href="#the-necessity-of-communication-fusion" title="Permalink to this headline"></a></h3>
<p>The time overhead of network communication can be measured by the following equation, where <span class="math notranslate nohighlight">\(m\)</span> is the size of the data transmission, <span class="math notranslate nohighlight">\(\alpha\)</span> is the network transmission rate, and <span class="math notranslate nohighlight">\(\beta\)</span> is the inherent overhead of network startup. As can be seen, when the number of transmitted messages becomes larger, the inherent overhead share of network shartup will decrease, transmitting small messages does not make efficient use of network bandwidth resources. Even communication primitives in the HPC domain, such as <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> and <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, follow this principle. Therefore, communication fusion technology can effectively improve network resource utilization and reduce network synchronization delay.</p>
<div class="math notranslate nohighlight">
\[t = \alpha m+\beta\]</div>
</section>
<section id="communication-fusion-implementation">
<h3>Communication Fusion Implementation<a class="headerlink" href="#communication-fusion-implementation" title="Permalink to this headline"></a></h3>
<p>Currently, fusion is supported for each of the three communication operators <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>, <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> and <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>, with the configuration item being a dict type, e.g.</p>
<p>comm_fusion={“allreduce”: {“mode”: “auto”, “config”: None}}, where “mode” has three options:</p>
<p>“auto”: Automatic operator fusion according to the data volume threshold of 64MB, with the configuration parameter “config” as None.</p>
<p>“size”: Communication operator fusion is performed by manually setting the data volume threshold, with the configuration parameter “config” of type int, in MB.</p>
<p>“index”: Only “allreduce” supports the configuration of index, which indicates the way of fusion according to the sequence number of communication operator, and the configuration parameter “config” is of type list. For example, [20, 35], means the first 20 AllReduce are fused into 1, the 20th to 35th AllReduce are fused into 1, and the remaining AllReduce are fused into 1.</p>
</section>
<section id="communication-fusion-usage">
<h3>Communication Fusion Usage<a class="headerlink" href="#communication-fusion-usage" title="Permalink to this headline"></a></h3>
<p>MindSpore provides two interfaces to enable communication fusion, each of which is described below.</p>
<section id="configuration-in-the-automatic-parallel-scenario">
<h4>Configuration in the Automatic Parallel Scenario<a class="headerlink" href="#configuration-in-the-automatic-parallel-scenario" title="Permalink to this headline"></a></h4>
<p>In automatic parallel or semi-automatic parallel scenarios, users can use the <code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> parameter provided by this interface to set the parallel strategy when configuring the parallel strategy via <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code>. Users can specify whether to use the index method or the fusion buffer method.</p>
</section>
<section id="using-the-interfaces-provided-by-cell">
<h4>Using the Interfaces Provided by <code class="docutils literal notranslate"><span class="pre">Cell</span></code><a class="headerlink" href="#using-the-interfaces-provided-by-cell" title="Permalink to this headline"></a></h4>
<p>Regardless of the parallel mode scenario, users can set the index for the parameters of a layer in the model through the <code class="docutils literal notranslate"><span class="pre">Cell.set_comm_fusion</span></code> interface, and MindSpore will fuse the parameters with the same index. In auto-parallel and semi-auto-parallel scenarios, it is recommended that the <code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> parameter be used in preference for configuration.</p>
</section>
</section>
</section>
<section id="operation-practice">
<h2>Operation Practice<a class="headerlink" href="#operation-practice" title="Permalink to this headline"></a></h2>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>You can download the full sample code here:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.0.0-alpha/docs/sample_code/distributed_comm_fusion">https://gitee.com/mindspore/docs/tree/r2.0.0-alpha/docs/sample_code/distributed_comm_fusion</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distributed_comm_fusion
        ├── fusion_example_cell.py
        ├── rank_table_2pcs.json
        ├── rank_table_8pcs.json
        └── run_fusion_example.sh
</pre></div>
</div>
<p>The function of each file is as follows:</p>
<ul class="simple">
<li><p>fusion_example_cell.py: Example of communication fusion by using the interface provided by <code class="docutils literal notranslate"><span class="pre">Cell</span></code>.</p></li>
<li><p>rank_table_2pcs.json: 2-card configuration file of RANK_TABLE_FILE.</p></li>
<li><p>rank_table_8pcs.json: 8-card configuration file of RANK_TABLE_FILE.</p></li>
<li><p>run_fusion_example.sh: Startup script for communication fusion.</p></li>
</ul>
</section>
<section id="configuring-the-communication-fusion">
<h3>Configuring the Communication Fusion<a class="headerlink" href="#configuring-the-communication-fusion" title="Permalink to this headline"></a></h3>
<p>The following introduces the configuration of two usage methods through the practical sample.</p>
<section id="comm-fusion-parameter">
<h4><code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> Parameter<a class="headerlink" href="#comm-fusion-parameter" title="Permalink to this headline"></a></h4>
<p>As shown in the following code, the <code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code> interface is used to configure the fusion mode for the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator to be <code class="docutils literal notranslate"><span class="pre">auto</span></code>, implying that the fusion buffer size is set to 64MB by default.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">comm_fusion</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;allreduce&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}})</span>
<span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
<p>If all similar communication operators are fused into one operator, in the current training iteration, the transmission needs to wait until the computation is completely finished before it can be executed, which will cause the device to wait.</p>
<p>In order to avoid the above problem, the network parameters can be fused in groups: while the next group of parameters is computed, the communication of the previous group of parameters is carried out, so that the computation and communication can be hidden from each other, to perform group fusion either by limiting the size of the fusion buffer, or by index partitioning.</p>
<p>For more usage, you can refer to MindSpore <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/tests/ut/python/parallel/test_comm_fusion.py">test cases</a>.</p>
<blockquote>
<div><p>Users can try the size and index modes of <code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> on their own, which are essentially methods of the fusion buffer class.</p>
</div></blockquote>
</section>
<section id="cell-set-comm-fusion-interface">
<h4><code class="docutils literal notranslate"><span class="pre">Cell.set_comm_fusion</span></code> Interface<a class="headerlink" href="#cell-set-comm-fusion-interface" title="Permalink to this headline"></a></h4>
<p>As shown in the following code, the <code class="docutils literal notranslate"><span class="pre">set_comm_fusion</span></code> method is called for the instantiated DenseLayer to set the fusion value for each layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Cell Fusion Example&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DEVICE_ID&quot;</span><span class="p">]))</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">DenseLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A base layer with two dense layer&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_mapping</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_mapping</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_mapping</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_mapping</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An network with many dense layers&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">DenseLayer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">DenseLayer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">DenseLayer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The parameter </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;s fusion id is </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">comm_fusion</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The corresponding output, representing the fusion index value for each layer of a particular dense, is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The parameter layer1.input_mapping.weight&#39;s fusion id is 0
The parameter layer1.input_mapping.bias&#39;s fusion id is 0
The parameter layer1.output_mapping.weight&#39;s fusion id is 0
The parameter layer1.output_mapping.bias&#39;s fusion id is 0
The parameter layer2.input_mapping.weight&#39;s fusion id is 1
The parameter layer2.input_mapping.bias&#39;s fusion id is 1
The parameter layer2.output_mapping.weight&#39;s fusion id is 1
The parameter layer2.output_mapping.bias&#39;s fusion id is 1
The parameter layer3.input_mapping.weight&#39;s fusion id is 2
The parameter layer3.input_mapping.bias&#39;s fusion id is 2
The parameter layer3.output_mapping.weight&#39;s fusion id is 2
The parameter layer3.output_mapping.bias&#39;s fusion id is 2
</pre></div>
</div>
</section>
</section>
<section id="running-the-code">
<h3>Running the Code<a class="headerlink" href="#running-the-code" title="Permalink to this headline"></a></h3>
<p>The above code needs to be configured with distributed variables before it can run. The Ascend environment needs to be configured with RANK_TABLE_FILE, RANK_ID and DEVICE_ID. For the configuration process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_ascend.html#configuring-distributed-environment-variables">here</a>. The GPU environment needs to be configured with <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_gpu.html#configuring-distributed-environment">OpenMPI</a>, NCCL and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_gpu.html#multi-host-training">HOST_FILE</a>. For the configuration process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_gpu.html#configuring-distributed-environment">here</a>.</p>
<p>Environment variables related to Ascend distributed are:</p>
<ul class="simple">
<li><p>RANK_TABLE_FILE: the path of networking information file. The rank_table_file file can be generated by using hccl_tools.py in the models code repository, which can be obtained from <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0.0-alpha/utils/hccl_tools">here</a>.</p></li>
<li><p>DEVICE_ID: The actual serial number of the current card on the machine.</p></li>
<li><p>RANK_ID: The logical serial number of the current card.</p></li>
</ul>
<p>Environment variables related to GPU distributed are:</p>
<ul class="simple">
<li><p>HOST_FILE: describes the IP and number of devices for multi-card training. Each line of the file has the format [hostname] slots=[slotnum], and hostname can be an ip or hostname. Note that the username needs to be the same on different machines, but the hostname cannot be the same.</p></li>
</ul>
<p>The user can access the above script in this document via <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.0.0-alpha/docs/sample_code/distributed_optimizer_parallel">here</a>. Execute the following <code class="docutils literal notranslate"><span class="pre">bash</span></code> script to run the program and output the log in the device0/train.log0 file.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">set</span><span class="w"> </span>-e
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_fusion_example.sh DATA_PATH RANK_SIZE&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_fusion_example.sh 8&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;This example is expected to run on the Ascend environment.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="nv">$1</span>

<span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>

test_dist_8pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_8pcs.json
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>
<span class="o">}</span>

test_dist_2pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_2pcs.json
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">2</span>
<span class="o">}</span>

test_dist_<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span>pcs

<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span>
<span class="k">do</span>
<span class="w">    </span>rm<span class="w"> </span>-rf<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>mkdir<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>cp<span class="w"> </span>./fusion_example_cell.py<span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device </span><span class="nv">$i</span><span class="s2">&quot;</span>
<span class="w">    </span>env<span class="w"> </span>&gt;<span class="w"> </span>env<span class="nv">$i</span>.log
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./fusion_example_cell.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="nv">$i</span><span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>../
<span class="k">done</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;The program launch succeed, the log is under device0/train.log0.&quot;</span>
</pre></div>
</div>
<p>After configuring RANK_TABLE_FILE in the current directory, the following command requires the user to have 8 Ascend 910 devices. Run the command as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_fusion_example.sh<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<p>[1] Xu Y, Lee H J, Chen D, et al. GSPMD: general and scalable parallelization for ML computation graphs[J]. arXiv preprint arXiv:2105.04663, 2021.</p>
<p>[2] Li M, Zhou L, Yang Z, et al. Parameter server for distributed machine learning[C]//Big learning NIPS workshop. 2013, 6: 2.</p>
<p>[3] Dean J, Corrado G, Monga R, et al. Large scale distributed deep networks[J]. Advances in neural information processing systems, 2012, 25.</p>
<p>[4] Narayanan D, Harlap A, Phanishayee A, et al. PipeDream: generalized pipeline parallelism for DNN training[C]//Proceedings of the 27th ACM Symposium on Operating Systems Principles. 2019: 1-15.</p>
<p>[5] Zhang H, Zheng Z, Xu S, et al. Poseidon: An efficient communication architecture for distributed deep learning on {GPU} clusters[C]//2017 USENIX Annual Technical Conference (USENIX ATC 17). 2017: 181-193.</p>
<p>[6] Peng Y, Zhu Y, Chen Y, et al. A generic communication scheduler for distributed dnn training acceleration[C]//Proceedings of the 27th ACM Symposium on Operating Systems Principles. 2019: 16-29.</p>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="parameter_server_training.html" class="btn btn-neutral float-left" title="Parameter Server Mode" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dataset_slice.html" class="btn btn-neutral float-right" title="Dataset Slicing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>