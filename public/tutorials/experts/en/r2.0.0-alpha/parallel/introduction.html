<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallel Training Mode &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quick Start Distributed Parallel Training" href="parallel_training_quickstart.html" />
    <link rel="prev" title="Incremental Operator Build" href="../debug/op_compilation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Parallel Training Mode</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/introduction.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallel-training-mode">
<h1>Distributed Parallel Training Mode<a class="headerlink" href="#distributed-parallel-training-mode" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/tutorials/experts/source_en/parallel/introduction.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<p>In deep learning, as the size of the dataset and the number of parameters grows, the time and hardware resources required for training increase, which eventually become a bottleneck that constrains training. Distributed parallel training, which can reduce the need for hardware such as memory and computational performance, is an important optimization tool for conducting training. According to the different principles and modes of parallelism, the following types of parallelism are mainstream in the industry:</p>
<ul class="simple">
<li><p>Data Parallel: The parallel mode of slicing the data is generally sliced according to the batch dimension, and the data is assigned to each computational unit (worker) for model computation.</p></li>
<li><p>Model Parallel: Parallel mode for slicing models. Model parallel can be classified as: operator-level model parallel, pipeline model parallel, optimizer model parallel, etc.</p></li>
<li><p>Hybrid Parallel: Refers to a parallel model that covers both data parallel and model parallel.</p></li>
</ul>
<p>Currently MindSpore also provides distributed parallel training, which supports a variety of models including:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code>: data parallel mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code>: automatic parallel mode, a distributed parallel mode that incorporates data parallel and operator-level model parallel, which can automatically build cost models, find the parallel strategy with shorter training times, and select the appropriate parallel mode for the user. MindSpore currently supports automatic search for the operator-level parallel strategy, providing three different strategy search algorithms as follows:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>: Dynamic programming strategy search algorithm. Capable of searching the optimal strategy inscribed by the cost model, but time consuming in searching parallel strategy for huge network models. Its cost model models training time based on the memory-based computational overhead and communication overhead of the Ascend 910 chip.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code>: Double recursive strategy search algorithm. The optimal strategy can be generated instantaneously for huge networks and large-scale multi-card slicing. Its cost model based on symbolic operations can be freely adapted to different accelerator clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code>: Sharding strategy propagation algorithm. The parallel strategy are propagated from operators configured with parallel strategy to operators that are not configured. When propagating, the algorithm tries to select the strategy that triggers the least amount of tensor rescheduling communication. For the parallel strategy configuration and tensor rescheduling of the operator, refer to this <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/design/distributed_training_design.html#principle-of-automatic-parallelism">design document</a>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code>: Semi-automatic parallel mode, compared to automatic parallel, requires the user to manually configure the shard strategy for the operator to achieve parallelism.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HYBRID_PARALLEL</span></code>: In MindSpore, it refers specifically to scenarios where the user achieves hybrid parallel by manually slicing the model.</p></li>
</ul>
<section id="distributed-parallel-training-mode-1">
<h2>Distributed Parallel Training Mode<a class="headerlink" href="#distributed-parallel-training-mode-1" title="Permalink to this headline"></a></h2>
<p>MindSpore currently supports the following four parallelism modes:</p>
<ul class="simple">
<li><p>Data Parallel: Used when the size of the user’s network parameters can be calculated on a single card. This mode will replicate the same network parameters on each card, with different training data input during training, and suitable for most users.</p></li>
<li><p>Semi-automatic Parallel: Used when neural network of the user cannot be computed on a single card and there is a large demand for the performance of slicing. Users can set this operation mode to manually specify the shard strategy for each operator to achieve better training performance.</p></li>
<li><p>Automatic Parallel: Used when neural network of the user cannot be computed on a single card, but does not know how to configure the operator strategy. When users start this mode, MindSpore will automatically configure the strategy for each operator, suitable for users who want to perform parallel train but don’t know how to configure the strategy.</p></li>
<li><p>Hybrid Parallel: It is entirely up to the user to design the logic and implementation of parallel training, and the user can define the communication operators such as <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> in the network by himself. Suitable for users who are familiar with parallel training.</p></li>
</ul>
<p>The usage and considerations of these four modes will be described in detail in the following document.</p>
<p>Currently MindSpore provides distributed parallel training. It supports multiple modes as mentioned above, and the corresponding parallel mode can be set via the <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context()</span></code> interface.</p>
<p>When users invoke the distributed training process, they need to call the following code to initialize the communication and configure the corresponding rank_table_file, which can be found the <strong>Multi-host Training</strong> section in the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_ascend.html#multi-host-training">Distributed Training (Ascend)</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rank_id is </span><span class="si">{}</span><span class="s2">, device_num is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">device_num</span><span class="p">))</span>
<span class="n">ms</span><span class="o">.</span><span class="n">reset_auto_parallel_context</span><span class="p">()</span>
<span class="c1"># The following parallel configuration users only need to configure one of these modes</span>
<span class="c1"># Data parallel mode</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">)</span>
<span class="c1"># Semi-automatic parallel mode</span>
<span class="c1"># ms.set_auto_parallel_context(parallel_mode=ms.ParallelMode.SEMI_AUTO_PARALLEL)</span>
<span class="c1"># Automatic parallel mode</span>
<span class="c1"># ms.set_auto_parallel_context(parallel_mode=ms.ParallelMode.AUTO_PARALLEL)</span>
<span class="c1"># Hybrid parallel mode</span>
<span class="c1"># ms.set_auto_parallel_context(parallel_mode=ms.ParallelMode.HYBRID_PARALLEL)</span>
</pre></div>
</div>
<p>The following involves automatic parallel interfaces, such as the interface configuration in <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code>. Distributed parallel training is supported in the following table for each scenario.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parallel modes</p></th>
<th class="head"><p>Configuration</p></th>
<th class="head"><p>Dynamic graph</p></th>
<th class="head"><p>Static graph</p></th>
<th class="head"><p>Supported devices</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data parallel</p></td>
<td><p>DATA_PARALLEL</p></td>
<td><p>Support</p></td>
<td><p>Support</p></td>
<td><p>CPU, GPU, Ascend 910</p></td>
</tr>
<tr class="row-odd"><td><p>Semi-automatic parallel</p></td>
<td><p>SEMI_AUTO_PARALLEL</p></td>
<td><p>Not support</p></td>
<td><p>Support</p></td>
<td><p>GPU, Ascend 910</p></td>
</tr>
<tr class="row-even"><td><p>Automatic parallel</p></td>
<td><p>AUTO_PARALLEL</p></td>
<td><p>Not support</p></td>
<td><p>Support</p></td>
<td><p>GPU, Ascend 910</p></td>
</tr>
<tr class="row-odd"><td><p>Hybrid parallel</p></td>
<td><p>HYBRID_PARALLEL</p></td>
<td><p>Not support</p></td>
<td><p>Support</p></td>
<td><p>GPU, Ascend 910</p></td>
</tr>
</tbody>
</table>
<section id="data-parallelism">
<h3>Data Parallelism<a class="headerlink" href="#data-parallelism" title="Permalink to this headline"></a></h3>
<p>In data parallelism, the way that the user defines the network is the same as a standalone script, but call <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.communication.html#mindspore.communication.init">init()</a> before the network definition to initialize the device communication state.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">DataParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DataParallelNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initialize weights</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">init</span><span class="p">()</span>
<span class="c1"># Set parallel mode to data parallelism, and other modes  are the same</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DataParallelNet</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="semi-automatic-parallism">
<h3>Semi-automatic Parallism<a class="headerlink" href="#semi-automatic-parallism" title="Permalink to this headline"></a></h3>
<p>Compared to automatic parallelism, the semi-automatic parallel mode requires the user to manually configure the shard <strong>strategy</strong> for the operator to achieve parallelism. For the operator parallel strategy definition, refer to this <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/design/distributed_training_design.html#principle-of-automatic-parallelism">design document</a>.</p>
<ul>
<li><p>When starting semi-automatic and automatic modes for training, training <strong>must</strong> be performed via the <code class="docutils literal notranslate"><span class="pre">model.train(*args,</span> <span class="pre">**kwargs)</span></code> interface. Custom loops for network training are not supported.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training method 1: Call through the Model interface, and only this method is supported</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Training method 2: custom loop, and this method is not supported</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
        <span class="n">train_net</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">net_with_criterion</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</li>
<li><p>Semi-automatic parallel mode requires the user to <strong>manually configure</strong> the <strong>shard</strong> interface of each operator to tune the parallel strategy compared to automatic parallel mode.</p>
<p>Taking <code class="docutils literal notranslate"><span class="pre">SemiAutoParallelNet</span></code> as an example, the script code in semi-automatic parallel mode is as follows. The shard strategy of <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> is <code class="docutils literal notranslate"><span class="pre">((1,</span> <span class="pre">1),(1,</span> <span class="pre">2))</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.weight</span></code> is specified to be sliced in two copies in the second dimension.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">SemiAutoParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SemiAutoParallelNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initialize weights</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="c1"># Set the shard strategy. There are two inputs of fc in construct, and the first input is x and the second input is the weight self.weight</span>
        <span class="c1"># Therefore, the shard needs to provide a tuple, which corresponds to the number of copies of each input tensor in the corresponding dimension</span>
        <span class="c1"># (1,1) means that each dimension of the input x is unsliced</span>
        <span class="c1"># (1,2) means that the second dimension of self.weight is sliced into two parts</span>
        <span class="c1"># The slicing process is during the graph compilation, and the shape of self.weight is changed after the compilation is completed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="c1"># When to initialize and parallelly call operation operator in the construct function, it is equivalent to the user not setting the strategy for the matmul operator. Then the default strategy will automatically configure data parallelism, i.e. ((8, 1), (1, 1)). 8 indicates the number of cards the user runs this time</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">SemiAutoParallelNet</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>In case the device matrices of the preceding and following operators do not match, a <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/design/distributed_training_design.html#principle-of-automatic-parallelism">rescheduling</a> is automatically inserted to ensure that the shard state of <code class="docutils literal notranslate"><span class="pre">tensor</span></code> matches the next operator input requirement. For example, the following example code is used in the training of single machine eight-card：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>
<span class="k">class</span> <span class="nc">SemiAutoParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SemiAutoParallelNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initialize weights</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="c1"># Setting a shard strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># In __init__, we configure the second input of fc as (1,2)</span>
        <span class="c1"># So the output tensor after fc is cut into two in the second dimension of the output, from 128 to 64, so its output shape is [batch, 64]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="c1"># In __init__, we configure (8,1) for the 0th input of fc2 by way of shard, which means that the 0th dimension of this input is requested to be cut into 8 parts</span>
        <span class="c1"># The output of the last operator fc is still [batch,64], and the 0th dimension is not cut, so there is a problem of inconsistent tensor shape</span>
        <span class="c1"># So the auto-parallel framework will insert the StrideSlice operator here, which is not declared in the user script, to slice x</span>
        <span class="c1"># to ensure the consistency of the tensor shape before and after.</span>
        <span class="c1"># In addition, the 1st dimension of the output of fc is cut into 2 parts, but the 1st dimension of the 0th input of fc2 is taken as a whole part, so the allgather operator will be inserted.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
        <span class="c1"># The framework will automatically insert an AllGather operator and a StridedSlice operation here</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Therefore, the inserted rescheduling operators may be <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, <code class="docutils literal notranslate"><span class="pre">Split</span></code>, <code class="docutils literal notranslate"><span class="pre">Concat</span></code> and <code class="docutils literal notranslate"><span class="pre">StridedSlice</span></code> operators if the operators before and after have different requirements for input slicing, which will increase the computation and communication time consuming of the network. The user can <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0.0-alpha/design/mindir.html">save ir graph</a> to view the operator status of the whole network. The automatic parallel process produces <code class="docutils literal notranslate"><span class="pre">ir</span></code> graphs named <code class="docutils literal notranslate"><span class="pre">step_parallel_begin_xxxx.ir</span></code> and <code class="docutils literal notranslate"><span class="pre">step_parallel_end_xxxx.ir</span></code>. The former indicates the graph state before entering the parallel process, and the latter indicates the graph state after the automatic parallel process. Users can view this latter one to find the operators inserted in automatic parallelism.</p>
<blockquote>
<div><ul class="simple">
<li><p>In semi-automatic parallel mode, the operators that are not configured with strategy is executed in data parallel by default, corresponding to the data parallelism of all cards.</p></li>
<li><p>Automatic parallel mode supports automatic acquisition of efficient operator parallel strategy through strategy search algorithms, and also supports users manually configure specific parallel strategy for operators.</p></li>
<li><p>If a <code class="docutils literal notranslate"><span class="pre">parameter</span></code> is used by more than one operator, the shard strategy of each operator for this <code class="docutils literal notranslate"><span class="pre">parameter</span></code> needs to be consistent, otherwise an error will be reported.</p></li>
</ul>
</div></blockquote>
<p>Pipeline parallel is also possible in automatic and semi-automatic modes by configuring the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> property on the <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. The corresponding tutorial on pipeline parallelism can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/pipeline_parallel.html">Applying Pipeline Parallel</a>.</p>
</section>
<section id="fully-automatic-parallelism">
<h3>Fully Automatic Parallelism<a class="headerlink" href="#fully-automatic-parallelism" title="Permalink to this headline"></a></h3>
<p>Automatic parallel mode, a distributed parallel mode that combines data parallel, model parallel and hybrid parallel in, can automatically build cost models, find parallel strategies with shorter training time, and select the appropriate parallel mode for users. MindSpore provides the following three different strategy search algorithms:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>: Dynamic programming strategy search algorithm. It is able to search for the optimal strategy inscribed by the cost model, but it is time-consuming to search for parallel policies for huge network models. Its cost model is modeled around the memory-based computational overhead and communication overhead of the Ascend 910 chip for training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code>: Double recursive strategy search algorithm. The optimal strategy can be generated instantaneously for huge networks and large-scale multi-card slicing. Its cost model based on symbolic operations can be freely adapted to different accelerator clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code>: Sharding strategy propagation algorithm. The parallel strategy are propagated from operators configured with parallel strategy to operators that are not configured. When propagating, the algorithm tries to select the strategy that triggers the least amount of tensor rescheduling communication. For the parallel strategy configuration and tensor rescheduling of the operator, refer to <span class="xref myst">semi-automatic parallel</span>.</p></li>
</ul>
<p>The user can set the above-mentioned strategy search algorithm with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="c1"># Set dynamic programming algorithm for strategy search</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;dynamic_programming&quot;</span><span class="p">)</span>
<span class="c1"># Set a double recursive method for strategy search</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;recursive_programming&quot;</span><span class="p">)</span>
<span class="c1"># Set the shard strategy propagation algorithm</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code> mode, the algorithm is propagated throughout the model according to the <code class="docutils literal notranslate"><span class="pre">shard</span></code> strategy set by the user. In <code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code> mode, the <code class="docutils literal notranslate"><span class="pre">shard</span></code> strategy set by the user will also take effect and will not be overwritten by the searched strategy.</p></li>
<li><p>In fully-automatic parallel mode, if you need to manually configure the data parallel strategy for all the operators in a Cell, you can use Cell.set_data_parallel() to set it uniformly.</p></li>
</ul>
</div></blockquote>
</section>
<section id="hybrid-parallelism">
<h3>Hybrid Parallelism<a class="headerlink" href="#hybrid-parallelism" title="Permalink to this headline"></a></h3>
<p>In MindSpore, specifically refer to the scenario where the user implements hybrid parallelism by manually slicing the model. The user can define the communication operator primitives <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> and <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, etc. in the network structure and execute the parallel process manually. In this case, the user needs to implement the parameter slicing, communication after the operator slicing. For example, the code example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">HybridParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HybridParallelNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># The following 2-card running scenario is used as an example to implement distributed matrix multiplication to simulate the results of single-card matrix multiplication.</span>
        <span class="c1"># The original logic</span>
        <span class="c1">#         Shapes of the inputs x and weight are (32, 512), (512, 128) respectively</span>
        <span class="c1">#        after calculating matmul(x, weight)</span>
        <span class="c1">#        The output is a tensor with shape (32, 128)</span>
        <span class="c1"># Here we implement the above matrix multiplication logic manually</span>
        <span class="c1"># We need to manually specify the shape of the current weight of the slice, which we want to slice in the relevant dimension of matmul. In the case of correlated dimensional slicing,</span>
        <span class="c1"># an AllReduce operation needs to be performed on the matmul results to ensure that the values are consistent with those of the standalone machine</span>
        <span class="c1">#</span>
        <span class="c1"># distributed logic</span>
        <span class="c1">#         The shapes of inputs x and weight are (32, 256), (256, 128) respectively</span>
        <span class="c1">#         after calculating output = matmul(x, weight)</span>
        <span class="c1">#                  output = allreduce(output)</span>
        <span class="c1">#         The output is a tensor with shape (32, 128)</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">HYBRID_PARALLEL</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">HybridParallelNet</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="multi-card-startup-method">
<h2>Multi-card Startup Method<a class="headerlink" href="#multi-card-startup-method" title="Permalink to this headline"></a></h2>
<p>Currently GPU, Ascend and CPU support multiple startup methods respectively. The three main methods are OpenMPI, dynamic networking and multi-process startup.</p>
<ul class="simple">
<li><p>Multi-process startup method. The user needs to start the processes corresponding to the number of cards, as well as configure the rank_table table. You can visit <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_ascend.html#running-the-script">Running Script</a> to learn how to start multi-card tasks by multi-processing.</p></li>
<li><p>OpenMPI. The user can start running the script with the mpirun command, at which point the user needs to provide the host file. Users can visit <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_ascend.html#running-the-script-through-openmpi">Run Scripts via OpenMPI</a> to learn how to use OpenMPI to start multi-card tasks.</p></li>
<li><p>Dynamic Networking. MindSpore uses built-in dynamic networking module and has no need to rely on external configuration files or modules to help implement multi-card tasks. Users can visit <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_gpu.html#training-without-relying-on-openmpi">Training without relying on OpenMPI</a> to learn how to use dynamic networking to start multi-card tasks.</p></li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>Ascend</p></th>
<th class="head"><p>CPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>OpenMPI</p></td>
<td><p>Support</p></td>
<td><p>Support</p></td>
<td><p>Not support</p></td>
</tr>
<tr class="row-odd"><td><p>Multi-process startup</p></td>
<td><p>Not support</p></td>
<td><p>Support</p></td>
<td><p>Not support</p></td>
</tr>
<tr class="row-even"><td><p>Dynamic Networking</p></td>
<td><p>Support</p></td>
<td><p>Support</p></td>
<td><p>Support</p></td>
</tr>
</tbody>
</table>
</section>
<section id="data-import-method">
<h2>Data Import Method<a class="headerlink" href="#data-import-method" title="Permalink to this headline"></a></h2>
<p>In parallel training, three types of data import are supported.</p>
<ul>
<li><p>Full import. Only works in <strong>semi-automatic</strong> and <strong>fully automatic</strong> parallel mode. User can turn it on with <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context(full_batch=True)</span></code>. After turning on full import, the read <code class="docutils literal notranslate"><span class="pre">batch</span></code> is considered as a complete shape of the network input in the automatic parallel process. For example, in the 8-card training, suppose the shape returned by each card <code class="docutils literal notranslate"><span class="pre">dataset</span></code> is <code class="docutils literal notranslate"><span class="pre">[32,</span> <span class="pre">8]</span></code>, the data trained in the current iteration of training is <code class="docutils literal notranslate"><span class="pre">[32,</span> <span class="pre">8]</span></code>. Therefore, <strong>the user needs to ensure that the data input is consistent for each card in each iteration</strong>. For example, make sure that the <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> order is consistent for each card dataset.</p></li>
<li><p>Data parallel import. If the user does not set <code class="docutils literal notranslate"><span class="pre">full_batch</span></code>, the data read in each card is a slice of the current training iteration, so the content of the data read in each card is required to be <strong>different</strong>. For example, in the 8-card training, the <code class="docutils literal notranslate"><span class="pre">shape</span></code> of the read data per card is <code class="docutils literal notranslate"><span class="pre">[32,8]</span></code>, so the total amount of data for the current iteration of training is <code class="docutils literal notranslate"><span class="pre">[32*8,</span> <span class="pre">8]</span></code>.</p></li>
<li><p>Model parallel import. The model parallel import is used in the scenrio where the image size is too large to calculate in the single-card, and the image is sliced right in the input process. MindSpore provides <code class="docutils literal notranslate"><span class="pre">dataset_strategy</span></code> interface in <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code>, and users can configure the input strategy more flexible through this interface. It should be noted that when the users use this interface, <code class="docutils literal notranslate"><span class="pre">tensor</span></code> returned by <code class="docutils literal notranslate"><span class="pre">dataset</span></code> needs to match corresponding shard strategy. The code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="c1"># Set the input to be sliced in dimension 1, which requires the user to ensure that the input returned by the dataset is sliced in dimension 1</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
<span class="c1"># Equivalent to setting full_batch=False</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">)</span>
<span class="c1"># Equivalent to setting full_batch=True</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>Therefore, after the user sets the above configuration, it is necessary to <strong>manually</strong> set the obtaining order of the dataset to ensure that the data is expected for each card.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../debug/op_compilation.html" class="btn btn-neutral float-left" title="Incremental Operator Build" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="parallel_training_quickstart.html" class="btn btn-neutral float-right" title="Quick Start Distributed Parallel Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>