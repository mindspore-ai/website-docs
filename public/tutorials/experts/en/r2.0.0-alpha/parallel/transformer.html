

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed Parallel Training of Transformer Models &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PengCheng·PanGu Model Network Multi-dimension Hydrid Parallel Analysis" href="pangu_alpha.html" />
    <link rel="prev" title="Distributed Parallel Training Base Sample (CPU)" href="train_cpu.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/gpu_mindir.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_910_mindir.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed Case</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="train_ascend.html">Distributed Parallel Training Example (Ascend)</a></li>
<li class="toctree-l2"><a class="reference internal" href="train_gpu.html">Distributed Parallel Training Example (GPU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="train_cpu.html">Distributed Parallel Training Base Sample (CPU)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Parallel Training of Transformer Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="pangu_alpha.html">PengCheng·PanGu Model Network Multi-dimension Hydrid Parallel Analysis</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed Case</a> &raquo;</li>
      <li>Distributed Parallel Training of Transformer Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/transformer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-parallel-training-of-transformer-models">
<h1>Distributed Parallel Training of Transformer Models<a class="headerlink" href="#distributed-parallel-training-of-transformer-models" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/tutorials/experts/source_en/parallel/transformer.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In recent years, the number of Transformer-based pre-trained model parameters has been increasing, and the growth of memory in Ascend 910, GPU and other devices is significantly smaller than the growth of model size. Therefore, it has been a very urgent need to perform parallel train on the Transformer model. MindSpore provides a distributed Transformer interface <code class="docutils literal notranslate"><span class="pre">mindspore.nn.transformer.transformer</span></code> that configures each operator used inside the Transformer with a parallel strategy, and the user only needs to configure the global <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> and <code class="docutils literal notranslate"> <span class="pre">model_parallel</span></code> attributes to complete the configuration of the distributed parallel strategy. It can greatly facilitate users to apply Transformer for distributed training. Currently, distributed training supports Ascend 910 and GPU environments, as summarized below:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Transformer</span></code> provides a simple parallel configuration to achieve both operator-level parallelism and pipeline parallelism.</p></li>
</ul>
<blockquote>
<div><p>Download complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.0.0-alpha/docs/sample_code/distributed_training_transformer">distributed_training_transformer</a></p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distribute_training_transformer
        ├── dataset.py
        ├── model.py
        ├── parallel_recover_train.py
        ├── parallel_save_ckpt_train.py
        ├── preprocess.py
        ├── rank_table_16pcs.json
        ├── rank_table_2pcs.json
        ├── rank_table_8pcs.json
        ├── run_cluster.sh
        ├── run_parallel_recover_ckpt.sh
        ├── run_parallel_save_ckpt.sh
        ├── run.sh
        └── train.py
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code> and <code class="docutils literal notranslate"><span class="pre">rank_table_2pcs.json</span></code> are networking information files to configure the current multi-card environment. The files <code class="docutils literal notranslate"><span class="pre">model.py</span></code>, <code class="docutils literal notranslate"><span class="pre">dataset.py</span></code> and <code class="docutils literal notranslate"><span class="pre">train.py</span></code> are the scripts that define the data import, network structure and training files. <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the execution script.</p>
<p>Using the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> library in <code class="docutils literal notranslate"><span class="pre">mindspore.parallel</span></code>, the user needs to decide on the inputs for both the parallel configuration and the model to complete the distributed configuration. <strong>Distributed configuration only works in semi-automatic and automatic parallel mode</strong>.</p>
</section>
<section id="parallel-configuration-definition">
<h2>Parallel Configuration Definition<a class="headerlink" href="#parallel-configuration-definition" title="Permalink to this headline"></a></h2>
<p>For the definition and implementation of the network in <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>, we set the corresponding shard strategy for each operator. Users can achieve parallel configuration of <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> network by setting the global parallel configuration according to their needs.</p>
<p><code class="docutils literal notranslate"><span class="pre">Transformer</span></code> currently defines three main categories of parallel configurations <code class="docutils literal notranslate"><span class="pre">TransformerOpParallelConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">OpParallelConfig</span></code> and <code class="docutils literal notranslate"><span class="pre">EmbeddingOpParallelConfig</span></code>.</p>
<p>The import path for <code class="docutils literal notranslate"><span class="pre">TransformerOpParallelConfig</span></code> is <code class="docutils literal notranslate"><span class="pre">mindspore.nn.transformer</span></code>, and the attributes it can configure are shown below:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data_parallel</span> <span class="pre">(int)</span></code>: Set the number of data parallelism, and the default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_parallel</span> <span class="pre">(int)</span></code>: Set the number of model parallelism, and the default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pipeline_stage</span> <span class="pre">(int)</span></code>: Set the number of Pipeline Stages, and the default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">micro_batch_num</span> <span class="pre">(int)</span></code>: Set the number of input Batch slices, i.e. a Batch is sliced into multiple small batches. The default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizer_shard</span> <span class="pre">(bool)</span></code>: Whether to enable optimizer parallelism. The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gradient_aggregation_group</span> <span class="pre">(int)</span></code>: Optimizer parallelism corresponds to the number of gradient aggregations, and the default value is 4.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">recompute</span> <span class="pre">(bool)</span></code>: Whether to enable recalculation. The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_emb_dp</span> <span class="pre">(bool)</span></code>: Whether to configure Embedding as data parallelism. The default value is True.</p></li>
</ul>
<p>We will discuss their differences next. Now, as an example of training a <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> model with a single-machine eight-card, we set the parallel configuration of the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> model based on the current number of 8-card. We can set <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>=1 and <code class="docutils literal notranslate"><span class="pre">model_parallel</span></code>=8 as the basic parallelism configuration. Note that in the case of parallel configuration, <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> *<code class="docutils literal notranslate"><span class="pre">model_parallel</span></code> *<code class="docutils literal notranslate"><span class="pre">pipeline_stages</span></code> &lt;= total number of cards. The <strong>parallel configuration</strong> in the corresponding code is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerOpParallelConfig</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="n">parallel_config</span> <span class="o">=</span> <span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-definition">
<h2>Model Definition<a class="headerlink" href="#model-definition" title="Permalink to this headline"></a></h2>
<p>After defining the configuration, we can start constructing a network. Since MindSpore provides the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>, the user only needs to add an additional <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer, an output layer and a loss function. The following describes the configuration of each module in turn.</p>
<section id="embedding-layer">
<h3>Embedding Layer<a class="headerlink" href="#embedding-layer" title="Permalink to this headline"></a></h3>
<p>The Embedding layer in Tranformer consists of two main parts: word vector embedding and location vector embedding.</p>
<p>We provide <code class="docutils literal notranslate"><span class="pre">VocabEmbedding</span></code> as a parallel Embedding layer, which needs to be initialized by passing in <code class="docutils literal notranslate"><span class="pre">EmbeddingOpParallelConfig</span></code>. Unlike <code class="docutils literal notranslate"><span class="pre">OpParallelConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">EmbeddingOpParallelConfig</span></code> has the following attributes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>: Set the number of data parallelism, and the default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_parallel</span></code>: Set the number of model parallelism, and the default value is 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_emb_dp</span></code>: Whether to configure Embedding as data parallelism. The default value is True.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">vocab_emb_dp</span></code> is used to distinguish between two parallel modes of <code class="docutils literal notranslate"><span class="pre">embedding_lookup</span></code> operations, <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">parallelism</span></code> and <code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">slicing</span> <span class="pre">parallelism</span></code>. When <code class="docutils literal notranslate"><span class="pre">vocab_emb_dp</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the process of embedding lookups will be set to data parallelism with a parallelism of <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>. When <code class="docutils literal notranslate"><span class="pre">vocab_emb_dp</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the embedding weights will be evenly divided by <code class="docutils literal notranslate"><span class="pre">model_parallel</span></code> in dimension 0, which can reduce the storage of variables.</p>
<p>Here we define an <code class="docutils literal notranslate"><span class="pre">EmbeddingLayer</span></code> that sums the query word vector and the position vector. Note that we set the <code class="docutils literal notranslate"><span class="pre">add</span></code> and <code class="docutils literal notranslate"><span class="pre">dropout</span></code> operations here. Since the input tensor size is <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">seq_length,</span> <span class="pre">hidden_size]</span></code> and the word vector lookup process is data parallel, we call the <code class="docutils literal notranslate"><span class="pre">shard</span></code> method of the operator to set the parallel strategy of these two operators separately according to the data parallel value <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> in <code class="docutils literal notranslate"><span class="pre">OpParallelConfig</span></code>. If the user does not set the <code class="docutils literal notranslate"><span class="pre">shard</span></code> method, the default operator parallelism strategy is <strong>data parallism with the degree of parallism as card number</strong>. The corresponding code is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">VocabEmbedding</span>
<span class="k">class</span> <span class="nc">EmbeddingLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">position_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">VocabEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                                             <span class="n">embedding_size</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span>
                                             <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">VocabEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">position_size</span><span class="p">,</span>
                                                 <span class="n">embedding_size</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span>
                                                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">input_position</span><span class="p">):</span>
        <span class="n">word_embedding</span><span class="p">,</span> <span class="n">word_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">position_embedding</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">(</span><span class="n">input_position</span><span class="p">)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">word_embedding</span><span class="p">,</span> <span class="n">position_embedding</span><span class="p">)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embed</span><span class="p">,</span> <span class="n">word_table</span>
</pre></div>
</div>
<p>Note that we also return the embedding_table of the word embedding as a return value.</p>
</section>
<section id="transformer-layer">
<h3>Transformer Layer<a class="headerlink" href="#transformer-layer" title="Permalink to this headline"></a></h3>
<p>There are three interfaces that users can call as the main construction API: <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>, <code class="docutils literal notranslate"><span class="pre">TransformerEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">TransformerDecoder</span></code>. They both need to pass <code class="docutils literal notranslate"><span class="pre">TransformerOpParallelConfig</span></code> as the configuration for parallel settings. We set the corresponding parallel strategy for the operator used inside <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> according to the parallel configuration configured in <code class="docutils literal notranslate"><span class="pre">TransformerOpParallelConfig</span></code>.</p>
<blockquote>
<div><p>The method <code class="docutils literal notranslate"><span class="pre">pipeline_func</span></code> sets the <code class="docutils literal notranslate"><span class="pre">stage</span></code> to which each <code class="docutils literal notranslate"><span class="pre">block</span></code> in the transformer belongs, whether to turn on recompute and the fusion flag for optimizer slicing. For example, in the following example, we calculate the <code class="docutils literal notranslate"><span class="pre">stage</span></code> corresponding to the current <code class="docutils literal notranslate"><span class="pre">block</span></code> according to the configuration of even division, <code class="docutils literal notranslate"><span class="pre">layer_id</span></code> and <code class="docutils literal notranslate"><span class="pre">offset</span></code> passed-in (in the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> interface, the <code class="docutils literal notranslate"><span class="pre">offset</span></code> passed in when instantiating <code class="docutils literal notranslate"><span class="pre">Encoder</span></code> is 0, and the value of <code class="docutils literal notranslate"><span class="pre">offset</span></code> passed in <code class="docutils literal notranslate"><span class="pre">Decoder</span></code> is the number of layers of <code class="docutils literal notranslate"><span class="pre">Encoder</span></code>), the total number of layers of <code class="docutils literal notranslate"><span class="pre">Encoder_layer</span></code> and <code class="docutils literal notranslate"><span class="pre">Decoder_layer</span></code>, the number of specified <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code>. By default, if the user does not pass in <code class="docutils literal notranslate"><span class="pre">lambda_func</span></code>, it is also set to evenly divide according to the number of layers.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pipeline_func</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="n">layers_per_stage</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">pp_id</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">layers_per_stage</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">network</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">pp_id</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pipeline id is:</span><span class="si">{</span><span class="n">pp_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>In the following code, we instantiate the <code class="docutils literal notranslate"><span class="pre">EmbeddingLayer</span></code> defined above and call <code class="docutils literal notranslate"><span class="pre">set_comm_fusion</span></code> to mark its corresponding reverse gradient fusion as group 0, and call the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> method to set the weight of the corresponding embedding as the 0th <code class="docutils literal notranslate"><span class="pre">stage</span></code>. Place the last <code class="docutils literal notranslate"><span class="pre">Head</span></code> class, a simple <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer, on the last <code class="docutils literal notranslate"><span class="pre">stage</span></code>. In case the user does not set the operator parallelism strategy in the Linear, the default is data parallelism within the current <code class="docutils literal notranslate"><span class="pre">stage</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">Transformer</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">,</span> <span class="n">CrossEntropyLoss</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Dense</span> <span class="k">as</span> <span class="n">Linear</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Single Transformer Model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span>
                 <span class="n">en_layer</span><span class="p">,</span> <span class="n">de_layer</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">,</span> <span class="n">return_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">position_size</span><span class="o">=</span><span class="n">src_len</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">embedding_dp_mp_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">position_size</span><span class="o">=</span><span class="n">tgt_len</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">embedding_dp_mp_config</span><span class="p">)</span>
        <span class="n">total_layers</span> <span class="o">=</span> <span class="n">en_layer</span> <span class="o">+</span> <span class="n">de_layer</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="n">layers_per_stage</span> <span class="o">=</span> <span class="n">total_layers</span> <span class="o">//</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_loss</span> <span class="o">=</span> <span class="n">return_loss</span>

        <span class="k">def</span> <span class="nf">pipeline_func</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
            <span class="n">pp_id</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">layers_per_stage</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">network</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">pp_id</span><span class="p">)</span>
            <span class="n">gradient_aggregation_group</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="n">dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">gradient_aggregation_group</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">network</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">dis</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pipeline id is:</span><span class="si">{</span><span class="n">pp_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base1</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">encoder_layers</span><span class="o">=</span><span class="n">en_layer</span><span class="p">,</span>
                                 <span class="n">decoder_layers</span><span class="o">=</span><span class="n">de_layer</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                 <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_len</span><span class="p">,</span>
                                 <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_len</span><span class="p">,</span>
                                 <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                 <span class="n">lambda_func</span><span class="o">=</span><span class="n">pipeline_func</span><span class="p">,</span>
                                 <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">AttentionMask</span><span class="p">(</span><span class="n">seq_length</span><span class="o">=</span><span class="n">tgt_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_equal</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>

</pre></div>
</div>
</section>
<section id="defining-the-loss-function">
<h3>Defining the Loss Function<a class="headerlink" href="#defining-the-loss-function" title="Permalink to this headline"></a></h3>
<p>MindSpore also provides a cross-quotient loss function <code class="docutils literal notranslate"><span class="pre">mindspore.nn.transformer.CrossEntroyLoss</span></code> that supports parallelism. This function takes an <code class="docutils literal notranslate"><span class="pre">OpParallelConfig</span></code> to configure the parallelism attributes.</p>
<p><code class="docutils literal notranslate"><span class="pre">OpParallelConfig</span></code> actually contains two attributes <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> and <code class="docutils literal notranslate"><span class="pre">model_parallel</span></code>. These two attributes allow you to configure the parallel configuration of the loss function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">TransformerOpParallelConfig</span>
<span class="n">parallel_config</span> <span class="o">=</span> <span class="n">TransformerOpParallelConfig</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="end-to-end-process">
<h2>End-to-end Process<a class="headerlink" href="#end-to-end-process" title="Permalink to this headline"></a></h2>
<p>After defining the parallel configuration, model and loss function, we further integrate the above code. Before starting the training, we call <code class="docutils literal notranslate"><span class="pre">auto_parallel_context</span></code> to set the parallelism option and set the parallelism mode to <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code>. In the case of pipeline parallelism, MindSpore provides additional configurations to further slice the gradient accumulation variables to the cards of the data parallel dimension to save memory footprint. The process is as follows: first turn on optimizer slicing (<code class="docutils literal notranslate"><span class="pre">enable_parallel_optimizer=True</span></code>).
Then set <code class="docutils literal notranslate"><span class="pre">parallel_optimizer_config=</span> <span class="pre">{&quot;gradient_accumulation_shard&quot;:True}</span></code> to further slice the accumulated variables during pipeline parallel training to save memory, and will introduce communication operators between each <code class="docutils literal notranslate"><span class="pre">micro_step</span></code> for gradient synchronization. Note that the default corresponding value of <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_shard</span></code> is True. If the user wants to improve the performance, he can set this parameter to False.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">loss_repeated_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel_optimizer_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_accumulation_shard&quot;</span><span class="p">:</span> <span class="n">gradient_accumulation_shard</span><span class="p">})</span>
</pre></div>
</div>
<p>The description of <code class="docutils literal notranslate"><span class="pre">stage_num</span></code> is as follows. MindSpore uses <code class="docutils literal notranslate"><span class="pre">stage_num</span></code> to determine whether to enter pipeline parallel training.</p>
<ul class="simple">
<li><p>Perform operator-level parallelism by setting <code class="docutils literal notranslate"><span class="pre">stage_num=1</span></code>. Users can configure the parallel policy by setting the <code class="docutils literal notranslate"><span class="pre">model_parallel</span></code> and <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> attributes in <code class="docutils literal notranslate"><span class="pre">TransformerOpParallelConfig</span></code>.</p></li>
<li><p>In case <code class="docutils literal notranslate"><span class="pre">stage_num&gt;1</span></code> is set, it will enter pipeline parallel mode. In the pipeline parallel mode, you need to set the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> attribute of each <code class="docutils literal notranslate"><span class="pre">cell</span></code> to assign the <code class="docutils literal notranslate"><span class="pre">cell</span></code> to the corresponding device for execution. In addition, after instantiating the network, we need to call <code class="docutils literal notranslate"><span class="pre">PipelineCell</span></code> again to encapsulate the defined network. The role of this <code class="docutils literal notranslate"><span class="pre">Cell</span></code> is to slice the input of the network into <code class="docutils literal notranslate"><span class="pre">mirco_batch_num</span></code> numbers of small data in order to maximize the use of computational resources. Note that we need to call <code class="docutils literal notranslate"><span class="pre">net.infer_param_pipeline_stage()</span></code> instead of <code class="docutils literal notranslate"><span class="pre">net.trainable_params()</span></code> to get the training weights corresponding to the current device <code class="docutils literal notranslate"><span class="pre">stage</span></code> and the number of cards within the stage of the pipeline is at least 8. A detailed tutorial of the pipeline can be found <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/pipeline_parallel.html">here</a>.</p></li>
</ul>
<p>The code of the integrated master file is as follows. Note that the definitions of some parameters are omitted here, and the complete list of parameters can be found in the <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/docs/sample_code/distributed_training_transformer/train.py">use case source code</a>. The code address of which is given in the beginning of this article.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">TimeMonitor</span><span class="p">,</span> <span class="n">LossMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerOpParallelConfig</span>
<span class="kn">import</span> <span class="nn">mindspore.communication</span> <span class="k">as</span> <span class="nn">D</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">PipelineCell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">AdamWeightDecay</span>
<span class="kn">from</span> <span class="nn">dataset</span> <span class="kn">import</span> <span class="n">ToyDataset</span><span class="p">,</span> <span class="n">Tokenzier</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">Net</span>


<span class="k">def</span> <span class="nf">set_weight_decay</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">decay_filter</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;layernorm&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">decay_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">decay_filter</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
    <span class="n">other_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="ow">not</span> <span class="n">decay_filter</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">params</span><span class="p">))</span>
    <span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">decay_params</span><span class="p">,</span>
        <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">1e-1</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">other_params</span><span class="p">,</span>
        <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">params</span>
    <span class="p">}]</span>
    <span class="k">return</span> <span class="n">group_params</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Transformer training&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--distribute&quot;</span><span class="p">,</span>
                        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;false&quot;</span><span class="p">,</span>
                        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">],</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Run distribute, default is true.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--micro_batch_num&quot;</span><span class="p">,</span>
                        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The micro batch num.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--pipeline_stage&#39;</span><span class="p">,</span>
                        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;The pipeline stage number.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--mp&#39;</span><span class="p">,</span>
                        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;The model parallel way.&#39;</span><span class="p">)</span>
    <span class="n">args_opt</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">distribute</span> <span class="o">==</span> <span class="s1">&#39;true&#39;</span><span class="p">:</span>
        <span class="n">D</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
        <span class="n">device_num</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_group_size</span><span class="p">()</span>
        <span class="n">rank_id</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
        <span class="n">dp</span> <span class="o">=</span> <span class="n">device_num</span> <span class="o">//</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">mp</span> <span class="o">//</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">pipeline_stage</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rank_id is </span><span class="si">{}</span><span class="s2">, device_num is </span><span class="si">{}</span><span class="s2">, dp is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">,</span> <span class="n">device_num</span><span class="p">,</span> <span class="n">dp</span><span class="p">))</span>
        <span class="n">gradient_accumulation_shard</span> <span class="o">=</span> <span class="n">dp</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">ms</span><span class="o">.</span><span class="n">reset_auto_parallel_context</span><span class="p">()</span>
        <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span>
               <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">loss_repeated_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">device_num</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">parallel_optimizer_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;gradient_accumulation_shard&quot;</span><span class="p">:</span> <span class="n">gradient_accumulation_shard</span><span class="p">})</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dp</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">parallel_config</span> <span class="o">=</span> <span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">pipeline_stage</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">,</span>
                                                  <span class="n">micro_batch_num</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">micro_batch_num</span><span class="p">,</span>
                                                  <span class="n">model_parallel</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">mp</span><span class="p">,</span>
                                                  <span class="n">data_parallel</span><span class="o">=</span><span class="n">dp</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">batch</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">micro_batch_num</span> <span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="k">else</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
              <span class="n">src_len</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">src_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">tgt_len</span><span class="p">,</span>
              <span class="n">vocab_size</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
              <span class="n">hidden_size</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
              <span class="n">en_layer</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">encoder_layer</span><span class="p">,</span>
              <span class="n">de_layer</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">decoder_layer</span><span class="p">,</span>
              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">,</span> <span class="n">return_loss</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenzier</span><span class="p">()</span>
    <span class="n">task</span> <span class="o">=</span> <span class="n">ToyDataset</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">file_path</span><span class="p">,</span>
                      <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
                      <span class="n">seq_length</span><span class="o">=</span><span class="p">(</span><span class="n">args_opt</span><span class="o">.</span><span class="n">src_len</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">tgt_len</span><span class="p">))</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">task</span><span class="o">.</span><span class="n">get_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">PipelineCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">micro_batch_num</span><span class="p">)</span>
        <span class="n">param</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">infer_param_pipeline_stage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;params is:</span><span class="si">{</span><span class="n">param</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">group_params</span> <span class="o">=</span> <span class="n">set_weight_decay</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_params</span> <span class="o">=</span> <span class="n">set_weight_decay</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">train</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>

    <span class="n">callback_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ckpt_config</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">callback_size</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                      <span class="n">integrated_save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
                                    <span class="n">config</span><span class="o">=</span><span class="n">ckpt_config</span><span class="p">)</span>
    <span class="n">callback</span> <span class="o">=</span> <span class="p">[</span><span class="n">TimeMonitor</span><span class="p">(</span><span class="n">callback_size</span><span class="p">),</span> <span class="n">LossMonitor</span><span class="p">(</span><span class="n">callback_size</span><span class="p">),</span> <span class="n">ckpoint_cb</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://statmt.org/wmt14/test-full.tgz">Download WMT14 En-Fr dataset</a>. If you download unsuccessfully to click the link, please try to download it after copying the link address.</p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">newstest2014-fren-ref.en.sgm</span></code> as the training set for this task, combine and clean this dataset. Extract the dataset to the <code class="docutils literal notranslate"><span class="pre">docs/sample_code/distributed_training_transformer</span></code> directory.</p>
</section>
<section id="pre-processing">
<h3>Pre-processing<a class="headerlink" href="#pre-processing" title="Permalink to this headline"></a></h3>
<p>Executing the following code to pre-process the data will generate the <code class="docutils literal notranslate"><span class="pre">output</span></code> directory in the current directory, which will produce the files <code class="docutils literal notranslate"><span class="pre">wmt14.en_fr.txt</span></code> and <code class="docutils literal notranslate"><span class="pre">wmt14.fr_en.txt</span></code>, each line of which is a sentence pair in French and English. We will use <code class="docutils literal notranslate"><span class="pre">wmt14.fr_en.txt</span></code> as the training data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">preprocess</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="configuring-the-distributed-environment-variables">
<h3>Configuring the Distributed Environment Variables<a class="headerlink" href="#configuring-the-distributed-environment-variables" title="Permalink to this headline"></a></h3>
<p>When performing distributed training in a bare-metal environment (compared to an on-cloud environment, i.e. with Ascend 910 AI processors locally), you need to configure the networking information file for the current multi-card environment. If you use Huawei cloud environment, you can skip this subsection because the cloud service itself is well configured.</p>
<p>Taking Ascend 910 AI processor as an example, a sample json configuration file for one 8-card environment is as follows. This sample names the configuration file as <code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code>. 2-card environment configuration can refer to the <code class="docutils literal notranslate"><span class="pre">rank_table_2pcs.json</span></code> file in the sample code.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;server_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;server_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;10.155.111.140&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.6&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.1.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.2.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.3.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;6&quot;</span><span class="p">},</span>
<span class="w">                </span><span class="p">{</span><span class="nt">&quot;device_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">,</span><span class="nt">&quot;device_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;192.4.27.7&quot;</span><span class="p">,</span><span class="nt">&quot;rank_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7&quot;</span><span class="p">}],</span>
<span class="w">             </span><span class="nt">&quot;host_nic_ip&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;reserve&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;status&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;completed&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Among the parameter items that need to be modified according to the actual training environment are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">server_count</span></code> represents the number of machines involved in the training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">server_id</span></code> represents IP address of the current machine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_id</span></code> represents the physical serial number of the card, i.e. the actual serial number in the machine where the card is located.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_ip</span></code> represents the IP address of the integration NIC. You can execute the command <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/etc/hccn.conf</span></code> on the current machine, and the key value of <code class="docutils literal notranslate"><span class="pre">address_x</span></code> is the NIC IP address.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_id</span></code> represents the logic serial number of card, fixed numbering from 0.</p></li>
</ul>
</section>
<section id="calling-the-collective-communication-repository">
<h3>Calling the Collective Communication Repository<a class="headerlink" href="#calling-the-collective-communication-repository" title="Permalink to this headline"></a></h3>
<p>The MindSpore distributed parallel training communication uses the Huawei Collective Communication Library <code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Collective</span> <span class="pre">Communication</span> <span class="pre">Library</span></code> (hereinafter referred to as HCCL), which can be found in the package that accompanies the Ascend AI processor. <code class="docutils literal notranslate"><span class="pre">mindspore.communication.management</span></code> encapsulates the collective communication interface provided by HCCL to facilitate user configuration of distributed information.</p>
<blockquote>
<div><p>HCCL implements multi-machine multi-card communication based on Ascend AI processor. There are some usage restrictions. We list the common ones by using distributed services, and you can check the corresponding usage documentation of HCCL for details.</p>
<ul class="simple">
<li><p>Support 1, 2, 4, 8-card device clusters in single machine scenario and 8*n-card device clusters in multi-machine scenario.</p></li>
<li><p>0-3 cards and 4-7 cards of each machine are consisted of two clusters respectively. 2-card and 4-card must be connected and do not support cross-group creation of clusters during training.</p></li>
<li><p>When building a multi-machine cluster, you need to ensure that each machine uses the same switch.</p></li>
<li><p>The server hardware architecture and operating system needs to be SMP (Symmetrical Multi-Processing) processing mode.</p></li>
</ul>
</div></blockquote>
<p>The following is sample code for calling the collection communication repository:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DEVICE_ID&quot;</span><span class="p">]))</span>
    <span class="n">init</span><span class="p">()</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>where,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=GRAPH_MODE</span></code>: Using distributed training requires specifying the run mode as graph mode (PyNative mode does not support parallelism).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_id</span></code>: The physical serial number of the card, i.e. the actual serial number in the machine where the card is located.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init</span></code>: Enables HCCL communication and completes distributed training initialization operations.</p></li>
</ul>
</section>
</section>
<section id="running-the-script">
<h2>Running the Script<a class="headerlink" href="#running-the-script" title="Permalink to this headline"></a></h2>
<p>The scripts required for training is edited above, and the corresponding scripts are called by the command.</p>
<p>The current MindSpore distributed execution uses a single-card, single-process operation, i.e., one process runs on each card, with the number of processes matching the number of used cards. In this case, card 0 is executed in the foreground, while the other cards are executed in the background. Each process creates 1 directory to store log information as well as operator compilation information. The following is an example of a distributed training script by using 8 cards to demonstrate how to run the script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># applicable to Ascend</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run.sh DATA_PATH RANK_SIZE&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run.sh /path/dataset 8&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">set</span><span class="w"> </span>-e
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="nv">$2</span>

<span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>

test_dist_8pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_8pcs.json
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>
<span class="o">}</span>

test_dist_2pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_2pcs.json
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">2</span>
<span class="o">}</span>

test_dist_<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span>pcs

<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">1</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span>
<span class="k">do</span>
<span class="w">    </span>rm<span class="w"> </span>-rf<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>mkdir<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>cp<span class="w"> </span>./train.py<span class="w"> </span>./model.py<span class="w"> </span>./dataset.py<span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device </span><span class="nv">$i</span><span class="s2">&quot;</span>
<span class="w">    </span>env<span class="w"> </span>&gt;<span class="w"> </span>env<span class="nv">$i</span>.log
<span class="w">    </span>python<span class="w"> </span>./train.py<span class="w"> </span>--distribute<span class="o">=</span><span class="nb">true</span><span class="w"> </span>--file_path<span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span><span class="w"> </span>--mp<span class="o">=</span><span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="nv">$i</span><span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>../
<span class="k">done</span>
rm<span class="w"> </span>-rf<span class="w"> </span>device0
mkdir<span class="w"> </span>device0
cp<span class="w"> </span>./train.py<span class="w"> </span>./model.py<span class="w"> </span>./dataset.py<span class="w"> </span>./device0
<span class="nb">cd</span><span class="w"> </span>./device0
<span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="m">0</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device 0&quot;</span>
env<span class="w"> </span>&gt;<span class="w"> </span>env0.log
python<span class="w"> </span>./train.py<span class="w"> </span>--distribute<span class="o">=</span><span class="nb">true</span><span class="w"> </span>--file_path<span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span><span class="w"> </span>--mp<span class="o">=</span><span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="w"> </span>&gt;<span class="w"> </span>train.log0<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-eq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;training success&quot;</span>
<span class="k">else</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;training failed&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">2</span>
<span class="k">fi</span>
<span class="nb">cd</span><span class="w"> </span>../
</pre></div>
</div>
<p>The script needs to pass in the variables <code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">RANK_SIZE</span></code>, which indicate the absolute path to the <code class="docutils literal notranslate"><span class="pre">wmt14.fr_en.txt</span></code> dataset and the number of cards, respectively.</p>
<p>Distributed-related environment variables are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>: The path to the network information file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code>: The actual serial number of the current card on the machine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_ID</span></code>: The logical serial number of the current card.</p></li>
</ul>
<p>For the rest of the environment variables, please refer to the configuration items in the <a class="reference external" href="https://www.mindspore.cn/install">Installation Tutorial</a>.</p>
<p>The running time is about 5 minutes, main part of which is spent on the compilation of the operators, and the actual training time is within 20 seconds. The user can monitor the task process by <code class="docutils literal notranslate"><span class="pre">ps</span> <span class="pre">-ef</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">python</span></code>.</p>
<p>The log files are saved to the <code class="docutils literal notranslate"><span class="pre">device0</span></code>, <code class="docutils literal notranslate"><span class="pre">device1</span></code> …… directories corresponding to <code class="docutils literal notranslate"><span class="pre">rank</span></code> directory. <code class="docutils literal notranslate"><span class="pre">env.log</span></code> records information about the environment variables, and the results about the Loss part are saved in <code class="docutils literal notranslate"><span class="pre">train.log</span></code>. The example is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 9.9034
epoch: 1 step: 2, loss is 9.9033
epoch: 1 step: 3, loss is 9.9031
epoch: 1 step: 4, loss is 9.9025
epoch: 1 step: 5, loss is 9.9022
</pre></div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h2>
<p>Distributed parallel training can significantly improve the performance of network training. From actual experiments, the performance of distributed training on Transformer 8-card exceeds 5 times that of a single card. The distributed parallelization process of the network introduces some code and configuration complexity, but the benefits are worth it compared to the performance gains.</p>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="train_cpu.html" class="btn btn-neutral float-left" title="Distributed Parallel Training Base Sample (CPU)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pangu_alpha.html" class="btn btn-neutral float-right" title="PengCheng·PanGu Model Network Multi-dimension Hydrid Parallel Analysis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>