

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Multi Dimensional &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Operator-level Parallelism" href="operator_parallel.html" />
    <link rel="prev" title="Distributed Fault Recovery" href="fault_recover.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Inference on the Ascend 310 AI Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multi Dimensional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l2"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_graph_partition.html">Distributed Graph Partition</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">Other Features</a></li>
</ul>
<p class="caption"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Multi Dimensional</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/parallel/multi_dimensional.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="multi-dimensional">
<h1>Multi Dimensional<a class="headerlink" href="#multi-dimensional" title="Permalink to this headline">¶</a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/docs/blob/r2.0/tutorials/experts/source_en/parallel/multi_dimensional.rst"><img alt="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png" /></a>
<div class="toctree-wrapper compound">
</div>
<p>As deep learning evolves, models get larger and larger. For example, in
the field of NLP, in just a few years, the amount of parameters has
developed from BERT’s 100 million to GPT-3’s 170 billion, and then to
Pangu alpha 200 billion, and the current industry has even proposed a
million billion. It can be seen that the scale of parameters has shown
an exponential growth trend in recent years. On the other hand, with the
development of related technologies in the fields of big data and the
Internet, the datasets available for model training are also rapidly
expanding, such as recommendations, natural language processing and
other scenarios of the dataset that can reach terabytes.</p>
<p>In the face of large-scale data and large-scale parameter training, a
single device either takes a long time to complete model training, or it
cannot be trained due to insufficient display memory. Therefore,
distributed training technology needs to be introduced.</p>
<p>Currently, the most commonly used distributed training technique is data
parallelism. Data parallelization splits the training data into multiple
devices, each maintaining the same model parameters and the same size of
computing tasks, but processing different data. In the process of
backpropagation, the parameter gradient generated by each device is
globally AllReduce synchronously summed. When the dataset is large and
the model is small, there is an advantage to choosing data parallelism,
such as ResNet50. However, when the model is large, or the dataset and
model are larger, other distributed features need to be used.</p>
<p>MindSpore provides the following advanced features to support
distributed training of large models, and users can flexibly combine
them according to their own needs.</p>
<div class="section" id="operator-parallel">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/operator_parallel.html">Operator Parallel</a><a class="headerlink" href="#operator-parallel" title="Permalink to this headline">¶</a></h2>
<p>Operator-level parallelism is a distributed computation of operators by
splitting their input tensors into multiple devices in units. On the one
hand, data samples and model parameters can be split into multiple
devices at the same time to complete the training of large models. On
the other hand, you can make full use of cluster resources for parallel
computing to improve the overall speed.</p>
<p>The users can set the sharding strategy of each operator in the forward
network, and the framework models each operator and its input tensor
according to the sharding strategy of the operator, so that the
computational logic of the operator remains mathematically equivalent
before and after the sharding.</p>
</div>
<div class="section" id="pipeline-parallel">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/pipeline_parallel.html">Pipeline Parallel</a><a class="headerlink" href="#pipeline-parallel" title="Permalink to this headline">¶</a></h2>
<p>When there are a large number of cluster devices, if only the operator
level is used in parallel, communication needs to be carried out on the
communication domain of the entire cluster, which may make communication
inefficient and reduce overall performance.</p>
<p>Pipeline parallel can split the neural network structure into multiple
stages, and each stage runs in a part of the device. The communication
domain of the set communication limits to this part of the device, and
the stage uses point-to-point communication.</p>
<p>The advantages of pipeline parallel are that they can improve
communication efficiency and easily handle layered neural network
structures. The disadvantage is that some nodes may be idle at the same
time.</p>
</div>
<div class="section" id="optimizer-parallel">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/pipeline_parallel.html">Optimizer Parallel</a><a class="headerlink" href="#optimizer-parallel" title="Permalink to this headline">¶</a></h2>
<p>When training in parallel with data or operators, the parameters of the
model may have the same copy on multiple devices. This allows the
optimizer to have redundant calculations across multiple devices when
updating this weight. In this case, the optimizer’s computational volume
can be spread across multiple devices through optimizer parallelism. It
has the advantage of reducing static memory consumption and reducing the
amount of computation in the optimizer. The disadvantage is that it
increases the communication overhead.</p>
</div>
<div class="section" id="host-device-training">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/host_device_training.html">Host Device Training</a><a class="headerlink" href="#host-device-training" title="Permalink to this headline">¶</a></h2>
<p>When training large models, the overall size of the model that can be
trained will be limited by the number of devices due to the limited
memory capacity of each device (accelerator). In order to complete
larger-scale model training, you can use the host and device
heterogeneous training modes. It takes advantage of both the large
memory on the host side and the fast calculation on the accelerator
side, and is an effective way to reduce the number of devices during the
training of the super-large model.</p>
</div>
<div class="section" id="recompute">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/recompute.html">Recompute</a><a class="headerlink" href="#recompute" title="Permalink to this headline">¶</a></h2>
<p>MindSpore automatically derives the reverse graph according to the
forward graph calculation process, and the forward graph and the inverse
graph together form a complete calculation graph. When calculating some
reverse operators, it may be necessary to use the calculation results of
some forward operators, resulting in the calculation results of these
forward operators, which need to reside in memory until these reverse
operators are calculated, and the memory they occupy will not be reused
by other operators. The compute results of these forward operators,
which reside in memory for a long time, push up the peak memory
footprint of the computation, especially in large-scale network models.
In order to reduce memory peaks, the recomputing technique can not save
the calculation results of the forward activation layer, so that the
memory can be reused, and then when calculating the reverse part,
recalculate the results of the forward activation layer.</p>
</div>
<div class="section" id="distributed-graph-partition">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/distributed_graph_partition.html">Distributed graph partition</a><a class="headerlink" href="#distributed-graph-partition" title="Permalink to this headline">¶</a></h2>
<p>MindSpore supports user-defined slicing of a compute graph, and MindSpore can slice any operator in the compute graph to any process according to the user parameters, making full use of the compute resources on the nodes where different processes are located, to perform distributed training and other tasks. After distributed graph partition, the execution results of compute tasks remain the same as those of a single-machine and single-card copy.</p>
</div>
<div class="section" id="description-of-the-interface-related-to-the-feature">
<h2>Description of the Interface Related to the Feature<a class="headerlink" href="#description-of-the-interface-related-to-the-feature" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 31%" />
<col style="width: 27%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p>
<p>category</p>
</th>
<th class="head"><p>Feature interface</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Auto-parallel</p></td>
<td><p>set_auto_paralle_context(search_mode=mode)</p></td>
<td><p>Specify the
policy search
algorithm, with a
value of type
string, and the
optional value:
1.
“sharding_propagation”:
indicate a policy
search by using
sharding strategy
propagation
algorithm;2.
“dynamic_programming”:
indicate the use
of dynamic
programming
algorithms for
policy search;3.
“recursive_programming”:
indicate the use
of a double
recursive
algorithm for
policy search;</p></td>
<td><p>Automatic
parallel allows
the user to
search for
sharding strategy
without
configuring or
configuring a
small number of
operators, and
the framework
searches for the
sharding
strategy.</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>set_algo_parametrs(fully_use_devces=bool_value)</p></td>
<td><p>Whether operators
need to be split
across all
devices when
setting up search
policies. Its
value is of type
bool, which
defaults to True.</p></td>
<td><p>If the operator
is split into all
devices, the
search space can
be reduced and
the search speed
can be improved,
but the search
strategy is not
globally optimal.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>set_auto_paralle_context(all_redce_fusion_configconfig)</p></td>
<td><p>Configure the
gradient
AllReduce
operator fusion
strategy with a
value of type
list. For
example: [20,
35], which means
that the first 20
AllReduces are
fused into 1, the
20th to 35th
AllReduce are
fused into 1, and
the remaining
AllReduce are
fused into 1.</p></td>
<td><p>Reduce the number
of operations of
the AllReduce
communication
operator and
improve
communication
efficiency.</p></td>
</tr>
<tr class="row-odd"><td><p>comm_fusion</p></td>
<td><p>set_auto_paralle_context(comm_fuion=config)</p></td>
<td><p>Set the fusion
configuration of
the communication
operator, and
support the
configuration of
the AllReduce,
AllGather, and
ReduceScatter
communication
operators
currently. Its
value is of type
dict, such as
comm_fusion={“allreduce”:
{“mode”: “auto”,
“config”: None}}.
There are three
options for
“mode” among
them: “auto”:
Automatically
perform operator
fusion according
to the data
volume threshold
of 64MB, and the
configuration
parameter
“config” is None.
“size”:
Communicate
operator fusion
according to the
method of
manually setting
the data volume
threshold, and
the configuration
parameter
“config” type is
int, with the
unit of MB.
“index”: Only
“allreduce”
supports
configuring
index, which
means that the
configuration
parameter
“config” type is
list according to
the way the
sequence number
of the
communication
operator is
fused. For
example: [20,
35], which means
that the first 20
AllReduces are
fused into 1, the
20th to 35th
AllReduce are
fused into 1, and
the remaining
AllReduce are
fused into 1.</p></td>
<td><p>Reduce the number
of operations of
the
AllReduce/AllGath
er/ReduceScatter
communication
operator and
improve
communication
efficiency.</p></td>
</tr>
<tr class="row-even"><td><p>Dataset
slicing</p></td>
<td><p>set_auto_parallel_context(dataset_strategy=config)</p></td>
<td><p>Configure the
sharding policy
for the dataset.
where config is
Union[str,
tuple]. When a
string is passed
in, there are two
options:
“full_batch”:
indicates that
the dataset is
not tangential,
and
“data_parallel”:
indicates that
the dataset is
sliced in
parallel with the
data. When passed
in tuple, the
content in tuple
represents the
shard() interface
of the dataset,
similar to the
premiumive
shard()
interface. if
this interface is
not called, it
defaults to the
“data_parallel”
mode.</p></td>
<td><p>When the number
of samples is
smaller than the
number of cards,
it can be
imported in the
way of
“full_batch”;
when the number
of samples is
large and the
model parameters
are small, it can
be imported in
the way of
“data_parallel”;
when the data set
is
high-resolution
image data, it
can be imported
by configuring
the tuple
sharding
strategy.</p></td>
</tr>
<tr class="row-odd"><td><p>Distributed
inference</p></td>
<td><p>infer_predict_layout(*predict_data)</p></td>
<td><p>Use inference
data to perform
precompilation,
which outputs the
splitting
information of
the operator.</p></td>
<td><p>Obtain the
sharding
information of
the ownership
weight at the
time of
inference.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>load_distributed_checkpoint(network,checkpoint_filenames,predict_strategy=None,train_strategy_filename=None)</p></td>
<td><p>Load the
distributed
weights. Each
machine needs to
pre-place the
full amount of
ckpt. where
network
represents the
inference
network,
checkpoint_filenames
represents the
checkpoint file,
predict_strategy
is the output of
the
infer_predict_layout(),
and
train_strategy_filename
is the operator
slicing strategy
information saved
during training.</p></td>
<td><p>Load distributed
weights for
distributed
inference.</p></td>
</tr>
<tr class="row-odd"><td><p>Functional
operator
sharding</p></td>
<td><p>shard(in_strategy,out_strategy=None,parameter_plan=None,device=”Ascend”,level=1)
In Cell class.</p></td>
<td><p>Set the sharding
strategy of the
input/output tensors
and key parameters
of the cell, and the
parallel strategy
of the remaining
operators is
propagated by the
sharding
strategy.
in_strategy/
out_strategy
specify the
sharding strategy
for the
input/output tensor.
parameter_plan
specifies the
sharding strategy for
key parameters.
device specifies the
execution device,
and level
specifies the
pattern of the
sharding policy
propagation
algorithm.</p></td>
<td><p>In PyNative mode,
specify that a
cell instance
executes in graph
mode, and
synchronizes the
operator-level
model according
to the specified the
strategy for
input-output and key
parameters, while
the rest of the
model is still
executed in
Python mode.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>mindspore.shard(fn,in_strategy,out_strategy=None,parameter_plan=None,device=”Ascend”,level=0)</p></td>
<td><p>The incoming fn
is a cell instance or
function. The
description of
remaining parameters
are the same as above
shard interface,
and the return value
is a function. When
this function is
called, the
operator-level
model is executed
in graph mode in
parallel.</p></td>
<td><p>This usage allows
you to specify
that a function
performs model
parallelism at
the operator
level, with the
same function as
cell’s shard
method.</p></td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="operator_parallel.html" class="btn btn-neutral float-right" title="Operator-level Parallelism" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="fault_recover.html" class="btn btn-neutral float-left" title="Distributed Fault Recovery" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>