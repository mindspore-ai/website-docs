<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Other Features &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sharding Propagation" href="sharding_propagation.html" />
    <link rel="prev" title="Distributed Resilience Training and Inference" href="resilience_train_and_predict.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Graph Compilation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Training Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Advanced Usage of Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Automatic Vectorization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Debugging and Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">Function Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r2.0/accuracy_problem_preliminary_location.html">Precision Optimization↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Distributed Parallel Training Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">Quick Start Distributed Parallel Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">Distributed Set Communication Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">Multi Dimensional</a></li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Other Features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sharding_propagation.html">Sharding Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_fusion.html">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_subgraph.html">Communication Subgraph Extraction and Reuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2"><a class="reference internal" href="pynative_shard_function_parallel.html">Functional Operator Sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="ms_operator.html">Performing Distributed Training on K8S Clusters</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">Environment Variables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Other Features</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/other_features.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="other-features">
<h1>Other Features<a class="headerlink" href="#other-features" title="Permalink to this headline"></a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/docs/blob/r2.0/tutorials/experts/source_en/parallel/other_features.rst"><img alt="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png" /></a>
<div class="toctree-wrapper compound">
</div>
<section id="sharding-propagation-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/sharding_propagation.html">Sharding Propagation</a><a class="headerlink" href="#sharding-propagation-1" title="Permalink to this headline"></a></h2>
<p>In operator-level parallelism, the user is required to configure a
slicing strategy for each operator in the forward network (if not
configured, the data-parallel policy is used by default). The slicing
strategy propagation feature can configure only a few operators to
automatically generate a feasible sharding strategy for operators
without a sharding strategy, and achieve the effect of minimizing
communication overhead.</p>
</section>
<section id="parameter-server-training-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/parameter_server_training.html">Parameter Server Training</a><a class="headerlink" href="#parameter-server-training-1" title="Permalink to this headline"></a></h2>
<p>Parameter Server is a widely used architecture in distributed training,
which has better flexibility, scalability, and node disaster tolerance
than the AllReduce training method of data parallel synchronization. The
parameter server supports both synchronous SGD (Stochastic Gradient
Descent) and asynchronous SGD training algorithms. In terms of
scalability, the calculation of the model and the update of the model
are deployed in the worker and server processes respectively, so that
the resources of the worker and server can be scaled horizontally
independently (adding or removing the worker and server resources). In
addition, in the environment of large-scale data centers, computing
equipment, networks and storage often have various failures that lead to
some node abnormalities, and under the architecture of parameter
servers, such failures can be easily handled without affecting the tasks
in training.</p>
</section>
<section id="communication-operator-fusion-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/comm_fusion.html">Communication Operator Fusion</a><a class="headerlink" href="#communication-operator-fusion-1" title="Permalink to this headline"></a></h2>
<p>In the distributed training scenario, cross-device or even cross-node
data transmission is a bottleneck that restricts scalability and
computing power utilization. Communication operator fusion is an
important method to improve the utilization of network resources and
accelerate the efficiency of data transmission, which packages the
communication operators of the same source node and the destination node
and executes them at the same time to avoid the additional overhead
caused by multiple single operator execution.</p>
</section>
<section id="communication-subgraph-extraction-and-reuse-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/comm_subgraph.html">Communication Subgraph Extraction and Reuse</a><a class="headerlink" href="#communication-subgraph-extraction-and-reuse-1" title="Permalink to this headline"></a></h2>
<p>In distributed training, as the model size increases, the number of communication operators required also rises significantly. On one hand, it will boost the communication time in model compilation; on the other hand, it will consume a large amount of streams, and when the required number of streams exceeds the hardware limit, the model cannot scale even more, thus becoming a bottleneck in the development of large models. By classifying communication operators, extracting communication subgraphs for operators of the same class and reusing these extracted subgraphs, we can reduce the number of communication operators in the graph compilation. It will decrease communication time and require less streams so that the model size can further expand.</p>
</section>
<section id="dataset-slicing-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/dataset_slice.html">Dataset Slicing</a><a class="headerlink" href="#dataset-slicing-1" title="Permalink to this headline"></a></h2>
<p>When doing distributed training, you need to import the training dataset
to each device. There are two common ways to import: 1) Import in
parallel with the data, that is, the data is split into match
dimensions, and each device is imported as part; 2) Import full amount
of data per device. In addition, when some dimensions of the data are
particularly large (such as the H/W dimension of the remote sensing
picture may be particularly large), even if the sample size is small,
the picture needs to be split, that is, the data is split in the H/W
dimension, and each device reads a part of the picture. This special
performance supports splitting datasets into specific dimensions to meet
training requirements in the field of large-format image processing.</p>
</section>
<section id="functional-operator-splitting-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/pynative_shard_function_parallel.html">Functional Operator Splitting</a><a class="headerlink" href="#functional-operator-splitting-1" title="Permalink to this headline"></a></h2>
<p>In dynamic graph mode, you specify that a part of the network structure
executes in graph mode and performs various parallel operations.</p>
</section>
<section id="performing-distributed-training-on-k8s-clusters-1">
<h2><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/ms_operator.html">Performing Distributed Training on K8S Clusters</a><a class="headerlink" href="#performing-distributed-training-on-k8s-clusters-1" title="Permalink to this headline"></a></h2>
<p>MindSpore Operator is a plugin that follows Kubernetes’ Operator pattern
(based on the CRD-Custom Resource Definition feature) and implements
distributed training on Kubernetes. MindSpore Operator defines
Scheduler, PS, worker three roles in CRD, and users can easily use
MindSpore on K8S for distributed training through simple YAML file
configuration. The code repository of mindSpore Operator is described
in: <a class="reference external" href="https://gitee.com/mindspore/ms-operator/">ms-operator</a>.</p>
</section>
<section id="description-of-the-interface-related-to-the-feature">
<h2>Description of the Interface Related to the Feature<a class="headerlink" href="#description-of-the-interface-related-to-the-feature" title="Permalink to this headline"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 30%" />
<col style="width: 26%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p>
<p>category</p>
</th>
<th class="head"><p>Feature interface</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>operator
parallel</p></td>
<td><p>shard(in_strategy=None,out_strategy=None)In Primitive class</p></td>
<td><p>Set the sharding
strategy of the
input and output
tensors of the
operator (where
the sharding
strategy of the
output tensor
only supports
some operators,
such as Gauther
and MatMul.)</p></td>
<td><p>Reduce the memory
capacity of a
single device by
slicing the
tensor involved
in each operator
in the network
model to complete
the large model
training/inferenc
e.
Or use cluster
resources to
perform
distributed
computing to
reduce the
overall execution
time.</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>add_prim_attr(name, value)In Primitive class</p></td>
<td><p>Gather
Operator:add_prim_attr(“manual_split”,
config):
Configure anon-uniformsharding strategy
for its first
input, where
config type is
tuple, which
describes how the
first parameter,
dimension 0, is
split. For
example , ( 10 ,20 , 30 , 4 )
means that the
0th dimension of
the first input
of the operator
is tangent into 4
parts , and the
shape size of
each part is 10 ,
20 , 30 , 4,
respectively.</p></td>
<td><p>In the
recommended
field, there is a
scene where each
column of the
dataset
corresponds to a
subtable. In this
scenario, using
this
configuration can
reduce traffic
and improve
overall
performance.</p></td>
</tr>
<tr class="row-even"><td></td>
<td></td>
<td><p>EmbeddingLookUp
Operator:add_prim_attr(“primitive_target”,
“CPU”): Configure
it to execute on
the CPU for
heterogeneous
scenarios.</p></td>
<td><p>In the
recommended
field, there is a
particularly
large scene of
the Embedding
Table, in order
to save device
memory, you can
use this
configuration to
put
EmbeddingLookUp
on the CPU to
execute to
complete the
training of the
recommended large
model.</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>set_auto_parallel_context(enable_alltoall=bool_value)</p></td>
<td><p>Indicate whether
the AllToAll
communication
operator is
allowed to be
generated when
communicating,
and its value is
the bool type,
which defaults to
False.</p></td>
<td><p>AllToAll
communication can
reduce the amount
of communication
data and improve
communication
efficiency, but
it requires
environmental
support.</p></td>
</tr>
<tr class="row-even"><td><p>Pipeline</p>
<p>parallel</p>
</td>
<td><p>set_auto_parallel_context(pipeline_stages=stage_num)</p></td>
<td><p>Set the number of
pipes in pipeline
parallelism, the
value of which is
a positive
integer, and the
value range is
[1, number of
devices].</p></td>
<td><p>Specify the
number of stages,
limiting the
communication
domain of the
collection
communication to
the stage, and
the
point-to-point
communication
between the
stages.</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>pipeline_stage(value) In Cell class</p></td>
<td><p>Set which stage
the Cell executes
in.</p></td>
<td><p>Set which stage
the Cell executes
in.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>PipelineCell(network, micro_size)</p></td>
<td><p>Specify the
number of
MicroSizes for
the training
network, where
the network is
the network to be
trained and the
micro_size is a
positive integer.</p></td>
<td><p>Specify
micro_size can
reduce the idle
wait time between
stages and
improve the
overall
efficiency of
pipeline
parallel.</p></td>
</tr>
<tr class="row-odd"><td><p>Optimizer</p>
<p>parallel</p>
</td>
<td><p>set_auto_parallel_context(enable_parallel_optimizer=bool_value)</p></td>
<td><p>Indicate whether
optimizer
parallelism is
enabled. Its
value is bool
type, and the
default is False.</p></td>
<td><p>Optimizer
parallel saves
static memory
overhead, but
increases
communication
overhead.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>set_auto_parallel_context(parallel_optimizer_config=config)</p></td>
<td><p>This
configuration
takes effect only
after optimizer
parallel is
turned on. The
config is a dict
and supports two
key values:
gradient_accumulation_shard(bool):
If True, the
cumulative
gradient variable
will be sharded
on the data
parallelism,
defaulting to
False.parallel_optimizer_threshold(int):
This value
represents the
optimizer
sharding
threshold in KB
(default value is
64KB). When the
parameter size
does not exceedthis value, it
will not be
split.</p></td>
<td><p>gradient_accumulation_shard
true saves a
portion of the
parameter size of
static memory,
but increases
communication
overhead.
Optimizer
sharding
thresholds allow
smaller shape
parameters to be
not optimized for
splitting to save
communication
resources.</p></td>
</tr>
<tr class="row-odd"><td><p>Recompute</p></td>
<td><p>recompute(mode=True)In primitive class</p></td>
<td><p>Used to specify
whether the
operator needs to
be recalculated,
and its value is
bool type, which
defaults to True
and means that
the operator
recalculation is
enabled.</p></td>
<td><p>After enabling
operator
recalculation,
you can reduce
the peak of
dynamic memory,
but increase the
overall
computation
amount.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>recompute(**kwargs) In Cell class</p></td>
<td><p>When this
interface is
called, the
operator in this
Cell is
recalculated.The
input parameter
has two bool
class
options:mp_comm_recompute:
Whether to enable
model parallel
communication
operator
recalculation,
and the default
is
True.parallel_optimizer_comm_recompute:
Whether to enable
optimizer
parallel
communication
operator
recompute, and
the default is
False.</p></td>
<td><p>Enable Cell
recompute and
configure whether
the model
parallel
communication
operator and the
optimizer
parallel
communication
operator are
recomputed. When
the communication
operator is
recomputed, it
consumes
communication
resources but
reduces the peak
of dynamic
memory.</p></td>
</tr>
<tr class="row-odd"><td><p>Communication SubgraphExtraction and Reuse</p></td>
<td><p>export MS_COMM_COMPILER_OPT=integer_value</p></td>
<td><p>Specify the maximum number of communication operators that can be replaced by corresponding communication subgraph. It can be set to -1 or a positive value. -1 means thatthe default value will be used.</p></td>
<td><p>This can decreasethe number of commucation operators in the graph compilation, hencerequire less streams and decreasethe communication time, improvingcompilation performance.</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="resilience_train_and_predict.html" class="btn btn-neutral float-left" title="Distributed Resilience Training and Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sharding_propagation.html" class="btn btn-neutral float-right" title="Sharding Propagation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>