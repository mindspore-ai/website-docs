<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Graph Partition &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Fault Recovery" href="fault_recover.html" />
    <link rel="prev" title="Dataset Slicing" href="dataset_slice.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Static Graph Usage Sepecifications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">Process Control Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">Calling the Custom Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">Dependency Control</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">Distributed Basic Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">Pipeline Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pangu_alpha.html">PengCheng·PanGu Model Network Multi-dimension Hybrid Parallel Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_fusion.html">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_subgraph.html">Communication Subgraph Extraction and Reuse</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Graph Partition</a></li>
<li class="toctree-l2"><a class="reference internal" href="fault_recover.html">Distributed Fault Recovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="resilience_train_and_predict.html">Distributed Resilience Training and Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="save_load.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="sharding_propagation.html">Sharding Propagation</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r2.1/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">Compiling Performance Optimization for Static Graph Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">Adaptive Gradient Summation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">Dimension Reduction Training Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_case.html">Distributed High-Level Configuration Case</a> &raquo;</li>
      <li>Distributed Graph Partition</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/distributed_graph_partition.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-graph-partition">
<h1>Distributed Graph Partition<a class="headerlink" href="#distributed-graph-partition" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.1/tutorials/experts/source_en/parallel/distributed_graph_partition.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In large model training tasks, users often use various types of parallel algorithms to distribute computational tasks to various nodes, to make full use of computational resources, e.g., through MindSpore <code class="docutils literal notranslate"><span class="pre">operator-level</span> <span class="pre">parallelism</span></code>, <code class="docutils literal notranslate"><span class="pre">pipeline</span> <span class="pre">parallelism</span></code> and other features. However, in some scenarios, the user needs to slice the graph into multiple subgraphs based on a custom algorithm and distribute them to different processes for distributed execution. MindSpore <code class="docutils literal notranslate"><span class="pre">distributed</span> <span class="pre">graph</span> <span class="pre">partition</span></code> feature provides an operator-granularity Python layer API to allow users to freely perform graph slicing and build distributed training/inference tasks based on custom algorithms.</p>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>Distributed tasks need to be executed in a cluster. MindSpore reuses built-in <code class="docutils literal notranslate"><span class="pre">Dynamic</span> <span class="pre">Networking</span></code> module of MindSpore in order to have better scalability and reliability in distributed graph partition scenarios. This module is also used in the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.1/parallel/train_gpu.html#training-without-relying-on-openmpi">Training without relying on OpenMPI</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.1/parallel/parameter_server_training.html">Parameter Server mode</a> sections.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">distributed</span> <span class="pre">graph</span> <span class="pre">partition</span></code>, each process represents a compute node (called <code class="docutils literal notranslate"><span class="pre">Worker</span></code>), and the scheduling node (called <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>) started by the <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">networking</span></code> module mentioned above allows discovering each compute node and thus forming a compute cluster.</p>
<p>After <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">networking</span></code>, MindSpore assigns <code class="docutils literal notranslate"><span class="pre">role</span></code> and <code class="docutils literal notranslate"><span class="pre">rank</span></code>, the <code class="docutils literal notranslate"><span class="pre">role</span></code> and <code class="docutils literal notranslate"><span class="pre">id</span></code> of each process, based on the user startup configuration, both of which form a unique <code class="docutils literal notranslate"><span class="pre">tag</span></code> for each process and are input to the Python layer API <code class="docutils literal notranslate"><span class="pre">place</span></code>. With the correspondence, the user can set process labels for any operator by calling the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface, and the MindSpore graph compiler module processes it to slice the computed graph into multiple subgraphs for distribution to different processes for execution. For the detailed use, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/ops/mindspore.ops.Primitive.html#mindspore.ops.Primitive.place">Primitive.place</a> and <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.place">Cell.place</a> interface document.
As an example, the compute topology after distributed graph partition might be as follows:</p>
<p><img alt="image" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/tutorials/experts/source_zh_cn/parallel/images/distributed_graph_partition.png" /></p>
<p>As shown in the figure above, each <code class="docutils literal notranslate"><span class="pre">Worker</span></code> has a portion of subgraphs that have been sliced by the user, with their own weights and inputs, and each <code class="docutils literal notranslate"><span class="pre">Worker</span></code> interacts with each other through the built-in <code class="docutils literal notranslate"><span class="pre">Rpc</span> <span class="pre">communication</span> <span class="pre">operator</span></code> for data interaction.</p>
<p>To ensure ease of use and user-friendliness, MindSpore also supports users to start dynamic networking and distributed training (without distinguishing between <code class="docutils literal notranslate"><span class="pre">Worker</span></code> and <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>) with a few modifications to a single script. See the following operation practice section for more details.</p>
</section>
<section id="operation-practice">
<h2>Operation Practice<a class="headerlink" href="#operation-practice" title="Permalink to this headline"></a></h2>
<p>Taking LeNet training on GPU based on MNIST dataset as an example, different parts of the graph in the training task are split to different computational nodes for execution. You can download complete code here: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.1/docs/sample_code/distributed_graph_partition">https://gitee.com/mindspore/docs/tree/r2.1/docs/sample_code/distributed_graph_partition</a>.</p>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>distributed_graph_partition/
├── lenet.py
├── run.sh
└── train.py
</pre></div>
</div>
<blockquote>
<div><p>This tutorial does not involve starting across physical nodes, where all processes are on the same node. For MindSpore, there is no difference in the implementation of intra-node and cross-node distributed graph partition: after dynamic networking, graph slicing, and graph compilation processes, data interaction is performed via Rpc communication operators.</p>
</div></blockquote>
<section id="preparation-for-training-python-script">
<h3>Preparation for Training Python Script<a class="headerlink" href="#preparation-for-training-python-script" title="Permalink to this headline"></a></h3>
<p>Refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.1/research/cv/lenet">https://gitee.com/mindspore/models/tree/r2.1/research/cv/lenet</a>, using the <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>, to learn how to train a LeNet network. Sample code for each part of the training script is given below according to steps.</p>
<section id="loading-the-dataset">
<h4>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">C</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">CV</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.vision</span> <span class="kn">import</span> <span class="n">Inter</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span>
                   <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    create dataset for train or test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># define dataset</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">rescale_nml</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">0.3081</span>
    <span class="n">shift_nml</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="mf">0.1307</span> <span class="o">/</span> <span class="mf">0.3081</span>

    <span class="c1"># define map operations</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">Inter</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">)</span>  <span class="c1"># Bilinear mode</span>
    <span class="n">rescale_nml_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale_nml</span><span class="p">,</span> <span class="n">shift_nml</span><span class="p">)</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">hwc2chw_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># apply map operations on images</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">rescale_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">rescale_nml_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">hwc2chw_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>

    <span class="c1"># apply DatasetOps</span>
    <span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mnist_ds</span>
</pre></div>
</div>
<p>The above code creates the MNIST dataset.</p>
</section>
<section id="constructing-the-lenet-network">
<h4>Constructing the LeNet Network<a class="headerlink" href="#constructing-the-lenet-network" title="Permalink to this headline"></a></h4>
<p>In order to make slicing to a single-machine single-card task, we need to first construct a single-machine single-card copy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">TruncatedNormal</span>

<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for conv layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                     <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                     <span class="n">weight_init</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fc_with_initialize</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for fc layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">weight_variable</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span> <span class="o">=</span> <span class="n">num_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="calling-the-interface-for-distributed-graph-partition">
<h4>Calling the Interface for Distributed Graph Partition<a class="headerlink" href="#calling-the-interface-for-distributed-graph-partition" title="Permalink to this headline"></a></h4>
<p>For this training task we slice <code class="docutils literal notranslate"><span class="pre">fc1</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">fc2</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">fc3</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">conv1</span></code> to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">3</span></code>, and conv2 to process <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">4</span></code>.</p>
<p>Distributed graph partition can be completed by adding the following graph partition statement to the <code class="docutils literal notranslate"><span class="pre">LeNet.__init__</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="o">...</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>The first parameter of the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface, <code class="docutils literal notranslate"><span class="pre">role</span></code>, is the process role, and the second parameter is the process <code class="docutils literal notranslate"><span class="pre">rank</span></code>, which means that the operator is executed on a process of such role. Currently the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface only supports the <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code> role, which represents the <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">X</span></code> process described above.</p>
<p>It follows that a user can quickly implement a distributed training task by simply describing a custom algorithm through a single-machine copy and then setting the label of the compute node where the operator is located through the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface. The advantages of this approach are:</p>
<p><strong>1.Instead of writing separate execution scripts for each compute node, users can execute distributed tasks with just one script, MindSpore.</strong>
<strong>2.Provides a more general and user-friendly interface. Through the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface, users can intuitively describe their distributed training algorithms</strong></p>
</section>
<section id="defining-the-optimizer-and-loss-function">
<h4>Defining the Optimizer and Loss Function<a class="headerlink" href="#defining-the-optimizer-and-loss-function" title="Permalink to this headline"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">mom_optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mom_optimizer</span>
<span class="k">def</span> <span class="nf">get_loss</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="executing-the-training-code">
<h4>Executing the Training Code<a class="headerlink" href="#executing-the-training-code" title="Permalink to this headline"></a></h4>
<p>The entry script train.py of training code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">set_seed</span>
<span class="kn">from</span> <span class="nn">mindspore.train.metrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>


<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">get_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================= Start training =================&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">),</span> <span class="s1">&#39;train&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">TimeMonitor</span><span class="p">()],</span><span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================= Start testing =================&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">),</span> <span class="s1">&#39;test&#39;</span><span class="p">))</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">if</span> <span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy is:&quot;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code trains first and then infers, where all processes are executed in a distributed manner.</p>
<ul class="simple">
<li><p>In a distributed graph partition scenario, the user must call <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code>, an interface that is the entry point for MindSpore <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">networking</span></code> module, which is used to help form compute clusters, and initialize communication operators. If not called, MindSpore performs single-machine single-card training, i.e. the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface will not take effect.</p></li>
<li><p>Since only some subgraphs are executed on some processes, their inference precision or loss is not meaningful. The user only needs to focus on the precision on <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">0</span></code>.</p></li>
</ul>
</section>
</section>
<section id="preparation-for-training-shell-script">
<h3>Preparation for Training Shell Script<a class="headerlink" href="#preparation-for-training-shell-script" title="Permalink to this headline"></a></h3>
<section id="starting-scheduler-and-worker-processes">
<h4>Starting Scheduler and Worker Processes<a class="headerlink" href="#starting-scheduler-and-worker-processes" title="Permalink to this headline"></a></h4>
<p>Since multiple processes are started within a node, only one <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> process and multiple <code class="docutils literal notranslate"><span class="pre">Worker</span></code> processes need to be started via a Shell script. This dynamic networking is also described in detail and used similarly in these two sections: <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.1/parallel/train_gpu.html#training-without-relying-on-openmpi">Training without relying on OpenMPI</a> and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.1/parallel/parameter_server_training.html">Parameter Server mode</a>. For the meaning and usage of environment variables in the script, you can refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.1/parallel/parameter_server_training.html">Parameter Server mode</a> section.</p>
<p>The run.sh execution script is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">execute_path</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="nv">self_path</span><span class="o">=</span><span class="k">$(</span>dirname<span class="w"> </span><span class="nv">$0</span><span class="k">)</span>

<span class="c1"># Set public environment.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">5</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>

<span class="c1"># Launch scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/
mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/
<span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
python<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/../train.py<span class="w"> </span>&gt;<span class="w"> </span>sched.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="nv">sched_pid</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$!</span><span class="sb">`</span>


<span class="c1"># Launch workers.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="nv">worker_pids</span><span class="o">=()</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="nv">$MS_WORKER_NUM</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">  </span>rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/
<span class="w">  </span>mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
<span class="w">  </span>python<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/../train.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">  </span>worker_pids<span class="o">[</span><span class="si">${</span><span class="nv">i</span><span class="si">}</span><span class="o">]=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$!</span><span class="sb">`</span>
<span class="k">done</span>

<span class="c1"># Wait for workers to exit.</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span><span class="w"> </span>i&lt;<span class="si">${</span><span class="nv">MS_WORKER_NUM</span><span class="si">}</span><span class="p">;</span><span class="w"> </span>i++<span class="o">))</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span><span class="nb">wait</span><span class="w"> </span><span class="si">${</span><span class="nv">worker_pids</span><span class="p">[i]</span><span class="si">}</span>
<span class="w">  </span><span class="nv">status</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$?</span><span class="sb">`</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">      </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;[ERROR] train failed. Failed to wait worker_{</span><span class="nv">$i</span><span class="s2">}, status: </span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">      </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="k">fi</span>
<span class="k">done</span>

<span class="c1"># Wait for scheduler to exit.</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">  </span><span class="nb">wait</span><span class="w"> </span><span class="si">${</span><span class="nv">sched_pid</span><span class="si">}</span>
<span class="w">  </span><span class="nv">status</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$?</span><span class="sb">`</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;[ERROR] train failed. Failed to wait scheduler, status: </span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="k">fi</span>
<span class="k">fi</span>

<span class="nb">exit</span><span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
<p>In the above script, <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_WORKER_NUM=5</span></code> means that <code class="docutils literal notranslate"><span class="pre">5</span></code> <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code> processes need to be started for this distributed execution. <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_SCHED_HOST=127.0.0.1</span></code> means the address of <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> is <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>. <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_SCHED_PORT=8118</span></code> means the open port of <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> is 8118, and all processes will connect to this port for dynamic networking.</p>
<p>The above environment variables are exported for both <code class="docutils literal notranslate"><span class="pre">Worker</span></code> and <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> processes, and then the corresponding roles are exported separately to start the processes for the corresponding roles: after <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_ROLE=MS_SCHED</span></code>, start the Scheduler process, and after <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_ROLE=MS_WORKER</span></code>, start the MS_WORKER_NUM worker process cyclically.</p>
<blockquote>
<div><p>Note that each process is executed in the background, so there will be a wait for the process to exit statement at the end of the script.</p>
</div></blockquote>
<p>Executing instruction</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh<span class="w"> </span><span class="o">[</span>MNIST_DATA_PATH<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="viewing-the-execution-result">
<h3>Viewing the Execution Result<a class="headerlink" href="#viewing-the-execution-result" title="Permalink to this headline"></a></h3>
<p>View accuracy execution instruction:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>grep<span class="w"> </span>-rn<span class="w"> </span><span class="s2">&quot;Accuracy&quot;</span><span class="w"> </span>*/*.log
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Accuracy is: {&#39;Accuracy&#39;: 0.9888822115384616}
</pre></div>
</div>
<p>It shows that distributed graph Partition has no effect on LeNet training and inference results.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline"></a></h2>
<p>MindSpore distributed graph partition feature provides users with <code class="docutils literal notranslate"><span class="pre">place</span></code> interface for operator granularity, which supports users to slice the compute graph according to custom algorithms and perform distributed training in various scenarios through dynamic networking.
The user only needs to make the following modifies to the single-machine single-card copy to initiate the task:</p>
<ul class="simple">
<li><p>Call the <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code> interface at the top of the single-machine single-card copy to start dynamic networking.</p></li>
<li><p>Call the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface of <code class="docutils literal notranslate"><span class="pre">ops.Primitive</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> for the operator or layer in the graph, based on the user algorithm, to set the label of the process where the operator is located.</p></li>
<li><p>Start <code class="docutils literal notranslate"><span class="pre">Worker</span></code> and <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> processes via shell scripts to execute distributed tasks.</p></li>
</ul>
<p>At present, the <code class="docutils literal notranslate"><span class="pre">place</span></code> interface is in a limited state of support, and advanced usage is in the development stage. Users are welcome to make various comments or issues on the MindSpore website.
The <code class="docutils literal notranslate"><span class="pre">place</span></code> interface currently has the following limitations:</p>
<ul class="simple">
<li><p>The input parameter <code class="docutils literal notranslate"><span class="pre">role</span></code> is only supported to be set to <code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code>. This is because each node in the distributed graph partition scenario is a compute node <code class="docutils literal notranslate"><span class="pre">Worker</span></code>, and setting other roles is not required for now.</p></li>
<li><p>Unable to mix with Parameter Server, Data Parallelism, and Auto Parallelism. The compute graph of each process after distributed graph partition is not consistent, and all three features have copies of inter-process graphs or operators that may cause unknown errors when executed overlaid with this feature. Mixed-use features will be supported in subsequent releases.</p></li>
<li><p>Control flow + distributed graph partition is in a limited support state and errors may be reported. This scenario will also be supported in subsequent releases.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">place</span></code> interface is not supported in <code class="docutils literal notranslate"><span class="pre">Pynative</span></code> mode.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dataset_slice.html" class="btn btn-neutral float-left" title="Dataset Slicing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="fault_recover.html" class="btn btn-neutral float-right" title="Distributed Fault Recovery" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>