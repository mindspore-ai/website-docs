<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Functional Operator Shading &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed High-Level Configuration Case" href="distributed_case.html" />
    <link rel="prev" title="Distributed Graph Partition" href="distributed_graph_partition.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="others.html">Experimental Characteristics</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="distributed_graph_partition.html">Distributed Graph Partition</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Functional Operator Shading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r2.2/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="others.html">Experimental Characteristics</a> &raquo;</li>
      <li>Functional Operator Shading</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/pynative_shard_function_parallel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="functional-operator-shading">
<h1>Functional Operator Shading<a class="headerlink" href="#functional-operator-shading" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/tutorials/experts/source_en/parallel/pynative_shard_function_parallel.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Dynamic graphs support richer syntax and more flexible use, but currently MindSpore dynamic graph mode does not support the various features of automatic parallel. We have designed the shard function to support specify a part of the graph mode execution, and perform various parallel operations in the dynamic graph mode.</p>
<blockquote>
<div><p>Currently, functional operator slicing is only supported in parallel mode “auto_parallel” and strategy search algorithm “sharding_propagation”.</p>
</div></blockquote>
<p>Related interfaces:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_strategy</span><span class="p">,</span> <span class="n">out_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parameter_plan</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">shard_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_strategy</span><span class="p">,</span> <span class="n">out_strategy</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">in_strategy(tuple)</span></code>: Specify the sharding strategy of the input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, each element is a tuple indicating the sharding strategy of the corresponding input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, the length of each tuple should be equal to the dimension of the corresponding <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, indicating how each dimension is sliced. You can pass in <code class="docutils literal notranslate"><span class="pre">None</span></code>, the corresponding sharding strategy will be automatically deduced and generated.</p>
<p><code class="docutils literal notranslate"><span class="pre">out_strategy(None,</span> <span class="pre">tuple)</span></code>: Specify the sharding strategy for the output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, used in the same way as <code class="docutils literal notranslate"><span class="pre">in_strategy</span></code>, with a default value of None, which is not yet enabled and will be opened later. In deep learning models, the output strategy is replaced with data parallel (False) and repeated computation (True) depending on the value of full_batch.</p>
<p><code class="docutils literal notranslate"><span class="pre">parameter_plan(None,</span> <span class="pre">dict)</span></code>: Specify the sharding strategy of each parameter, when passed into the dictionary, the key is the parameter name of type str, the value is a one-dimensional integer tuple indicating the corresponding sharding strategy. If the parameter name is wrong or the corresponding parameter has already set the sharding strategy, the setting of this parameter will be skipped. Default: None, means not set.</p>
<p><code class="docutils literal notranslate"><span class="pre">device(string)</span></code>: Specify the device for execution, optional range <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPU</span></code> and <code class="docutils literal notranslate"><span class="pre">CPU</span></code>, default is <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, which is not enabled yet, and will be opened later.</p>
<p><code class="docutils literal notranslate"><span class="pre">level(int)</span></code>: Specify the search strategy for all operators, the cut strategy for input and output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is specified by the user, and the cut strategy for the rest of operators will be obtained by the framework search, this parameter specifies the objective function for searching, with the optional range of 0, 1, 2, which represent maximizing the computational communication ratio, minimizing memory consumption, and maximizing the operation speed. The default is 0, which is currently not yet enabled, and will be opened later.</p>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>in the MindSpore dynamic graph mode, you can use the <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code> decorator to specify a certain section to be compiled and executed in graph mode. In the forward execution at the same time, execute while the operators, subgraphs will be recorded, the forward execution is complete, will be automatically differentiated to get the whole graph to get the inverse graph. The specific process is shown in the following figure:</p>
<p><em>Figure 1: Schematic diagram of the execution of the &#64;jit decorator</em></p>
<p>The Shard function follows this pattern, with the difference that operator-level model parallel can be performed at the link where the graph pattern is compiled and executed.</p>
</section>
<section id="operation-practice">
<h2>Operation Practice<a class="headerlink" href="#operation-practice" title="Permalink to this headline"></a></h2>
<section id="example-code-description">
<h3>Example Code Description<a class="headerlink" href="#example-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>You can download the complete the example code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.2/docs/sample_code/pynative_shard_function_parallel">https://gitee.com/mindspore/docs/tree/r2.2/docs/sample_code/pynative_shard_function_parallel</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─shard_function_parallel
        ├── rank_table_8pcs.json
        ├── run_shard_function_example.sh
        ├── run_mpirun_shard_function_example.sh
        └── shard_function_example.py
</pre></div>
</div>
<p>The function of each file is as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">shard_function_example.py</span></code>: Sample code for the shard function, which describes how to use the shard function to specify part of the code to be executed in parallel.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_table_8pcs.json</span></code>: 8-card profile for RANK_TABLE_FILE.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_shard_function_example.sh</span></code>: Startup script for shard function example.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_mpirun_shard_function_example.sh</span></code>: shard function example startup script launched with mpirun.</p></li>
</ul>
</section>
<section id="importing-the-relevant-packages-and-setting-the-execution-mode">
<h3>Importing the Relevant Packages and Setting the Execution Mode<a class="headerlink" href="#importing-the-relevant-packages-and-setting-the-execution-mode" title="Permalink to this headline"></a></h3>
<p>As mentioned earlier, the shard function will execute a certain part of the operator-level model in graph mode in parallel with the dynamic graph mode, so you need to set the mode to PyNative when using the shard function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span>
                             <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="specifying-output-arrangement">
<h3>Specifying Output Arrangement<a class="headerlink" href="#specifying-output-arrangement" title="Permalink to this headline"></a></h3>
<p>Specifying the output arrangement for data parallel and repeated calculations is currently supported, and can be controlled by the <code class="docutils literal notranslate"><span class="pre">dataset_strategy</span></code> or <code class="docutils literal notranslate"><span class="pre">full_batch</span></code> attributes in auto_parallel_context, which are set as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set via dataset_strategy, recommended</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span>  <span class="c1"># The dataset is not sliced and the output tensor of the shard is not sliced; (default configuration)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">)</span>  <span class="c1"># The dataset is sliced in a data-parallel fashion, and the output tensor of the shard is also sliced in a data-parallel fashion</span>

<span class="c1"># This attribute is about to be deprecated through the full_batch setting</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>   <span class="c1"># The dataset is not sliced and the output tensor of the shard is not sliced; (default configuration)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># The dataset is sliced in a data-parallel fashion, and the output tensor of the shard is also sliced in a data-parallel fashion</span>
</pre></div>
</div>
</section>
<section id="cell-uses-functional-sharding">
<h3>Cell Uses Functional Sharding<a class="headerlink" href="#cell-uses-functional-sharding" title="Permalink to this headline"></a></h3>
<p>There are currently two ways to use the shard function, using the following network as an example to introduce the use of the shard function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block3</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Here all blocks are executed in PyNative mode</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<ul>
<li><p>Self-call via Cell member method <code class="docutils literal notranslate"><span class="pre">shard</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net1</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block1 is executed in graph mode</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block2 and block3 are executed in PyNative mode</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net1</span><span class="p">()</span>
<span class="c1"># Slicing along the second dimension of the input makes the output into data parallel arrangement</span>
<span class="n">net</span><span class="o">.</span><span class="n">block1</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),),</span> <span class="n">parameter_plan</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;self.block1.dense2.weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)})</span>
</pre></div>
</div>
</li>
<li><p>Using the functional interface <code class="docutils literal notranslate"><span class="pre">mindspore.shard</span></code>. Since the return value of the <code class="docutils literal notranslate"><span class="pre">shard</span></code> function is a function, you can’t assign an instance of a class that has already been instantiated to the return value of <code class="docutils literal notranslate"><span class="pre">shard</span></code> when using the functional interface, because MindSpore doesn’t support assigning class instances to other types.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NetError</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),),</span>
                                <span class="n">parameter_plan</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;self.block1.dense2.weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)})</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Such an execution will encounter an error:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: For &#39;Cell&#39;, the type of block1 should be cell, but got function.
</pre></div>
</div>
<p>The right usage is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net2</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Set the return value of the Cell instance after passing it through ms.shard to a different name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1_graph</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),),</span>
                                      <span class="n">parameter_plan</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;self.block1.dense2.weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># block1 is executed in graph mode and along the first dimensional slice</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1_graph</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block2 is also executed in graph mode</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block3 is executed in PyNative mode</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="function-uses-functional-sharding">
<h3>Function Uses Functional Sharding<a class="headerlink" href="#function-uses-functional-sharding" title="Permalink to this headline"></a></h3>
<p>function can be functionally sliced using <code class="docutils literal notranslate"><span class="pre">mindspore.shard</span></code>. Using the matmul+bias_add+relu function as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span> <span class="c1"># This is an example where the dataset is not sliced and the output tensor of the shard is not sliced.</span>

<span class="k">def</span> <span class="nf">dense_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Specify the sharding strategy for x to be (4, 2), and the weight and bias sharding strategies to be set to None via in_strategy to indicate automatic derivation generation.</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">dense_relu</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;result.shape:&#39;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>Note that the initialization of parameters depends on Cell parameter management, and when the fn type passed in shard is function, its definition should not contain parameters (e.g., operations such as Conv2D, Dense, etc.).</p>
</div></blockquote>
</section>
<section id="running-the-code">
<h3>Running the Code<a class="headerlink" href="#running-the-code" title="Permalink to this headline"></a></h3>
<p>Currently MindSpore can pull up distributed parallel tasks through both multi-process startup and mpirun.</p>
<section id="multi-process-startup">
<h4>Multi-process Startup<a class="headerlink" href="#multi-process-startup" title="Permalink to this headline"></a></h4>
<p>Distributed parallelism can be initiated by multi-process when executing on Ascend and there is no subGroup communication.</p>
<blockquote>
<div><p>Model parallelism generates sub-Group communication when there are dimensions of an object that are not sliced full or for which at least two dimensions are sliced.</p>
<p>That is, when started in this way, the communication generated by model parallelism within <code class="docutils literal notranslate"><span class="pre">shard</span></code> can only occur within <code class="docutils literal notranslate"><span class="pre">world</span> <span class="pre">group</span></code>, so the specified sharding strategy can currently only support slicing one dimension.</p>
</div></blockquote>
<p>The above code needs to be configured with distributed variables before it can run. The Ascend environment needs to be configured with RANK_TABLE_FILE, RANK_ID, and DEVICE_ID. The configuration procedure can be found <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/rank_table.html">here</a>.</p>
<p>Ascend Distributed related environment variables are:</p>
<ul class="simple">
<li><p>RANK_TABLE_FILE: Path to the grouping information file. rank_table_file files can be generated using hccl_tools.py in the models code repository, which can be obtained from <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.2/utils/hccl_tools">here</a>.</p></li>
<li><p>DEVICE_ID: the actual serial number of the current card on the machine.</p></li>
<li><p>RANK_ID: logical serial number of the current card.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">set</span><span class="w"> </span>-e
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_shard_function_example.sh RANK_SIZE RANK_TABLE_FILE&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_fusion_example.sh 8 ../rank_table_8pcs.json&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;This example is expected to run on the Ascend environment.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="nv">$#</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">]</span>
<span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: bash run_shard_function_example.sh RANK_SIZE RANK_TABLE_FILE&quot;</span>
<span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>
<span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data.zip&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span>wget<span class="w"> </span>http://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip
<span class="w">    </span><span class="k">fi</span>
<span class="w">    </span>unzip<span class="w"> </span>MNIST_Data.zip
<span class="k">fi</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/MNIST_Data/train/
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="nv">$2</span>
test_dist_8pcs<span class="o">()</span>
<span class="o">{</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">RANK_TABLE_FILE</span><span class="si">}</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>
<span class="o">}</span>
test_dist_<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span>pcs

<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span>
<span class="k">do</span>
<span class="w">    </span>rm<span class="w"> </span>-rf<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>mkdir<span class="w"> </span>device<span class="nv">$i</span>
<span class="w">    </span>cp<span class="w"> </span>./shard_function_example.py<span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>./device<span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span><span class="nv">$i</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training for device </span><span class="nv">$i</span><span class="s2">&quot;</span>
<span class="w">    </span>env<span class="w"> </span>&gt;<span class="w"> </span>env<span class="nv">$i</span>.log
<span class="w">    </span>python<span class="w"> </span>./shard_function_example.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="nv">$i</span><span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span>../
<span class="k">done</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;The program launch succeed, the log is under device0/train.log0.&quot;</span>
</pre></div>
</div>
<p>After configuring RANK_TABLE_FILE in the current directory, the following command requires the user to have 8 Ascend 910 devices. Run the command as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_shard_function_example.sh<span class="w"> </span><span class="m">8</span><span class="w"> </span>../rank_table_8pcs.json
</pre></div>
</div>
<p>During the execution process, the framework automatically performs operator-level model parallelization for the input function of <code class="docutils literal notranslate"><span class="pre">shard</span></code>, and the parallelization strategy for each operator is obtained by the framework search, and the whole process is user-agnostic. The graph can be saved as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The parallelization strategy for each specific operator can be seen in <code class="docutils literal notranslate"><span class="pre">step_parallel_end.ir</span></code>.</p>
</section>
<section id="startup-via-mpirun">
<h4>Startup via mpirun<a class="headerlink" href="#startup-via-mpirun" title="Permalink to this headline"></a></h4>
<p>On Ascend and GPU, distributed parallel can be started by means of mpirun, and <strong>this startup method supports the creation of subgroup communication</strong>. The run command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_NUM</span><span class="si">}</span><span class="w"> </span>--output-filename<span class="w"> </span>log_output<span class="w"> </span>--allow-run-as-root<span class="w"> </span>python<span class="w"> </span><span class="si">${</span><span class="nv">PYTHON_SCRIPT_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>Take the sample code as an example, and start 8 cards, the corresponding command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_mpirun_shard_function_example.sh
</pre></div>
</div>
<blockquote>
<div><p>Note that when startup from mpirun on Ascend with a large number of subgroups, you may run into an error that creating a communication domain fails. The error message is “Ascend collective Error: “HcclCommInitRootInfo failed. | Error Number 2”. You can reduce the <code class="docutils literal notranslate"><span class="pre">max_device_memory</span></code> in <code class="docutils literal notranslate"><span class="pre">context</span></code> to reserve enough memory for hccl to create the communication domain.</p>
</div></blockquote>
</section>
</section>
<section id="running-results">
<h3>Running Results<a class="headerlink" href="#running-results" title="Permalink to this headline"></a></h3>
<p>After the running is completed, the following is the part of Loss results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 0, step: 0, loss is 2.3093076
epoch: 0, step: 100, loss is 2.299726
epoch: 0, step: 200, loss is 2.3076267
epoch: 0, step: 300, loss is 2.288056
epoch: 0, step: 400, loss is 2.2772775
epoch: 0, step: 500, loss is 2.1903486
epoch: 0, step: 600, loss is 1.2501067
epoch: 0, step: 700, loss is 0.7540306
...
</pre></div>
</div>
</section>
</section>
<section id="usage-limitations">
<h2>Usage Limitations<a class="headerlink" href="#usage-limitations" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The execution mode needs to be set to <code class="docutils literal notranslate"><span class="pre">PYNATIVE_MODE</span></code>, the parallel configuration to <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code>, and the <code class="docutils literal notranslate"><span class="pre">search_mode</span></code> to <code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code>.</p></li>
<li><p>Nested <code class="docutils literal notranslate"><span class="pre">vmap</span></code> use is supported, and must be used with <code class="docutils literal notranslate"><span class="pre">shard</span></code> outside and <code class="docutils literal notranslate"><span class="pre">vmap</span></code> inside.</p></li>
<li><p>Nested use of <code class="docutils literal notranslate"><span class="pre">shard</span></code> is not supported.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed_graph_partition.html" class="btn btn-neutral float-left" title="Distributed Graph Partition" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_case.html" class="btn btn-neutral float-right" title="Distributed High-Level Configuration Case" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>