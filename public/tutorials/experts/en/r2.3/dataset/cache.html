<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Single-Node Data Cache &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimizing the Data Processing" href="optimize.html" />
    <link rel="prev" title="Auto Augmentation" href="augment.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="augment.html">Auto Augmentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Single-Node Data Cache</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-cache-process">Data Cache Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#1-start-the-cache-server">1. Start the Cache Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="#2-create-the-cache-session">2. Create the Cache Session</a></li>
<li class="toctree-l3"><a class="reference internal" href="#3-create-a-cache-instance">3. Create a Cache Instance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#4-insert-a-cache-instance">4. Insert a Cache Instance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#caching-the-original-dataset-data">Caching the Original Dataset Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#caching-the-data-processed-by-argumentation">Caching the Data Processed by Argumentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#5-destroy-the-cache-session">5. Destroy the Cache Session</a></li>
<li class="toctree-l3"><a class="reference internal" href="#6-stop-the-cache-server">6. Stop the Cache Server</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cache-sharing">Cache Sharing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cache-acceleration">Cache Acceleration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cache-performance-tuning">Cache Performance Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Single-Node Data Cache</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/dataset/cache.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="single-node-data-cache">
<h1>Single-Node Data Cache<a class="headerlink" href="#single-node-data-cache" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/tutorials/experts/source_en/dataset/cache.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<p>If you need to repeatedly access remote datasets or load datasets from disks, you can use the single-node cache to cache datasets in the local memory to accelerate dataset loading.</p>
<p>The cache operation depends on the cache server started on the current node. Functioning as a daemon process and independent of the training script, the cache server is mainly used to manage cached data, including storing, querying, and loading data, and writing cached data when the cache is not hit.</p>
<p>If the memory space is insufficient to cache all datasets, you can configure a cache operation to cache the remaining data to disks.</p>
<p>Currently, the cache service supports only single-node cache. That is, the client and server are deployed on the same machine. This service can be used in the following scenarios:</p>
<ul>
<li><p>Cache the loaded original dataset.</p>
<p>You can use the cache in the dataset loading operation. The loaded data is stored in the cache server. If the same data is required subsequently, the data can be directly load from the cache server, avoiding repeated loading from the disk.</p>
<p><img alt="cache on leaf pipeline" src="../_images/cache_dataset.png" /></p>
</li>
<li><p>Cache the data processed by argumentation.</p>
<p>You can also use the cache in the <code class="docutils literal notranslate"><span class="pre">map</span></code> operation. The data processed by argumentation (such as image cropping or resizing) is directly cached, avoiding repeated data argumentation operations and reducing unnecessary computations.</p>
<p><img alt="cache on map pipeline" src="../_images/cache_processed_data.png" /></p>
</li>
</ul>
<section id="data-cache-process">
<h2>Data Cache Process<a class="headerlink" href="#data-cache-process" title="Permalink to this headline"></a></h2>
<p>Before using the cache service, you need to install MindSpore and set the relevant environment variables.</p>
<blockquote>
<div><p>At present, data cache can only be performed in the Linux environment. Ubuntu, EulerOS and CentOS can refer to the <a class="reference external" href="https://help.ubuntu.com/community/SwapFaq#How_do_I_add_a_swap_file.3F">relevant tutorials</a> to learn how to increase the swap memory space. In addition, since the use of cache may cause the server’s memory shortage, it is recommended that users increase the server’s swap memory space to more than 100GB before using the cache.</p>
</div></blockquote>
<section id="1-start-the-cache-server">
<h3>1. Start the Cache Server<a class="headerlink" href="#1-start-the-cache-server" title="Permalink to this headline"></a></h3>
<p>Before using the single-node cache serve, you need to enter the following command at the command line to start the cache server:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--start
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Cache server startup completed successfully!
The cache server daemon has been created as process id 14678 and listening on port 50052.

Recommendation:
Since the server is detached into its own daemon process, monitor the server logs (under /tmp/mindspore/cache/log) for any issues that may happen after startup
</pre></div>
</div>
<p>If the above information is output, it means that the cache server starts successfully.</p>
<p>The preceding commands can use the <code class="docutils literal notranslate"><span class="pre">-h</span></code> and <code class="docutils literal notranslate"><span class="pre">-p</span></code> parameters to specify the server, or the user can specify it by configuring environment variables <code class="docutils literal notranslate"><span class="pre">MS_CACHE_HOST</span></code> and <code class="docutils literal notranslate"><span class="pre">MS_CACHE_PORT</span></code>. If not specified, the default operation is performed on servers with IP 127.0.0.1 and port number 50052.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ps</span> <span class="pre">-ef|grep</span> <span class="pre">cache_server</span></code> command can be used to check if the server is started and query server parameters.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--server_info</span></code> command can also be used to check a detailed list of parameters for the server.</p>
<p>To enable the data overflow feature, the user must set the overflow path with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> parameter when starting the cache server, or the feature is turned off by default.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--server_info
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Cache Server Configuration:
----------------------------------------
         config name          value
----------------------------------------
            hostname      127.0.0.1
                port          50052
   number of workers              8
           log level              1
           spill dir           None
----------------------------------------
Active sessions:
No active sessions.
</pre></div>
</div>
<p>The Cache Server Configuration table lists the IP address, port number, number of worker threads, log level, overflow path and other detailed configuration information of the current server. The Active sessions module displays a list of session IDs that are enabled in the current server.</p>
<p>The cache server log file is named in the format “cache_server.&lt;host name&gt;.&lt;user name&gt;.log.&lt;log level&gt;.&lt;data-time&gt;.&lt;process number&gt;”.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">GLOG_v=0</span></code>, DEBUG log may be displayed on the screen.</p>
</section>
<section id="2-create-the-cache-session">
<h3>2. Create the Cache Session<a class="headerlink" href="#2-create-the-cache-session" title="Permalink to this headline"></a></h3>
<p>If there is no cache session in the cache server, you need to create a cache session and get the cache session id:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>-g
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Session created for server on port 50052: 780643335
</pre></div>
</div>
<p>where 780643335 is the cache session id assigned to the server on port 50052, and the cache session id is assigned by the server.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command can be used to check all cache session information existing in the current server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--list_sessions
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
   780643335         n/a         n/a         n/a             n/a       n/a
</pre></div>
</div>
<p>Output parameters description:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Session</span></code>: cache session id.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cache</span> <span class="pre">Id</span></code>: cache instance id in the current cache session. <code class="docutils literal notranslate"><span class="pre">n/a</span></code> indicates that the cache instance has not been created at the moment.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Mem</span> <span class="pre">cached</span></code>: the amount of data cached in memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Disk</span> <span class="pre">cached</span></code>: the amount of data cached in disk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Avg</span> <span class="pre">cache</span> <span class="pre">size</span></code>: the average size of each row of data currently cached.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Numa</span> <span class="pre">hit</span></code>: the number of Numa hits. The higher value will get the better time performance.</p></li>
</ul>
</section>
<section id="3-create-a-cache-instance">
<h3>3. Create a Cache Instance<a class="headerlink" href="#3-create-a-cache-instance" title="Permalink to this headline"></a></h3>
<p>In the Python training script, use the <code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> API to define a cache instance named <code class="docutils literal notranslate"><span class="pre">test_cache</span></code>, and put a cache session ID created in the previous step to the <code class="docutils literal notranslate"><span class="pre">session_id</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="c1"># define a variable named `session_id` to receive the cache session ID created in the previous step</span>
<span class="n">session_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">popen</span><span class="p">(</span><span class="s1">&#39;cache_admin --list_sessions | tail -1 | awk -F &quot; &quot; </span><span class="se">\&#39;</span><span class="s1">{{print $1;}}</span><span class="se">\&#39;</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="n">test_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="n">session_id</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spilling</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> supports the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">session_id</span></code>: specifies the cache session ID, which can be created and obtained by running the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">-g</span></code> command.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size</span></code>: specifies the maximum memory space occupied by the cache. The unit is MB. For example, if the cache space is 512 GB, set <code class="docutils literal notranslate"><span class="pre">size=524288</span></code>. The default value is 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spilling</span></code>: determines whether to spill the remaining data to disks when the memory space exceeds the upper limit. The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hostname</span></code>: specifies the IP address for connecting to the cache server. The default value is 127.0.0.1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">port</span></code>: specifies the port number for connecting to the cache server. The default value is 50052.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_connections</span></code>: specifies the number of established TCP/IP connections. The default value is 12.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefetch_size</span></code>: specifies the number of prefetched rows. The default value is 20.</p></li>
</ul>
<p>The following things that needs to be noted:</p>
<p>In actual use, you are advised to run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">-g</span></code> command to obtain a cache session id from the cache server and use it as the parameter of <code class="docutils literal notranslate"><span class="pre">session_id</span></code> to prevent errors caused by cache session nonexistence.</p>
<p><code class="docutils literal notranslate"><span class="pre">size=0</span></code> indicates that the memory space used by the cache is not limited manually, but automically controlled by the cache server according to system’s total memory resources, and cache server’s memory usage would be limited to within 80% of the total system memory.</p>
<p>Users can also manually set <code class="docutils literal notranslate"><span class="pre">size</span></code> to a proper value based on the idle memory of the machine. Note that before setting the <code class="docutils literal notranslate"><span class="pre">size</span></code> parameter, make sure to check the available memory of the system and the size of the dataset to be loaded. If the memory space occupied by the cache_server or the space of the dataset to be loaded exceeds the available memory of the system, it may cause problems such as machine downtime/restart, automatic shutdown of cache_server, and failure of training process execution.</p>
<p><code class="docutils literal notranslate"><span class="pre">spilling=True</span></code> indicates that the remaining data is written to disks when the memory space is insufficient. Therefore, ensure that you have the writing permission and the sufficient disk space on the configured disk path is  to store the cache data that spills to the disk. Note that if no spilling path is set when cache server starts, setting <code class="docutils literal notranslate"><span class="pre">spilling=True</span></code> will raise an error when calling the API.</p>
<p><code class="docutils literal notranslate"><span class="pre">spilling=False</span></code> indicates that no data is written once the configured memory space is used up on the cache server.</p>
<p>If a dataset that does not support random access (such as <code class="docutils literal notranslate"><span class="pre">TFRecordDataset</span></code>) is used to load data and the cache service is enabled, ensure that the entire dataset is stored locally. In this scenario, if the local memory space is insufficient to store all data, spilling must be enabled to spill data to disks.</p>
</section>
<section id="4-insert-a-cache-instance">
<h3>4. Insert a Cache Instance<a class="headerlink" href="#4-insert-a-cache-instance" title="Permalink to this headline"></a></h3>
<p>Currently, the cache service can be used to cache both original datasets and datasets processed by argumentation. The following example shows two usage methods.</p>
<p>Note that both examples need to create a cache instance according to the method in step 3, and pass in the created <code class="docutils literal notranslate"><span class="pre">test_cache</span></code> as <code class="docutils literal notranslate"><span class="pre">cache</span></code> parameters in the dataset load or map operation.</p>
<p>CIFAR-10 dataset is used in the following examples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">download</span> <span class="kn">import</span> <span class="n">download</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz&quot;</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s2">&quot;./datasets&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;tar.gz&quot;</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">test_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/test&quot;</span>
<span class="n">train_path</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="s2">&quot;test_batch.bin&quot;</span><span class="p">)):</span>
    <span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="p">,</span> <span class="n">test_path</span><span class="p">)</span>
<span class="p">[</span><span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/&quot;</span><span class="o">+</span><span class="n">i</span><span class="p">,</span> <span class="n">train_path</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;./datasets/cifar-10-batches-bin/&quot;</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">i</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.html&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">i</span><span class="p">))]</span>
</pre></div>
</div>
<p>The directory structure of the extracted dataset file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>./datasets/cifar-10-batches-bin
├── readme.html
├── test
│   └── test_batch.bin
└── train
    ├── batches.meta.txt
    ├── data_batch_1.bin
    ├── data_batch_2.bin
    ├── data_batch_3.bin
    ├── data_batch_4.bin
    └── data_batch_5.bin
</pre></div>
</div>
<section id="caching-the-original-dataset-data">
<h4>Caching the Original Dataset Data<a class="headerlink" href="#caching-the-original-dataset-data" title="Permalink to this headline"></a></h4>
<p>Cache the original dataset, and the datat is loaded by the MindSpore system.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_dir</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>

<span class="c1"># apply cache to dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">test_cache</span><span class="p">)</span>

<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># each data is a dictionary</span>
    <span class="c1"># in this example, each dictionary has a key &quot;image&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> image shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0 image shape: (32, 32, 3)
1 image shape: (32, 32, 3)
2 image shape: (32, 32, 3)
3 image shape: (32, 32, 3)
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether there are four data records in the current session. If yes, the data is successfully cached.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--list_sessions
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
   780643335  2044459912           4         n/a            3226         4
</pre></div>
</div>
</section>
<section id="caching-the-data-processed-by-argumentation">
<h4>Caching the Data Processed by Argumentation<a class="headerlink" href="#caching-the-data-processed-by-argumentation" title="Permalink to this headline"></a></h4>
<p>Cache data after data enhancement processing <code class="docutils literal notranslate"><span class="pre">transforms</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>

<span class="n">dataset_dir</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>

<span class="c1"># apply cache to dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># apply cache to map</span>
<span class="n">rescale_op</span> <span class="o">=</span> <span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">test_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="n">session_id</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spilling</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">rescale_op</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">test_cache</span><span class="p">)</span>

<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># each data is a dictionary</span>
    <span class="c1"># in this example, each dictionary has a keys &quot;image&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> image shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0 image shape: (32, 32, 3)
1 image shape: (32, 32, 3)
2 image shape: (32, 32, 3)
3 image shape: (32, 32, 3)
4 image shape: (32, 32, 3)
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether there are five data records in the current session. If yes, the data is successfully cached.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--list_sessions
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
   780643335   112867845           5         n/a           12442         5
   780643335  2044459912           4         n/a            3226         4
</pre></div>
</div>
</section>
</section>
<section id="5-destroy-the-cache-session">
<h3>5. Destroy the Cache Session<a class="headerlink" href="#5-destroy-the-cache-session" title="Permalink to this headline"></a></h3>
<p>After the training is complete, you can destroy the current cache and release the memory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">destroy_session</span> <span class="o">=</span> <span class="s1">&#39;cache_admin --destroy_session&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">session_id</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>destroy_session
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Drop session successfully for server on port 50052
</pre></div>
</div>
<p>The preceding command is used to destroy the cache with the session ID 1456416665 on the server with the port number 50052.</p>
<p>If you choose not to destroy the cache, the cached data still exists in the cache session. You can use the cache when starting the training script next time.</p>
</section>
<section id="6-stop-the-cache-server">
<h3>6. Stop the Cache Server<a class="headerlink" href="#6-stop-the-cache-server" title="Permalink to this headline"></a></h3>
<p>After using the cache server, you can stop it. This operation will destroy all cache sessions on the current server and release the memory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--stop
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
<p>The preceding command is used to shut down the server with the port number 50052.</p>
<p>If you choose not to shut down the server, the cache sessions on the server will be retained for future use. During the next training, you can create a cache session or reuse the existing cache.</p>
</section>
</section>
<section id="cache-sharing">
<h2>Cache Sharing<a class="headerlink" href="#cache-sharing" title="Permalink to this headline"></a></h2>
<p>During the single-node multi-device distributed training, the cache operation allows multiple same training scripts to share the same cache and read and write data from the cache.</p>
<ol class="arabic">
<li><p>Start the cache server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">$cache_admin</span><span class="w"> </span>--start
Cache<span class="w"> </span>server<span class="w"> </span>startup<span class="w"> </span>completed<span class="w"> </span>successfully!
The<span class="w"> </span>cache<span class="w"> </span>server<span class="w"> </span>daemon<span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>created<span class="w"> </span>as<span class="w"> </span>process<span class="w"> </span>id<span class="w"> </span><span class="m">39337</span><span class="w"> </span>and<span class="w"> </span>listening<span class="w"> </span>on<span class="w"> </span>port<span class="w"> </span><span class="m">50052</span>
Recommendation:
Since<span class="w"> </span>the<span class="w"> </span>server<span class="w"> </span>is<span class="w"> </span>detached<span class="w"> </span>into<span class="w"> </span>its<span class="w"> </span>own<span class="w"> </span>daemon<span class="w"> </span>process,<span class="w"> </span>monitor<span class="w"> </span>the<span class="w"> </span>server<span class="w"> </span>logs<span class="w"> </span><span class="o">(</span>under<span class="w"> </span>/tmp/mindspore/cache/log<span class="o">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>any<span class="w"> </span>issues<span class="w"> </span>that<span class="w"> </span>may<span class="w"> </span>happen<span class="w"> </span>after<span class="w"> </span>startup
</pre></div>
</div>
</li>
<li><p>Create a cache session.</p>
<p>Create the Shell script <code class="docutils literal notranslate"><span class="pre">cache.sh</span></code> for starting Python training and run the following command to generate a cache session ID:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># This shell script will launch parallel pipelines</span>

<span class="c1"># get path to dataset directory</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$#</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">]</span>
<span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: sh cache.sh DATASET_PATH&quot;</span>
<span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>
<span class="nv">dataset_path</span><span class="o">=</span><span class="nv">$1</span>

<span class="c1"># generate a session id that these parallel pipelines can share</span>
<span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>-g<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="k">)</span>
<span class="nv">rc</span><span class="o">=</span><span class="nv">$?</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$rc</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;some error&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># grab the session id from the result string</span>
<span class="nv">session_id</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$result</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $NF}&#39;</span><span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Pass the cache session id to the training script.</p>
<p>Continue to write the Shell script and add the following command to pass <code class="docutils literal notranslate"><span class="pre">session_id</span></code> and other parameters when the Python training is started:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># make the session_id available to the python scripts</span>
<span class="nv">num_devices</span><span class="o">=</span><span class="m">4</span>

<span class="k">for</span><span class="w"> </span>p<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="k">$((</span><span class="si">${</span><span class="nv">num_devices</span><span class="si">}</span><span class="o">-</span><span class="m">1</span><span class="k">)))</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span>python<span class="w"> </span>my_training_script.py<span class="w"> </span>--num_devices<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$num_devices</span><span class="s2">&quot;</span><span class="w"> </span>--device<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$p</span><span class="s2">&quot;</span><span class="w"> </span>--session_id<span class="w"> </span><span class="nv">$session_id</span><span class="w"> </span>--dataset_path<span class="w"> </span><span class="nv">$dataset_path</span>
<span class="k">done</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/sample_code/cache/cache.sh">cache.sh</a>.</p>
</div></blockquote>
</li>
<li><p>Create and apply a cache instance.</p>
<p>CIFAR-10 dataset is used in the following example.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├─cache.sh
├─my_training_script.py
└─cifar-10-batches-bin
    ├── batches.meta.txt
    ├── data_batch_1.bin
    ├── data_batch_2.bin
    ├── data_batch_3.bin
    ├── data_batch_4.bin
    ├── data_batch_5.bin
    ├── readme.html
    └── test_batch.bin
</pre></div>
</div>
<p>Create and write the Python script <code class="docutils literal notranslate"><span class="pre">my_training_script.py</span></code>. Use the following code to receive <code class="docutils literal notranslate"><span class="pre">session_id</span></code> and pass it as a parameter when defining a cache instance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Cache Example&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_devices&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Device num.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Device id.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--session_id&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Session id.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dataset_path&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Dataset path&#39;</span><span class="p">)</span>
<span class="n">args_opt</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># apply cache to dataset</span>
<span class="n">test_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">session_id</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spilling</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">num_shards</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">test_cache</span><span class="p">)</span>
<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2"> samples on device </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/sample_code/cache/my_training_script.py">my_training_script.py</a></p>
</div></blockquote>
</li>
<li><p>Execute the training script.</p>
<p>Execute the Shell script <code class="docutils literal notranslate"><span class="pre">cache.sh</span></code> to enable distributed training.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sh<span class="w"> </span>cache.sh<span class="w"> </span>cifar-10-batches-bin/
Got<span class="w"> </span><span class="m">4</span><span class="w"> </span>samples<span class="w"> </span>on<span class="w"> </span>device<span class="w"> </span><span class="m">0</span>
Got<span class="w"> </span><span class="m">4</span><span class="w"> </span>samples<span class="w"> </span>on<span class="w"> </span>device<span class="w"> </span><span class="m">1</span>
Got<span class="w"> </span><span class="m">4</span><span class="w"> </span>samples<span class="w"> </span>on<span class="w"> </span>device<span class="w"> </span><span class="m">2</span>
Got<span class="w"> </span><span class="m">4</span><span class="w"> </span>samples<span class="w"> </span>on<span class="w"> </span>device<span class="w"> </span><span class="m">3</span>
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether only one group of data exists in the current session. If yes, cache sharing is successful.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>cache_admin<span class="w"> </span>--list_sessions
Listing<span class="w"> </span>sessions<span class="w"> </span><span class="k">for</span><span class="w"> </span>server<span class="w"> </span>on<span class="w"> </span>port<span class="w"> </span><span class="m">50052</span>

Session<span class="w">    </span>Cache<span class="w"> </span>Id<span class="w">  </span>Mem<span class="w"> </span>cached<span class="w"> </span>Disk<span class="w"> </span>cached<span class="w">  </span>Avg<span class="w"> </span>cache<span class="w"> </span>size<span class="w">  </span>Numa<span class="w"> </span>hit
<span class="m">3392558708</span><span class="w">   </span><span class="m">821590605</span><span class="w">          </span><span class="m">16</span><span class="w">         </span>n/a<span class="w">            </span><span class="m">3227</span><span class="w">        </span><span class="m">16</span>
</pre></div>
</div>
</li>
<li><p>Destroy the cache session.</p>
<p>After the training is complete, you can destroy the current cache and release the memory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>cache_admin<span class="w"> </span>--destroy_session<span class="w"> </span><span class="m">3392558708</span>
Drop<span class="w"> </span>session<span class="w"> </span>successfully<span class="w"> </span><span class="k">for</span><span class="w"> </span>server<span class="w"> </span>on<span class="w"> </span>port<span class="w"> </span><span class="m">50052</span>
</pre></div>
</div>
</li>
<li><p>Stop the cache server, after using the cache server, you can stop it.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>cache_admin<span class="w"> </span>--stop
Cache<span class="w"> </span>server<span class="w"> </span>on<span class="w"> </span>port<span class="w"> </span><span class="m">50052</span><span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>stopped<span class="w"> </span>successfully.
</pre></div>
</div>
</li>
</ol>
</section>
<section id="cache-acceleration">
<h2>Cache Acceleration<a class="headerlink" href="#cache-acceleration" title="Permalink to this headline"></a></h2>
<p>In order to share large data sets among multiple servers and alleviate the disk space requirements of a single server, users can usually choose to use NFS (Network File System), such as HUAWEI CLOUD-NFS Storage Server.</p>
<p>However, access to NFS datasets is often expensive, resulting in longer training sessions by using NFS datasets.</p>
<p>In order to improve the training performance of the NFS dataset, we can choose to use a cache service to cache the dataset in memory as Tensor.</p>
<p>Once cached, post-sequence epochs can read data directly from memory, avoiding the overhead of accessing remote NAS.</p>
<p>It should be noted that in the data processing process of the training process, and the dataset usually needs to be augmentated with randomness after loading, such as <code class="docutils literal notranslate"><span class="pre">RandomCropDecodeResize</span></code>. If the cache is added to the operation with randomness, it will cause the results of the first enhancement operation to be cached, and the results read from the cache server in the later sequence are the first cached data, resulting in the loss of data randomness and affecting the accuracy of the training network.</p>
<p>Therefore, we can choose to add a cache directly after the data set reads the operation. This section takes this approach, using the MobileNetV2 network as a sample for an example.</p>
<p>For complete sample code, refer to ModelZoo’s <a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/cv/MobileNet/mobilenetv2">MobileNetV2</a>.</p>
<ol class="arabic">
<li><p>Create Shell script <code class="docutils literal notranslate"><span class="pre">cache_util.sh</span></code> for managing cache:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bootup_cache_server<span class="o">()</span>
<span class="o">{</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Booting up cache server...&quot;</span>
<span class="w">  </span><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>--start<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="k">)</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">result</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>

generate_cache_session<span class="o">()</span>
<span class="o">{</span>
<span class="w">  </span><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>-g<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;END {print $NF}&#39;</span><span class="k">)</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">result</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/sample_code/cache/cache_util.sh">cache_util.sh</a>.</p>
</div></blockquote>
</li>
<li><p>In the Shell script <code class="docutils literal notranslate"><span class="pre">run_train_nfs_cache.sh</span></code> that starts NFS dataset training, turn on the cache server for the scenario trained with datasets located on NFS and generate a cache session saved in the Shell variable <code class="docutils literal notranslate"><span class="pre">CACHE_SESSION_ID</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CURPATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">dirname</span><span class="p"> </span><span class="s2">&quot;</span><span class="nv">$0</span><span class="s2">&quot;</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">source</span><span class="w"> </span><span class="si">${</span><span class="nv">CURPATH</span><span class="si">}</span>/cache_util.sh

bootup_cache_server
<span class="nv">CACHE_SESSION_ID</span><span class="o">=</span><span class="k">$(</span>generate_cache_session<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Pass in the <code class="docutils literal notranslate"><span class="pre">CACHE_SESSION_ID</span></code> and other parameters when starting Python training:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python train.py \
--platform=$1 \
--dataset_path=$5 \
--pretrain_ckpt=$PRETRAINED_CKPT \
--freeze_layer=$FREEZE_LAYER \
--filter_head=$FILTER_HEAD \
--enable_cache=True \
--cache_session_id=$CACHE_SESSION_ID \
&amp;&gt; log$i.log &amp;
</pre></div>
</div>
</li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">train_parse_args()</span></code> function of Python’s parameter parsing script <code class="docutils literal notranslate"><span class="pre">args.py</span></code>, the incoming <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> is received by the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>

<span class="k">def</span> <span class="nf">train_parse_args</span><span class="p">():</span>
<span class="o">...</span>
    <span class="n">train_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--enable_cache&#39;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Caching the dataset in memory to speedup dataset processing, default is False.&#39;</span><span class="p">)</span>
    <span class="n">train_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--cache_session_id&#39;</span><span class="p">,</span>
         <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
         <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
         <span class="n">help</span><span class="o">=</span><span class="s1">&#39;The session id for cache service.&#39;</span><span class="p">)</span>
<span class="n">train_args</span> <span class="o">=</span> <span class="n">train_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</pre></div>
</div>
<p>And call the <code class="docutils literal notranslate"><span class="pre">train_parse_args()</span></code> function in Python’s training script <code class="docutils literal notranslate"><span class="pre">train.py</span></code> to parse the parameters such as the <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> passed in, and pass it in as an argument when defining the dataset <code class="docutils literal notranslate"><span class="pre">dataset</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.args</span> <span class="kn">import</span> <span class="n">train_parse_args</span>
<span class="n">args_opt</span> <span class="o">=</span> <span class="n">train_parse_args</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span>
    <span class="n">dataset_path</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">,</span>
    <span class="n">do_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">enable_cache</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">enable_cache</span><span class="p">,</span>
    <span class="n">cache_session_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">cache_session_id</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>In the Python script <code class="docutils literal notranslate"><span class="pre">dataset.py</span></code> that defines the data processing process, an instance of <code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> is created and inserted after <code class="docutils literal notranslate"><span class="pre">ImageFolderDataset</span></code> based on  the parameters <code class="docutils literal notranslate"><span class="pre">enable_cache</span></code> and <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> passed in:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">do_train</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_session_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="o">...</span>
    <span class="k">if</span> <span class="n">enable_cache</span><span class="p">:</span>
        <span class="n">nfs_dataset_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">cache_session_id</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nfs_dataset_cache</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">rank_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;RANK_SIZE&quot;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">))</span>
        <span class="n">rank_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;RANK_ID&quot;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rank_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">nfs_dataset_cache</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">nfs_dataset_cache</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">run_train_nfs_cache.sh</span></code>, and obtain the following results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: [  0/ 200], step:[ 2134/ 2135], loss:[4.682/4.682], time:[3364893.166], lr:[0.780]
epoch time: 3384387.999, per step time: 1585.193, avg loss: 4.682
epoch: [  1/ 200], step:[ 2134/ 2135], loss:[3.750/3.750], time:[430495.242], lr:[0.724]
epoch time: 431005.885, per step time: 201.876, avg loss: 4.286
epoch: [  2/ 200], step:[ 2134/ 2135], loss:[3.922/3.922], time:[420104.849], lr:[0.635]
epoch time: 420669.174, per step time: 197.035, avg loss: 3.534
epoch: [  3/ 200], step:[ 2134/ 2135], loss:[3.581/3.581], time:[420825.587], lr:[0.524]
epoch time: 421494.842, per step time: 197.421, avg loss: 3.417
...
</pre></div>
</div>
<p>The following table shows the average epoch time on gpu servers of using cache versus or not using cache:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>| 4p, MobileNetV2, imagenet2012            | without cache | with cache |
| ---------------------------------------- | ------------- | ---------- |
| first epoch time                         | 1649s         | 3384s      |
| average epoch time (exclude first epoch) | 458s          | 421s       |
</pre></div>
</div>
<p>You can see that after using the cache, the completion time of the first epoch increases more than if the cache is not used, which is mainly due to the overhead of writing cache data to the cache server. However, each subsequent epoch after caching data writes can get a large performance gain. Therefore, the greater the total number of episodes trained, the more pronounced the benefits of using the cache.</p>
<p>Taking running 200 epochs as an example, using caching can reduce the total end-to-end training time from 92791 seconds to 87163 seconds, saving a total of about 5628 seconds.</p>
</li>
<li><p>When finish using, you can choose to shut down the cache server:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --stop
Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
</li>
</ol>
</section>
<section id="cache-performance-tuning">
<h2>Cache Performance Tuning<a class="headerlink" href="#cache-performance-tuning" title="Permalink to this headline"></a></h2>
<p>The cache service performance can be significantly improved in following scenarios:</p>
<ul class="simple">
<li><p>Cache the data processed by augmentation, especially when the data processing pipeline contains high complexity operations such as decode. In this scenario, you do not need to perform the data augmentation operation repeatedly on each epoch, which saves a lot of time.</p></li>
<li><p>Use cache services during simple network training and inference. Compared with complex networks, simple networks require less training time. Therefore, the time performance is significantly improved when cache services are used in this scenario.</p></li>
</ul>
<p>However, we may not benefit from cache in the following scenarios:</p>
<ul class="simple">
<li><p>The system memory is insufficient or the cache is not hit, resulting in poor cache service time performance. You can check whether the available system memory is sufficient and set a proper cache size before using the cache.</p></li>
<li><p>Too much cache spilling will deteriorate the time performance. Therefore, try not to spill cache to disks when datasets that support random access (such as <code class="docutils literal notranslate"><span class="pre">ImageFolderDataset</span></code>) are used for data loading.</p></li>
<li><p>Using cache on NLP network such as Bert does not perform. In the NLP scenarios, there are usually no high complexity data augmentation operations like decode.</p></li>
<li><p>There is expectable startup overhead when using cache in non-mappable datasets like <code class="docutils literal notranslate"><span class="pre">TFRecordDataset</span></code>. According to the current design, it is required to cache all rows to the cache server before the first epoch of training. So the first epoch time can be longer than the non-cache case.</p></li>
</ul>
</section>
<section id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Currently, dataset classes such as <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code>, <code class="docutils literal notranslate"><span class="pre">PaddedDataset</span></code>, and <code class="docutils literal notranslate"><span class="pre">NumpySlicesDataset</span></code> do not support cache. <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code>, <code class="docutils literal notranslate"><span class="pre">PaddedDataset</span></code>, and <code class="docutils literal notranslate"><span class="pre">NumpySlicesDataset</span></code> belong to <code class="docutils literal notranslate"><span class="pre">GeneratorOp</span></code>, so their error message is displayed as “There is currently no support for GeneratorOp under cache.”</p></li>
<li><p>Data processed by <code class="docutils literal notranslate"><span class="pre">batch</span></code>, <code class="docutils literal notranslate"><span class="pre">concat</span></code>, <code class="docutils literal notranslate"><span class="pre">filter</span></code>, <code class="docutils literal notranslate"><span class="pre">repeat</span></code>, <code class="docutils literal notranslate"><span class="pre">skip</span></code>, <code class="docutils literal notranslate"><span class="pre">split</span></code>, <code class="docutils literal notranslate"><span class="pre">take</span></code>, and <code class="docutils literal notranslate"><span class="pre">zip</span></code> does not support cache.</p></li>
<li><p>Data processed by random data argumentation operations (such as <code class="docutils literal notranslate"><span class="pre">RandomCrop</span></code>) does not support cache.</p></li>
<li><p>The same cache instance cannot be nested in different locations of the same pipeline.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="augment.html" class="btn btn-neutral float-left" title="Auto Augmentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="optimize.html" class="btn btn-neutral float-right" title="Optimizing the Data Processing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>