<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Heterogeneous Storage &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Training Communication Fusion" href="comm_fusion.html" />
    <link rel="prev" title="Host&amp;Device Heterogeneous" href="host_device_training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="strategy_select.html">Strategy Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="split_technique.html">Sharding Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiple_copy.html">Multi-copy Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_gradient_accumulation.html">Gradient Accumulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="recompute.html">Recomputation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_slice.html">Dataset Slicing</a></li>
<li class="toctree-l2"><a class="reference internal" href="host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Heterogeneous Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_fusion.html">Distributed Training Communication Fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="comm_subgraph.html">Communication Subgraph Extraction and Reuse</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">Incremental Operator Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="optimize_technique.html">Optimization Techniques</a> &raquo;</li>
      <li>Heterogeneous Storage</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/memory_offload.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="heterogeneous-storage">
<h1>Heterogeneous Storage<a class="headerlink" href="#heterogeneous-storage" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/tutorials/experts/source_en/parallel/memory_offload.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In recent years Transformer-based large models have made rapid progress in various downstream tasks in Natural Language Processing and Computer Vision, and often the larger the model, the higher the accuracy achieved in downstream tasks. The model size develops from hundreds of millions to hundreds of billions, however, large model training consumes a large amount of computational storage resources and the training overhead is huge.</p>
<p>Large model training is limited by the size of the video memory, and the number of model parameters that can be stored on a single card is limited. With model parallel, we can split large models into different machines, and after introducing the necessary inter-process communication, we can conduct collaborative training in clusters, where the model size is proportional to the machine size. At the same time, when the model size exceeds the memory capacity of a single machine, the overhead of inter-machine communication in model parallel will become larger, and the resource utilization will decrease significantly. How to train larger models on a single machine and avoid inter-machine communication in model parallel has become the key to improve the performance of large model training.</p>
<p>Heterogeneous storage management enables 10x to 100x storage expansion of model parameters, thus breaking the memory limitation of large model training and realizing low-cost large model training. This tutorial will explain the basic principles of heterogeneous storage management and introduce the related configuration parameters and their use. With this feature, developers can use the same hardware to train larger models.</p>
<p>The related configuration and switch cide;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>

<span class="n">offload_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;auto_offload&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                  <span class="s2">&quot;offload_cpu_size&quot;</span><span class="p">:</span> <span class="s2">&quot;512GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_disk_size&quot;</span><span class="p">:</span> <span class="s2">&quot;1024GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./offload/&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;host_mem_block_size&quot;</span><span class="p">:</span><span class="s2">&quot;1GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;enable_aio&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s2">&quot;enable_pinned_mem&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">memory_offload</span><span class="o">=</span><span class="s1">&#39;ON&#39;</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="s1">&#39;30GB&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="o">=</span><span class="n">offload_config</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">memory_offload</span></code>: : Whether to enable heterogeneous storage to temporarily copy free data to Host-side memory in out-of-memory scenarios.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_device_memory</span></code>: Sets the maximum memory available to the device.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">offload_config</span></code> is a configuration option for heterogeneous storage where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;:</span> <span class="pre">&quot;cpu&quot;</span></code>: The parameters of the setup model are stored on the cpu memory and loaded to the device side only when the data needs to be used during the training process, and then unloaded to the cpu memory once the use is complete.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;auto_offload&quot;:</span> <span class="pre">False</span></code>: set off the auto-offload strategy, parameter data will strictly follow the previous configuration option.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;offload_cpu_size&quot;:</span> <span class="pre">&quot;512GB&quot;,</span> <span class="pre">&quot;offload_disk_size&quot;:</span> <span class="pre">&quot;1024GB&quot;</span></code>: The cpu memory and disk size available for offload are set respectively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;offload_path&quot;:</span> <span class="pre">&quot;./offload/&quot;</span></code>: sets the path to the disk file to be used for offload.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enable_pinned_mem&quot;:</span> <span class="pre">True</span></code>: set to turn on page locking, which when turned on speeds up copying between HBM-CPU memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;host_mem_block_size&quot;:&quot;1GB&quot;</span></code>: set the cpu lock page memory pool block size.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enable_aio&quot;:</span> <span class="pre">True</span></code>: set to turn on file asynchronous IO, which when turned on speeds up DDR-to-disk copying. (Requires compilation with the -o option, and only supports Linux environments with aio installed)</p></li>
</ul>
</li>
</ul>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>During training, the main stored data consists of parameters and intermediate results:</p>
<ul class="simple">
<li><p>Parameters: data such as the weights of the model and the amount of state of the optimizer, which need to be stored all the time during the training process.</p></li>
<li><p>Intermediate results: data generated by calculations in the forward/backward and optimization processes can be released and deleted after the corresponding calculations are completed.</p></li>
</ul>
<p>Through heterogeneous storage management, parameters or intermediate results that do not need to participate in computation temporarily can be copied to the memory of Host side or even hard disk storage during the training process, and then copied and restored to the device side when the data is needed to participate in computation. By the above means, the model size that can be trained by the same hardware device can be increased.</p>
<p><img alt="image.png" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/experts/source_zh_cn/parallel/images/memory_offload.png" /></p>
</section>
<section id="operation-practice">
<h2>Operation Practice<a class="headerlink" href="#operation-practice" title="Permalink to this headline"></a></h2>
<p>The following is an illustration of heterogeneous storage operation using Ascend as an example:</p>
<section id="example-code-description">
<h3>Example Code Description<a class="headerlink" href="#example-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Download the complete example code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.3/docs/sample_code/memory_offload">memory_offload</a>.</p>
</div></blockquote>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ memory_offload
       ├── train.py
       └── run.sh
    ...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train.py</span></code> is the script that defines the network structure and the training process. <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the execution script.</p>
</section>
<section id="configuring-a-distributed-environment">
<h3>Configuring a Distributed Environment<a class="headerlink" href="#configuring-a-distributed-environment" title="Permalink to this headline"></a></h3>
<p>Specify the run mode, run device, run card number via the context interface. The parallel mode in this sample uses data parallel and initializes HCCL or NCCL communication with init.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">max_device_memory</span><span class="o">=</span><span class="s2">&quot;1GB&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">memory_offload</span> <span class="o">==</span> <span class="s2">&quot;ON&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">memory_offload</span><span class="o">=</span><span class="s2">&quot;ON&quot;</span><span class="p">)</span>
    <span class="n">offload_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;offload_path&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">offload_path</span><span class="p">,</span> <span class="s2">&quot;auto_offload&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">auto_offload</span><span class="p">,</span>
                      <span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">offload_param</span><span class="p">,</span> <span class="s2">&quot;offload_cpu_size&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">offload_cpu_size</span><span class="p">,</span>
                      <span class="s2">&quot;offload_disk_size&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">offload_disk_size</span><span class="p">,</span>
                      <span class="s2">&quot;host_mem_block_size&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">host_mem_block_size</span><span class="p">,</span>
                      <span class="s2">&quot;enable_aio&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">enable_aio</span><span class="p">,</span> <span class="s2">&quot;enable_pinned_mem&quot;</span><span class="p">:</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">enable_pinned_mem</span><span class="p">}</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=====offload_config====</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">offload_config</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="o">=</span><span class="n">offload_config</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">offload_config</span></code> is the configuration dictionary for heterogeneous storage, and see the relevant configuration notes in the overview of this chapter for details of the configuration. Here <code class="docutils literal notranslate"><span class="pre">max_device_memory</span></code> is configured to <code class="docutils literal notranslate"><span class="pre">1GB</span></code> to trigger heterogeneous storage by preventing the video memory from loading the full network. The “1GB” here only represents the borderline video memory we tested on the Ascend 910, which may vary from device to device.</p>
</section>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h3>
<p>This example uses the CIFAR-10 dataset for training, so the data processing method corresponding to CIFAR-10 is used, and since the parallel method is data parallel, <code class="docutils literal notranslate"><span class="pre">num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id</span></code> also need to be configured, and the code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">dataset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">)</span>
    <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
    <span class="n">cifar_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="n">resize_height</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="n">random_crop_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">random_horizontal_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">()</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">))</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">normalize_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4465</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2023</span><span class="p">,</span> <span class="mf">0.1994</span><span class="p">,</span> <span class="mf">0.2010</span><span class="p">))</span>
    <span class="n">changeswap_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">c_trans</span> <span class="o">=</span> <span class="p">[</span><span class="n">random_crop_op</span><span class="p">,</span> <span class="n">random_horizontal_op</span><span class="p">]</span>
    <span class="n">c_trans</span> <span class="o">+=</span> <span class="p">[</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">rescale_op</span><span class="p">,</span> <span class="n">normalize_op</span><span class="p">,</span> <span class="n">changeswap_op</span><span class="p">]</span>
    <span class="n">cifar_ds</span> <span class="o">=</span> <span class="n">cifar_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">)</span>
    <span class="n">cifar_ds</span> <span class="o">=</span> <span class="n">cifar_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">c_trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">)</span>
    <span class="n">cifar_ds</span> <span class="o">=</span> <span class="n">cifar_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">cifar_ds</span> <span class="o">=</span> <span class="n">cifar_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cifar_ds</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">args_opt</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="defining-the-network">
<h3>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h3>
<p>The definition of a network is consistent with a single card network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="training-the-network">
<h3>Training the Network<a class="headerlink" href="#training-the-network" title="Permalink to this headline"></a></h3>
<p>In this step, we need to define the loss function, the optimizer, and the training process, which is written in the same way as the data parallel, also calling the <code class="docutils literal notranslate"><span class="pre">nn.DistributedGradReducer</span></code> interface to aggregate the gradients, with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DistributedGradReducer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_set</span><span class="p">:</span>
    <span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;step: </span><span class="si">%s</span><span class="s2">, loss is </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss_value</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">step</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="running-the-script">
<h3>Running the Script<a class="headerlink" href="#running-the-script" title="Permalink to this headline"></a></h3>
<p>Next, the corresponding script is called by command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh<span class="w"> </span><span class="m">96</span><span class="w"> </span>OFF
</pre></div>
</div>
<p>When training with batch_size=96 without turning on heterogeneous storage, an error ‘Memory not enough’ is reported due to insufficient memory space:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>----------------------------------------------------
-<span class="w"> </span>Framework<span class="w"> </span>Error<span class="w"> </span>Message:
----------------------------------------------------
Out<span class="w"> </span>of<span class="w"> </span>Memory!!!<span class="w"> </span>Request<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>1088627200B,<span class="w"> </span>Memory<span class="w"> </span>Statistic:
Device<span class="w"> </span>HBM<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>32768M
MindSpore<span class="w"> </span>Used<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>1024M
MindSpore<span class="w"> </span>memory<span class="w"> </span>base<span class="w"> </span>address:<span class="w"> </span>0x124140000000
Total<span class="w"> </span>Static<span class="w"> </span>Memory<span class="w"> </span>size:<span class="w"> </span>56M
Total<span class="w"> </span>Dynamic<span class="w"> </span>memory<span class="w"> </span>size:<span class="w"> </span>0M
Dynamic<span class="w"> </span>memory<span class="w"> </span>size<span class="w"> </span>of<span class="w"> </span>this<span class="w"> </span>graph:<span class="w"> </span>0M

Please<span class="w"> </span>try<span class="w"> </span>to<span class="w"> </span>reduce<span class="w"> </span><span class="s1">&#39;batch_size&#39;</span><span class="w"> </span>or<span class="w"> </span>check<span class="w"> </span>whether<span class="w"> </span>exists<span class="w"> </span>extra<span class="w"> </span>large<span class="w"> </span>shape.<span class="w"> </span>For<span class="w"> </span>more<span class="w"> </span>details,<span class="w"> </span>please<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span><span class="s1">&#39;Out of Memory&#39;</span><span class="w"> </span>at<span class="w"> </span>https://www.mindspore.cn<span class="w"> </span>.
</pre></div>
</div>
<p>After turning on heterogeneous storage, it is able to train normally with batch_size=96:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh<span class="w"> </span><span class="m">96</span><span class="w"> </span>ON
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>step:<span class="w"> </span><span class="m">0</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.3294048
step:<span class="w"> </span><span class="m">1</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.3190398
step:<span class="w"> </span><span class="m">2</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.314652
step:<span class="w"> </span><span class="m">3</span>,<span class="w"> </span>loss<span class="w"> </span>is<span class="w"> </span><span class="m">2</span>.3037016
...
</pre></div>
</div>
</section>
<section id="automatically-generating-offload-strategies">
<h3>Automatically Generating offload Strategies<a class="headerlink" href="#automatically-generating-offload-strategies" title="Permalink to this headline"></a></h3>
<p>In addition to copying data strictly according to the user <code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;</span></code> configuration, MindSpore also supports automatic generation of heterogeneous storage strategies. MindSpore can analyze the network video memory usage information and combine the user-configured <code class="docutils literal notranslate"><span class="pre">&quot;max_device_memory&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;offload_cpu_size&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;offload_disk_size&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;hbm_ratio&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&quot;cpu_ratio&quot;</span></code> to generate eterogeneous storage strategies, and then follow the established strategy to move data across multiple storage media.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>

<span class="n">offload_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;offload_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./offload/&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;auto_offload&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_cpu_size&quot;</span><span class="p">:</span> <span class="s2">&quot;512GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;offload_disk_size&quot;</span><span class="p">:</span> <span class="s2">&quot;1024GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;host_mem_block_size&quot;</span><span class="p">:</span><span class="s2">&quot;1GB&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;enable_aio&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s2">&quot;enable_pinned_mem&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">memory_offload</span><span class="o">=</span><span class="s1">&#39;ON&#39;</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="s1">&#39;30GB&#39;</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="o">=</span><span class="n">offload_config</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">&quot;auto_offload&quot;:</span> <span class="pre">True</span></code> is set, <code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;</span></code> only affects the initial storage location of the parameter, and the framework adjusts the weights and intermediate results storage location during the computation process according to the generated strategy.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="host_device_training.html" class="btn btn-neutral float-left" title="Host&amp;Device Heterogeneous" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="comm_fusion.html" class="btn btn-neutral float-right" title="Distributed Training Communication Fusion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>