<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>msrun Launching &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dynamic Cluster Startup" href="dynamic_cluster.html" />
    <link rel="prev" title="Distributed Parallel Startup Methods" href="startup_method.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">msrun Launching</a></li>
<li class="toctree-l2"><a class="reference internal" href="dynamic_cluster.html">Dynamic Cluster Startup</a></li>
<li class="toctree-l2"><a class="reference internal" href="mpirun.html">mpirun Startup</a></li>
<li class="toctree-l2"><a class="reference internal" href="rank_table.html">rank table Startup</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="startup_method.html">Distributed Parallel Startup Methods</a> &raquo;</li>
      <li>msrun Launching</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/msrun_launcher.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="msrun-launching">
<h1>msrun Launching<a class="headerlink" href="#msrun-launching" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/tutorials/experts/source_en/parallel/msrun_launcher.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">msrun</span></code> is an encapsulation of the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/parallel/dynamic_cluster.html">Dynamic Cluster</a> startup method. Users can use <code class="docutils literal notranslate"><span class="pre">msrun</span></code> to pull multi-process distributed tasks across nodes with a single command line instruction. Users can use <code class="docutils literal notranslate"><span class="pre">msrun</span></code> to pull up multi-process distributed tasks on each node with a single command line command, and there is no need to manually set <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/parallel/dynamic_cluster.html">dynamic networking environment variables</a>. <code class="docutils literal notranslate"><span class="pre">msrun</span></code> supports both <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPU</span></code> and <code class="docutils literal notranslate"><span class="pre">CPU</span></code> backends. As with the <code class="docutils literal notranslate"><span class="pre">Dynamic</span> <span class="pre">Cluster</span></code> startup, <code class="docutils literal notranslate"><span class="pre">msrun</span></code> has no dependencies on third-party libraries and configuration files.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">msrun</span></code> is available after the user installs MindSpore, and the command <code class="docutils literal notranslate"><span class="pre">msrun</span> <span class="pre">--help</span></code> can be used to view the supported parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">msrun</span></code> supports <code class="docutils literal notranslate"><span class="pre">graph</span> <span class="pre">mode</span></code> as well as <code class="docutils literal notranslate"><span class="pre">PyNative</span> <span class="pre">mode</span></code>.</p></li>
</ul>
</div></blockquote>
<p>A parameters list of command line:</p>
<table align="center">
    <tr>
        <th align="left">Parameters</th>
        <th align="left">Functions</th>
        <th align="left">Types</th>
        <th align="left">Values</th>
        <th align="left">Instructions</th>
    </tr>
    <tr>
        <td align="left">--worker_num</td>
        <td align="left">The total number of Worker processes participating in the distributed task.</td>
        <td align="left">Integer</td>
        <td align="left">An integer greater than 0. The default value is 8.</td>
        <td align="left">The total number of Workers started on each node should be equal to this parameter:<br> if the total number is greater than this parameter, the extra Worker processes will fail to register; <br>if the total number is less than this parameter, the cluster will wait for a certain period of timeout before prompting the task to pull up the failed task and exit, <br>and the size of the timeout window can be configured by the parameter <code>cluster_time_out</code>.</td>
    </tr>
    <tr>
        <td align="left">--local_worker_num</td>
        <td align="left">The number of Worker processes pulled up on the current node.</td>
        <td align="left">Integer</td>
        <td align="left">An integer greater than 0. The default value is 8.</td>
        <td align="left">When this parameter is consistent with <code>worker_num</code>, it means that all Worker processes are executed locally. <br>The <code>node_rank</code> value is ignored in this scenario.</td>
    </tr>
    <tr>
        <td align="left">--master_addr</td>
        <td align="left">Specifies the IP address of the Scheduler.</td>
        <td align="left">String</td>
        <td align="left">Legal IP address. The default is 127.0.0.1.</td>
        <td align="left">msrun will automatically detect on which node to pull up the Scheduler process, and users do not need to care. <br>If the corresponding address cannot be found, the training task will pull up and fail. <br>IPv6 addresses are not supported in the current version<br>The current version of msrun uses the <code>ip -j addr</code> command to query the current node address, which requires the user's environment to support this command.</td>
    </tr>
    <tr>
        <td align="left">--master_port</td>
        <td align="left">Specifies the Scheduler binding port number.</td>
        <td align="left">Integer</td>
        <td align="left">Port number in the range 1024 to 65535. The default is 8118.</td>
        <td align="left"></td>
    </tr>
    <tr>
        <td align="left">--node_rank</td>
        <td align="left">The index of the current node.</td>
        <td align="left">Integer</td>
        <td align="left">An integer greater than 0. The default value is -1.</td>
        <td align="left">This parameter is ignored in single-machine multi-card scenario.<br>In multi-machine and multi-card scenarios, if this parameter is not set, the rank_id of the Worker process will be assigned automatically; <br>if it is set, the rank_id will be assigned to the Worker process on each node according to the index.<br>If the number of Worker processes per node is different, it is recommended that this parameter not be configured to automatically assign the rank_id.</td>
    </tr>
    <tr>
        <td align="left">--log_dir</td>
        <td align="left">Worker, and Scheduler log output paths.</td>
        <td align="left">String</td>
        <td align="left">Folder path. Defaults to the current directory.</td>
        <td align="left">If the path does not exist, msrun creates the folder recursively.<br>The log format is as follows: for the Scheduler process, the log is named <code>scheduler.log</code>; <br>For Worker process, log name is <code>worker_[rank].log</code>, where <code>rank</code> suffix is the same as the <code>rank_id</code> assigned to the Worker, <br>but they may be inconsistent in multiple-machine and multiple-card scenarios where <code>node_rank</code> is not set. <br>It is recommended that <code>grep -rn "Global rank id"</code> is executed to view <code>rank_id</code> of each Worker.</td>
    </tr>
    <tr>
        <td align="left">--join</td>
        <td align="left">Whether msrun waits for the Worker as well as the Scheduler to exit.</td>
        <td align="left">Bool</td>
        <td align="left">True or False. Default: False.</td>
        <td align="left">If set to False, msrun will exit immediately after pulling up the process and check the logs to confirm that the distributed task is executing properly.<br>If set to True, msrun waits for all processes to exit, collects the exception log and exits.</td>
    </tr>
    <tr>
        <td align="left">--cluster_time_out</td>
        <td align="left">Cluster networking timeout in seconds.</td>
        <td align="left">Integer</td>
        <td align="left">Integer. Default: 600 seconds.</td>
        <td align="left">This parameter represents the waiting time in cluster networking. <br>If no <code>worker_num</code> number of Workers register successfully beyond this time window, the task pull-up fails.</td>
    </tr>
    <tr>
        <td align="left">task_script</td>
        <td align="left">User Python scripts.</td>
        <td align="left">String</td>
        <td align="left">Legal script path.</td>
        <td align="left">Normally, this parameter is the python script path, and msrun will pull up the process as <code>python task_script task_script_args</code> by default.<br>msrun also supports this parameter as pytest. <br>In this scenario the task script and task parameters are passed in the parameter <code>task_script_args</code>.</td>
    </tr>
    <tr>
        <td align="left">task_script_args</td>
        <td align="left">Parameters for the user Python script.</td>
        <td align="left"></td>
        <td align="left">Parameter list.</td>
        <td align="left">For example, <code>msrun --worker_num=8 --local_worker_num=8 train.py <b>--device_target=Ascend --dataset_path=/path/to/dataset</b></code></td>
    </tr>
</table>
</section>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline"></a></h2>
<p>The following table shows the environment variables can be used in user scripts, which are set by <code class="docutils literal notranslate"><span class="pre">msrun</span></code>:</p>
<table align="center">
    <tr>
        <th align="left">Environment Variables</th>
        <th align="left">Functions</th>
        <th align="left">Values</th>
    </tr>
    <tr>
        <td align="left">MS_ROLE</td>
        <td align="left">This process role.</td>
        <td align="left">
            The current version of <code>msrun</code> exports the following two values:
            <ul>
                <li>MS_SCHED: Represents the Scheduler process.</li>
                <li>MS_WORKER: Represents the Worker process.</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td align="left">MS_SCHED_HOST</td>
        <td align="left">The IP address of the user-specified Scheduler.</td>
        <td align="left">Same as parameter <code>--master_addr</code>.</td>
    </tr>
    <tr>
        <td align="left">MS_SCHED_PORT</td>
        <td align="left">User-specified Scheduler binding port number.</td>
        <td align="left">Same as parameter <code>--master_port</code>.</td>
    </tr>
    <tr>
        <td align="left">MS_WORKER_NUM</td>
        <td align="left">The total number of Worker processes specified by the user.</td>
        <td align="left">Same as parameter <code>--worker_num</code>.</td>
    </tr>
    <tr>
        <td align="left">MS_CLUSTER_TIMEOUT</td>
        <td align="left">Cluster Timeout Time.</td>
        <td align="left">Same as parameter <code>--cluster_time_out</code>.</td>
    </tr>
    <tr>
        <td align="left">RANK_SIZE</td>
        <td align="left">The total number of Worker processes specified by the user.</td>
        <td align="left">Same as parameter <code>--worker_num</code>.</td>
    </tr>
    <tr>
        <td align="left">RANK_ID</td>
        <td align="left">The rank_id assigned to the Worker process.</td>
        <td align="left">In a multi-machine multi-card scenario, if the parameter <code>--node_rank</code> is not set, <code>RANK_ID</code> will only be exported after the cluster is initialized.<br> So to use this environment variable, it is recommended to set the <code>--node_rank</code> parameter correctly.</td>
    </tr>
</table>
</section>
<section id="operating-practices">
<h2>Operating Practices<a class="headerlink" href="#operating-practices" title="Permalink to this headline"></a></h2>
<p>The startup script is consistent across hardware platforms. The following is an example of how to write a startup script for Ascend:</p>
<blockquote>
<div><p>You can download the full sample code here: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.3/docs/sample_code/startup_method">startup_method</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ startup_method
       ├── net.py
       ├── msrun_1.sh
       ├── msrun_2.sh
    ...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">net.py</span></code> defines the network structure and the training process, and <code class="docutils literal notranslate"><span class="pre">msrun_1.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">msrun_2.sh</span></code> are execution scripts started with <code class="docutils literal notranslate"><span class="pre">msrun</span></code> and executed on different nodes, respectively.</p>
<section id="1-preparing-python-training-scripts">
<h3>1. Preparing Python Training Scripts<a class="headerlink" href="#1-preparing-python-training-scripts" title="Permalink to this headline"></a></h3>
<p>Here is an example of data parallelism to train a recognition network for the MNIST dataset.</p>
<p>First specify the operation mode, hardware device, etc. Unlike single card scripts, parallel scripts also need to specify configuration items such as parallel mode and initialize the HCCL, NCCL or MCCL communication domain with <code class="docutils literal notranslate"><span class="pre">init()</span></code>. If <code class="docutils literal notranslate"><span class="pre">device_target</span></code> is not set here, it will be automatically specified as the backend hardware device corresponding to the MindSpore package.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Then build the following network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="n">bias_init</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">logits</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, the dataset is processed and the training process is defined:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">dataset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">)</span>
    <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>
    <span class="n">image_transforms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="p">]</span>
    <span class="n">label_transform</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">image_transforms</span><span class="p">,</span> <span class="s1">&#39;image&#39;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">label_transform</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DistributedGradReducer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_set</span><span class="p">:</span>
        <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch: </span><span class="si">%s</span><span class="s2">, step: </span><span class="si">%s</span><span class="s2">, loss is </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="2-preparing-the-startup-script">
<h3>2. Preparing the Startup Script<a class="headerlink" href="#2-preparing-the-startup-script" title="Permalink to this headline"></a></h3>
<section id="multi-machine-multi-card">
<h4>Multi-machine Multi-card<a class="headerlink" href="#multi-machine-multi-card" title="Permalink to this headline"></a></h4>
<blockquote>
<div><p>For msrun, execution commands of the single-machine multi-card and multi-machine multi-card are similar. The single-machine multi-card just needs to keep the parameters <code class="docutils literal notranslate"><span class="pre">worker_num</span></code> and <code class="docutils literal notranslate"><span class="pre">local_worker_num</span></code> the same, so this tutorial takes the multi-machine multi-card task as an example.</p>
</div></blockquote>
<p>The following is an example of executing 2-machine, 8-card training, with each machine executing the startup of 4 Workers:</p>
<p>The script <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/sample_code/startup_method/msrun_1.sh">msrun_1.sh</a> is executed on node 1 and uses the msrun command to pull up 1 <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> process and 4 <code class="docutils literal notranslate"><span class="pre">Worker</span></code> processes (msrun automatically detects that the current node ip matches the <code class="docutils literal notranslate"><span class="pre">master_addr</span></code> and pulls up the <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> process):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data.zip&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span>wget<span class="w"> </span>http://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip
<span class="w">    </span><span class="k">fi</span>
<span class="w">    </span>unzip<span class="w"> </span>MNIST_Data.zip
<span class="k">fi</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/MNIST_Data/train/

rm<span class="w"> </span>-rf<span class="w"> </span>msrun_log
mkdir<span class="w"> </span>msrun_log
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

msrun<span class="w"> </span>--worker_num<span class="o">=</span><span class="m">8</span><span class="w"> </span>--local_worker_num<span class="o">=</span><span class="m">4</span><span class="w"> </span>--master_addr<span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w"> </span>--master_port<span class="o">=</span><span class="m">8118</span><span class="w"> </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w"> </span>--log_dir<span class="o">=</span>msrun_log<span class="w"> </span>--join<span class="o">=</span>True<span class="w"> </span>--cluster_time_out<span class="o">=</span><span class="m">300</span><span class="w"> </span>net.py
</pre></div>
</div>
<p>The script <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/sample_code/startup_method/msrun_2.sh">msrun_2.sh</a> is executed on node 2 and uses the msrun command to pull up 4 <code class="docutils literal notranslate"><span class="pre">Worker</span></code> processes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-d<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>!<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span><span class="s2">/MNIST_Data.zip&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span>wget<span class="w"> </span>http://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip
<span class="w">    </span><span class="k">fi</span>
<span class="w">    </span>unzip<span class="w"> </span>MNIST_Data.zip
<span class="k">fi</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/MNIST_Data/train/

rm<span class="w"> </span>-rf<span class="w"> </span>msrun_log
mkdir<span class="w"> </span>msrun_log
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

msrun<span class="w"> </span>--worker_num<span class="o">=</span><span class="m">8</span><span class="w"> </span>--local_worker_num<span class="o">=</span><span class="m">4</span><span class="w"> </span>--master_addr<span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w"> </span>--master_port<span class="o">=</span><span class="m">8118</span><span class="w"> </span>--node_rank<span class="o">=</span><span class="m">1</span><span class="w"> </span>--log_dir<span class="o">=</span>msrun_log<span class="w"> </span>--join<span class="o">=</span>True<span class="w"> </span>--cluster_time_out<span class="o">=</span><span class="m">300</span><span class="w"> </span>net.py
</pre></div>
</div>
<blockquote>
<div><p>The difference between the instructions for node 2 and node 1 is that <code class="docutils literal notranslate"><span class="pre">node_rank</span></code> is different.</p>
</div></blockquote>
<p>Executed at node 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>msrun_1.sh
</pre></div>
</div>
<p>Executed at node 2:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>msrun_2.sh
</pre></div>
</div>
<p>The 2-machine, 8-card distributed training task can be executed, and the log files are saved to the <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/msrun_log</span></code> directory and the results are saved in <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/msrun_log/worker_*.log</span></code>. The Loss results are as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 0, step: 0, loss is 2.3499548
epoch: 0, step: 10, loss is 1.6682479
epoch: 0, step: 20, loss is 1.4237018
epoch: 0, step: 30, loss is 1.0437132
epoch: 0, step: 40, loss is 1.0643986
epoch: 0, step: 50, loss is 1.1021575
epoch: 0, step: 60, loss is 0.8510884
epoch: 0, step: 70, loss is 1.0581372
epoch: 0, step: 80, loss is 1.0076828
epoch: 0, step: 90, loss is 0.88950706
...
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="startup_method.html" class="btn btn-neutral float-left" title="Distributed Parallel Startup Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dynamic_cluster.html" class="btn btn-neutral float-right" title="Dynamic Cluster Startup" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>