<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pipeline Parallel &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Automatic Parallel" href="auto_parallel.html" />
    <link rel="prev" title="Optimizer Parallel" href="optimizer_parallel.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Distributed Parallel</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Distributed Parallelism Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="startup_method.html">Distributed Parallel Startup Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_parallel.html">Data Parallel</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="semi_auto_parallel.html">Semi-automatic Parallel</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="operator_parallel.html">Operator-level Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced_operator_parallel.html">Higher-order Operator-level Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer_parallel.html">Optimizer Parallel</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Pipeline Parallel</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Automatic Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_parallel.html">Manually Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_save_load.html">Model Saving and Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_technique.html">Optimization Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Experimental Characteristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">Distributed High-Level Configuration Case</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">Custom Operators (Custom-based)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid Syntax Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">Custom Operator Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">Advanced Usage of aot-type Custom Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">Sinking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">Memory Reuse</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithm Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">Second-order Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High-level Functional Programming</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">Automatic Vectorization (Vmap)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">Computing Jacobian and Hessian Matrices Using Functional Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">Auto Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">Single-Node Data Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">Optimizing the Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">Model Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Complex Problem Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">Ascend Optimization Engine (AOE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">Fault Recovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/sdc.html">Accuracy-Sensitive Detection</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="semi_auto_parallel.html">Semi-automatic Parallel</a> &raquo;</li>
      <li>Pipeline Parallel</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/pipeline_parallel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="pipeline-parallel">
<h1>Pipeline Parallel<a class="headerlink" href="#pipeline-parallel" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/tutorials/experts/source_en/parallel/pipeline_parallel.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In recent years, the scale of neural networks has increased exponentially. Limited by the memory on a single device, the number of devices used for training large models is also increasing. Due to the low communication bandwidth between servers, the performance of the conventional hybrid parallelism (data parallel + model parallel) is poor. Therefore, pipeline parallelism needs to be introduced. Pipeline parallel can divide a model in space based on stage. Each stage needs to execute only a part of the network, which greatly reduces memory overheads, shrinks the communication domain, and shortens the communication time. MindSpore can automatically convert a standalone model to the pipeline parallel mode based on user configurations.</p>
<blockquote>
<div><p>Hardware platforms supported by the pipeline parallel model include Ascend, GPU, and need to be run in Graph mode.</p>
</div></blockquote>
<p>Related interfaces:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mindspore.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL,</span> <span class="pre">pipeline_stages=NUM,</span> <span class="pre">pipeline_result_broadcast=True)</span></code>: Set semi-automatic parallel mode and set <code class="docutils literal notranslate"><span class="pre">pipeline_stages</span></code> to indicate that the total number of stages is NUM and call it before initializing the network. <code class="docutils literal notranslate"><span class="pre">pipeline_result_broadcast</span></code>: A switch that broadcast the last stage result to all other stage in pipeline parallel inference.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.PipelineCell(loss_cell,</span> <span class="pre">micro_size)</span></code>: pipeline parallelism requires wrapping a layer of <code class="docutils literal notranslate"><span class="pre">PipelineCell</span></code> around the LossCell and specifying the size of the MicroBatch. In order to improve machine utilization, MindSpore slices the MiniBatch into finer-grained MicroBatches, and the final loss is the sum of the loss values computed by all MicroBatches, where the size of the MicroBatch must be greater than or equal to the number of stages.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.PipelineGradReducer(parameters)</span></code>: pipeline parallelism requires using <code class="docutils literal notranslate"><span class="pre">PipelineGradReducer</span></code> for gradient reduction. Because the output of pipeline parallelism is derived by the addition of several micro-batch outputs, as the gradient do.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mindspore.parallel.sync_pipeline_shared_parameters(net)</span></code>: Synchronize pipeline parallel stage shared parameters.</p></li>
</ol>
</section>
<section id="basic-principle">
<h2>Basic Principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline"></a></h2>
<p>Pipeline parallel is the splitting of operators in a neural network into multiple stages, and then mapping the stages to different devices, so that different devices can compute different parts of the neural network. Pipeline parallel is suitable for graph structures where the model is linear. As shown in Figure 1, the network of 4 layers of MatMul is split into 4 stages and distributed to 4 devices. In forward calculations, each machine sends the result to the next machine through the communication operator after calculating the MatMul on the machine, and at the same time, the next machine receives (Receive) the MatMul result of the previous machine through the communication operator, and starts to calculate the MatMul on the machine; In reverse calculation, after the gradient of the last machine is calculated, the result is sent to the previous machine, and at the same time, the previous machine receives the gradient result of the last machine and begins to calculate the reverse of the current machine.</p>
<p><img alt="" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/experts/source_zh_cn/parallel/images/pipeline_parallel_image_0_zh.png" /></p>
<p><em>Figure 1: Schematic diagram of graph splitting in pipeline parallel</em></p>
<p>Simply splitting the model onto multiple devices does not bring about a performance gain, because the linear structure of the model has only one device at work at a time, while other devices are waiting, resulting in a waste of resources. In order to improve efficiency, the pipeline parallel further divides the small batch (MiniBatch) into more fine-grained micro batches (MicroBatch), and adopts a pipeline execution sequence in the micro batch, so as to achieve the purpose of improving efficiency, as shown in Figure 2. The small batches are cut into 4 micro-batches, and the 4 micro-batches are executed on 4 groups to form a pipeline. The gradient aggregation of the micro-batch is used to update the parameters, where each device only stores and updates the parameters of the corresponding group. where the white ordinal number represents the index of the micro-batch.</p>
<p><img alt="" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/experts/source_zh_cn/parallel/images/pipeline_parallel_image_1_zh.png" /></p>
<p><em>Figure 2: Schematic diagram of a pipeline parallel execution timeline with MicroBatch</em></p>
<p>In MindSpore’s pipeline parallel implementation, the execution order has been adjusted for better memory management. As shown in Figure 3, the reverse of the MicroBatch numbered 0 is performed immediately after its forward execution, so that the memory of the intermediate result of the numbered 0 MicroBatch is freed earlier (compared to Figure 2), thus ensuring that the peak memory usage is lower than in the way of Figure 2.</p>
<p><img alt="" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/experts/source_zh_cn/parallel/images/pipeline_parallel_image_2_zh.png" /></p>
<p><em>Figure 3: MindSpore Pipeline Parallel Execution Timeline Diagram</em></p>
</section>
<section id="training-operation-practices">
<h2>Training Operation Practices<a class="headerlink" href="#training-operation-practices" title="Permalink to this headline"></a></h2>
<p>The following is an illustration of pipeline parallel operation using Ascend or GPU single-machine 8-card as an example:</p>
<section id="sample-code-description">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Download the complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.3/docs/sample_code/distributed_pipeline_parallel">distributed_pipeline_parallel</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ distributed_pipeline_parallel
       ├── distributed_pipeline_parallel.py
       └── run.sh
    ...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">distributed_pipeline_parallel.py</span></code> is the script that defines the network structure and training process. <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> is the execution script.</p>
</section>
<section id="configuring-the-distributed-environment">
<h3>Configuring the Distributed Environment<a class="headerlink" href="#configuring-the-distributed-environment" title="Permalink to this headline"></a></h3>
<p>Specify the run mode, run device, run card number, etc. via the context interface. Unlike single-card scripts, parallel scripts also need to specify the parallel mode <code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code> to be semi-automatic parallel mode and initialize HCCL or NCCL communication via init. In addition, <code class="docutils literal notranslate"><span class="pre">pipeline_stages=2</span></code> should be configured to specify the total number of stages. Not setting <code class="docutils literal notranslate"><span class="pre">device_target</span></code> here automatically specifies the backend hardware device corresponding to the MindSpore package.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h3>
<p>In the pipeline parallel scenario, the dataset is loaded in the same way as a single card is loaded, with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">dataset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>
    <span class="n">image_transforms</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="p">]</span>
    <span class="n">label_transform</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">image_transforms</span><span class="p">,</span> <span class="s1">&#39;image&#39;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">label_transform</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="defining-the-network">
<h3>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h3>
<p>The pipeline parallel network structure is basically the same as the single-card network structure, and the difference is the addition of pipeline parallel strategy configuration. Pipeline parallel requires the user to define the parallel strategy by calling the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> interface to specify the stage on which each layer is to be executed. The granularity of the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> interface is <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. All <code class="docutils literal notranslate"><span class="pre">Cells</span></code> containing training parameters need to be configured with <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code>, and <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> should be configured in the order of network execution, from smallest to largest. After adding <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> configuration based on the single-card model is as follows:</p>
<blockquote>
<div><p>Under pipeline parallelism, when enabling Print/Summary/TensorDump related operators, the operator needs to be used in a Cell with the pipeline_stage attribute. Otherwise, there is a possibility that the operator will not take effect due to pipeline parallel split.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">HeUniform</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="k">class</span> <span class="nc">MatMulCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MatMulCell definition.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
        <span class="n">weight_init</span> <span class="o">=</span> <span class="n">HeUniform</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;param&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param</span> <span class="o">=</span> <span class="n">param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Print</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;out is:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">MatMulCell</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">net</span><span class="o">.</span><span class="n">relu1</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">net</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">net</span><span class="o">.</span><span class="n">relu2</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">net</span><span class="o">.</span><span class="n">layer3</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="training-the-network">
<h3>Training the Network<a class="headerlink" href="#training-the-network" title="Permalink to this headline"></a></h3>
<p>In this step, we need to define the loss function, the optimizer, and the training process, and unlike the single-card model, two interfaces need to be called in this section to configure the pipeline parallel:</p>
<ul class="simple">
<li><p>First define the LossCell. In this case the <code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code> interface is called to encapsulate the network and loss functions.</p></li>
<li><p>Finally, wrap the LossCell with <code class="docutils literal notranslate"><span class="pre">nn.PipelineCell</span></code>, and specify the size of MicroBatch. For detailed information, refer to the related interfaces in the overview.</p></li>
</ul>
<p>Besides, the interface <code class="docutils literal notranslate"><span class="pre">nn.PipelineGradReducer</span></code> is needed to handle gradient of pipeline parallelism, the first parameter of this interface is the network parameter to be updated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PipelineCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">net_with_loss</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">net_with_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">pp_grad_reducer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PipelineGradReducer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

<span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">train_one_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">pp_grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data_set</span><span class="p">:</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">train_one_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch: </span><span class="si">%s</span><span class="s2">, step: </span><span class="si">%s</span><span class="s2">, loss is </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<blockquote>
<div><p>Currently pipeline parallel does not support the automatic mixed precision.</p>
<p>Pipeline parallel training is more suitable to use <code class="docutils literal notranslate"><span class="pre">model.train</span></code> approach, because the TrainOneStep logic under pipeline parallelism is complex, while <code class="docutils literal notranslate"><span class="pre">model.train</span></code> internally encapsulates the TrainOneStepCell for pipeline parallel, which is much easier to use.</p>
</div></blockquote>
</section>
<section id="running-the-single-host-with-8-devices-script">
<h3>Running the Single-host with 8 Devices Script<a class="headerlink" href="#running-the-single-host-with-8-devices-script" title="Permalink to this headline"></a></h3>
<p>Next, the corresponding scripts are called by commands, using the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> startup method and the 8-card distributed training script as an example of distributed training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh
</pre></div>
</div>
<p>After training, the log files are saved to the <code class="docutils literal notranslate"><span class="pre">log_output</span></code> directory, where part of the file directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ log_output
    └─ 1
        ├─ rank.0
        |   └─ stdout
        ├─ rank.1
        |   └─ stdout
...
</pre></div>
</div>
<p>The results are saved in <code class="docutils literal notranslate"><span class="pre">log_output/1/rank.*/stdout</span></code>, and the example is as below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 0 step: 0, loss is 9.137518
epoch: 0 step: 10, loss is 8.826559
epoch: 0 step: 20, loss is 8.675843
epoch: 0 step: 30, loss is 8.307994
epoch: 0 step: 40, loss is 7.856993
epoch: 0 step: 50, loss is 7.0662785
...
</pre></div>
</div>
<p>The results of operator <code class="docutils literal notranslate"><span class="pre">Print</span></code> is:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>out is:
Tensor(shape=[8, 512], dtype=Float32, value=
[[ 4.61914062e-01 5.78613281e-01 1.34995094e-01 ... 8.54492188e-02 7.91992188e-01 2.13378906e-01]
...
[  4.89746094e-01 3.56689453e-01 -4.90966797e-01 ... -3.30078125e-e01 -2.38525391e-01 7.33398438e-01]])
</pre></div>
</div>
<p>Other startup methods such as dynamic cluster and <code class="docutils literal notranslate"><span class="pre">rank</span> <span class="pre">table</span></code> startup can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/parallel/startup_method.html">startup methods</a>.</p>
</section>
</section>
<section id="inference-operation-practices">
<h2>Inference Operation Practices<a class="headerlink" href="#inference-operation-practices" title="Permalink to this headline"></a></h2>
<p>The following is an illustration of pipeline parallel inference operation using Ascend or GPU single-machine 8-card as an example:</p>
<section id="sample-code-description-1">
<h3>Sample Code Description<a class="headerlink" href="#sample-code-description-1" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Download the complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.3/docs/sample_code/distributed_pipeline_parallel">distributed_pipeline_parallel</a>.</p>
</div></blockquote>
<p>The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ sample_code
    ├─ distributed_pipeline_parallel
       ├── distributed_pipeline_parallel_inference.py
       └── run_inference.sh
    ...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">distributed_pipeline_parallel_inference.py</span></code> is the script that defines the network structure and inference process. <code class="docutils literal notranslate"><span class="pre">run_inference.sh</span></code> is the execution script.</p>
</section>
<section id="configuring-the-distributed-environment-1">
<h3>Configuring the Distributed Environment<a class="headerlink" href="#configuring-the-distributed-environment-1" title="Permalink to this headline"></a></h3>
<p>Specify the run mode, run device, run card number, etc. via the context interface. Unlike single-card scripts, parallel scripts also need to specify the parallel mode <code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code> to be semi-automatic parallel mode and initialize HCCL or NCCL communication via init. In addition, <code class="docutils literal notranslate"><span class="pre">pipeline_stages=4</span></code> should be configured to specify the total number of stages. Not setting <code class="docutils literal notranslate"><span class="pre">device_target</span></code> here automatically specifies the backend hardware device corresponding to the MindSpore package. <code class="docutils literal notranslate"><span class="pre">pipeline_result_broadcast=True</span></code> specifies broadcast last stage inference to other stages. It is useful during auto-regression inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;full_batch&quot;</span><span class="p">,</span>
                             <span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pipeline_result_broadcast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

</pre></div>
</div>
</section>
<section id="defining-the-network-1">
<h3>Defining the Network<a class="headerlink" href="#defining-the-network-1" title="Permalink to this headline"></a></h3>
<p>The pipeline parallel network structure is basically the same as the single-card network structure, and the difference is the addition of pipeline parallel strategy configuration. Pipeline parallel requires the user to define the parallel strategy by calling the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> interface to specify the stage on which each layer is to be executed. The granularity of the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> interface is <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. All <code class="docutils literal notranslate"><span class="pre">Cells</span></code> containing training parameters need to be configured with <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code>, and <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> should be configured in the order of network execution, from smallest to largest. Configuration after adding <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> based on the single-card model is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">lazy_inline</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel.checkpoint_transform</span> <span class="kn">import</span> <span class="n">sync_pipeline_shared_parameters</span>

<span class="k">class</span> <span class="nc">VocabEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vocab Embedding&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">]),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">embed</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">embed</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Network&quot;&quot;&quot;</span>
    <span class="nd">@lazy_inline</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">VocabEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">embedding_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">Head</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">embed</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Define network and set pipeline stage</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">word_embedding</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">net</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">net</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">net</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">3</span>

</pre></div>
</div>
</section>
<section id="inferring-the-network">
<h3>Inferring the Network<a class="headerlink" href="#inferring-the-network" title="Permalink to this headline"></a></h3>
<p>wrap the netork with <code class="docutils literal notranslate"><span class="pre">PipelineCellInference</span></code>, and specify the size of MicroBatch. <code class="docutils literal notranslate"><span class="pre">PipelineCellInference</span></code> splits input into several micro batch, then executes the network, and finally concats the results along the batch axis through <code class="docutils literal notranslate"><span class="pre">ops.Concat</span></code> operator.</p>
<p>In the previous step, the parameter <code class="docutils literal notranslate"><span class="pre">embed</span></code> is shared by <code class="docutils literal notranslate"><span class="pre">self.word_embedding</span></code> and <code class="docutils literal notranslate"><span class="pre">self.head</span></code> layer, and these two layers are split into different stages. Before inference, executing <code class="docutils literal notranslate"><span class="pre">inference_network.compile()</span></code> and <code class="docutils literal notranslate"><span class="pre">sync_pipeline_shared_parameters(inference_network)</span></code>, the framework will synchronize the shared parameter automatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="k">class</span> <span class="nc">PipelineCellInference</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pipeline Cell Inference wrapper&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">micro_batch_num</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">micro_batch_num</span> <span class="o">=</span> <span class="n">micro_batch_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the pipeline inference&quot;&quot;&quot;</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">micro_batch_num</span><span class="p">):</span>
            <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">micro_batch_num</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">micro_batch_size</span> <span class="o">*</span> <span class="n">i</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">micro_batch_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">micro_input</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
            <span class="n">micro_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">micro_input</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span> <span class="o">+</span> <span class="p">(</span><span class="n">micro_output</span><span class="p">,)</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

<span class="n">inference_network</span> <span class="o">=</span> <span class="n">PipelineCellInference</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">micro_batch_num</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">inference_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Compile and synchronize shared parameter.</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">inference_network</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">sync_pipeline_shared_parameters</span><span class="p">(</span><span class="n">inference_network</span><span class="p">)</span>

<span class="c1"># Execute the inference network</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">inference_network</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>

</pre></div>
</div>
</section>
<section id="running-the-single-host-with-8-devices-script-1">
<h3>Running the Single-host with 8 Devices Script<a class="headerlink" href="#running-the-single-host-with-8-devices-script-1" title="Permalink to this headline"></a></h3>
<p>Next, the corresponding scripts are called by commands, using the <code class="docutils literal notranslate"><span class="pre">msrun</span></code> startup method and the 8-card distributed inference script as an example of distributed inference:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_inference.sh
</pre></div>
</div>
<p>After training, the log files are saved to the <code class="docutils literal notranslate"><span class="pre">log_output</span></code> directory, where part of the file directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ pipeline_inference_logs
   ├── scheduler.log
   ├── worker_0.log
   ├── worker_1.log
   ├── worker_2.log
...
</pre></div>
</div>
<p>The results are saved in <code class="docutils literal notranslate"><span class="pre">pipeline_inference_logs/worker_0.log</span></code>, and the example is as below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556
  0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556
  0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556
  0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556
  0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556 0.01181556
  0.01181556 0.01181556]
  ...]
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="optimizer_parallel.html" class="btn btn-neutral float-left" title="Optimizer Parallel" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="auto_parallel.html" class="btn btn-neutral float-right" title="Automatic Parallel" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>