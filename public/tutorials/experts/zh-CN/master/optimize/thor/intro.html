<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>二阶优化器THOR介绍 &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="在ResNet-50网络上应用二阶优化实践" href="resnet50.html" />
    <link rel="prev" title="二阶优化" href="../thor.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">静态图使用规范</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../network/control_flow.html">流程控制语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/jit_class.html">调用自定义类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/constexpr.html">网络内构造常量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../network/dependency_control.html">依赖控制</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/overview.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/basic_cases.html">分布式基础案例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/operator_parallel.html">算子级并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/pipeline_parallel.html">流水线并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/optimizer_parallel.html">优化器并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/recompute.html">重计算</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/host_device_training.html">Host&amp;Device异构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/parameter_server_training.html">Parameter Server模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/startup_method.html">分布式并行启动方式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_inference.html">分布式推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_case.html">分布式高阶配置案例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom_adv.html">自定义算子注册</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">性能优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op_overload.html">静态图网络编译性能优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graph_fusion_engine.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op_compilation.html">算子增量编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mem_reuse.html">内存复用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">算法优化</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adaptive_summation.html">自适应梯度求和算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dimention_reduce_training.html">降维训练算法</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../thor.html">二阶优化</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">二阶优化器THOR介绍</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#优化器背景介绍">优化器背景介绍</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#一阶优化器">一阶优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="#二阶优化器">二阶优化器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#thor的介绍">THOR的介绍</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#降低二阶信息矩阵更新频率">降低二阶信息矩阵更新频率</a></li>
<li class="toctree-l4"><a class="reference internal" href="#硬件感知矩阵切分">硬件感知矩阵切分</a></li>
<li class="toctree-l4"><a class="reference internal" href="#实验结果">实验结果</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="resnet50.html">在ResNet-50网络上应用二阶优化实践</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高阶函数式编程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vmap/vmap.html">自动向量化Vmap</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">复杂问题调试</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../debug/dump.html">Dump功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/aoe.html">AOE调优工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/fault_recover.html">故障恢复</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../thor.html">二阶优化</a> &raquo;</li>
      <li>二阶优化器THOR介绍</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/optimize/thor/intro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="二阶优化器thor介绍">
<h1>二阶优化器THOR介绍<a class="headerlink" href="#二阶优化器thor介绍" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_zh_cn/optimize/thor/intro.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png"></a></p>
<p>深度学习训练过程可以看成损失函数损失值下降过程，合适的优化器可以让深度学习训练时间大大减少。优化器可以分为一阶优化器和二阶优化器，目前业界主流使用的仍然是一阶优化器，二阶优化器因为单步训练时间过久而没有被广泛应用，而近年来，将二阶优化应用到深度学习训练中有了理论突破，并取得了不错的结果。</p>
<p>本文会介绍下优化器的背景，以及MindSpore团队自研二阶优化器THOR。</p>
<section id="优化器背景介绍">
<h2>优化器背景介绍<a class="headerlink" href="#优化器背景介绍" title="Permalink to this headline"></a></h2>
<p>假设训练样本数据集：<span class="math notranslate nohighlight">\(D = {(x_1,y_1),...,(x_i,y_i),...,(x_N,y_N)},x_i \in X,y_i\in Y\)</span>，参数θ表述的深度神经网络模型为： <span class="math notranslate nohighlight">\(\hat{y} = f(x;\theta),x\in{X}\)</span>，定义在模型输出和真实标签y之间的损失函数为：<span class="math notranslate nohighlight">\(L(y,\hat y),y \in Y\)</span>，网络参数学习的过程是最小化损失函数的过程：<span class="math notranslate nohighlight">\(\min\limits_{\theta}L(y,\hat{y})\)</span>。给定数据集、模型、损失函数后，深度学习训练问题归结为优化问题，深度神经网络训练优化问题参数规模巨大，需要大量的计算，难以计算出解析解。因此该过程也常常被比喻成下山，如图1 所示，一个人站在山顶的时候如何在有限视距内寻找最快路径下山呢？</p>
<p><img alt="The process of deeplearning training" src="../../_images/deeplearning_train_process.png" /></p>
<p><em>图1 深度学习训练过程模拟</em></p>
<p>而优化器就是在做这件事情，业界的优化算法可分为一阶优化算法和二阶优化算法。下面简单介绍下业界的优化器情况。</p>
<section id="一阶优化器">
<h3>一阶优化器<a class="headerlink" href="#一阶优化器" title="Permalink to this headline"></a></h3>
<p>梯度下降算法（Gradient Descent, GD）是机器学习中最经典的一阶优化算法，也是众多机器学习算法中最常用的优化算法。常用的一阶优化算法（比如SGD算法）中对参数的更新采用如下规则：<span class="math notranslate nohighlight">\(\theta = \theta -\eta \nabla L_\theta\)</span>，其中<span class="math notranslate nohighlight">\(\theta\)</span>是需要更新的参数，<span class="math notranslate nohighlight">\(\eta\)</span>是学习率，<span class="math notranslate nohighlight">\(\nabla L_\theta\)</span>是损失函数对于参数的梯度。</p>
<p>但是主流随机梯度下降方法有以下问题：太小的学习率会导致网络收敛过于缓慢；学习率太高可能会影响收敛，并导致损失函数在最小值上波动，甚至出现发散，对参数比较敏感；容易收敛到局部最优，难以跳出鞍点。</p>
<p>因此业界提出了很多随机梯度下降方法的改良算法，例如Momentum、Nesterov、AdaGrad、RMSprop、Adadelta和Adam等。这些改进后的优化算法可以利用随机梯度的历史信息来自适应地更新步长，使得它们更容易调参，而且方便使用。</p>
</section>
<section id="二阶优化器">
<h3>二阶优化器<a class="headerlink" href="#二阶优化器" title="Permalink to this headline"></a></h3>
<p>二阶优化算法利用目标函数的二阶导数进行曲率校正来加速一阶梯度下降。与一阶优化器相比，其收敛速度更快，能高度逼近最优值，几何上下降路径也更符合真实的最优下降路径。</p>
<p>例如，二阶优化算法中的牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。如图2 所示，左边下降路径表示牛顿法的下降曲线，右边表示一阶梯度的下降曲线，二阶算法与一阶算法相比，可以更快的走到目的地，从而加速收敛。</p>
<p><img alt="The different process of deeplearning training" src="../../_images/different_train_process.png" /></p>
<p><em>图2 不同优化器下降路径</em></p>
<p>从数学公式上来看，与一阶优化算法相比，二阶优化算法则是先将<span class="math notranslate nohighlight">\(\nabla L_{\theta}\)</span>与一个矩阵<span class="math notranslate nohighlight">\(G^{-1}\)</span>相乘，产生如下的更新规则：<span class="math notranslate nohighlight">\(\theta = \theta -\eta G^{-1}\nabla L_{\theta}\)</span>，其中G即为二阶信息矩阵，不同的二阶优化算法中的G定义是不同的，常见的二阶优化算法有牛顿法，自然梯度法等，分别对应的二阶信息矩阵G为海森矩阵，费雪矩阵。</p>
<p>牛顿法有着很好的局部收敛性质，当函数L在最优值点<span class="math notranslate nohighlight">\(\theta^{*}\)</span>点满足<span class="math notranslate nohighlight">\(\nabla L_{\theta^{*}}=0,\nabla^{2} L_{\theta^{*}}\)</span>是正定矩阵，且海森矩阵在极值点附近是李普希兹连续时，牛顿法二次收敛到最优值点。海森矩阵是一个由多变量实值函数的所有二阶偏导数组成的方块矩阵。海森矩阵可以表示为：<span class="math notranslate nohighlight">\(H_{ij} = \frac{\partial^2L}{\partial \theta_i \partial \theta_j}\)</span>，其中L即为损失函数，<span class="math notranslate nohighlight">\(\theta\)</span>是需要更新的参数。</p>
<p>在SGD中，参数空间和函数空间的度量用的都是欧式距离，但欧式距离在一些情况下不能作为函数空间准确的距离度量。例如神经网络中，参数引起的目标函数变化是概率的变化，这并不适合在欧几里得空间度量，它不是概率属性变化的合理表征。KL散度是分布之间距离的合理度量。当使用KL散度作为概率分布之间距离的度量时。此时参数更新时，用到的梯度就是自然梯度。自然梯度法中的费雪矩阵可以表示为：<span class="math notranslate nohighlight">\(F=\mathrm{E}[\frac{\partial \mathrm {log} p(y|x,\theta)}{\partial \theta}{\frac{\partial \mathrm {log} p(y|x,\theta)}{\partial \theta}}^T]\)</span>，其中P(y|x,θ)是网络模型的预测分布，p(y|x,θ)是其概率密度，θ是需要网络模型的参数。</p>
<p>二阶优化算法虽然收敛速度快，但是计算二阶矩阵的逆的时间复杂度为<span class="math notranslate nohighlight">\(\mathrm O(n^3)\)</span>，当模型参数量为<span class="math notranslate nohighlight">\(n_\theta\)</span>时，对应的二阶信息矩阵的大小为<span class="math notranslate nohighlight">\(n_\theta \times n_\theta\)</span>。在深度学习模型中，<span class="math notranslate nohighlight">\(n_\theta\)</span>常常在数百万的量级，此时二阶信息矩阵的逆无法计算。因此如何降低二阶信息矩阵求逆的计算复杂度成为关键问题。接下来介绍下深度学习中的二阶优化器。</p>
</section>
</section>
<section id="thor的介绍">
<h2>THOR的介绍<a class="headerlink" href="#thor的介绍" title="Permalink to this headline"></a></h2>
<p>当前业界已有的二阶优化算法计算量较大，与一阶相比优势不明显或者使用场景较为单一。MindSpore提出了自研算法<a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/16867">THOR(Trace-based Hardware-driven layer-ORiented Natural Gradient Descent Computation)</a>，
算法已被AAAI录用，THOR在多个场景中均有明显收益，如在BERT和ResNet50中，收敛速度均有明显优势。THOR主要做了以下两点创新：</p>
<section id="降低二阶信息矩阵更新频率">
<h3>降低二阶信息矩阵更新频率<a class="headerlink" href="#降低二阶信息矩阵更新频率" title="Permalink to this headline"></a></h3>
<p>通过实验观察费雪矩阵的F范数（Frobenius norm）在前期变化剧烈，后期逐渐变稳定，从而假设<span class="math notranslate nohighlight">\(\Big\{{F^k}\Big\}^{n}_{k=1}\)</span>是一个马尔可夫过程，可以收敛到一个稳态分布π，其中<span class="math notranslate nohighlight">\(F^k\)</span>代表第k个迭代时的费雪矩阵。因此，在训练过程中逐步增大费雪矩阵的更新间隔，可以在不影响收敛速度的情况下，减少训练时间。例如在ResNet50中，更新间隔步数随着训练的进行越来越大，到后期每个epoch只需更新一次二阶信息矩阵。</p>
<p>THOR受KFAC启发，将费雪矩阵按层解耦来降低矩阵复杂度，分别针对每一层的费雪矩阵做实验，发现有些层的费雪矩阵趋于稳态的速度更快，因此在统一的更新间隔上，更加细粒度的去调整每一层的更新频率。THOR使用矩阵的迹作为判断条件，当迹的变化情况大于某一阈值时更新该层的二阶信息矩阵，否则沿用上一个迭代的二阶信息矩阵，并且引入了停止更新机制，当迹的变化量小于某个阈值时停止更新该层二阶信息矩阵，具体更新公式如下：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
update\ F^{k}_{i} , \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\ \ if \ \Delta^{k} \in (\omega_{1},+\infty)\\
do\ not\ update\ F^{k}_{i}\ and\ set \  F^{k}_{i}=F^{k-1}_{i}, \ \quad\qquad\qquad\qquad\quad if \ \Delta^{k} \in [\omega_{2},\omega_{1}]\\
stop\ update\ F^{k}_{i}\ and\ set \  F^{k+t}_{t}\equiv F^{k-1}_{i}\ for\ all\ t=1,2,...\quad if \ \Delta^{k} \in [0,\omega_{2})
\end{cases}
\end{split}\]</div>
<p>其中：</p>
<div class="math notranslate nohighlight">
\[\Delta^k=\frac{||tr(F^k_i+\lambda I)|-|tr(F^{k-1}_i+\lambda I)||}{|tr(F^{k-1}_i+\lambda I)|}\]</div>
</section>
<section id="硬件感知矩阵切分">
<h3>硬件感知矩阵切分<a class="headerlink" href="#硬件感知矩阵切分" title="Permalink to this headline"></a></h3>
<p>THOR在将费雪矩阵按层解耦的基础上，进一步假设每个网络层中的输入和输出块之间也是独立的，例如将每层网络的输入输出切分为n个块，这n个块之间即是独立的，根据该假设对二阶信息矩阵做进一步的切分，从而提高了计算效率。THOR结合矩阵信息损失数据和矩阵性能数据确定了矩阵分块维度，从而大大提升费雪矩阵求逆时间。</p>
<p>那么如何确定矩阵分块维度的呢。具体方法为：</p>
<p>（1）根据费雪矩阵中维度最大的那一层，确定矩阵切分维度，拿ResNet50举例，网络层中的最大维度为2048，确定矩阵切分维度为[1,16,32,64,128,256,512,1024,2048]。</p>
<p>（2）根据确定的矩阵维度，根据谱范数计算每个维度下的矩阵损失，具体公式为：</p>
<div class="math notranslate nohighlight">
\[L=1-\sqrt{\frac{\lambda_{max}\ \ (\hat{A}\hat{A}^T)}{\lambda_{max}\ \ (AA^T)}}\]</div>
<p>其中<span class="math notranslate nohighlight">\(\lambda_{max}(X)\)</span>表示矩阵<span class="math notranslate nohighlight">\(X\)</span>的最大特征值，<span class="math notranslate nohighlight">\(A\)</span>表示原始未分割矩阵，<span class="math notranslate nohighlight">\(\hat A\)</span>表示分割后的矩阵。然后统计在该维度下损失小于1%的矩阵数量，最后通过除以总的矩阵数量得到标准化后的矩阵损失信息。</p>
<p>（3）根据确定的矩阵维度，计算每个维度下的矩阵求逆时间，再通过公式<span class="math notranslate nohighlight">\(normalized_n = \frac{p_1}{p_n}\)</span>得到每个维度下标准化后性能数据，其中<span class="math notranslate nohighlight">\(p_1\)</span>表示维度最小的矩阵的性能数据，<span class="math notranslate nohighlight">\(p_n\)</span>表示第n个维度下的性能数据。</p>
<p>（4）根据标注化后的矩阵损失信息和标准化后的性能数据绘图，如以ResNet50为例，如图3所示，图中下降曲线为性能曲线，上升曲线表示矩阵损失曲线，图中交叉点为106，与128最接近，最后确定矩阵切分维度为128。</p>
<p><img alt="The split dimension of matrix" src="../../_images/split_dimension.png" /></p>
<p><em>图3 切分维度确定示意图</em></p>
</section>
<section id="实验结果">
<h3>实验结果<a class="headerlink" href="#实验结果" title="Permalink to this headline"></a></h3>
<p>图4展示了THOR在ResNet50+ImageNet，batchsize为256时一二阶上的训练曲线图，其中train loss表示训练误差，test accuracy表示测试精度，epoch表示迭代数，wall-clock time表示时间，其中下降较快的曲线和上升较快的曲线是本算法曲线，另外差距较明显的曲线是momentum的训练曲线。</p>
<p><img alt="The result of ResNet50" src="../../_images/thor_in_resnet.png" /></p>
<p><em>图4 THOR在ResNet50上的实验结果</em></p>
<p>图4中的THOR，THOR_stop，THOR_NT分别表示(<span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>)=(0.01,0)，(<span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>)=(0.01,0.001)，(<span class="math notranslate nohighlight">\(w_1\)</span>,<span class="math notranslate nohighlight">\(w_2\)</span>)=(0,0)，从图中可以看到THOR收敛所需迭代数大约是一阶的一半，且单step的时间与一阶相差也不大。相比一阶算法需要117min，二阶优化器端到端时间提速约40%。</p>
<p>THOR还测试了在不同batchsize下ResNet50+ImageNet的收敛结果，结果见下图5，其中Hardware表示硬件平台，Software是指使用的深度学习框架，Batch size是每次训练的图片数量，Optimizer表示使用的优化器，Time指总体训练时间，Accuracy是指最后收敛精度。当batchsize为8192，使用256块Ascend 910时，只需2.7分钟精度即可收敛到75.9%。</p>
<p><img alt="The large batchsize result of ResNet50" src="../../_images/thor_largebs_in_resnet.png" /></p>
<p><em>图5 大batchsize下THOR在ResNet50上的实验结果</em></p>
<p>在BERT+WIkipedia中，THOR也有不错的表现效果，以MLPerf为标准，精度达到71.2%，与一阶相比端到端提升30%，实验结果见图6，图中横坐标表示训练时间，纵坐标表示测试精度，上升较快的曲线是THOR的训练曲线，另一条为Lamb的训练曲线。</p>
<p><img alt="The result of BERT" src="../../_images/thor_in_bert.png" /></p>
<p><em>图6 THOR在BERT上的实验结果</em></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../thor.html" class="btn btn-neutral float-left" title="二阶优化" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="resnet50.html" class="btn btn-neutral float-right" title="在ResNet-50网络上应用二阶优化实践" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>