<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>在ResNet-50网络上应用二阶优化实践 &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="自动向量化Vmap" href="../../vmap/vmap.html" />
    <link rel="prev" title="二阶优化器THOR介绍" href="intro.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/overview.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/startup_method.html">分布式并行启动方式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/data_parallel.html">数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/semi_auto_parallel.html">半自动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/auto_parallel.html">自动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/manual_parallel.html">手动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/parameter_server_training.html">参数服务器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/model_save_load.html">模型保存与加载</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/recover.html">故障恢复</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/optimize_technique.html">优化方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/others.html">实验特性</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallel/distributed_case.html">分布式高阶配置案例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom_adv.html">自定义算子注册</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operation/op_custom_aot.html">aot类型自定义算子进阶用法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">性能优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graph_fusion_engine.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../op_compilation.html">算子增量编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mem_reuse.html">内存复用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">算法优化</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../thor.html">二阶优化</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">二阶优化器THOR介绍</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">在ResNet-50网络上应用二阶优化实践</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#准备环节">准备环节</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#准备数据集">准备数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置分布式环境变量">配置分布式环境变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#加载处理数据集">加载处理数据集</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义网络">定义网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义损失函数及thor优化器">定义损失函数及THOR优化器</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#定义损失函数">定义损失函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#定义优化器">定义优化器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#训练网络">训练网络</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#配置模型保存">配置模型保存</a></li>
<li class="toctree-l4"><a class="reference internal" href="#配置训练网络">配置训练网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#运行脚本">运行脚本</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#模型推理">模型推理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#定义推理网络">定义推理网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#执行推理">执行推理</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高阶函数式编程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vmap/vmap.html">自动向量化Vmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func_programming/Jacobians_Hessians.html">使用函数变换计算雅可比矩阵和黑塞矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">复杂问题调试</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../debug/dump.html">Dump功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/aoe.html">AOE调优工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../debug/fault_recover.html">故障恢复</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../thor.html">二阶优化</a> &raquo;</li>
      <li>在ResNet-50网络上应用二阶优化实践</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/optimize/thor/resnet50.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="在resnet-50网络上应用二阶优化实践">
<h1>在ResNet-50网络上应用二阶优化实践<a class="headerlink" href="#在resnet-50网络上应用二阶优化实践" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_zh_cn/optimize/thor/resnet50.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>常见的优化算法可分为一阶优化算法和二阶优化算法。经典的一阶优化算法如SGD等，计算量小、计算速度快，但是收敛的速度慢，所需的迭代次数多。而二阶优化算法使用目标函数的二阶导数来加速收敛，能更快地收敛到模型最优值，所需要的迭代次数少，但由于二阶优化算法过高的计算成本，导致其总体执行时间仍然慢于一阶，故目前在深度神经网络训练中二阶优化算法的应用并不普遍。二阶优化算法的主要计算成本在于二阶信息矩阵（Hessian矩阵、<a class="reference external" href="https://arxiv.org/pdf/1808.07172.pdf">FIM矩阵</a>等）的求逆运算，时间复杂度约为<span class="math notranslate nohighlight">\(O(n^3)\)</span>。</p>
<p>MindSpore开发团队在现有的自然梯度算法的基础上，对FIM矩阵采用近似、切分等优化加速手段，极大的降低了逆矩阵的计算复杂度，开发出了可用的二阶优化器THOR。使用8块Ascend 910 AI处理器，THOR可以在72min内完成ResNet50-v1.5网络和ImageNet数据集的训练，相比于SGD+Momentum速度提升了近一倍。</p>
<p>本篇教程将主要介绍如何在Ascend 910 以及GPU上，使用MindSpore提供的二阶优化器THOR训练ResNet50-v1.5网络和ImageNet数据集。</p>
<blockquote>
<div><p>下载完整示例代码：<a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/cv/ResNet">Resnet</a>。</p>
</div></blockquote>
<p>示例代码目录结构</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── resnet
    ├── README.md
    ├── scripts
        ├── run_distribute_train.sh         # launch distributed training for Ascend 910
        ├── run_eval.sh                     # launch inference for Ascend 910
        ├── run_distribute_train_gpu.sh     # launch distributed training for GPU
        ├── run_eval_gpu.sh                 # launch inference for GPU
    ├── src
        ├── dataset.py                      # data preprocessing
        ├── CrossEntropySmooth.py           # CrossEntropy loss function
        ├── lr_generator.py                 # generate learning rate for every step
        ├── resnet.py                       # ResNet50 backbone
        ├── model_utils
            ├── config.py                   # parameter configuration
    ├── eval.py                             # infer script
    ├── train.py                            # train script
</pre></div>
</div>
<p>整体执行流程如下：</p>
<ol class="arabic simple">
<li><p>准备ImageNet数据集，处理需要的数据集；</p></li>
<li><p>定义ResNet50网络；</p></li>
<li><p>定义损失函数和THOR优化器；</p></li>
<li><p>加载数据集并进行训练，训练完成后，查看结果及保存模型文件；</p></li>
<li><p>加载保存的模型，进行推理。</p></li>
</ol>
</section>
<section id="准备环节">
<h2>准备环节<a class="headerlink" href="#准备环节" title="Permalink to this headline"></a></h2>
<p>实践前，确保已经正确安装MindSpore。如果没有，可以通过<a class="reference external" href="https://www.mindspore.cn/install">MindSpore安装页面</a>安装MindSpore。</p>
<section id="准备数据集">
<h3>准备数据集<a class="headerlink" href="#准备数据集" title="Permalink to this headline"></a></h3>
<p>下载完整的ImageNet2012数据集，将数据集解压分别存放到本地工作区的<code class="docutils literal notranslate"><span class="pre">ImageNet2012/ilsvrc</span></code>、<code class="docutils literal notranslate"><span class="pre">ImageNet2012/ilsvrc_eval</span></code>路径下。</p>
<p>目录结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─ImageNet2012
    ├─ilsvrc
    │      n03676483
    │      n04067472
    │      n01622779
    │      ......
    └─ilsvrc_eval
    │      n03018349
    │      n02504013
    │      n07871810
    │      ......
</pre></div>
</div>
</section>
<section id="配置分布式环境变量">
<h3>配置分布式环境变量<a class="headerlink" href="#配置分布式环境变量" title="Permalink to this headline"></a></h3>
<section id="ascend-910">
<h4>Ascend 910<a class="headerlink" href="#ascend-910" title="Permalink to this headline"></a></h4>
<p>Ascend 910 AI处理器的分布式环境变量配置参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/rank_table.html">rank table启动方式</a>。</p>
</section>
<section id="gpu">
<h4>GPU<a class="headerlink" href="#gpu" title="Permalink to this headline"></a></h4>
<p>GPU的分布式环境配置参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/mpirun.html">mpirun启动方式</a>。</p>
</section>
</section>
</section>
<section id="加载处理数据集">
<h2>加载处理数据集<a class="headerlink" href="#加载处理数据集" title="Permalink to this headline"></a></h2>
<p>分布式训练时，通过并行的方式加载数据集，同时通过MindSpore提供的数据增强接口对数据集进行处理。加载处理数据集的脚本在源码的<code class="docutils literal notranslate"><span class="pre">src/dataset.py</span></code>脚本中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">vision</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>


<span class="k">def</span> <span class="nf">create_dataset2</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">do_train</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">distribute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_session_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a training or evaluation ImageNet2012 dataset for ResNet50.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset_path(string): the path of dataset.</span>
<span class="sd">        do_train(bool): whether the dataset is used for training or evaluation.</span>
<span class="sd">        repeat_num(int): the repeat times of dataset. Default: 1</span>
<span class="sd">        batch_size(int): the batch size of dataset. Default: 32</span>
<span class="sd">        target(str): the device target. Default: Ascend</span>
<span class="sd">        distribute(bool): data for distribute or not. Default: False</span>
<span class="sd">        enable_cache(bool): whether tensor caching service is used for evaluation. Default: False</span>
<span class="sd">        cache_session_id(int): if enable_cache is set, cache session_id need to be provided. Default: None</span>

<span class="sd">    Returns:</span>
<span class="sd">        dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">device_num</span><span class="p">,</span> <span class="n">rank_id</span> <span class="o">=</span> <span class="n">_get_rank_info</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">distribute</span><span class="p">:</span>
            <span class="n">init</span><span class="p">()</span>
            <span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
            <span class="n">device_num</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_num</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">device_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">num_shards</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">)</span>

    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">224</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.456</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.406</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.224</span> <span class="o">*</span> <span class="mi">255</span><span class="p">,</span> <span class="mf">0.225</span> <span class="o">*</span> <span class="mi">255</span><span class="p">]</span>

    <span class="c1"># define map operations</span>
    <span class="k">if</span> <span class="n">do_train</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">RandomCropDecodeResize</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.333</span><span class="p">)),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Decode</span><span class="p">(),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">),</span>
            <span class="n">vision</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
        <span class="p">]</span>

    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="c1"># only enable cache for eval</span>
    <span class="k">if</span> <span class="n">do_train</span><span class="p">:</span>
        <span class="n">enable_cache</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">enable_cache</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_session_id</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;A cache session_id must be provided to use cache.&quot;</span><span class="p">)</span>
        <span class="n">eval_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">cache_session_id</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                <span class="n">cache</span><span class="o">=</span><span class="n">eval_cache</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="c1"># apply batch operations</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># apply dataset repeat operation</span>
    <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_set</span>
</pre></div>
</div>
<blockquote>
<div><p>MindSpore支持进行多种数据处理和增强的操作，各种操作往往组合使用，具体可以参考<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/advanced/dataset.html">数据处理</a>和<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/advanced/dataset.html">数据增强</a>章节。</p>
</div></blockquote>
</section>
<section id="定义网络">
<h2>定义网络<a class="headerlink" href="#定义网络" title="Permalink to this headline"></a></h2>
<p>本示例中使用的网络模型为ResNet50-v1.5，定义<a class="reference external" href="https://gitee.com/mindspore/models/blob/master/official/cv/ResNet/src/resnet.py">ResNet50网络</a>。</p>
<p>网络构建完成以后，在<code class="docutils literal notranslate"><span class="pre">__main__</span></code>函数中调用定义好的ResNet50：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">src.resnet</span> <span class="kn">import</span> <span class="n">resnet50</span> <span class="k">as</span> <span class="n">resnet</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define net</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="定义损失函数及thor优化器">
<h2>定义损失函数及THOR优化器<a class="headerlink" href="#定义损失函数及thor优化器" title="Permalink to this headline"></a></h2>
<section id="定义损失函数">
<h3>定义损失函数<a class="headerlink" href="#定义损失函数" title="Permalink to this headline"></a></h3>
<p>MindSpore支持的损失函数有<code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>、<code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>、<code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>等。THOR优化器需要使用<code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>损失函数。</p>
<p>损失函数的实现步骤在<code class="docutils literal notranslate"><span class="pre">src/CrossEntropySmooth.py</span></code>脚本中。这里使用了深度网络模型训练中的一个常用trick：label smoothing，通过对真实标签做平滑处理，提高模型对分类错误标签的容忍度，从而可以增加模型的泛化能力。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropySmooth</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CrossEntropy&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">smooth_factor</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CrossEntropySmooth</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">smooth_factor</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">smooth_factor</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logit</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>在<code class="docutils literal notranslate"><span class="pre">__main__</span></code>函数中调用定义好的损失函数：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">src.CrossEntropySmooth</span> <span class="kn">import</span> <span class="n">CrossEntropySmooth</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define the loss function</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">use_label_smooth</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropySmooth</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
                              <span class="n">smooth_factor</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="定义优化器">
<h3>定义优化器<a class="headerlink" href="#定义优化器" title="Permalink to this headline"></a></h3>
<p>THOR优化器的参数更新公式如下：</p>
<div class="math notranslate nohighlight">
\[ \theta^{t+1} = \theta^t + \alpha F^{-1}\nabla E\]</div>
<p>参数更新公式中各参数的含义如下：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span>：网络中的可训参数；</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>：迭代次数；</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>：学习率值，参数的更新步长；</p></li>
<li><p><span class="math notranslate nohighlight">\(F^{-1}\)</span>：FIM矩阵，在网络中计算获得；</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla E\)</span>：一阶梯度值。</p></li>
</ul>
<p>从参数更新公式中可以看出，THOR优化器需要额外计算的是每一层的FIM矩阵。FIM矩阵可以对每一层参数更新的步长和方向进行自适应的调整，加速收敛的同时可以降低调参的复杂度。</p>
<p>在调用MindSpore封装的二阶优化器THOR时，优化器会自动调用转换接口，把之前定义好的ResNet50网络中的Conv2d层和Dense层分别转换成对应的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/nn/layer/thor_layer.py">Conv2dThor</a>和<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/nn/layer/thor_layer.py">DenseThor</a>。
而在Conv2dThor和DenseThor中可以完成二阶信息矩阵的计算和存储。</p>
<blockquote>
<div><p>THOR优化器转换前后的网络backbone一致，网络参数保持不变。</p>
</div></blockquote>
<p>在训练主脚本中调用THOR优化器：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">thor</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># learning rate setting and damping setting</span>
    <span class="kn">from</span> <span class="nn">src.lr_generator</span> <span class="kn">import</span> <span class="n">get_thor_lr</span><span class="p">,</span> <span class="n">get_thor_damping</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">get_thor_lr</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_init</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_decay</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">lr_end_epoch</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">decay_epochs</span><span class="o">=</span><span class="mi">39</span><span class="p">)</span>
    <span class="n">damping</span> <span class="o">=</span> <span class="n">get_thor_damping</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">damping_init</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">damping_decay</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
    <span class="c1"># define the optimizer</span>
    <span class="n">split_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">26</span><span class="p">,</span> <span class="mi">53</span><span class="p">]</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">thor</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">damping</span><span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span>
               <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">split_indices</span><span class="o">=</span><span class="n">split_indices</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">frequency</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="训练网络">
<h2>训练网络<a class="headerlink" href="#训练网络" title="Permalink to this headline"></a></h2>
<section id="配置模型保存">
<h3>配置模型保存<a class="headerlink" href="#配置模型保存" title="Permalink to this headline"></a></h3>
<p>MindSpore提供了callback机制，可以在训练过程中执行自定义逻辑，这里使用框架提供的<code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>函数。
<code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>可以保存网络模型和参数，以便进行后续的fine-tuning操作。
<code class="docutils literal notranslate"><span class="pre">TimeMonitor</span></code>、<code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code>是MindSpore官方提供的callback函数，可以分别用于监控训练过程中单步迭代时间和<code class="docutils literal notranslate"><span class="pre">loss</span></code>值的变化。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>
<span class="o">...</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define callbacks</span>
    <span class="n">time_cb</span> <span class="o">=</span> <span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">cb</span> <span class="o">=</span> <span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">loss_cb</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">:</span>
        <span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint_epochs</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span>
                                     <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">keep_checkpoint_max</span><span class="p">)</span>
        <span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">ckpt_save_dir</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
        <span class="n">cb</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ckpt_cb</span><span class="p">]</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="配置训练网络">
<h3>配置训练网络<a class="headerlink" href="#配置训练网络" title="Permalink to this headline"></a></h3>
<p>通过MindSpore提供的<code class="docutils literal notranslate"><span class="pre">model.train</span></code>接口可以方便地进行网络的训练。THOR优化器通过降低二阶矩阵更新频率，来减少计算量，提升计算速度，故重新定义一个<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/train/train_thor/model_thor.py">ModelThor</a>类，继承MindSpore提供的Model类。在ModelThor类中获取THOR的二阶矩阵更新频率控制参数，用户可以通过调整该参数，优化整体的性能。
MindSpore提供Model类向ModelThor类的一键转换接口。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">amp</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">ConvertModelUtils</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="n">loss_scale</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
                  <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eval_network</span><span class="o">=</span><span class="n">dist_eval_network</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s2">&quot;Thor&quot;</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">ConvertModelUtils</span><span class="p">()</span><span class="o">.</span><span class="n">convert_to_thor_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
                                                          <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">},</span>
                                                          <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="运行脚本">
<h3>运行脚本<a class="headerlink" href="#运行脚本" title="Permalink to this headline"></a></h3>
<p>训练脚本定义完成之后，调<code class="docutils literal notranslate"><span class="pre">scripts</span></code>目录下的shell脚本，启动分布式训练进程。</p>
<section id="ascend-910-1">
<h4>Ascend 910<a class="headerlink" href="#ascend-910-1" title="Permalink to this headline"></a></h4>
<p>目前MindSpore分布式在Ascend上执行采用单卡单进程运行方式，即每张卡上运行1个进程，进程数量与使用的卡的数量一致。进程均放在后台执行，每个进程创建1个目录，目录名称为<code class="docutils literal notranslate"><span class="pre">train_parallel</span></code>+ <code class="docutils literal notranslate"><span class="pre">device_id</span></code>，用来保存日志信息，算子编译信息以及训练的checkpoint文件。下面以使用8张卡的分布式训练脚本为例，演示如何运行脚本。</p>
<p>使用以下命令运行脚本：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_distribute_train.sh<span class="w"> </span>&lt;RANK_TABLE_FILE&gt;<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span><span class="o">[</span>CONFIG_PATH<span class="o">]</span>
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>，<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>：组网信息文件的路径。(rank table文件的生成，参考<a class="reference external" href="https://gitee.com/mindspore/models/tree/master/utils/hccl_tools">HCCL_TOOL</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：训练数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>：配置文件路径。</p></li>
</ul>
<p>其余环境变量请参考安装教程中的配置项。</p>
<p>训练过程中loss打印示例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
epoch: 1 step: 5004, loss is 4.4182425
epoch: 2 step: 5004, loss is 3.740064
epoch: 3 step: 5004, loss is 4.0546017
epoch: 4 step: 5004, loss is 3.7598825
epoch: 5 step: 5004, loss is 3.3744206
...
epoch: 40 step: 5004, loss is 1.6907625
epoch: 41 step: 5004, loss is 1.8217756
epoch: 42 step: 5004, loss is 1.6453942
...
</pre></div>
</div>
<p>训练完后，每张卡训练产生的checkpoint文件保存在各自训练目录下，<code class="docutils literal notranslate"><span class="pre">device_0</span></code>产生的checkpoint文件示例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─train_parallel0
    ├─ckpt_0
        ├─resnet-1_5004.ckpt
        ├─resnet-2_5004.ckpt
        │      ......
        ├─resnet-42_5004.ckpt
        │      ......
</pre></div>
</div>
<p>其中，
<code class="docutils literal notranslate"><span class="pre">*.ckpt</span></code>：指保存的模型参数文件。checkpoint文件名称具体含义：<em>网络名称</em>-<em>epoch数</em>_<em>step数</em>.ckpt。</p>
</section>
<section id="gpu-1">
<h4>GPU<a class="headerlink" href="#gpu-1" title="Permalink to this headline"></a></h4>
<p>在GPU硬件平台上，MindSpore采用OpenMPI的<code class="docutils literal notranslate"><span class="pre">mpirun</span></code>进行分布式训练，进程创建1个目录，目录名称为<code class="docutils literal notranslate"><span class="pre">train_parallel</span></code>，用来保存日志信息和训练的checkpoint文件。下面以使用8张卡的分布式训练脚本为例，演示如何运行脚本。</p>
<p>使用以下命令运行脚本：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_distribute_train_gpu.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CONFIG_PATH&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：训练数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>：配置文件路径。</p></li>
</ul>
<p>在GPU训练时，无需设置<code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code>环境变量，因此在主训练脚本中不需要调用<code class="docutils literal notranslate"><span class="pre">int(os.getenv('DEVICE_ID'))</span></code>来获取卡的物理序号，同时<code class="docutils literal notranslate"><span class="pre">context</span></code>中也无需传入<code class="docutils literal notranslate"><span class="pre">device_id</span></code>。我们需要将device_target设置为GPU，并需要调用<code class="docutils literal notranslate"><span class="pre">init()</span></code>来使能NCCL。</p>
<p>训练过程中loss打印示例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
epoch: 1 step: 5004, loss is 4.2546034
epoch: 2 step: 5004, loss is 4.0819564
epoch: 3 step: 5004, loss is 3.7005644
epoch: 4 step: 5004, loss is 3.2668946
epoch: 5 step: 5004, loss is 3.023509
...
epoch: 36 step: 5004, loss is 1.645802
...
</pre></div>
</div>
<p>训练完后，保存的模型文件示例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─train_parallel
    ├─ckpt_0
        ├─resnet-1_5004.ckpt
        ├─resnet-2_5004.ckpt
        │      ......
        ├─resnet-36_5004.ckpt
        │      ......
    ......
    ├─ckpt_7
        ├─resnet-1_5004.ckpt
        ├─resnet-2_5004.ckpt
        │      ......
        ├─resnet-36_5004.ckpt
        │      ......
</pre></div>
</div>
</section>
</section>
</section>
<section id="模型推理">
<h2>模型推理<a class="headerlink" href="#模型推理" title="Permalink to this headline"></a></h2>
<p>使用训练过程中保存的checkpoint文件进行推理，验证模型的泛化能力。首先通过<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code>接口加载模型文件，然后调用<code class="docutils literal notranslate"><span class="pre">Model</span></code>的<code class="docutils literal notranslate"><span class="pre">eval</span></code>接口对输入图片类别作出预测，再与输入图片的真实类别做比较，得出最终的预测精度值。</p>
<section id="定义推理网络">
<h3>定义推理网络<a class="headerlink" href="#定义推理网络" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>使用<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code>接口加载模型文件。</p></li>
<li><p>使用<code class="docutils literal notranslate"><span class="pre">model.eval</span></code>接口读入测试数据集，进行推理。</p></li>
<li><p>计算得出预测精度值。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="o">...</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># define net</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>

    <span class="c1"># load checkpoint</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">args_opt</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># define loss</span>
    <span class="k">if</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">dataset</span> <span class="o">==</span> <span class="s2">&quot;imagenet2012&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">use_label_smooth</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropySmooth</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                                  <span class="n">smooth_factor</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smooth_factor</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>

    <span class="c1"># define model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>

    <span class="c1"># eval model</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;result:&quot;</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="s2">&quot;ckpt=&quot;</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="执行推理">
<h3>执行推理<a class="headerlink" href="#执行推理" title="Permalink to this headline"></a></h3>
<p>推理网络定义完成之后，调用<code class="docutils literal notranslate"><span class="pre">scripts</span></code>目录下的shell脚本，进行推理。</p>
<section id="ascend-910-2">
<h4>Ascend 910<a class="headerlink" href="#ascend-910-2" title="Permalink to this headline"></a></h4>
<p>在Ascend 910硬件平台上，推理的执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_eval.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CHECKPOINT_PATH&gt;<span class="w"> </span>&lt;CONFIG_PATH&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>，<code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">&lt;CONFIG_PATH&gt;</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：推理数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>：保存的checkpoint路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>：配置文件路径。</p></li>
</ul>
<p>目前推理使用的是单卡（默认device 0）进行推理，推理的结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result: {&#39;top_5_accuracy&#39;: 0.9295574583866837, &#39;top_1_accuracy&#39;: 0.761443661971831} ckpt=train_parallel0/resnet-42_5004.ckpt
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">top_5_accuracy</span></code>：对于一个输入图片，如果预测概率排名前五的标签中包含真实标签，即认为分类正确；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_1_accuracy</span></code>：对于一个输入图片，如果预测概率最大的标签与真实标签相同，即认为分类正确。</p></li>
</ul>
</section>
<section id="gpu-2">
<h4>GPU<a class="headerlink" href="#gpu-2" title="Permalink to this headline"></a></h4>
<p>在GPU硬件平台上，推理的执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>bash<span class="w"> </span>run_eval_gpu.sh<span class="w"> </span>&lt;DATASET_PATH&gt;<span class="w"> </span>&lt;CHECKPOINT_PATH&gt;<span class="w"> </span>&lt;CONFIG_PATH&gt;
</pre></div>
</div>
<p>脚本需要传入变量<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>，<code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>和<code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>，其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>：推理数据集路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHECKPOINT_PATH</span></code>：保存的checkpoint路径。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CONFIG_PATH</span></code>：配置文件路径。</p></li>
</ul>
<p>推理的结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>result: {&#39;top_5_accuracy&#39;: 0.9287972151088348, &#39;top_1_accuracy&#39;: 0.7597031049935979} ckpt=train_parallel/resnet-36_5004.ckpt
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="intro.html" class="btn btn-neutral float-left" title="二阶优化器THOR介绍" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../vmap/vmap.html" class="btn btn-neutral float-right" title="自动向量化Vmap" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>