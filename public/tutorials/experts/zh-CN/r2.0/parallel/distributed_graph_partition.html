<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>分布式图切分 &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="分布式弹性训练与推理" href="resilience_train_and_predict.html" />
    <link rel="prev" title="重计算" href="recompute.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">图编译</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">流程控制语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">静态图网络编译性能优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">调用自定义类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">网络内构造常量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">依赖控制</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型训练优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">自适应梯度求和算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">降维训练算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">二阶优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">自定义算子进阶用法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自动向量化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">自动向量化Vmap</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">调试调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/zh-CN/r2.0/accuracy_problem_preliminary_location.html">精度调优↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">快速入门分布式并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">分布式集合通信原语</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">分布式案例</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">分布式推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">保存和加载模型（HyBrid Parallel模式）</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">分布式故障恢复</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="multi_dimensional.html">多维度混合并行</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="operator_parallel.html">算子级并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeline_parallel.html">流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer_parallel.html">优化器并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="host_device_training.html">Host&amp;Device异构</a></li>
<li class="toctree-l2"><a class="reference internal" href="recompute.html">重计算</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">分布式图切分</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">分布式弹性训练与推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">其他特性</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">环境变量</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="multi_dimensional.html">多维度混合并行</a> &raquo;</li>
      <li>分布式图切分</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/distributed_graph_partition.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="分布式图切分">
<h1>分布式图切分<a class="headerlink" href="#分布式图切分" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/tutorials/experts/source_zh_cn/parallel/distributed_graph_partition.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source.png"></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>在大模型训练任务中，用户往往使用各类并行算法将计算任务分配到各个节点，以充分利用计算资源，例如通过MindSpore的<code class="docutils literal notranslate"><span class="pre">算子级并行</span></code>，<code class="docutils literal notranslate"><span class="pre">流水线并行</span></code>等特性。但是在某些场景下，用户需要根据自定义算法将图切分为多个子图，分发到不同进程分布式执行。MindSpore的<code class="docutils literal notranslate"><span class="pre">分布式图切分</span></code>特性从这一需求出发，提供了算子粒度的Python层API，让用户能自由进行图切分和构建分布式训练/推理等任务。</p>
</section>
<section id="基本原理">
<h2>基本原理<a class="headerlink" href="#基本原理" title="Permalink to this headline"></a></h2>
<p>分布式任务需要在一个集群中执行，MindSpore为了在分布式图切分场景中拥有更好的可扩展性和可靠性，复用了MindSpore内置的<code class="docutils literal notranslate"><span class="pre">动态组网</span></code>模块，此模块在<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/train_gpu.html#%E4%B8%8D%E4%BE%9D%E8%B5%96openmpi%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">不依赖OpenMPI进行训练</a>和<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/parameter_server_training.html">Parameter Server模式</a>章节也有使用。</p>
<p>对于<code class="docutils literal notranslate"><span class="pre">分布式图切分</span></code>来说，每一个进程都代表一个计算节点(称之为<code class="docutils literal notranslate"><span class="pre">Worker</span></code>)，通过上述的<code class="docutils literal notranslate"><span class="pre">动态组网</span></code>模块，启动的调度节点(称之为<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>)可以让发现各个计算节点，进而组成一个计算集群。
在<code class="docutils literal notranslate"><span class="pre">动态组网</span></code>后，MindSpore会根据用户启动配置，为每个进程分配<code class="docutils literal notranslate"><span class="pre">role</span></code>和<code class="docutils literal notranslate"><span class="pre">rank</span></code>，即每个进程的<code class="docutils literal notranslate"><span class="pre">角色</span></code>和<code class="docutils literal notranslate"><span class="pre">id</span></code>，两者组成了每个进程的唯一<code class="docutils literal notranslate"><span class="pre">标签</span></code>，并且是Python层API<code class="docutils literal notranslate"><span class="pre">place</span></code>的入参。有了这层对应关系，用户可以通过调用<code class="docutils literal notranslate"><span class="pre">place</span></code>接口，对任意算子设置进程标签，MindSpore图编译模块处理后，将计算图切分成多个子图分发到不同进程上执行。<code class="docutils literal notranslate"><span class="pre">place</span></code>具体用法可参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/ops/mindspore.ops.Primitive.html#mindspore.ops.Primitive.place">Primitive.place</a>以及<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.place">Cell.place</a>接口文档。
举例来说，经过分布式图切分后的计算拓扑图可能如下：</p>
<p><img alt="image" src="../_images/distributed_graph_partition.png" /></p>
<p>如上图所示，每个<code class="docutils literal notranslate"><span class="pre">Worker</span></code>上都有一部分用户已经切分的子图，拥有各自的权重和输入，各<code class="docutils literal notranslate"><span class="pre">Worker</span></code>间通过内置<code class="docutils literal notranslate"><span class="pre">Rpc通信算子</span></code>进行数据交互。</p>
<p>为了保证易用性和用户友好，MindSpore还支持用户只要对一份脚本作少许修改，即可启动动态组网和分布式训练(无需区分<code class="docutils literal notranslate"><span class="pre">Worker</span></code>还是<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>)。详见以下操作实践章节。</p>
</section>
<section id="操作实践">
<h2>操作实践<a class="headerlink" href="#操作实践" title="Permalink to this headline"></a></h2>
<p>以LeNet基于MNIST数据集在GPU上训练为例，将训练任务中图的不同部分拆分到不同计算节点上执行。你可以在这里下载到完整代码：<a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.0/docs/sample_code/distributed_graph_partition">https://gitee.com/mindspore/docs/tree/r2.0/docs/sample_code/distributed_graph_partition</a>。</p>
<p>目录结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>distributed_graph_partition/
├── lenet.py
├── run.sh
└── train.py
</pre></div>
</div>
<blockquote>
<div><p>此教程不涉及跨物理节点启动，所有进程都在同一节点。对MindSpore来说，节点内和跨节点分布式图切分的实现是没有区别的：通过动态组网，图切分，图编译流程后，通过Rpc通信算子进行数据交互。</p>
</div></blockquote>
<section id="训练python脚本准备">
<h3>训练Python脚本准备<a class="headerlink" href="#训练python脚本准备" title="Permalink to this headline"></a></h3>
<p>参考<a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/research/cv/lenet">https://gitee.com/mindspore/models/tree/r2.0/research/cv/lenet</a>，使用<a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST数据集</a>，了解如何训练一个LeNet网络。下面按照步骤给出训练脚本各部分代码示例。</p>
<section id="数据集加载">
<h4>数据集加载<a class="headerlink" href="#数据集加载" title="Permalink to this headline"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.transforms</span> <span class="k">as</span> <span class="nn">C</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.vision</span> <span class="k">as</span> <span class="nn">CV</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.vision</span> <span class="kn">import</span> <span class="n">Inter</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span>
                   <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    create dataset for train or test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># define dataset</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">MnistDataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">rescale_nml</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">0.3081</span>
    <span class="n">shift_nml</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="mf">0.1307</span> <span class="o">/</span> <span class="mf">0.3081</span>

    <span class="c1"># define map operations</span>
    <span class="n">resize_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">resize_height</span><span class="p">,</span> <span class="n">resize_width</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">Inter</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">)</span>  <span class="c1"># Bilinear mode</span>
    <span class="n">rescale_nml_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale_nml</span><span class="p">,</span> <span class="n">shift_nml</span><span class="p">)</span>
    <span class="n">rescale_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="n">rescale</span><span class="p">,</span> <span class="n">shift</span><span class="p">)</span>
    <span class="n">hwc2chw_op</span> <span class="o">=</span> <span class="n">CV</span><span class="o">.</span><span class="n">HWC2CHW</span><span class="p">()</span>
    <span class="n">type_cast_op</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">TypeCast</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># apply map operations on images</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">resize_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">rescale_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">rescale_nml_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">hwc2chw_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="n">num_parallel_workers</span><span class="p">)</span>

    <span class="c1"># apply DatasetOps</span>
    <span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">mnist_ds</span> <span class="o">=</span> <span class="n">mnist_ds</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mnist_ds</span>
</pre></div>
</div>
<p>以上代码创建MNIST数据集。</p>
</section>
<section id="构建lenet网络">
<h4>构建LeNet网络<a class="headerlink" href="#构建lenet网络" title="Permalink to this headline"></a></h4>
<p>为了对一个单机单卡任务进行切图，我们需要先构造一个单机单卡副本：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">TruncatedNormal</span>

<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for conv layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                     <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                     <span class="n">weight_init</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fc_with_initialize</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial for fc layer&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">weight_variable</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">weight_variable</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;weight initial&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TruncatedNormal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span> <span class="o">=</span> <span class="n">num_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="调用接口进行分布式图切分">
<h4>调用接口进行分布式图切分<a class="headerlink" href="#调用接口进行分布式图切分" title="Permalink to this headline"></a></h4>
<p>此次训练任务我们切分<code class="docutils literal notranslate"><span class="pre">fc1</span></code>到<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">0</span></code>，<code class="docutils literal notranslate"><span class="pre">fc2</span></code>到<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">1</span></code>，fc3到<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">2</span></code>，<code class="docutils literal notranslate"><span class="pre">conv1</span></code>到<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">3</span></code>，conv2到<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">4</span></code>进程。</p>
<p>在<code class="docutils literal notranslate"><span class="pre">LeNet.__init__</span></code>函数中，添加以下切图语句，即可做到分布式图切分：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">place</span><span class="p">(</span><span class="s2">&quot;MS_WORKER&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="o">...</span>
    <span class="o">...</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">place</span></code>接口第一个入参<code class="docutils literal notranslate"><span class="pre">role</span></code>为进程角色，第二个参数为进程<code class="docutils literal notranslate"><span class="pre">rank</span></code>，即代表算子在此类角色的某进程上执行。目前<code class="docutils literal notranslate"><span class="pre">place</span></code>接口只支持<code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code>角色，代表着上述的<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">X</span></code>进程。</p>
<p>由此可见，用户只需将自定义算法通过一个单机副本描述出来，再通过<code class="docutils literal notranslate"><span class="pre">place</span></code>接口设置算子所在计算节点标签，即可快速实现一个分布式训练任务。此方式的优势在于：</p>
<p><strong>1.用户无需单独对每个计算节点编写执行脚本，只需一个脚本MindSpore即可执行分布式任务</strong></p>
<p><strong>2.提供了更加通用和用户友好的接口，通过<code class="docutils literal notranslate"><span class="pre">place</span></code>接口，用户能直观的描述自己的分布式训练算法</strong></p>
</section>
<section id="定义优化器和损失函数">
<h4>定义优化器和损失函数<a class="headerlink" href="#定义优化器和损失函数" title="Permalink to this headline"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">mom_optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mom_optimizer</span>
<span class="k">def</span> <span class="nf">get_loss</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="执行训练代码">
<h4>执行训练代码<a class="headerlink" href="#执行训练代码" title="Permalink to this headline"></a></h4>
<p>训练代码入口脚本train.py：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">set_seed</span>
<span class="kn">from</span> <span class="nn">mindspore.train.metrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>


<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">get_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================= Start training =================&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">),</span> <span class="s1">&#39;train&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">TimeMonitor</span><span class="p">()],</span><span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================= Start testing =================&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;DATA_PATH&quot;</span><span class="p">),</span> <span class="s1">&#39;test&#39;</span><span class="p">))</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">if</span> <span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy is:&quot;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
<p>以上代码先训练，后推理，其中所有过程都是以分布式的方式执行的。</p>
<ul class="simple">
<li><p>在分布式图切分场景下，用户必须调用<code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code>，此接口是MindSpore的<code class="docutils literal notranslate"><span class="pre">动态组网</span></code>模块入口，用于帮助组建计算集群，通信算子初始化等。若没有调用，则MindSpore会执行单机单卡训练，即<code class="docutils literal notranslate"><span class="pre">place</span></code>接口不会生效。</p></li>
<li><p>由于某些进程上只有部分子图执行，因此它们的推理精度或者loss并没有意义。用户只需关注<code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">0</span></code>上的精度即可。</p></li>
</ul>
</section>
</section>
<section id="训练shell脚本准备">
<h3>训练Shell脚本准备<a class="headerlink" href="#训练shell脚本准备" title="Permalink to this headline"></a></h3>
<section id="启动scheduler和worker进程">
<h4>启动Scheduler和Worker进程<a class="headerlink" href="#启动scheduler和worker进程" title="Permalink to this headline"></a></h4>
<p>由于是在节点内启动多个进程，因此只需要通过一个Shell脚本启动一个<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>进程和多个<code class="docutils literal notranslate"><span class="pre">Worker</span></code>进程。此动态组网在这两章节也有详细介绍和类似用法：<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/train_gpu.html#%E4%B8%8D%E4%BE%9D%E8%B5%96openmpi%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">不依赖OpenMPI进行训练</a>和<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/parameter_server_training.html">Parameter Server模式</a>，对于脚本中的环境变量含义以及用法，可以参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/parameter_server_training.html">Parameter Server模式</a>章节。</p>
<p>run.sh执行脚本如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">execute_path</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="nv">self_path</span><span class="o">=</span><span class="k">$(</span>dirname<span class="w"> </span><span class="nv">$0</span><span class="k">)</span>

<span class="c1"># Set public environment.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">5</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>

<span class="c1"># Launch scheduler.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED
rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/
mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/
<span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/sched/<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
python<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/../train.py<span class="w"> </span>&gt;<span class="w"> </span>sched.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="nv">sched_pid</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$!</span><span class="sb">`</span>


<span class="c1"># Launch workers.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER
<span class="nv">worker_pids</span><span class="o">=()</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="nv">$MS_WORKER_NUM</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">  </span>rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/
<span class="w">  </span>mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/
<span class="w">  </span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/worker_<span class="nv">$i</span>/<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
<span class="w">  </span>python<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/../train.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">  </span>worker_pids<span class="o">[</span><span class="si">${</span><span class="nv">i</span><span class="si">}</span><span class="o">]=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$!</span><span class="sb">`</span>
<span class="k">done</span>

<span class="c1"># Wait for workers to exit.</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span><span class="w"> </span>i&lt;<span class="si">${</span><span class="nv">MS_WORKER_NUM</span><span class="si">}</span><span class="p">;</span><span class="w"> </span>i++<span class="o">))</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span><span class="nb">wait</span><span class="w"> </span><span class="si">${</span><span class="nv">worker_pids</span><span class="p">[i]</span><span class="si">}</span>
<span class="w">  </span><span class="nv">status</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$?</span><span class="sb">`</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">      </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;[ERROR] train failed. Failed to wait worker_{</span><span class="nv">$i</span><span class="s2">}, status: </span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">      </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="k">fi</span>
<span class="k">done</span>

<span class="c1"># Wait for scheduler to exit.</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">  </span><span class="nb">wait</span><span class="w"> </span><span class="si">${</span><span class="nv">sched_pid</span><span class="si">}</span>
<span class="w">  </span><span class="nv">status</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$?</span><span class="sb">`</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;[ERROR] train failed. Failed to wait scheduler, status: </span><span class="si">${</span><span class="nv">status</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="k">fi</span>
<span class="k">fi</span>

<span class="nb">exit</span><span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
<p>以上脚本中，<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_WORKER_NUM=5</span></code>代表此次分布式执行需要启动<code class="docutils literal notranslate"><span class="pre">5</span></code>个<code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code>进程；<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_SCHED_HOST=127.0.0.1</span></code>代表<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>的地址为<code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>；<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_SCHED_PORT=8118</span></code>代表<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>开放端口为8118，所有进程会向此端口连接进行动态组网。</p>
<p>上述的环境变量对于<code class="docutils literal notranslate"><span class="pre">Worker</span></code>和<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>进程都要导出，然后分别导出对应角色，启动对应角色的进程：<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_ROLE=MS_SCHED</span></code>后启动Scheduler进程；<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_ROLE=MS_WORKER</span></code>后循环启动MS_WORKER_NUM个Worker进程。</p>
<blockquote>
<div><p>注意每个进程都为后台执行，因此在脚本最后会有等待进程退出语句。</p>
</div></blockquote>
<p>执行指令</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh<span class="w"> </span><span class="o">[</span>MNIST_DATA_PATH<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="查看执行结果">
<h3>查看执行结果<a class="headerlink" href="#查看执行结果" title="Permalink to this headline"></a></h3>
<p>查看精度执行指令：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>grep<span class="w"> </span>-rn<span class="w"> </span><span class="s2">&quot;Accuracy&quot;</span><span class="w"> </span>*/*.log
</pre></div>
</div>
<p>结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Accuracy is: {&#39;Accuracy&#39;: 0.9888822115384616}
</pre></div>
</div>
<p>说明分布式图切分对LeNet训练和推理结果没有影响。</p>
</section>
</section>
<section id="总结">
<h2>总结<a class="headerlink" href="#总结" title="Permalink to this headline"></a></h2>
<p>MindSpore分布式图切分特性提供给用户算子粒度的<code class="docutils literal notranslate"><span class="pre">place</span></code>接口，支持用户根据自定义算法对计算图进行切分，通过动态组网，执行各类场景下的分布式训练等任务。
用户只需要对单机单卡副本作出以下几点修改，即可启动任务：</p>
<ul class="simple">
<li><p>在单机单卡副本最前面调用<code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code>接口，启动动态组网。</p></li>
<li><p>根据用户算法，对图中算子或者layer调用<code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code>或者<code class="docutils literal notranslate"><span class="pre">ops.Primitive</span></code>的<code class="docutils literal notranslate"><span class="pre">place</span></code>接口，设置算子所在的进程标签。</p></li>
<li><p>通过Shell脚本启动<code class="docutils literal notranslate"><span class="pre">Worker</span></code>和<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>进程，执行分布式任务。</p></li>
</ul>
<p>目前<code class="docutils literal notranslate"><span class="pre">place</span></code>接口为有限支持状态，进阶用法处于开发阶段，欢迎用户在MindSpore官网提出各类意见或issue。
<code class="docutils literal notranslate"><span class="pre">place</span></code>接口在当前存在以下限制：</p>
<ul class="simple">
<li><p>入参<code class="docutils literal notranslate"><span class="pre">role</span></code>只支持设置为<code class="docutils literal notranslate"><span class="pre">MS_WORKER</span></code>。这是因为在分布式图切分场景下每个节点都是计算节点<code class="docutils literal notranslate"><span class="pre">Worker</span></code>，设置其他角色暂不需要。</p></li>
<li><p>无法和参数服务器，数据并行，自动并行混合使用。分布式图切分后每个进程的计算图不一致，这三种特性都存在进程间图或者算子的拷贝，与本特性叠加执行可能出现未知的错误。混合使用特性将会在后续版本中支持。</p></li>
<li><p>控制流+分布式图切分处于有限支持状态，可能会出现未错误。此场景也会在后续版本中支持。</p></li>
<li><p>在<code class="docutils literal notranslate"><span class="pre">Pynative</span></code>模式下不支持<code class="docutils literal notranslate"><span class="pre">place</span></code>接口。</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="recompute.html" class="btn btn-neutral float-left" title="重计算" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="resilience_train_and_predict.html" class="btn btn-neutral float-right" title="分布式弹性训练与推理" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>