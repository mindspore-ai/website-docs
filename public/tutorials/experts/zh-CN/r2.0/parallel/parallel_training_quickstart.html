<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>快速入门分布式并行训练 &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="分布式集合通信原语" href="communicate_ops.html" />
    <link rel="prev" title="分布式并行总览" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">图编译</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">流程控制语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">静态图网络编译性能优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">调用自定义类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">网络内构造常量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">依赖控制</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型训练优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">自适应梯度求和算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">降维训练算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">二阶优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">自定义算子进阶用法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自动向量化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">自动向量化Vmap</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">调试调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/zh-CN/r2.0/accuracy_problem_preliminary_location.html">精度调优↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">分布式并行总览</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">快速入门分布式并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">分布式集合通信原语</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">分布式案例</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">分布式推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">保存和加载模型（HyBrid Parallel模式）</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">分布式故障恢复</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_dimensional.html">多维度混合并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">分布式弹性训练与推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">其他特性</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">环境变量</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>快速入门分布式并行训练</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/parallel_training_quickstart.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="快速入门分布式并行训练">
<h1>快速入门分布式并行训练<a class="headerlink" href="#快速入门分布式并行训练" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/tutorials/experts/source_zh_cn/parallel/parallel_training_quickstart.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source.png"></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>本篇教程通过一个单隐藏层全连接神经网络的简单示例，展示如何通过<strong>OpenMPI</strong>，在单机8卡的<strong>GPU</strong>环境下，进行MindSpore分布式并行训练。</p>
<p>在GPU平台上，对ResNet网络进行分布式并行训练的教程见<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/train_gpu.html">分布式并行训练基础样例（GPU）</a>。相比之下：（1）该示例使用更加复杂的ResNet网络；（2）除使用OpenMPI的方式拉起训练外，该示例还介绍使用脚本的方式拉起训练。</p>
<blockquote>
<div><p>你可以在这里下载完整的样例代码：</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.0/docs/sample_code/distributed_training_quickstart">https://gitee.com/mindspore/docs/tree/r2.0/docs/sample_code/distributed_training_quickstart</a></p>
</div></blockquote>
<p>目录结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─distributed_training_quickstart
        ├── net.py
        ├── run_with_mpi.sh
    ...
</pre></div>
</div>
<p>其中，<code class="docutils literal notranslate"><span class="pre">net.py</span></code>为网络定义脚本，<code class="docutils literal notranslate"><span class="pre">run_with_mpi.sh</span></code>是执行脚本。</p>
<blockquote>
<div><p>此外，在Ascend 910平台上进行分布式并行训练的教程详见<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/train_ascend.html">分布式并行训练基础样例（Ascend）</a>。</p>
</div></blockquote>
</section>
<section id="准备环节">
<h2>准备环节<a class="headerlink" href="#准备环节" title="Permalink to this headline"></a></h2>
<section id="数据集">
<h3>数据集<a class="headerlink" href="#数据集" title="Permalink to this headline"></a></h3>
<p>本样例随机构造一组输入数据与标签，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">label_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_per_epoch</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">label_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generate</span>
</pre></div>
</div>
<p>其中，<code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code>为训练每epoch进行的step数，<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>为批大小，<code class="docutils literal notranslate"><span class="pre">in_dim</span></code>为输入向量长度，<code class="docutils literal notranslate"><span class="pre">out_dim</span></code>为输出向量长度。</p>
</section>
<section id="网络结构">
<h3>网络结构<a class="headerlink" href="#网络结构" title="Permalink to this headline"></a></h3>
<p>本样例使用的网络代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;define net&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">]),</span> <span class="s2">&quot;w2&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>其中，<code class="docutils literal notranslate"><span class="pre">in_dim</span></code>为网络输入维度，<code class="docutils literal notranslate"><span class="pre">out_dim</span></code>为输出维度，需与数据维度匹配，而<code class="docutils literal notranslate"><span class="pre">hidden_dim</span></code>为网络隐藏层节点数。</p>
</section>
</section>
<section id="通过openmpi进行半自动并行分布式训练">
<h2>通过OpenMPI进行半自动并行分布式训练<a class="headerlink" href="#通过openmpi进行半自动并行分布式训练" title="Permalink to this headline"></a></h2>
<section id="openmpi环境配置">
<h3>OpenMPI环境配置<a class="headerlink" href="#openmpi环境配置" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://www.open-mpi.org/">OpenMPI</a>是一种高性能消息传递库，是MindSpore采用的多进程通讯库，相关环境配置见：<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/train_ascend.html#%E9%80%9A%E8%BF%87openmpi%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC">通过OpenMPI运行脚本</a>。</p>
<blockquote>
<div><p>此外，MindSpore还支持不依赖OpenMPI进行分布式训练，详见：<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/train_gpu.html#%E4%B8%8D%E4%BE%9D%E8%B5%96openmpi%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">不依赖OpenMPI进行训练</a>。</p>
</div></blockquote>
</section>
<section id="半自动并行">
<h3>半自动并行<a class="headerlink" href="#半自动并行" title="Permalink to this headline"></a></h3>
<p>目前MindSpore支持四种并行模式，详见：<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/introduction.html#%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F">分布式并行训练模式</a>。</p>
<p>本例中演示全自动并行，通过<code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context()</span></code>接口配置<code class="docutils literal notranslate"><span class="pre">parallel_mode=ms.ParallelMode.AUTO_PARALLEL</span></code>实现。
全自动并行下共有三种可配置的并行策略搜索算法，详见：<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/introduction.html#%E5%85%A8%E8%87%AA%E5%8A%A8%E5%B9%B6%E8%A1%8C">全自动并行</a>。本例中，选择<strong>切分策略传播算法</strong>，通过<code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context()</span></code>接口配置<code class="docutils literal notranslate"><span class="pre">search_mode=&quot;sharding_propagation&quot;</span></code>实现，并手动设置<code class="docutils literal notranslate"><span class="pre">matmul</span></code>算子切分策略，其他算子的切分策略由并行策略搜索算法自动给出，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;define net&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">]),</span> <span class="s2">&quot;w2&quot;</span><span class="p">)</span>

        <span class="c1"># 对matmul算子手动设置切分策略</span>
        <span class="c1"># 其中(2, 4)表示matmul算子的输入数据在batch维切分为两份，在width维切分为四份</span>
        <span class="c1"># (4, 1)表示matmul算子的权重在height维切分为四份</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>其中，<code class="docutils literal notranslate"><span class="pre">shard()</span></code>方法的详细介绍见<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.0/design/distributed_training_design.html#%E8%87%AA%E5%8A%A8%E5%B9%B6%E8%A1%8C%E5%8E%9F%E7%90%86">自动并行原理</a>，接口介绍见<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/pynative_shard_function_parallel.html">函数式算子切分</a>。</p>
<p>对于上述例子中设置的并行切分策略，在单机8卡环境下，前向传播过程的<code class="docutils literal notranslate"><span class="pre">matmul</span></code>算子计算过程示意图如下：</p>
<p><img alt="image" src="../_images/matmul_shard.png" /></p>
<p>其中，上半部分为数据切分示意图，下半部分为在逻辑号（rank）0-7号的GPU卡各自执行的计算与通信过程示意图。</p>
<section id="代码运行">
<h4>代码运行<a class="headerlink" href="#代码运行" title="Permalink to this headline"></a></h4>
<p>本例中，损失函数、优化器与训练过程的定义与单卡训练类似，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">var_step_per_epoch</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">var_single_batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">var_in_dim</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">var_hidden_dim</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">var_out_dim</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s2">&quot;../saved_graph&quot;</span><span class="p">)</span>

<span class="c1"># 单机8卡环境，并行模式为全自动并行，策略搜索设置为策略传播算法</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">,</span> <span class="n">dataset_strategy</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">)</span>

<span class="c1"># 初始化通信环境，并获取当前卡的逻辑序号，即rank_id</span>
<span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>

<span class="c1"># 随机构造数据集</span>
<span class="n">fake_dataset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">var_single_batch_size</span><span class="p">,</span> <span class="n">var_step_per_epoch</span><span class="p">,</span> <span class="n">var_in_dim</span><span class="p">,</span> <span class="n">var_out_dim</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">fake_dataset</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>

<span class="c1"># 定义网络结构</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">var_in_dim</span><span class="p">,</span> <span class="n">var_hidden_dim</span><span class="p">,</span> <span class="n">var_out_dim</span><span class="p">)</span>

<span class="c1"># 定义损失函数、callback</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
<span class="n">callback</span> <span class="o">=</span> <span class="p">[</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank_id</span><span class="p">))]</span>

<span class="c1"># 定义优化器</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">epoch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>

<span class="c1"># 模型训练</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>可以通过OpenMPI的<code class="docutils literal notranslate"><span class="pre">mpirun</span></code>命令进行训练，具体代码见脚本<code class="docutils literal notranslate"><span class="pre">run_with_mpi.sh</span></code>。</p>
<p>运行后，该脚本在后台进行，训练日志保存在<code class="docutils literal notranslate"><span class="pre">./device</span></code>目录下，逻辑编号为<code class="docutils literal notranslate"><span class="pre">rank_id</span></code>的卡的模型保存在<code class="docutils literal notranslate"><span class="pre">./device/{rank_id}</span></code>目录下。</p>
<p>此外，通过<code class="docutils literal notranslate"><span class="pre">ms.set_context()</span></code>接口配置<code class="docutils literal notranslate"><span class="pre">save_graphs=2</span></code>保存模型中间表示<code class="docutils literal notranslate"><span class="pre">MindIR</span></code>，逻辑编号为<code class="docutils literal notranslate"><span class="pre">rank_id</span></code>的卡的<code class="docutils literal notranslate"><span class="pre">MindIR</span></code>保存在<code class="docutils literal notranslate"><span class="pre">./saved_graph/{rank_id}</span></code>目录下。其中，MindSpore IR(MindIR)是MindSpore框架程序编译过程中介于源语言和目标语言之间的程序表示，以方便编译器进行程序分析和优化，详见<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.0/design/mindir.html">MindIR</a>。</p>
</section>
<section id="验证">
<h4>验证<a class="headerlink" href="#验证" title="Permalink to this headline"></a></h4>
<p>运行<code class="docutils literal notranslate"><span class="pre">run_with_mpi.sh</span></code>脚本后，记录的loss应下降，如：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ./device/train.log: #
...
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 2, loss is 0.367389976978302
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 3, loss is 0.35383114218711853
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 3 step: 4, loss is 0.3312329947948456
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 1, loss is 0.295515775680542
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
epoch: 4 step: 2, loss is 0.2440134435892105
...
</pre></div>
</div>
<p>可以在<code class="docutils literal notranslate"><span class="pre">./saved_graph/rank_x/step_parallel_begin_xxxx.ir</span></code>中，查看各算子的切分策略配置，如：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ./saved_graph/rank_0/step_parallel_begin_0041.ir: #
...
%3(out) = MatMul(%1, %2) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 4), (4, 1)), output_names: [output], transpose_a: false, transpose_x1: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (16, 32)&gt;, &lt;Tensor[Float32], (32, 16)&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%4(out) = ReLU(%3) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]} {in_strategy: ((2, 4))}
    : (&lt;Tensor[Float32], (16, 16)&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%5([CNode]472) = Load($(@1_construct_wrapper.337:para4_w2), %para12_u)
    : (&lt;Ref[Tensor(F32)], (16, 16), ref_key=:w2&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell)
%6(out) = MatMul(%4, %5) {instance name: matmul2} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (16, 16)&gt;, &lt;Tensor[Float32], (16, 16)&gt;) -&gt; (&lt;Tensor[Float32], (16, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
...
</pre></div>
</div>
<p>可以看到，<code class="docutils literal notranslate"><span class="pre">%4(out)</span></code>行对应的<code class="docutils literal notranslate"><span class="pre">relu</span></code>算子<code class="docutils literal notranslate"><span class="pre">%6(out)</span></code>行对应的<code class="docutils literal notranslate"><span class="pre">matmul2</span></code>算子被自动配置了切分策略。</p>
<p>进一步，可以在<code class="docutils literal notranslate"><span class="pre">./saved_graph/rank_x/18_execute_xxxx.ir</span></code>中，查看各卡实际执行的切分算子维度，如：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ./saved_graph/rank_0/18_execute_0185.ir: #
...
%12(equivout) = MatMul(%10, %11) {instance name: matmul} primitive_attrs: {input_names: [x1, x2], out_strategy: None, transpose_x2: false, transpose_b: false, in_strategy: ((2, 4), (4, 1)), output_names: [output], transpose_a: false, transpose_x1: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (8, 8)&gt;, &lt;Tensor[Float32], (8, 16)&gt;) -&gt; (&lt;Tensor[Float32], (8, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
    # In file /home/jenkins/my_dir/parallel_training_quick_start/device/./matmul.py(45)/        out = self.matmul(x, self.weight)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(114)/        out = self._backbone(data)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(376)/        loss = self.network(*inputs)/
%13(equiv[CNode]520) = AllReduce(%12) {instance name: forward_op_11795743325248501408} primitive_attrs: {group: 4-6301172352641561019, fusion: 0, op: sum, rank_list: (0, 1, 2, 3), group_ranks: 0-1-2-3, index: 0, group_rank_ids: (0, 1, 2, 3), no_eliminate: true} cnode_attrs: {comm_reuse: true}
    : (&lt;Tensor[Float32], (8, 16)&gt;) -&gt; (&lt;Tensor[Float32], (8, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%14(equiv[CNode]519) = StridedSlice(%13, (0, 0), (8, 4), (1, 1)) {instance name: redistribution_op_16390315056374637535StridedSlice} primitive_attrs: {new_axis_mask: 0, shrink_axis_mask: 0, end_mask: 0, input_names: [x, begin, end, strides], output_names: [output], keep_value_node_input: true, begin_mask: 0, ellipsis_mask: 0}
    : (&lt;Tensor[Float32], (8, 16)&gt;, &lt;Tuple[Int64*2], sequence_nodes={node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (0, 0), elements_use_flags: {ptr: 0x560e8fef5fa0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}}&gt;, &lt;Tuple[Int64*2], sequence_nodes={node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (8, 4), elements_use_flags: {ptr: 0x560e8fed50d0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}}&gt;, &lt;Tuple[Int64*2], sequence_nodes={node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={ValueNode&lt;ValueTuple&gt; (1, 1), elements_use_flags: {ptr: 0x560e8ffb4ff0, value: [const vector][1, 1]}}, node={&lt;freed node&gt;}}&gt;) -&gt; (&lt;Tensor[Float32], (8, 4)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
%15(equivout) = ReLU(%14) {instance name: relu} primitive_attrs: {output_names: [output], input_names: [x]} {in_strategy: ((2, 4))}
    : (&lt;Tensor[Float32], (8, 4)&gt;) -&gt; (&lt;Tensor[Float32], (8, 4)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
    # In file /home/jenkins/my_dir/parallel_training_quick_start/device/./matmul.py(46)/        out = self.relu(out)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(114)/        out = self._backbone(data)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(376)/        loss = self.network(*inputs)/
%16(equiv[CNode]472) = Load(%para4_w2, U)
    : (&lt;Ref[Tensor(F32)], (4, 16), ref_key=:w2&gt;, &lt;UMonad&gt;) -&gt; (&lt;Tensor[Float32], (4, 16)&gt;)
    # scope: (Default/network-WithLossCell)
%17(equivout) = MatMul(%15, %16) {instance name: matmul2} primitive_attrs: {output_names: [output], transpose_a: false, input_names: [x1, x2], transpose_x2: false, transpose_x1: false, transpose_b: false} {in_strategy: ((2, 4), (4, 1))}
    : (&lt;Tensor[Float32], (8, 4)&gt;, &lt;Tensor[Float32], (4, 16)&gt;) -&gt; (&lt;Tensor[Float32], (8, 16)&gt;)
    # scope: (Default/network-WithLossCell/_backbone-Net)
    # In file /home/jenkins/my_dir/parallel_training_quick_start/device/./matmul.py(47)/        out = self.matmul2(out, self.weight2)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(114)/        out = self._backbone(data)/
    # In file /home/miniconda3/envs/my_env/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py(376)/        loss = self.network(*inputs)/
...
</pre></div>
</div>
<p>可以看到，<code class="docutils literal notranslate"><span class="pre">%12(equivout)</span></code>行对应的<code class="docutils literal notranslate"><span class="pre">matmul</span></code>算子维度与图示中一致。</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-left" title="分布式并行总览" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="communicate_ops.html" class="btn btn-neutral float-right" title="分布式集合通信原语" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>