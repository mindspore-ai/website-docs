<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>动态组网启动方式 &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="在K8S集群上进行分布式训练" href="ms_operator.html" />
    <link rel="prev" title="分布式并行启动方式" href="startup_method.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">静态图使用规范</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">流程控制语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">调用自定义类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">网络内构造常量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">依赖控制</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_cases.html">分布式基础案例</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_parallel.html">算子级并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallel.html">流水线并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_parallel.html">优化器并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="recompute.html">重计算</a></li>
<li class="toctree-l1"><a class="reference internal" href="host_device_training.html">Host&amp;Device异构</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_server_training.html">Parameter Server模式</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="startup_method.html">分布式并行启动方式</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">动态组网启动方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="ms_operator.html">在K8S集群上进行分布式训练</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">分布式推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">分布式高阶配置案例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">自定义算子注册</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">性能优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r2.1/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_overload.html">静态图网络编译性能优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">算子增量编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">内存复用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">算法优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">自适应梯度求和算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">降维训练算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">二阶优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高阶函数式编程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">自动向量化Vmap</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">复杂问题调试</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Dump功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">AOE调优工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">故障恢复</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="startup_method.html">分布式并行启动方式</a> &raquo;</li>
      <li>动态组网启动方式</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/parallel/dynamic_cluster.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="动态组网启动方式">
<h1>动态组网启动方式<a class="headerlink" href="#动态组网启动方式" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.1/tutorials/experts/source_zh_cn/parallel/dynamic_cluster.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>出于训练时的可靠性要求，MindSpore提供了<strong>动态组网</strong>特性，用户能够不依赖任何第三方库(OpenMPI)来启动Ascend/GPU/CPU分布式训练任务，并且训练脚本无需做任何修改。我们建议用户优先使用此种启动方式。用户可以点击<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.1/parallel/startup_method.html">多卡启动方式</a>查看多卡启动方式在不同平台的支持情况。</p>
<p>OpenMPI在分布式训练的场景中，起到在Host侧同步数据以及进程间组网的功能；而MindSpore<strong>动态组网</strong>特性通过<strong>复用Parameter Server模式训练架构</strong>，取代了OpenMPI能力，可参考<a class="reference external" href="https://mindspore.cn/tutorials/experts/zh-CN/r2.1/parallel/parameter_server_training.html">Parameter Server模式</a>训练教程。</p>
<p><strong>动态组网</strong>特性将多个MindSpore训练进程作为<code class="docutils literal notranslate"><span class="pre">Worker</span></code>启动，并且额外启动一个<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>负责组网和容灾恢复。用户只需对启动脚本做少量修改，即可执行分布式训练。</p>
<blockquote>
<div><p>动态组网启动脚本能在多种硬件平台间快速迁移，无需对其进行额外修改。</p>
</div></blockquote>
</section>
<section id="注意事项">
<h2>注意事项<a class="headerlink" href="#注意事项" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>动态组网当前不支持<code class="docutils literal notranslate"><span class="pre">PyNative</span></code>模式。</p></li>
</ul>
</section>
<section id="环境变量">
<h2>环境变量<a class="headerlink" href="#环境变量" title="Permalink to this headline"></a></h2>
<p>动态组网启动训练脚本前需要导出若干环境变量，如下表格所示：</p>
<table align="center">
    <tr>
        <th align="left">环境变量</th>
        <th align="left">功能</th>
        <th align="left">类型</th>
        <th align="left">取值</th>
        <th align="left">说明</th>
    </tr>
    <tr>
        <td align="left">MS_ROLE</td>
        <td align="left">指定本进程角色。</td>
        <td align="left">String</td>
        <td align="left">
            <ul>
                <li>MS_SCHED: 代表Scheduler进程，一个训练任务只启动一个Scheduler，负责组网，容灾恢复等，<b>不会执行训练代码</b>。</li>
                <li>MS_WORKER: 代表Worker进程，一般设置分布式训练进程为此角色。</li>
                <li>MS_PSERVER: 代表Parameter Server进程，只有在Parameter Server模式下此角色生效，具体请参考<a href="https://mindspore.cn/tutorials/experts/zh-CN/r2.1/parallel/parameter_server_training.html">Parameter Server模式</a>。</li>
            </ul>
        </td>
        <td align="left">Worker和Parameter Server进程会向Scheduler进程注册从而完成组网。</td>
    </tr>
    <tr>
        <td align="left">MS_SCHED_HOST</td>
        <td align="left">指定Scheduler的IP地址。</td>
        <td align="left">String</td>
        <td align="left">合法的IP地址。</td>
        <td align="left">当前版本暂不支持IPv6地址。</td>
    </tr>
    <tr>
        <td align="left">MS_SCHED_PORT</td>
        <td align="left">指定Scheduler绑定端口号。</td>
        <td align="left">Integer</td>
        <td align="left">1024～65535范围内的端口号。</td>
        <td align="left"></td>
    </tr>
    <tr>
        <td align="left">MS_NODE_ID</td>
        <td align="left">指定本进程的ID，集群内唯一。</td>
        <td align="left">String</td>
        <td align="left">代表本进程的唯一ID，默认由MindSpore自动生成。</td>
        <td align="left">
            MS_NODE_ID在在以下情况需要设置，一般情况下无需设置，由MindSpore自动生成：
            <ul>
                <li>开启容灾场景：容灾恢复时需要获取当前进程ID，从而向Scheduler重新注册。</li>
                <li>开启GLOG日志重定向场景：为了保证各训练进程日志独立保存，需设置进程ID，作为日志保存路径后缀。</li>
                <li>指定进程rank id场景：用户可通过设置MS_NODE_ID为某个整数，来指定本进程的rank id。</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td align="left">MS_WORKER_NUM</td>
        <td align="left">指定角色为MS_WORKER的进程数量。</td>
        <td align="left">Integer</td>
        <td align="left">大于0的整数。</td>
        <td align="left">
            用户启动的Worker进程数量应当与此环境变量值相等。若小于此数值，组网失败；若大于此数值，Scheduler进程会根据Worker注册先后顺序完成组网，多余的Worker进程会启动失败。
        </td>
    </tr>
    <tr>
        <td align="left">MS_SERVER_NUM</td>
        <td align="left">指定角色为MS_PSERVER的进程数量。</td>
        <td align="left">Integer</td>
        <td align="left">大于0的整数。</td>
        <td align="left">只在Parameter Server训练模式下需要设置。</td>
    </tr>
    <tr>
        <td align="left">MS_ENABLE_RECOVERY</td>
        <td align="left">开启容灾。</td>
        <td align="left">Integer</td>
        <td align="left">1代表开启，0代表关闭。默认为0。</td>
        <td align="left"></td>
    </tr>
    <tr>
        <td align="left">MS_RECOVERY_PATH</td>
        <td align="left">持久化路径文件夹。</td>
        <td align="left">String</td>
        <td align="left">合法的用户目录。</td>
        <td align="left">Worker和Scheduler进程在执行过程中会进行必要的持久化，如用于恢复组网的节点信息以及训练业务中间状态等，并通过文件保存。</td>
    </tr>
    <tr>
        <td align="left">MS_HCCL_CM_INIT</td>
        <td align="left">是否使用CM方式初始化HCCL。</td>
        <td align="left">Integer</td>
        <td align="left">1代表是，0代表否。默认为0。</td>
        <td align="left">此环境变量只在<b>Ascend硬件平台并且通信域数量较多</b>的情况下建议开启。开启此环境变量后，能够降低HCCL集合通信库的内存占用，并且训练任务执行方式与`rank table`启动方式相同。</td>
    </tr>
</table>
<blockquote>
<div><p>以上环境变量在各进程启动前都需设置且<code class="docutils literal notranslate"><span class="pre">MS_SCHED_HOST</span></code>，<code class="docutils literal notranslate"><span class="pre">MS_SCHED_PORT</span></code>，<code class="docutils literal notranslate"><span class="pre">MS_WORKER_NUM</span></code>内容保持一致，否则会由于各进程配置不一致导致组网失败。</p>
</div></blockquote>
</section>
<section id="执行训练任务">
<h2>执行训练任务<a class="headerlink" href="#执行训练任务" title="Permalink to this headline"></a></h2>
<p>由于<strong>动态组网</strong>启动脚本在各硬件平台下能够保持一致，下面仅以GPU硬件平台下使用8卡分布式训练为例，演示如何编写启动脚本：</p>
<blockquote>
<div><p>样例的运行目录：<a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.1/docs/sample_code/distributed_training">distributed_training</a>。</p>
</div></blockquote>
<section id="1-准备python训练脚本">
<h3>1. 准备Python训练脚本<a class="headerlink" href="#1-准备python训练脚本" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
    <span class="n">init</span><span class="p">()</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>其中，</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=GRAPH_MODE</span></code>：使用分布式训练需要指定运行模式为图模式（当前版本<strong>动态组网</strong>特性暂不支持PyNative模式）。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init()</span></code>：初始化组网，根据<code class="docutils literal notranslate"><span class="pre">set_context</span></code>接口中指定后端，初始化集合通信库（此案例下为NCCL），完成分布式训练初始化操作。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ms.ParallelMode.DATA_PARALLEL</span></code>：设置训练模式为数据并行模式。</p></li>
</ul>
<p>动态组网还支持<strong>安全加密通道</strong>特性，支持<code class="docutils literal notranslate"><span class="pre">TLS/SSL</span></code>协议，满足用户的安全性需求。默认情况下，安全加密通道是关闭的，若需要开启，则通过<code class="docutils literal notranslate"><span class="pre">set_ps_context</span></code>正确配置安全加密通道后，才能调用init()，否则初始化组网会失败。若想使用安全加密通道，请配置：</p>
<p><code class="docutils literal notranslate"><span class="pre">set_ps_context(config_file_path=&quot;/path/to/config_file.json&quot;,</span> <span class="pre">enable_ssl=True,</span> <span class="pre">client_password=&quot;123456&quot;,</span> <span class="pre">server_password=&quot;123456&quot;)</span></code></p>
<blockquote>
<div><p>详细参数配置说明请参考Python API <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.1/api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">mindspore.set_ps_context</a>，以及本文档<a class="reference internal" href="#安全认证"><span class="std std-doc">安全认证</span></a>章节。</p>
</div></blockquote>
</section>
<section id="2-准备启动脚本">
<h3>2. 准备启动脚本<a class="headerlink" href="#2-准备启动脚本" title="Permalink to this headline"></a></h3>
<section id="单机多卡">
<h4>单机多卡<a class="headerlink" href="#单机多卡" title="Permalink to this headline"></a></h4>
<p>单机多卡启动脚本内容<code class="docutils literal notranslate"><span class="pre">run_gpu_cluster.sh</span></code>如下，在启动Worker和Scheduler之前，需要添加相关环境变量设置：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># 循环启动8个Worker训练进程</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">          </span><span class="c1"># 设置集群中Worker进程数量为8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">  </span><span class="c1"># 设置Scheduler IP地址为本地环路地址</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">       </span><span class="c1"># 设置Scheduler端口</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">        </span><span class="c1"># 设置启动的进程为MS_WORKER角色</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">                      </span><span class="c1"># 设置进程id，可选</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">                             </span><span class="c1"># 启动训练脚本</span>
<span class="k">done</span>

<span class="c1"># 启动1个Scheduler进程</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># 设置集群中Worker进程数量为8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># 设置Scheduler IP地址为本地环路地址</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># 设置Scheduler端口</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># 设置启动的进程为MS_SCHED角色</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># 启动训练脚本</span>
</pre></div>
</div>
<blockquote>
<div><p>Scheduler和Worker进程的训练脚本内容和启动方式完全一致，这是因为在MindSpore已经差异化处理了两种角色内部流程。用户只需按照普通的训练方式拉起进程即可，无需按照角色修改Python代码。这是动态组网启动脚本在多硬件平台能够保持一致的原因之一。</p>
</div></blockquote>
<p>执行如下指令，即可执行单机8卡分布式训练：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
</section>
<section id="多机多卡">
<h4>多机多卡<a class="headerlink" href="#多机多卡" title="Permalink to this headline"></a></h4>
<p>多机训练场景下，需拆分启动脚本。下面以执行2机8卡训练，每台机器执行启动4 Worker为例：</p>
<p>脚本<code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_1.sh</span></code>在节点1上启动1<code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>和<code class="docutils literal notranslate"><span class="pre">Worker1</span></code>到<code class="docutils literal notranslate"><span class="pre">Worker4</span></code>：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># 循环启动Worker1到Worker4，4个Worker训练进程</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">4</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">                    </span><span class="c1"># 设置集群中Worker进程总数为8（包括其他节点进程）</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w">  </span><span class="c1"># 设置Scheduler IP地址为节点1 IP地址</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">                 </span><span class="c1"># 设置Scheduler端口</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">                  </span><span class="c1"># 设置启动的进程为MS_WORKER角色</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">                      </span><span class="c1"># 设置进程id，可选</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">                                       </span><span class="c1"># 启动训练脚本</span>
<span class="k">done</span>

<span class="c1"># 在节点1启动1个Scheduler进程</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">                        </span><span class="c1"># 设置集群中Worker进程总数为8（包括其他节点进程）</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w">      </span><span class="c1"># 设置Scheduler IP地址为节点1 IP地址</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">                     </span><span class="c1"># 设置Scheduler端口</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">                       </span><span class="c1"># 设置启动的进程为MS_SCHED角色</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">     </span><span class="c1"># 启动训练脚本</span>
</pre></div>
</div>
<p>脚本<code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_2.sh</span></code>在节点2上启动<code class="docutils literal notranslate"><span class="pre">Worker5</span></code>到<code class="docutils literal notranslate"><span class="pre">Worker8</span></code>（无需执行Scheduler）：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># 循环启动Worker5到Worker8，4个Worker训练进程</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">4</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">                    </span><span class="c1"># 设置集群中Worker进程总数为8（包括其他节点进程）</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span>&lt;node_1<span class="w"> </span>ip<span class="w"> </span>address&gt;<span class="w">  </span><span class="c1"># 设置Scheduler IP地址为节点1 IP地址</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">                 </span><span class="c1"># 设置Scheduler端口</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">                  </span><span class="c1"># 设置启动的进程为MS_WORKER角色</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span><span class="nv">$i</span><span class="w">                      </span><span class="c1"># 设置进程id，可选</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span><span class="w">                                       </span><span class="c1"># 启动训练脚本</span>
<span class="k">done</span>
</pre></div>
</div>
<blockquote>
<div><p>多机任务<code class="docutils literal notranslate"><span class="pre">MS_WORKER_NUM</span></code>应当为集群中Worker节点总数。
节点间网络需保持连通，可使用<code class="docutils literal notranslate"><span class="pre">telnet</span> <span class="pre">&lt;scheduler</span> <span class="pre">ip&gt;</span> <span class="pre">&lt;scheduler</span> <span class="pre">port&gt;</span></code>指令测试本节点是否和已启动的Scheduler节点连通。</p>
</div></blockquote>
<p>在节点1执行：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster_1.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
<p>在节点2执行：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_gpu_cluster_2.sh<span class="w"> </span>/path/to/dataset/
</pre></div>
</div>
<p>即可执行2机8卡分布式训练任务。</p>
<blockquote>
<div><p>上述启动脚本在<code class="docutils literal notranslate"><span class="pre">Ascend</span></code>以及<code class="docutils literal notranslate"><span class="pre">CPU</span></code>硬件平台下保持一致，只需对Python训练脚本中<code class="docutils literal notranslate"><span class="pre">device_target</span></code>等硬件相关代码修改即可执行动态组网分布式训练。</p>
</div></blockquote>
</section>
</section>
<section id="3-执行结果">
<h3>3. 执行结果<a class="headerlink" href="#3-执行结果" title="Permalink to this headline"></a></h3>
<p>脚本会在后台运行，日志文件会保存到当前目录下，共跑了10个epoch，每个epoch有234个step，关于Loss部分结果保存在worker_*.log中。将loss值grep出来后，示例如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 234, loss is 2.0084016
epoch: 2 step: 234, loss is 1.6407638
epoch: 3 step: 234, loss is 1.6164391
epoch: 4 step: 234, loss is 1.6838071
epoch: 5 step: 234, loss is 1.6320667
epoch: 6 step: 234, loss is 1.3098773
epoch: 7 step: 234, loss is 1.3515002
epoch: 8 step: 234, loss is 1.2943741
epoch: 9 step: 234, loss is 1.2316195
epoch: 10 step: 234, loss is 1.1533381
</pre></div>
</div>
</section>
</section>
<section id="容灾恢复">
<h2>容灾恢复<a class="headerlink" href="#容灾恢复" title="Permalink to this headline"></a></h2>
<p>模型训练对分布式训练架构的可靠性、可服务性要求比较高，MindSpore支持数据并行下容灾恢复，多卡数据并行训练场景集群(多个Worker和1个Scheduler)中存在进程异常退出，被重新拉起后，训练任务继续能正常执行。</p>
<p>场景约束：
在图模式下，采用<code class="docutils literal notranslate"><span class="pre">MindData</span></code>进行数据下沉模式训练，开启数据并行模式，采用上述的非<code class="docutils literal notranslate"><span class="pre">OpenMPI</span></code>的方式拉起Worker进程。</p>
<p>在上述场景下，训练过程中如果有节点挂掉，保证在相同的环境变量（<code class="docutils literal notranslate"><span class="pre">MS_ENABLE_RECOVERY</span></code> 和 <code class="docutils literal notranslate"><span class="pre">MS_RECOVERY_PATH</span></code>）下，重新拉起对应进程对应的脚本后训练可继续，并且不影响精度收敛。</p>
<p>1）开启容灾：</p>
<p>通过环境变量开启容灾：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                 </span><span class="c1"># 开启容灾</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/path/to/recovery/<span class="w">  </span><span class="c1"># 配置持久化路径文件</span>
</pre></div>
</div>
<p>2）配置checkpoint保存间隔，样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span>

<span class="n">ckptconfig</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ckpoint_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt_of_rank_/&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">get_rank</span><span class="p">()),</span> <span class="n">config</span><span class="o">=</span><span class="n">ckptconfig</span><span class="p">)</span>
</pre></div>
</div>
<p>每个Worker都开启保存checkpoint，并用不同的路径（如上述样例中的directory的设置使用了rank id，保证路径不会相同），防止同名checkpoint保存冲突。checkpoint用于异常进程恢复和正常进程回滚，训练的回滚是指集群中各个Worker都恢复到最新的checkpoint对应的状态，同时数据侧也回退到对应的step，然后继续训练。保存checkpoint的间隔是可配置的，这个间隔决定了容灾恢复的粒度，间隔越小，恢复到上次保存checkpoint所回退的step数就越小，但保存checkpoint频繁也可能会影响训练效率，间隔越大则效果相反。keep_checkpoint_max至少设置为2(防止checkpoint保存失败)。</p>
<blockquote>
<div><p>样例的运行目录：<a class="reference external" href="https://gitee.com/mindspore/docs/tree/r2.1/docs/sample_code/distributed_training">distributed_training</a>。</p>
</div></blockquote>
<p>涉及到的脚本有<code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_recovery.sh</span></code>、<code class="docutils literal notranslate"><span class="pre">resnet50_distributed_training_gpu_recovery.py</span></code>、<code class="docutils literal notranslate"><span class="pre">resnet.py</span></code>。脚本内容<code class="docutils literal notranslate"><span class="pre">run_gpu_cluster_recovery.sh</span></code>如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==========================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu_cluster_recovery.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu_cluster_recovery.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;===========================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                </span><span class="c1"># 开启容灾</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/path/to/recovery/<span class="w"> </span><span class="c1"># 配置持久化路径文件夹</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>

<span class="c1"># 启动1个Scheduler进程</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># 设置集群中Worker进程数量为8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># 设置Scheduler IP地址为本地环路地址</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># 设置Scheduler端口</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># 设置启动的进程为MS_SCHED角色</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">             </span><span class="c1"># 设置本节点Node ID为&#39;sched&#39;</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># 循环启动8个Worker训练进程</span>
<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># 设置集群中Worker进程数量为8</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># 设置Scheduler IP地址为本地环路地址</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># 设置Scheduler端口</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_WORKER<span class="w">            </span><span class="c1"># 设置启动的进程为MS_WORKER角色</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>worker_<span class="nv">$i</span><span class="w">         </span><span class="c1"># 设置本节点Node ID为&#39;worker_$i&#39;</span>
<span class="w">    </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>worker_<span class="nv">$i</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>在启动Worker和Scheduler之前，需要添加相关环境变量设置，如Scheduler的IP和Port，当前进程的角色是Worker还是Scheduler。</p>
<p>执行下面的命令即可启动一个单机8卡的数据并行训练</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run_gpu_cluster_recovery.sh<span class="w"> </span>/path/to/recovery/
</pre></div>
</div>
<p>分布式训练开始，若训练过程中遇到异常，如进程异常退出，然后再重新启动对应的进程，训练流程即可恢复：
例如训练中途Scheduler进程异常退出，可执行下列命令重新启动Scheduler：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span>YOUR_DATA_PATH
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ENABLE_RECOVERY</span><span class="o">=</span><span class="m">1</span><span class="w">                </span><span class="c1"># 开启容灾功能</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_RECOVERY_PATH</span><span class="o">=</span>/path/to/recovery/<span class="w"> </span><span class="c1"># 设置容灾文件保存路径</span>

<span class="nb">cd</span><span class="w"> </span>./device

<span class="c1"># 启动1个Scheduler进程</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_WORKER_NUM</span><span class="o">=</span><span class="m">8</span><span class="w">              </span><span class="c1"># 设置集群中Worker进程数量为8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_HOST</span><span class="o">=</span><span class="m">127</span>.0.0.1<span class="w">      </span><span class="c1"># 设置Scheduler IP地址为本地环路地址</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_SCHED_PORT</span><span class="o">=</span><span class="m">8118</span><span class="w">           </span><span class="c1"># 设置Scheduler端口</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_ROLE</span><span class="o">=</span>MS_SCHED<span class="w">             </span><span class="c1"># 设置启动的进程为MS_SCHED角色</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MS_NODE_ID</span><span class="o">=</span>sched<span class="w">             </span><span class="c1"># 设置本节点Node ID为&#39;sched&#39;</span>
pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training_gpu_recovery.py<span class="w"> </span>&gt;<span class="w"> </span>scheduler.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Worker和Scheduler的组网会自动恢复。</p>
<p>Worker进程出现异常退出处理方式类似(注：Worker进程出现异常退出，需要等30s后再拉起才能恢复训练，在这之前，Scheduler为了防止网络抖动和恶意注册，拒绝相同node id的Worker再次注册)。</p>
</section>
<section id="安全认证">
<h2>安全认证<a class="headerlink" href="#安全认证" title="Permalink to this headline"></a></h2>
<p>要支持节点/进程间的SSL安全认证，要开启安全认证，通过Python API <code class="docutils literal notranslate"><span class="pre">mindspore.set_ps_context</span></code>配置<code class="docutils literal notranslate"><span class="pre">enable_ssl=True</span></code>(不传入时默认为False，表示不启用SSL安全认证)，config_file_path指定的config.json配置文件需要添加如下字段：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;server_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;server.p12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;crl_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;client_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;client.p12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;ca_cert_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ca.crt&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cipher_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ECDHE-R SA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-DSS-AES256-GCM-SHA384:DHE-PSK-AES128-GCM-SHA256:DHE-PSK-AES256-GCM-SHA384:DHE-PSK-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-PSK-CHACHA20-POLY1305:DHE-RSA-AES128-CCM:DHE-RSA-AES256-CCM:DHE-RSA-CHACHA20-POLY1305:DHE-PSK-AES128-CCM:DHE-PSK-AES256-CCM:ECDHE-ECDSA-AES128-CCM:ECDHE-ECDSA-AES256-CCM:ECDHE-ECDSA-CHACHA20-POLY1305&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;cert_expire_warning_time_in_day&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">90</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>server_cert_path：服务端包含了证书和秘钥的密文的p12文件（SSL专用证书文件）路径。</p></li>
<li><p>crl_path：吊销列表（用于区分无效不可信证书和有效可信证书）的文件路径。</p></li>
<li><p>client_cert_path：客户端包含了证书和秘钥的密文的p12文件（SSL专用证书文件）路径。</p></li>
<li><p>ca_cert_path：根证书路径。</p></li>
<li><p>cipher_list：密码套件（支持的SSL加密类型列表）。</p></li>
<li><p>cert_expire_warning_time_in_day：证书过期的告警时间。</p></li>
</ul>
<p>p12文件中的秘钥为密文存储，在启动时需要传入密码，具体参数请参考Python API <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.1/api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">mindspore.set_ps_context</a>中的<code class="docutils literal notranslate"><span class="pre">client_password</span></code>以及<code class="docutils literal notranslate"><span class="pre">server_password</span></code>字段。</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="startup_method.html" class="btn btn-neutral float-left" title="分布式并行启动方式" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ms_operator.html" class="btn btn-neutral float-right" title="在K8S集群上进行分布式训练" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>