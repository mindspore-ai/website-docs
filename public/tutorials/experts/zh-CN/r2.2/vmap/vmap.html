<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>自动向量化Vmap &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="使用函数变换计算雅可比矩阵和黑塞矩阵" href="../func_programming/Jacobians_Hessians.html" />
    <link rel="prev" title="在ResNet-50网络上应用二阶优化实践" href="../optimize/thor/resnet50.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/overview.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/startup_method.html">分布式并行启动方式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/data_parallel.html">数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/semi_auto_parallel.html">半自动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/auto_parallel.html">自动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/manual_parallel.html">手动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parameter_server_training.html">参数服务器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/model_save_load.html">模型保存与加载</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/recover.html">故障恢复</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/optimize_technique.html">优化方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/others.html">实验特性</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">分布式高阶配置案例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">自定义算子注册</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_aot.html">aot类型自定义算子进阶用法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">性能优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r2.2/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/op_compilation.html">算子增量编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">内存复用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">算法优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">二阶优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高阶函数式编程</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">自动向量化Vmap</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#向量化思维">向量化思维</a></li>
<li class="toctree-l2"><a class="reference internal" href="#手动向量化">手动向量化</a></li>
<li class="toctree-l2"><a class="reference internal" href="#自动向量化">自动向量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#高阶函数的嵌套">高阶函数的嵌套</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cell的自动向量化">Cell的自动向量化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#模型集成场景">模型集成场景</a></li>
<li class="toctree-l3"><a class="reference internal" href="#更多实践案例">更多实践案例</a></li>
<li class="toctree-l3"><a class="reference internal" href="#总结">总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">使用函数变换计算雅可比矩阵和黑塞矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">复杂问题调试</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Dump功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">AOE调优工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">故障恢复</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>自动向量化Vmap</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/vmap/vmap.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="自动向量化vmap">
<h1>自动向量化Vmap<a class="headerlink" href="#自动向量化vmap" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r2.2/tutorials/experts/zh_cn/vmap/mindspore_vmap.ipynb"><img alt="下载Notebook" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_notebook.svg" /></a> <a class="reference external" href="https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r2.2/tutorials/experts/zh_cn/vmap/mindspore_vmap.py"><img alt="下载样例代码" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_download_code.svg" /></a> <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/tutorials/experts/source_zh_cn/vmap/vmap.ipynb"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline"></a></h2>
<p>AI融合计算的蓬勃发展，对框架能力提出了新的需求和挑战。问题场景和模型设计逐渐复杂化，使得业务数据的维度和计算逻辑的嵌套深度也相应增长。结合向量化优化手段可以有效优化性能瓶颈，但实现向量化优化对于普通用户并非易事。虽然用户可以很容易地实现低维数据运算逻辑，但随着数据维度的增长，业务逻辑也变得更为复杂，用户需要清晰了解各操作间的数据维度的逻辑映射关系，给用户的模型设计和编码带来了巨大挑战。自动向量化特性（Vmap）帮助用户解决了这个头疼的问题，该技术允许用户将特定的批处理逻辑从函数中剥离。用户在编写函数时，只需要先考虑低维的运算逻辑即可，通过调用<code class="docutils literal notranslate"><span class="pre">vmap</span></code>接口自动实现高维运算，并且支持嵌套调用，有效降低问题复杂度。</p>
<p>本教程介绍自动向量化<code class="docutils literal notranslate"><span class="pre">vmap</span></code>接口的使用方式，将模型或函数中高度重复的运算逻辑转换为并行的向量运算逻辑，从而获得更加精简的代码逻辑以及更高效的执行性能。</p>
</section>
<section id="向量化思维">
<h2>向量化思维<a class="headerlink" href="#向量化思维" title="Permalink to this headline"></a></h2>
<p>向量化思维在提升计算性能的技术中十分常见，向量化思维可公式化表示为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{matrix}
a_{1} + b_{1} = c_{1} \\
a_{2} + b_{2} = c_{2} \\
a_{3} + b_{3} = c_{3} \\
a_{4} + b_{4} = c_{4}
\end{matrix} \Rightarrow \vec{a} + \vec{b} = \vec{c}\end{split}\]</div>
<p>其核心思想在于将多次for循环的计算逻辑转换为一次对于向量的计算。如果将单个操作抽象为一个函数或者一个模型的操作集合，同样可应用向量化思维方式来处理。</p>
</section>
<section id="手动向量化">
<h2>手动向量化<a class="headerlink" href="#手动向量化" title="Permalink to this headline"></a></h2>
<p>首先，我们先构造一个简单的卷积函数，适用于一维向量场景：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>
<span class="kn">import</span> <span class="nn">mindspore.numpy</span> <span class="k">as</span> <span class="nn">mnp</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">convolve</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span> <span class="n">w</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">mnp</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="n">convolve</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3], dtype=Float32, value= [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01])
</pre></div></div>
</div>
<p>当我们期望该函数运用于计算一批一维的卷积运算时，我们很自然地会想到调用for循环进行批处理：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_batch</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>
<span class="n">w_batch</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">manually_batch_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w_batch</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convolve</span><span class="p">(</span><span class="n">x_batch</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">mnp</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="n">manually_batch_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 3], dtype=Float32, value=
[[ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01]])
</pre></div></div>
</div>
<p>很显然，通过这种实现方式我们能够得到正确的计算结果，但效率并不高。 当然，您也可以通过自己手动重写函数实现更高效率的向量化计算逻辑，但这将涉及对数据的索引、轴等信息的处理。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">manually_vectorization_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w_batch</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x_batch</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">w_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">mnp</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">manually_vectorization_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 3], dtype=Float32, value=
[[ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01]])
</pre></div></div>
</div>
<p>在低维场景下，您可以很容易把握数据索引间的映射逻辑，但随着维度的增加，计算逻辑也变得更为复杂，您或许也会为此混乱的逻辑感到头疼。 幸运的是Vmap为我们提供了另一种实现方式。</p>
</section>
<section id="自动向量化">
<h2>自动向量化<a class="headerlink" href="#自动向量化" title="Permalink to this headline"></a></h2>
<p>Vmap可以帮助我们隐藏批处理维度，您只需要调用一个接口便可以将函数转换为向量化形式。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">vmap</span>

<span class="n">auto_vectorization_conv</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">)</span>
<span class="n">auto_vectorization_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w_batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 3], dtype=Float32, value=
[[ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01]])
</pre></div></div>
</div>
<p>Vmap除了为您提供了简易的编程体验外，将循环逻辑下沉至函数的各个基元操作中，结合分布式并行优化以获得更高的执行性能。 默认情况下，<code class="docutils literal notranslate"><span class="pre">vmap</span></code>的输入输出沿第一个轴进行批处理，如果您的输入和输出并不总是期望沿0轴批处理，可以通过<code class="docutils literal notranslate"><span class="pre">in_axes</span></code>和<code class="docutils literal notranslate"><span class="pre">out_axes</span></code>参数进行指定。您可以为所有输入或输出位置分别指定批处理轴索引，也可以为所有输入或输出指定相同的批处理轴索引。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_batch_t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w_batch</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="n">auto_vectorization_conv</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">auto_vectorization_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w_batch_t</span><span class="p">)</span>

<span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 3], dtype=Float32, value=
[[ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01]])
</pre></div></div>
</div>
<p>对于多个输入的场景，您还可以指定只对其中的某些入参进行批处理，如上述场景变为求一组一维向量与某一权重的卷积，可在<code class="docutils literal notranslate"><span class="pre">in_axes</span></code>参数中的输入对应位置配置<code class="docutils literal notranslate"><span class="pre">None</span></code>即可，<code class="docutils literal notranslate"><span class="pre">None</span></code>表示不沿任何轴进行批处理。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">auto_vectorization_conv</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">auto_vectorization_conv</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 3], dtype=Float32, value=
[[ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01],
 [ 8.00000000e+00,  1.40000000e+01,  2.00000000e+01]])
</pre></div></div>
</div>
<blockquote>
<div><p>为保证自动向量化计算逻辑的正确性，vmap内部会根据输入的维度和轴索引以及批处理大小等信息进行校验，详细参数限制可参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.2/api_python/mindspore/mindspore.vmap.html#mindspore.vmap">mindspore.vmap</a>。</p>
</div></blockquote>
<section id="高阶函数的嵌套">
<h3>高阶函数的嵌套<a class="headerlink" href="#高阶函数的嵌套" title="Permalink to this headline"></a></h3>
<p>Vmap本质上是一种高阶函数，它将函数作为输入，并返回可应用于批处理数据的向量化函数。用法上它允许和其他框架提供的高阶函数进行嵌套组合使用。</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vmap</span></code>与<code class="docutils literal notranslate"><span class="pre">vmap</span></code>嵌套使用，应用于两层以上的批处理逻辑。</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hyper_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">hyper_w</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">hyper_vmap_ger</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">hyper_vmap_ger</span><span class="p">(</span><span class="n">hyper_x</span><span class="p">,</span> <span class="n">hyper_w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 3, 3], dtype=Float32, value=
[[[ 6.00000000e+00,  9.00000000e+00,  1.20000000e+01],
  [ 1.20000000e+01,  1.80000000e+01,  2.40000000e+01],
  [ 1.80000000e+01,  2.70000000e+01,  3.60000000e+01]],
 [[ 9.00000000e+00,  1.20000000e+01,  1.50000000e+01],
  [ 1.80000000e+01,  2.40000000e+01,  3.00000000e+01],
  [ 2.70000000e+01,  3.60000000e+01,  4.50000000e+01]],
 [[ 1.20000000e+01,  1.50000000e+01,  1.80000000e+01],
  [ 2.40000000e+01,  3.00000000e+01,  3.60000000e+01],
  [ 3.60000000e+01,  4.50000000e+01,  5.40000000e+01]]])
</pre></div></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">grad</span></code>内部嵌套<code class="docutils literal notranslate"><span class="pre">vmap</span></code>使用，应用于计算向量化函数的梯度等场景。</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">grad</span>

<span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">x_hat</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">grad_vmap_ger</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">vmap</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">),</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">grad_vmap_ger</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(Tensor(shape=[2, 3], dtype=Float32, value=
 [[ 2.83662200e-01, -1.45500034e-01,  4.42569796e-03],
  [-1.45500034e-01,  4.42569796e-03,  1.36737213e-01]]),
 Tensor(shape=[2, 3], dtype=Float32, value=
 [[ 5.67324400e-01, -2.91000068e-01,  8.85139592e-03],
  [-2.91000068e-01,  8.85139592e-03,  2.73474425e-01]]))
</pre></div></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vmap</span></code>内部嵌套<code class="docutils literal notranslate"><span class="pre">grad</span></code>使用，应用于计算批量梯度、高阶梯度计算等场景，如计算Jacobian矩阵。</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vmap_grad_ger</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">vmap_grad_ger</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(Tensor(shape=[2, 3], dtype=Float32, value=
 [[ 2.83662200e-01, -1.45500034e-01,  4.42569796e-03],
  [-1.45500034e-01,  4.42569796e-03,  1.36737213e-01]]),
 Tensor(shape=[2, 3], dtype=Float32, value=
 [[ 5.67324400e-01, -2.91000068e-01,  8.85139592e-03],
  [-2.91000068e-01,  8.85139592e-03,  2.73474425e-01]]))
</pre></div></div>
</div>
<p>本教程中只简单介绍两层高阶函数组合嵌套的用法，您可以根据场景需求实现更多层次的嵌套。</p>
</section>
<section id="cell的自动向量化">
<h3>Cell的自动向量化<a class="headerlink" href="#cell的自动向量化" title="Permalink to this headline"></a></h3>
<p>之前的用例我们都是以函数对象作为输入，下面将介绍<code class="docutils literal notranslate"><span class="pre">Cell</span></code>对象结合<code class="docutils literal notranslate"><span class="pre">vmap</span></code>的用法。这是一个简单定义的全连接层的例子。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Dense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">input_a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_c</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">dense_net</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_net</span><span class="p">(</span><span class="n">input_a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_net</span><span class="p">(</span><span class="n">input_b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_net</span><span class="p">(</span><span class="n">input_c</span><span class="p">))</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">input_a</span><span class="p">,</span> <span class="n">input_b</span><span class="p">,</span> <span class="n">input_c</span><span class="p">])</span>

<span class="n">vmap_dense_net</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">dense_net</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vmap_dense_net</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[ 0.0219292  -0.01062493 -0.03378957 -0.02589925]
 [ 0.03091274 -0.04968021 -0.08098207 -0.07896652]]
[[ 0.02492371 -0.02364336 -0.0495204  -0.04358834]
 [ 0.03390725 -0.06269865 -0.09671289 -0.09665561]]
[[ 0.02791822 -0.03666179 -0.06525123 -0.06127743]
 [ 0.03690176 -0.07571708 -0.11244373 -0.1143447 ]]
[[[ 0.0219292  -0.01062493 -0.03378957 -0.02589925]
  [ 0.03091274 -0.04968021 -0.08098207 -0.07896652]]

 [[ 0.02492371 -0.02364336 -0.0495204  -0.04358834]
  [ 0.03390725 -0.06269865 -0.09671289 -0.09665561]]

 [[ 0.02791822 -0.03666179 -0.06525123 -0.06127743]
  [ 0.03690176 -0.07571708 -0.11244373 -0.1143447 ]]]
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Cell</span></code>和函数式的自动向量化用法基本一致，只需要将<code class="docutils literal notranslate"><span class="pre">vmap</span></code>的第一个入参替换为<code class="docutils literal notranslate"><span class="pre">Cell</span></code>实例即可，Vmap将<code class="docutils literal notranslate"><span class="pre">construct</span></code>转换为作用于批处理数据的向量化<code class="docutils literal notranslate"><span class="pre">construct</span></code>。另外，该用例中初始化函数定义了两个<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>参数, Vmap对于这类执行函数的自由变量的处理等同于将其作为入参并配置对应<code class="docutils literal notranslate"><span class="pre">in_axes</span></code>位置为<code class="docutils literal notranslate"><span class="pre">None</span></code>的场景。</p>
<p>通过这种方式，我们可以实现批量输入在同一个模型上进行训练或推理等功能，与现有网络模型输入支持batch轴输入的区别在于，利用Vmap实现的批处理维度更加灵活，不局限于<code class="docutils literal notranslate"><span class="pre">NCHW</span></code>等输入格式。</p>
</section>
<section id="模型集成场景">
<h3>模型集成场景<a class="headerlink" href="#模型集成场景" title="Permalink to this headline"></a></h3>
<p>模型集成场景将来自多个模型的预测结果组合在一起，传统的实现方式是通过分别在某些输入上运行各个模型，然后将各自的预测结果组合在一起。假如您正在运行的是具有相同架构的模型，那么您可以借助Vmap将它们进行向量化，从而实现加速效果。</p>
<p>该场景下涉及权重数据的向量化，如果您运行的模型是通过函数式编程形式实现，即权重参数在模型外部定义并通过入参传递给模型操作，那您可以直接通过配置<code class="docutils literal notranslate"><span class="pre">in_axes</span></code>的方式进行相应的批处理。而MindSpore框架为了提供便捷的模型定义功能，绝大部分<code class="docutils literal notranslate"><span class="pre">nn</span></code>接口的权重参数都在接口内部定义并初始化，这意味着模型中的权重参数在原始Vmap中无法对权重进行批处理，改造成通过入参传递的函数式实现需要额外工作量。不过您不必担心，MindSpore的<code class="docutils literal notranslate"><span class="pre">vmap</span></code>接口已经替您优化了该场景。您只需要将运行的多个模型实例以<code class="docutils literal notranslate"><span class="pre">CellList</span></code>的形式传入给<code class="docutils literal notranslate"><span class="pre">vmap</span></code>，框架即可自动实现权重参数的批处理。</p>
<p>让我们演示如何使用一组简单的CNN模型来实现模型集成推理和训练。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeNet5</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LeNet-5网络结构</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">num_class</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<p>假设我们正在验证同一模型架构在不同权重参数下的效果，让我们模拟四个已经训练好的模型实例和一份batch大小为16，尺寸为32 x 32的虚拟图像数据集的<code class="docutils literal notranslate"><span class="pre">minibatch</span></code>。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net1</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="n">net2</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="n">net3</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="n">net4</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>

<span class="n">minibatch</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>相较于利用for循环分别运行各个模型后将预测结果集合到一起，Vmap能够一次运行获得多个模型的预测结果。</p>
<blockquote>
<div><p>注意，由于vmap的实现机制，会对设备运行内存有要求，使用vmap可能会占用更多内存，请用户根据实际场景使用。</p>
</div></blockquote>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nets</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">([</span><span class="n">net1</span><span class="p">,</span> <span class="n">net2</span><span class="p">,</span> <span class="n">net3</span><span class="p">,</span> <span class="n">net4</span><span class="p">])</span>
<span class="n">vmap</span><span class="p">(</span><span class="n">nets</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)(</span><span class="n">minibatch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[4, 3, 10], dtype=Float32, value=
[[[ 4.66281290e-06, -7.24548045e-06,  8.68147254e-07 ...  1.42438457e-05,  1.49375774e-05, -1.18535736e-05],
  [ 9.10962353e-06, -5.63606591e-06, -7.06250285e-06 ...  1.68580664e-05,  1.41603141e-05, -3.55220163e-06],
  [ 1.11184154e-05, -6.08020900e-06, -5.08124231e-06 ...  1.37913748e-05,  1.20597506e-05, -1.01803471e-05]],
 [[ 3.22165624e-06,  6.22022753e-06,  2.60713023e-07 ... -1.53302244e-05,  2.34313102e-05, -4.16413786e-06],
  [ 2.82950850e-06,  1.54561894e-06,  5.19753303e-06 ... -1.53819674e-05,  1.58681542e-05, -7.10185304e-07],
  [ 1.77780748e-07,  4.33479636e-06, -1.35376536e-06 ... -1.06113021e-05,  1.58355688e-05, -5.78900153e-06]],
 [[ 6.66864071e-06, -1.99870119e-05, -1.30958688e-05 ...  3.68208202e-06, -9.69678968e-06,  9.59075351e-06],
  [ 7.99765985e-06, -1.16931469e-05, -1.06589669e-05 ... -1.24687813e-06, -8.65744005e-06,  6.81729716e-06],
  [ 6.87587362e-06, -1.23972441e-05, -1.05251866e-05 ...  1.44004912e-06, -5.40550172e-06,  6.71799853e-06]],
 [[-3.44783439e-06,  2.32537104e-07, -8.64402864e-06 ...  3.52633970e-06, -6.27670488e-06,  3.27721250e-06],
  [-6.90392517e-06, -9.97693860e-07, -6.48076320e-06 ...  7.61923275e-07, -2.54563452e-06,  3.08638573e-06],
  [-3.78440518e-06,  3.93633945e-06, -7.90367903e-06 ...  5.13138957e-07, -4.50420839e-06,  2.13702333e-06]]])
</pre></div></div>
</div>
<p>又或者，我们期望得到多个模型分别执行不同<code class="docutils literal notranslate"><span class="pre">minibatch</span></code>数据的预测结果。</p>
<blockquote>
<div><p>模型集成场景下，<code class="docutils literal notranslate"><span class="pre">vmap</span></code>的第一个入参应为<code class="docutils literal notranslate"><span class="pre">CellList</span></code>类型，需要确保每个模型的架构完全相同，否则无法保证计算正确性，如果<code class="docutils literal notranslate"><span class="pre">in_axes</span></code>不为<code class="docutils literal notranslate"><span class="pre">None</span></code>是需保证模型数量与映射轴索引对应的<code class="docutils literal notranslate"><span class="pre">axis_size</span></code>一致，以实现一一映射关系。</p>
</div></blockquote>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">minibatchs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">vmap</span><span class="p">(</span><span class="n">nets</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">minibatchs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[4, 3, 10], dtype=Float32, value=
[[[ 6.52808285e-06, -4.15002341e-06, -3.80283609e-06 ...  1.54428089e-05,  1.44425348e-05, -9.00016857e-06],
  [ 7.39091365e-06, -5.19960076e-06,  3.83916813e-07 ...  1.67857870e-05,  1.80104271e-05, -1.56435199e-05],
  [ 1.11604741e-05, -7.59019804e-06,  2.54263796e-07 ...  1.21071571e-05,  1.66683039e-05, -1.09967377e-05]],
 [[ 1.48978233e-06,  1.02267529e-06,  1.33801677e-06 ... -1.32894393e-05,  1.36311328e-05, -3.29658405e-06],
  [ 1.09956818e-06, -5.06103561e-07,  3.04885953e-06 ... -1.76028752e-05,  1.66466998e-05, -1.17290392e-06],
  [ 2.96090502e-06,  1.87074147e-06,  5.76813818e-06 ... -1.09994007e-05,  1.35614964e-05, -2.19983576e-06]],
 [[ 6.74323928e-06, -1.03955799e-05, -6.92168396e-06 ...  4.88165415e-06, -5.40378596e-06,  3.09346888e-06],
  [ 7.28906161e-06, -1.34921102e-05, -1.00995640e-05 ...  9.44596650e-07, -6.40979761e-06,  1.26146606e-05],
  [ 9.43304440e-06, -1.61852931e-05, -1.16265892e-05 ...  5.31926253e-06, -1.28484417e-05,  8.03831313e-07]],
 [[-5.51165886e-06, -1.09487860e-06, -6.07781249e-06 ...  7.51453626e-06, -3.29403338e-06,  3.45475746e-06],
  [-6.27516283e-06,  1.40756754e-06, -9.18502155e-06 ...  4.16079911e-06, -5.30383022e-06,  5.12517454e-06],
  [-6.19608954e-06,  5.12868655e-06, -1.00337056e-05 ...  2.93281119e-07, -6.52256404e-06,  3.62988635e-06]]])
</pre></div></div>
</div>
<p>除了支持模型集成推理外，结合Vmap特性同样能够实现模型集成训练。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">ParameterTuple</span>

<span class="k">class</span> <span class="nc">TrainOneStepNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOneStepNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adam_optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">use_amsgrad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">grad_weights</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)(</span><span class="n">batch</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adam_optim</span><span class="p">(</span><span class="n">grad_weights</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="n">train_net1</span> <span class="o">=</span> <span class="n">TrainOneStepNet</span><span class="p">(</span><span class="n">net1</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">train_net2</span> <span class="o">=</span> <span class="n">TrainOneStepNet</span><span class="p">(</span><span class="n">net2</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">train_net3</span> <span class="o">=</span> <span class="n">TrainOneStepNet</span><span class="p">(</span><span class="n">net3</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-3</span><span class="p">)</span>
<span class="n">train_net4</span> <span class="o">=</span> <span class="n">TrainOneStepNet</span><span class="p">(</span><span class="n">net4</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">)</span>

<span class="n">train_nets</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">([</span><span class="n">train_net1</span><span class="p">,</span> <span class="n">train_net2</span><span class="p">,</span> <span class="n">train_net3</span><span class="p">,</span> <span class="n">train_net4</span><span class="p">])</span>
<span class="n">model_ensembling_train_one_step</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">train_nets</span><span class="p">)</span>

<span class="n">images</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model_ensembling_train_one_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Step </span><span class="si">{}</span><span class="s2"> - loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>

<span class="n">vmap</span><span class="p">(</span><span class="n">nets</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)(</span><span class="n">minibatch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Step 1 - loss: [2.3025837 2.3025882 2.3025858 2.3025842]
Step 2 - loss: [2.260927  2.301028  2.2992857 2.2976868]
Step 3 - loss: [1.8539654 2.2993202 2.2951114 2.2899477]
Step 4 - loss: [0.77165794 2.2973287  2.288719   2.2726345 ]
Step 5 - loss: [0.9397469 2.2948549 2.2777178 2.2313874]
Step 6 - loss: [0.6747699 2.29158   2.2579656 2.1410708]
Step 7 - loss: [0.64673084 2.2870557  2.2232006  1.966973  ]
Step 8 - loss: [1.0506033 2.2806385 2.1645374 1.6848679]
Step 9 - loss: [0.612196  2.2714498 2.0706694 1.3499321]
Step 10 - loss: [0.8843982 2.258316  1.9299208 1.1472267]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[4, 3, 10], dtype=Float32, value=
[[[-1.91058636e+01, -1.92182674e+01,  1.06328402e+01 ... -1.87287464e+01, -1.87855473e+01, -2.02504387e+01],
  [-1.94767399e+01, -1.95909595e+01,  1.08379564e+01 ... -1.90921249e+01, -1.91503220e+01, -2.06434765e+01],
  [-1.96521702e+01, -1.97674465e+01,  1.09355783e+01 ... -1.92643051e+01, -1.93227654e+01, -2.08293762e+01]],
 [[-4.07293849e-02, -4.27918807e-02,  5.22112176e-02 ... -4.67570126e-02, -3.88025381e-02,  4.88412194e-02],
  [-3.91553082e-02, -4.11494374e-02,  5.00433967e-02 ... -4.48847115e-02, -3.73134986e-02,  4.68519926e-02],
  [-3.80369201e-02, -3.99325565e-02,  4.84890938e-02 ... -4.35365662e-02, -3.62745039e-02,  4.54225838e-02]],
 [[-5.08784056e-01, -5.05123973e-01,  5.20882547e-01 ...  4.72596169e-01, -5.00697553e-01, -4.60489392e-01],
  [-4.80103493e-01, -4.76664037e-01,  4.91507798e-01 ...  4.46062207e-01, -4.72493649e-01, -4.34652239e-01],
  [-4.81168061e-01, -4.77702975e-01,  4.92583781e-01 ...  4.47029382e-01, -4.73524809e-01, -4.35579300e-01]],
 [[-3.66236401e+00, -3.25362825e+00,  3.51312804e+00 ...  3.77490187e+00, -3.36864424e+00, -3.34358120e+00],
  [-3.49160767e+00, -3.10209608e+00,  3.34935308e+00 ...  3.59894991e+00, -3.21167707e+00, -3.18782210e+00],
  [-3.57623625e+00, -3.17717075e+00,  3.43059325e+00 ...  3.68615556e+00, -3.28948307e+00, -3.26504302e+00]]])
</pre></div></div>
</div>
<p>经过模型集成训练的模型除了可以集成推理之外，仍然可以单独进行推理。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net1</span><span class="p">(</span><span class="n">minibatch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3, 10], dtype=Float32, value=
[[-1.91058636e+01, -1.92182674e+01,  1.06328402e+01 ... -1.87287483e+01, -1.87855473e+01, -2.02504387e+01],
 [-1.94767399e+01, -1.95909595e+01,  1.08379564e+01 ... -1.90921249e+01, -1.91503220e+01, -2.06434765e+01],
 [-1.96521702e+01, -1.97674465e+01,  1.09355783e+01 ... -1.92643051e+01, -1.93227654e+01, -2.08293762e+01]])
</pre></div></div>
</div>
</section>
<section id="更多实践案例">
<h3>更多实践案例<a class="headerlink" href="#更多实践案例" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/sample_code/per_sample_gradient.py">差分隐私场景下利用Vmap加速逐样本梯度计算过程</a>;</p></li>
<li><p>科学计算领域AI电磁模型结合Vmap加速点源时域麦克斯韦方程求解;</p></li>
<li><p>强化学习场景下结合Vmap实现多Agent并行训练和推理;</p></li>
<li><p>自动微分场景中基于Vmap提供高效的雅可比矩阵计算接口<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.2/api_python/mindspore/mindspore.jacrev.html">jacrev</a>和<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.2/api_python/mindspore/mindspore.jacfwd.html">jacfwd</a>;</p></li>
</ul>
</section>
<section id="总结">
<h3>总结<a class="headerlink" href="#总结" title="Permalink to this headline"></a></h3>
<p>本教程重点在于介绍Vmap的场景使用说明，本质上自动向量化并非将循环逻辑执行于函数外部，而是将循环下沉至函数的各个基元操作中，并将映射轴信息在基元操作间传递，从而保证计算逻辑的正确性。Vmap的性能收益主要来自于各个基元操作所对应的<code class="docutils literal notranslate"><span class="pre">VmapRule</span></code>实现，由于循环下沉至算子层级，因而更容易结合并行技术进行性能优化，如果您有自定义算子的场景也可以尝试为自定义算子实现特定的<code class="docutils literal notranslate"><span class="pre">VmapRule</span></code>，从而获得更好的性能。对于性能极致追求的场景还可以再结合图算融合特性进行优化。</p>
<blockquote>
<div><p>Vmap特性当前支持GPU、CPU平台，Ascend平台功能仍在不断完善中。</p>
<p>在Vmap包含控制流的场景中，当前仅支持每个批处理分支具有相同处理操作或控制流逻辑中所有变量均无切分轴的场景。</p>
</div></blockquote>
<p>通过上列用例的展示，相信您已经对Vmap的能力有了大致了解，但Vmap的使用场景绝不仅限于此教程陈列的内容。此教程仅起到抛砖引玉的作用，您可以尝试更多有意思的场景，也欢迎加入我们的讨论和工作中。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../optimize/thor/resnet50.html" class="btn btn-neutral float-left" title="在ResNet-50网络上应用二阶优化实践" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../func_programming/Jacobians_Hessians.html" class="btn btn-neutral float-right" title="使用函数变换计算雅可比矩阵和黑塞矩阵" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>