<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>自定义算子（基于Custom表达） &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="MindSpore Hybrid 语法规范" href="ms_kernel.html" />
    <link rel="prev" title="在K8S集群上进行分布式训练" href="../parallel/ms_operator.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/overview.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/startup_method.html">分布式并行启动方式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/data_parallel.html">数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/semi_auto_parallel.html">半自动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/auto_parallel.html">自动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/manual_parallel.html">手动并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/parameter_server_training.html">参数服务器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/model_save_load.html">模型保存与加载</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/recover.html">故障恢复</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/optimize_technique.html">优化方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/others.html">实验特性</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/distributed_case.html">分布式高阶配置案例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">自定义算子</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">自定义算子（基于Custom表达）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#自定义算子入门-一个例子">自定义算子入门：一个例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="#采用jit编译的自定义算子">采用JIT编译的自定义算子</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hybrid类型的自定义算子开发">Hybrid类型的自定义算子开发</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tbe类型的自定义算子开发">tbe类型的自定义算子开发</a></li>
<li class="toctree-l3"><a class="reference internal" href="#akg类型的自定义算子开发">akg类型的自定义算子开发</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#采用aot编译的自定义算子">采用AOT编译的自定义算子</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aot类型的自定义算子开发">aot类型的自定义算子开发</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gpu示例">GPU示例</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cpu示例">CPU示例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#aicpu类型的自定义算子开发">aicpu类型的自定义算子开发</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#自定义算子接入第三方前端">自定义算子接入第三方前端</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#julia类型的自定义算子开发">julia类型的自定义算子开发</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_custom_adv.html">自定义算子注册</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_custom_aot.html">aot类型自定义算子进阶用法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">性能优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling.html">Profiling↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/graph_fusion_engine.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/mem_reuse.html">内存复用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">算法优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">梯度累加</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">二阶优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高阶函数式编程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">自动向量化Vmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/Jacobians_Hessians.html">使用函数变换计算雅可比矩阵和黑塞矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func_programming/per_sample_gradients.html">Per-sample-gradients</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">复杂问题调试</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/dump.html">Dump功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/aoe.html">AOE调优工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/rdr.html">Running Data Recorder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/fault_recover.html">故障恢复</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/sdc.html">精度敏感检测</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>自定义算子（基于Custom表达）</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/operation/op_custom.ipynb.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="自定义算子基于custom表达">
<h1>自定义算子（基于Custom表达）<a class="headerlink" href="#自定义算子基于custom表达" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.3/tutorials/experts/zh_cn/operation/mindspore_op_custom.ipynb"><img alt="下载Notebook" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_notebook.svg" /></a> <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.3/tutorials/experts/zh_cn/operation/mindspore_op_custom.py"><img alt="下载样例代码" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_download_code.svg" /></a> <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/tutorials/experts/source_zh_cn/operation/op_custom.ipynb"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>当开发网络遇到内置算子不足以满足需求时，你可以利用MindSpore的Python API中的<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a>原语方便快捷地进行不同类型自定义算子的定义和使用。</p>
<p>传统的添加一个自定义算子的方式，需要完成算子原语注册、算子实现、算子信息注册三部分工作。</p>
<p>其中：</p>
<ul class="simple">
<li><p>算子原语：定义了算子在网络中的前端接口原型，也是组成网络模型的基础单元，主要包括算子的名称、属性（可选）、输入输出名称、输出shape推理方法、输出数据类型推理方法等信息。</p></li>
<li><p>算子实现：在Python侧定义函数（Ascend自定义算子）或C++侧定义类（GPU和CPU自定义算子），描述算子内部计算逻辑的实现。</p></li>
<li><p>算子信息：描述自定义算子的基本信息，如算子名称、支持的输入输出数据类型、支持的输入输出数据格式和属性等。它是后端做算子选择和映射时的依据。</p></li>
</ul>
<p>相比于传统自定义算子方式，基于<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语自定义算子具有如下优势：</p>
<ul class="simple">
<li><p>不同的自定义算子对应的算子原语都是<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语，无需对每个自定义算子定义一个相应的算子原语。上述提到的三部分工作可以在网络脚本中以统一的接口进行实现，并作为网络表达的一部分，不需要对MindSpore框架进行侵入式修改和重新编译。</p></li>
<li><p>实现了不同方式自定义算子的接口和使用统一，方便网络开发者根据需要灵活选用不同的自定义方式。</p></li>
<li><p>新增支持hybrid等自定义算子方式，并且可以跨平台使用。</p></li>
</ul>
<p>基于<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a>原语的自定义算子支持的算子开发方式包括：hybrid、tbe、aicpu、aot、pyfunc、julia、akg。</p>
<p>不同的算子开发方式差异如下：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30%" />
<col style="width: 15%" />
<col style="width: 6%" />
<col style="width: 19%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>算子开发方式</p></th>
<th class="head"><p>开发语言</p></th>
<th class="head"><p>编译方式</p></th>
<th class="head"><p>支持平台</p></th>
<th class="head"><p>推荐场景</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="#自定义算子入门一个例子">pyfunc</a></p></td>
<td><p>Python</p></td>
<td><p>N/A</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>快速算法验证、需要与Python进行交互等场景</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="#hybrid类型的自定义算子开发">hybrid</a></p></td>
<td><p>MindSpore HYBRID DSL</p></td>
<td><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>全平台通用开发和快速验证</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="#tbe类型的自定义算子开发">tbe</a></p></td>
<td><p>TBE DSL</p></td>
<td><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
<td><p>Ascend AICORE自定义算子场景</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="#akg类型的自定义算子开发">akg</a></p></td>
<td><p>MindSpore AKG DSL</p></td>
<td><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
<td><p>用于开发验证场景，不建议普通用户使用</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="#aicpu类型的自定义算子开发">aicpu</a></p></td>
<td><p>C/C++</p></td>
<td><p>AOT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
<td><p>Ascend AICPU自定义算子场景</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="#aot类型的自定义算子开发">aot</a></p></td>
<td><p>C/C++/CUDA</p></td>
<td><p>AOT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>高性能手写、对接调用第三方算子库场景</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="#julia类型的自定义算子开发">julia</a></p></td>
<td><p>Julia</p></td>
<td><p>N/A</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>科学计算场景、需要使用Julia编程等场景</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>DSL全称是Domain Specific Language。</p></li>
<li><p>AOT（Ahead Of Time）编译方式指的是，算子实现函数需提前被编译为动态链接库，然后在网络运行时由框架自动调用；JIT（Just In Time）编译方式则不需要提前编译算子实现函数，而是在网络编译或运行期间被框架直接编译。</p></li>
<li><p>为了区别自定义算子的类型和编译方式，下面的文中用aot指代自定义算子的类型，用AOT指代自定义算子的编译方式。</p></li>
</ul>
</div></blockquote>
<p>不同平台的不同场景下的推荐开发方式如下：</p>
<ul class="simple">
<li><p>Ascend: hybrid（通用场景），aicpu（不规则运算的高性能实现）；</p></li>
<li><p>GPU: hybrid（通用场景），aot（基于CUDA的高性能实现）；</p></li>
<li><p>CPU: hybrid（通用场景），aot（基于C++的高性能实现）。</p></li>
</ul>
<p>不同的开发方式使用不同的开发语言实现算子计算逻辑，但是自定义算子的开发流程是一致的，包括算子实现、算子输出shape和数据类型推理和算子信息注册（可选）。网络开发者可以根据需要选用不同的自定义算子开发方式。下面分别介绍这几种自定义算子开发方式，每种开发方式均提供示例。</p>
<blockquote>
<div><p>更多示例可参考MindSpore源码中<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>下的用例。</p>
</div></blockquote>
</section>
<section id="自定义算子入门-一个例子">
<h2>自定义算子入门：一个例子<a class="headerlink" href="#自定义算子入门-一个例子" title="永久链接至标题"></a></h2>
<p>为了帮助用户快速入门自定义算子，这里以pyfunc类型自定义算子为例帮助用户理解自定义算子的定义流程。下面基于pyfunc模式定义一个实现sin计算的自定义算子。pyfunc类型的自定义算子使用原生Python语法定义算子实现函数，描述算子内部计算逻辑的实现。网络运行时框架会自动调用此函数。为了表达自定义算子的计算，我们写一个基于numpy的计算正弦函数的Python原生函数。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sin_by_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>然后我们要定义两个函数，一个是张量形状的推导函数（infer_shape），另一个是张量数据类型的推导函数（infer_dtype）。这里要注意：</p>
<ul class="simple">
<li><p>张量形状的推导函数是输入张量的形状；</p></li>
<li><p>张量数据类型的推导函数是输入张量的数据类型。</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>

    <span class="c1">#    1. 这里的输入x是算子输入张量的形状</span>
    <span class="c1">#    2. sin函数是逐元素计算，输入的形状和输出的一样</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>

    <span class="c1">#    1. 这里的输入x是算子输入张量的数据类型</span>
    <span class="c1">#    2. sin函数输入的数据类型和输出的一样</span>
    <span class="k">return</span> <span class="n">x</span>
<br/></pre></div>
</div>
</div>
<p>下面我们用上面的函数自定义一个算子，其输入包括</p>
<ul class="simple">
<li><p>func：自定义算子的函数表达，这里我们用<code class="docutils literal notranslate"><span class="pre">sin_by_numpy</span></code>函数；</p></li>
<li><p>out_shape: 输出形状的推导函数，这里我们用<code class="docutils literal notranslate"><span class="pre">infer_shape</span></code>函数；</p></li>
<li><p>out_dtype: 输出数据类型的推导函数，这里我们用<code class="docutils literal notranslate"><span class="pre">infer_dtype</span></code>函数；</p></li>
<li><p>func_type: 自定义算子类型，这里我们用<code class="docutils literal notranslate"><span class="pre">&quot;pyfunc&quot;</span></code>。</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="n">sin_by_numpy_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">sin_by_numpy</span><span class="p">,</span>     <span class="c1"># 这里填入自定义算子的函数表达</span>
                             <span class="n">out_shape</span><span class="o">=</span><span class="n">infer_shape</span><span class="p">,</span> <span class="c1"># 这里填入输出形状的推导函数</span>
                             <span class="n">out_dtype</span><span class="o">=</span><span class="n">infer_dtype</span><span class="p">,</span> <span class="c1"># 这里填入输出数据类型的推导函数</span>
                             <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;pyfunc&quot;</span>     <span class="c1"># 这里填入自定义算子类型</span>
                            <span class="p">)</span>
</pre></div>
</div>
</div>
<p>加上其他环境依赖依赖和算子调用语句，我们获得完整的自定义算子用例如下。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sin_by_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">sin_by_numpy_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">sin_by_numpy</span><span class="p">,</span>
                             <span class="n">out_shape</span><span class="o">=</span><span class="n">infer_shape</span><span class="p">,</span>
                             <span class="n">out_dtype</span><span class="o">=</span><span class="n">infer_dtype</span><span class="p">,</span>
                             <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;pyfunc&quot;</span><span class="p">)</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">result_cus</span> <span class="o">=</span> <span class="n">sin_by_numpy_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_cus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.         0.841471   0.19866933 0.29552022 0.38941833]
</pre></div></div>
</div>
<p>我们可以得到结果为，即上面输入对应的sin值。</p>
<p>如此我们完成一个pyfunc类型自定义算子的定义。对于更多完整的pyfunc类型自定义算子的例子，参见MindSpore源码中的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/tests/st/ops/graph_kernel/custom/test_custom_pyfunc.py">用例</a>。</p>
</section>
<section id="采用jit编译的自定义算子">
<h2>采用JIT编译的自定义算子<a class="headerlink" href="#采用jit编译的自定义算子" title="永久链接至标题"></a></h2>
<p>JIT（Just In Time）指算子在网络编译或运行期间被框架直接编译。用户可以直接用Python脚本在网络脚本中直接定义此种类型的自定义算子，然后根据算子和后端类型调用对应算子编译器自动编译。此种类型的自定义算子定义方便，而且有着更好的后端适应性。</p>
<section id="hybrid类型的自定义算子开发">
<h3>Hybrid类型的自定义算子开发<a class="headerlink" href="#hybrid类型的自定义算子开发" title="永久链接至标题"></a></h3>
<p>Hybrid类型的自定义算子是自定义算子的默认定义类型。通过使用Hybrid类型的自定义算子，用户可以用类Python的语法描述算子计算逻辑，且无需关注MindSpore框架对于算子定义的工程细节，让用户专注于算法本身。</p>
<p>Hybrid类型的自定义算子使用<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/ms_kernel.html#语法规则">MindSpore Hybrid DSL</a>描述算子内部计算逻辑的实现。用MindSpore Hybrid DSL定义的函数可以被<a class="reference external" href="https://gitee.com/mindspore/akg">AKG算子编译器</a>解析进行JIT编译生成高效算子，在大规模模型的训练推理中使用。同时，用MindSpore Hybrid DSL定义的函数可以当做一个<code class="docutils literal notranslate"><span class="pre">numpy</span></code>函数直接调用，方便用户调试的同时也可以灵活的切换到<a class="reference external" href="#自定义算子入门一个例子">pyfunc
类型的自定义算子</a>，做到一次开发，多个模式多个平台多个场景复用的自定义算子表达。</p>
<p>下面用例(test_custom_hybrid.py)介绍hybrid类型的自定义算子开发流程，其中自定义算子实现两个输入张量相加的功能。 值得注意的是，Hybrid类型的自定义算子采取源码变换的方式打通MindSpore的图编译器和算子编译器，用户可以直接使用MindSpore Hybrid DSL提供的关键词，例如下面的<code class="docutils literal notranslate"><span class="pre">output_tensor</span></code>，而无需引入对应Python函数。更多MindSpore Hybrid DSL关键词的介绍，参见<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/ms_kernel.html#关键词">MindSpore Hybrid DSL关键词</a>。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">kernel</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="c1"># 算子实现，Hybrid DSL</span>
<span class="nd">@kernel</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># 定义hybrid类型的自定义算子(Custom的默认模式)</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[2. 2.]
 [4. 4.]]
</pre></div></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p>Hybrid类型是Custom的默认类型。</p></li>
<li><p>Hybrid类型自定义算子的输入必须是一个带有<code class="docutils literal notranslate"><span class="pre">`&#64;kernel</span></code> &lt;<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/ops/mindspore.ops.kernel.html">https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/ops/mindspore.ops.kernel.html</a>&gt;`__的函数。</p></li>
<li><p>Hybrid类型自定义算子定义时可以使用自带的自动shape/dtype推导函数，也可以手动输入shape/dtype推导函数。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_custom_hybrid.py
</pre></div>
</div>
<p>执行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>对于更多完整的hybrid类型自定义算子的例子，参见MindSpore源码中的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/tests/st/ops/graph_kernel/custom/test_ms_kernel.py">用例</a>。</p>
</section>
<section id="tbe类型的自定义算子开发">
<h3>tbe类型的自定义算子开发<a class="headerlink" href="#tbe类型的自定义算子开发" title="永久链接至标题"></a></h3>
<p>tbe类型的自定义算子使用TBE（Tensor Boost Engine）算子DSL，描述算子内部计算逻辑的实现。算子DSL开发可以参考<a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/operatordev/tbedevg/tbedevg_000003.html">TBE文档</a>。</p>
<p>算子输出shape和数据类型推理可以通过定义Python函数实现，描述算子输出shape和数据类型的推导逻辑。</p>
<p>这种类型的自定义算子需要注册算子信息，算子信息生成方式请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/op_custom_adv.html#算子信息注册">算子信息注册</a>。</p>
<p>下面以test_custom_tbe.py为例介绍tbe类型的自定义算子开发流程，其中自定义算子实现两个输入张量相加的功能。</p>
<p>test_custom_tbe.py内容：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import numpy as np
import mindspore as ms
import mindspore.ops as ops
from mindspore.ops import DataType, CustomRegOp, custom_info_register

ms.set_context(device_target=&quot;Ascend&quot;)

# 算子实现，注册算子信息
@custom_info_register(CustomRegOp() \
                      .input(0, &quot;a&quot;) \
                      .input(1, &quot;b&quot;) \
                      .output(0, &quot;output&quot;) \
                      .dtype_format(DataType.F16_Default, DataType.F16_Default, DataType.F16_Default) \
                      .dtype_format(DataType.F32_Default, DataType.F32_Default, DataType.F32_Default) \
                      .target(&quot;Ascend&quot;) \
                      .get_op_info())
def add(a, b, output, kernel_name=&quot;add&quot;):
    import te.lang.cce
    from te import tvm
    data0 = tvm.placeholder(a.get(&quot;shape&quot;), name=&quot;data0&quot;, dtype=a.get(&quot;dtype&quot;).lower())
    data1 = tvm.placeholder(b.get(&quot;shape&quot;), name=&quot;data1&quot;, dtype=b.get(&quot;dtype&quot;).lower())
    res = te.lang.cce.vadd(data0, data1)
    with tvm.target.cce():
        sch = te.lang.cce.auto_schedule(res)
    config = {&quot;print_ir&quot;: False, &quot;name&quot;: kernel_name, &quot;tensor_list&quot;: [data0, data1, res]}
    te.lang.cce.cce_build_code(sch, config)

if __name__ == &quot;__main__&quot;:
    # 定义tbe类型的自定义算子
    op = ops.Custom(add, out_shape=lambda x, _: x, out_dtype=lambda x, _: x, func_type=&quot;tbe&quot;)

    x0 = np.array([[0.0, 0.0], [1.0, 1.0]]).astype(np.float32)
    x1 = np.array([[2.0, 2.0], [3.0, 3.0]]).astype(np.float32)
    output = op(ms.Tensor(x0), ms.Tensor(x1))
    print(output)
</pre></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p>用Python lambda函数定义输出shape和数据类型推理函数，并分别传给<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语的<code class="docutils literal notranslate"><span class="pre">out_shape</span></code>和<code class="docutils literal notranslate"><span class="pre">out_dtype</span></code>参数。本例中lambda函数表明输出shape和数据类型和第一个输入张量的信息相同。</p></li>
<li><p>通过<code class="docutils literal notranslate"><span class="pre">CustomRegOp</span></code>生成算子信息，并通过<code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code>装饰器注册算子信息。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_custom_tbe.py
</pre></div>
</div>
<p>执行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>对于更多完整的tbe类型自定义算子的例子，参见MindSpore源码中的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/tests/st/ops/graph_kernel/custom/test_custom_tbe.py">用例</a>。</p>
</section>
<section id="akg类型的自定义算子开发">
<h3>akg类型的自定义算子开发<a class="headerlink" href="#akg类型的自定义算子开发" title="永久链接至标题"></a></h3>
<p>akg类型的自定义算子使用<a class="reference external" href="https://gitee.com/mindspore/akg">MindSpore AKG</a>算子DSL，描述算子内部计算逻辑的实现。MindSpore AKG是基于TVM（Tensor Virtual Machine）和Polyhedral技术的算子开发和编译框架，支持Hybrid、IR builder和TVM compute等多种类型的算子DSL。</p>
<p>算子输出shape和数据类型推理可以通过定义Python函数实现，描述算子输出shape和数据类型的推导逻辑。</p>
<p>若算子包含属性或者只支持特定的输入输出数据类型或数据格式，则需要注册算子信息，算子信息生成方式请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/op_custom_adv.html#算子信息注册">算子信息注册</a>。若未注册算子信息，在后端做算子选择和映射的时候，将会从当前算子的输入中推导算子信息。</p>
<p>下面以test_custom_akg.py为例介绍akg类型的自定义算子开发流程，其中自定义算子实现两个输入张量相加的功能。</p>
<p>test_custom_akg.py内容：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="c1"># 算子实现，Hybrid DSL</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># 定义akg类型的自定义算子</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[2. 2.]
 [4. 4.]]
</pre></div></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">set_context(device_target=&quot;GPU&quot;)</span></code>表示算子运行在GPU平台，若要运行在Ascend平台，请编译Ascend版本的MindSpore，并将device_target的值设置为“Ascend”。</p></li>
<li><p>用Python lambda函数定义输出shape和数据类型推理函数，并分别传给<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语的<code class="docutils literal notranslate"><span class="pre">out_shape</span></code>和<code class="docutils literal notranslate"><span class="pre">out_dtype</span></code>参数。本例中lambda函数表明输出shape和数据类型和第一个输入张量的信息相同。</p></li>
<li><p>未注册算子信息，所以自定义算子的算子信息将会从算子输入中推理。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_custom_akg.py
</pre></div>
</div>
<p>执行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>对于更多完整的akg类型自定义算子的例子，参见MindSpore源码中的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/tests/st/ops/graph_kernel/custom/test_custom_akg.py">用例</a>。</p>
</section>
</section>
<section id="采用aot编译的自定义算子">
<h2>采用AOT编译的自定义算子<a class="headerlink" href="#采用aot编译的自定义算子" title="永久链接至标题"></a></h2>
<p>AOT类型的自定义算子指用户事先把算子编译成二进制文件后接入网络。通常用户通过C/C++/CUDA等编程语言手工优化算子实现，并把算子以动态库的形式接入MindSpore加速网络。如此，用户可以针对算子进行极致优化，发挥对应后端硬件的极致性能。这里我们会介绍AOT类型自定义算子的一些基础知识，对于AOT类型自定义算子的更多用法和功能，请参见<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/op_custom_aot.html">AOT类型自定义算子进阶用法</a></p>
<section id="aot类型的自定义算子开发">
<h3>aot类型的自定义算子开发<a class="headerlink" href="#aot类型的自定义算子开发" title="永久链接至标题"></a></h3>
<p>aot类型的自定义算子采用AOT编译方式，要求网络开发者基于特定接口，手写算子实现函数对应的源码文件，并提前将源码文件编译为动态链接库，然后在网络运行时框架会自动调用执行动态链接库中的函数。在算子实现的开发语言方面，GPU平台支持CUDA，CPU平台支持C和C++。源码文件中的算子实现函数的接口规范如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>extern &quot;C&quot; int CustomFunc(int nparam, void **params, int *ndims, int64_t **shapes, const char **dtypes, void *stream, void *extra);
</pre></div>
</div>
<p>其中，函数名<code class="docutils literal notranslate"><span class="pre">CustomFunc</span></code>可替换成任意有效函数名。返回值为int类型，约定0表示正常退出，非0表示发生异常。参数列表的含义如下：</p>
<ul class="simple">
<li><p>nparam (int): 输入输出总数。比如算子有2个输入，1个输出，则nparam的值为3。</p></li>
<li><p>params (void **): 输入输出指针数组。比如算子有2个输入，1个输出，params[0]指向第一个输入数据，params[1]指向第二个输入数据，params[2]指向输出数据。</p></li>
<li><p>ndims (int *): 输入输出shape维度数组。比如params[i]是个shape[1024, 1024]的张量，则ndims[i]的值为2。</p></li>
<li><p>shapes (int64_t **): 输入输出shape数组。比如params[i]是个shape[1024, 1024]的张量，则shapes[i][0]的值为1024，shapes[i][1]的值为1024。</p></li>
<li><p>dtypes (const char **): 输入输出数据类型数组。dtypes里的元素取值可为：“float32”, “float16”, “float”, “float64”, “int”, “int8”, “int16”, “int32”, “int64”, “uint”, “uint8”, “uint16”, “uint32”, “uint64”, “bool”。</p></li>
<li><p>stream (void *): CUDA流指针，仅定义GPU算子实现时需要。</p></li>
<li><p>extra (void *): 用于后续扩展。</p></li>
</ul>
<p>在Python脚本中，<code class="docutils literal notranslate"><span class="pre">Custom</span></code>接口中的<code class="docutils literal notranslate"><span class="pre">func</span></code>输入的格式为<code class="docutils literal notranslate"><span class="pre">Path_To_Func:CustomFunc</span></code>，其中<code class="docutils literal notranslate"><span class="pre">CustomFunc</span></code>为上面函数的名字，而<code class="docutils literal notranslate"><span class="pre">Path_To_Func</span></code>为对应函数源文件或者二进制库的地址。</p>
<blockquote>
<div><ul class="simple">
<li><p>MindSpore识别自动编译的方式为文件名后缀。为了使用自动编译功能，请使用后缀为<code class="docutils literal notranslate"><span class="pre">cpp</span></code>、<code class="docutils literal notranslate"><span class="pre">cc</span></code>或者<code class="docutils literal notranslate"><span class="pre">cu</span></code>的源文件。其他情况MindSpore将处理为二进制库的路径；</p></li>
<li><p>为了防止恶意第三方库篡改，请在环境变量<code class="docutils literal notranslate"><span class="pre">MS_CUSTOM_AOT_WHITE_LIST</span></code>设置合法第三方库的路径。只有在<code class="docutils literal notranslate"><span class="pre">MS_CUSTOM_AOT_WHITE_LIST</span></code>设置的目录及其子目录下文件才会被自定义算子调用。</p></li>
</ul>
</div></blockquote>
<p>算子输出shape和数据类型推理可以通过定义Python函数实现，描述算子输出shape和数据类型的推导逻辑。</p>
<p>若自定义算子只支持特定的输入输出数据类型，则需要定义算子信息，算子信息生成方式请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/op_custom_adv.html#算子信息注册">算子信息注册</a>。</p>
<p>下面通过例子介绍GPU平台和CPU平台上aot类型的自定义算子开发流程，其中自定义算子实现两个输入张量相加的功能。</p>
<section id="gpu示例">
<h4>GPU示例<a class="headerlink" href="#gpu示例" title="永久链接至标题"></a></h4>
<p>使用CUDA语言，编写算子实现的源码文件add.cu：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>#define THREADS 1024
__global__ void CustomAddKernel(float *input1, float *input2, float *output, size_t size) {
  auto idx = blockIdx.x * THREADS + threadIdx.x;
  if (idx &lt; size) {
    output[idx] = input1[idx] + input2[idx];
  }
}

extern &quot;C&quot; int CustomAdd(int nparam, void **params, int *ndims, int64_t **shapes, const char **dtypes, void *stream,
                         void *extra) {
  cudaStream_t custream = static_cast&lt;cudaStream_t&gt;(stream);
  if (nparam != 3) return 1;
  void *input1 = params[0];
  void *input2 = params[1];
  void *output = params[2];
  size_t size = 1;

  for (int i = 0; i &lt; ndims[2]; i++) {
    size *= shapes[2][i];
  }
  int n = size / THREADS;
  for (int i = 0; i &lt; nparam; i++) {
    if (strcmp(dtypes[i], &quot;float32&quot;) != 0) {
      return 2;
    }
  }
  CustomAddKernel&lt;&lt;&lt;n + 1, THREADS, 0, custream&gt;&gt;&gt;(static_cast&lt;float *&gt;(input1), static_cast&lt;float *&gt;(input2),
                                                   static_cast&lt;float *&gt;(output), size);
  return 0;
}
</pre></div>
</div>
<p>将add.cu编译成动态库add.so：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvcc --shared -Xcompiler -fPIC -o add.so add.cu
</pre></div>
</div>
<p>编写测试用例test_custom_aot.py：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import numpy as np
import mindspore as ms
import mindspore.ops as ops

ms.set_context(device_target=&quot;GPU&quot;)

if __name__ == &quot;__main__&quot;:
    # 定义aot类型的自定义算子
    op = ops.Custom(&quot;./add.so:CustomAdd&quot;, out_shape=lambda x, _: x, out_dtype=lambda x, _: x, func_type=&quot;aot&quot;)

    x0 = np.array([[0.0, 0.0], [1.0, 1.0]]).astype(np.float32)
    x1 = np.array([[2.0, 2.0], [3.0, 3.0]]).astype(np.float32)
    output = op(ms.Tensor(x0), ms.Tensor(x1))
    print(output)
</pre></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p>本例中需要将test_custom_aot.py和add.so放置在同一目录下，若add.so在其他目录，则需要将<code class="docutils literal notranslate"><span class="pre">Custom</span></code>第一个参数里路径修改为add.so的绝对路径。</p></li>
<li><p>用Python lambda函数定义输出shape和数据类型推理函数，并分别传给<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语的<code class="docutils literal notranslate"><span class="pre">out_shape</span></code>和<code class="docutils literal notranslate"><span class="pre">out_dtype</span></code>参数。本例中lambda函数表明输出shape和数据类型和第一个输入张量的信息相同。</p></li>
<li><p>未注册算子信息，所以自定义算子的算子信息将会从算子输入中推理。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_custom_aot.py
</pre></div>
</div>
<p>执行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="cpu示例">
<h4>CPU示例<a class="headerlink" href="#cpu示例" title="永久链接至标题"></a></h4>
<p>使用C或者C++语言，编写算子实现的源码文件add.cc：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>#include &lt;string.h&gt;
using size_t = decltype(sizeof(int));
using int64_t = decltype(sizeof(long));

extern &quot;C&quot; int CustomAdd(int nparam, void **params, int *ndims, int64_t **shapes, const char **dtypes, void *stream, void *extra) {
  if (nparam != 3) return 1;
  float *input1 = static_cast&lt;float *&gt;(params[0]);
  float *input2 = static_cast&lt;float *&gt;(params[1]);
  float *output = static_cast&lt;float *&gt;(params[2]);
  size_t size = 1;
  for (int i = 0; i &lt; nparam; i++) {
    size *= shapes[2][i];
  }
  for (int i = 0; i &lt; nparam; i++) {
    if (strcmp(dtypes[i], &quot;float32&quot;) != 0) {
      return 2;
    }
  }
  for (int i = 0; i &lt; size; i++) {
    output[i] = input1[i] + input2[i];
  }
  return 0;
}
</pre></div>
</div>
<p>将add.cc编译成动态库add.so：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>g++ --shared -fPIC -o add.so add.cc
</pre></div>
</div>
<p>编写测试用例test_custom_aot.py：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import numpy as np
import mindspore as ms
import mindspore.ops as ops

ms.set_context(device_target=&quot;CPU&quot;)

if __name__ == &quot;__main__&quot;:
    # 定义aot类型的自定义算子
    op = ops.Custom(&quot;./add.so:CustomAdd&quot;, out_shape=lambda x, _: x, out_dtype=lambda x, _: x, func_type=&quot;aot&quot;)

    x0 = np.array([[0.0, 0.0], [1.0, 1.0]]).astype(np.float32)
    x1 = np.array([[2.0, 2.0], [3.0, 3.0]]).astype(np.float32)
    output = op(ms.Tensor(x0), ms.Tensor(x1))
    print(output)
</pre></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p>本例中需要将test_custom_aot.py和add.so放置在同一目录下，若add.so在其他目录，则需要将<code class="docutils literal notranslate"><span class="pre">Custom</span></code>第一个参数里路径修改为add.so的绝对路径。</p></li>
<li><p>用Python lambda函数定义输出shape和数据类型推理函数，并分别传给<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语的<code class="docutils literal notranslate"><span class="pre">out_shape</span></code>和<code class="docutils literal notranslate"><span class="pre">out_dtype</span></code>参数。本例中lambda函数表明输出shape和数据类型和第一个输入张量的信息相同。</p></li>
<li><p>未注册算子信息，所以自定义算子的算子信息将会从算子输入中推理。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_custom_aot.py
</pre></div>
</div>
<p>执行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>对于更多完整的aot类型自定义算子的例子，参见MindSpore源码中的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/tests/st/ops/graph_kernel/custom/test_custom_aot.py">用例</a>。</p>
</section>
</section>
<section id="aicpu类型的自定义算子开发">
<h3>aicpu类型的自定义算子开发<a class="headerlink" href="#aicpu类型的自定义算子开发" title="永久链接至标题"></a></h3>
<p>aicpu类型的自定义算子采用AOT编译方式，要求算子开发者基于提供的特定接口，手写算子实现函数对应的源码文件，并提前将源码文件编译为动态链接库，然后框架会根据开发者在算子属性中配置的动态链接库名称，找到对应动态链接库并加载算子。具体算子实现参考<a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/51RC2/operatordev/aicpudevg/aicpudevg_000026.html">CANN AICPU 自定义算子开发</a>。</p>
<p>算子输出shape和数据类型推理可以通过定义Python函数实现，描述算子输出shape和数据类型的推导逻辑。</p>
<p>这种类型的自定义算子需要注册算子信息，算子信息生成方式请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/op_custom_adv.html#算子信息注册">算子信息注册</a>，aicpu类型的自定义算子，需要额外指定<code class="docutils literal notranslate"><span class="pre">attr(&quot;cust_aicpu&quot;,</span>&#160; <span class="pre">&quot;required&quot;,</span> <span class="pre">&quot;str&quot;,</span> <span class="pre">&quot;mindspore_aicpu_kernels&quot;)</span></code>的属性，用于MindSpore找到对应的算子实现的动态链接库。</p>
<blockquote>
<div><ul class="simple">
<li><p>需要注意的是，aicpu类型的自定义算子开发后编译成的动态链接库，需要存放到MindSpore的lib目录下，比如MindSpore安装在虚拟环境<code class="docutils literal notranslate"><span class="pre">/home/conda/envs/aicpu/lib/python3.7/site-packages/mindspore</span></code>下，则aicpu的so文件需要放到<code class="docutils literal notranslate"><span class="pre">/home/conda/envs/aicpu/lib/python3.7/site-packages/mindspore/lib/</span></code>目录下。</p></li>
<li><p>“cust_aicpu”的值为字符串，用算子动态链接库的名字去除<code class="docutils literal notranslate"><span class="pre">lib</span></code>前缀与<code class="docutils literal notranslate"><span class="pre">.so</span></code>后缀表示，如<code class="docutils literal notranslate"><span class="pre">libmindspore_aicpu_kernels.so</span></code>则设为<code class="docutils literal notranslate"><span class="pre">&quot;mindspore_aicpu_kernels&quot;</span></code>即可。</p></li>
</ul>
</div></blockquote>
<p>下面以test_dropout_aicpu.py为例介绍aicpu类型的自定义算子开发流程，其中自定义算子实现了dropout的功能，并且编译好的算子动态链接库，我们命名为libmindspore_aicpu_kernels.so，并已将该动态链接库放至mindspore根目录的lib下。</p>
<p>test_dropout_aicpu.py内容：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import numpy as np
import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops
from mindspore.ops import CustomRegOp, DataType

ms.set_context(mode=ms.GRAPH_MODE, device_target=&quot;Ascend&quot;)

# 算子实现，注册算子信息
acos_op_info = CustomRegOp(&quot;Abs&quot;) \
    .fusion_type(&quot;OPAQUE&quot;) \
    .input(0, &quot;x&quot;, &quot;required&quot;) \
    .output(0, &quot;y&quot;, &quot;required&quot;) \
    .attr(&quot;cust_aicpu&quot;, &quot;required&quot;, &quot;str&quot;, &quot;mindspore_aicpu_kernels&quot;) \
    .dtype_format(DataType.F16_Default, DataType.F16_Default) \
    .dtype_format(DataType.F32_Default, DataType.F32_Default) \
    .dtype_format(DataType.F64_Default, DataType.F64_Default) \
    .target(&quot;Ascend&quot;) \
    .get_op_info()


# 定义自定义算子网络
class NetAbs(nn.Cell):
    def __init__(self):
        super(NetAbs, self).__init__()
        self.op = ops.Custom(&quot;acos_aicpu&quot;, out_shape=lambda x, cust_attr: x,
                             out_dtype=lambda x, cust_attr: x, func_type=&quot;aicpu&quot;,
                             reg_info=acos_op_info)
        self.cust_aicpu_so_path = &quot;mindspore_aicpu_kernels&quot;

    def construct(self, inputs):
        return self.op(inputs, self.cust_aicpu_so_path)

if __name__ == &quot;__main__&quot;:
    # 定义aicpu类型的自定义算子
    input_tensor = ms.Tensor(np.ones([1, 1, 2, 3]), ms.float32)
    abs_nn = NetAbs()
    output = abs_nn(input_tensor)
    print(&quot;output shape: &quot;, output.shape)
</pre></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p>可以用多种方式指定<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语的<code class="docutils literal notranslate"><span class="pre">out_shape</span></code>和<code class="docutils literal notranslate"><span class="pre">out_dtype</span></code>参数，可以给定类型，也可以用Python lambda函数等设置。本例中lambda函数表明输出的两个shape与输入相同，第一个输出的数据类型和输入张量的信息相同，第二个输出的数据类型为bool类型。</p></li>
<li><p>通过<code class="docutils literal notranslate"><span class="pre">CustomRegOp</span></code>生成算子信息，并通过<code class="docutils literal notranslate"><span class="pre">Custom</span></code>的<code class="docutils literal notranslate"><span class="pre">reg_info</span></code>接口传入。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_dropout_aicpu.py
</pre></div>
</div>
<p>执行结果（由于dropout算子具有随机性，多次运行结果存在差异）：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>output shape:  (1, 1, 2, 3)
</pre></div>
</div>
</section>
</section>
<section id="自定义算子接入第三方前端">
<h2>自定义算子接入第三方前端<a class="headerlink" href="#自定义算子接入第三方前端" title="永久链接至标题"></a></h2>
<p>作为MindSpore未来的发展方向之一，AI和科学计算的融合越来越受到业界的重视。MindSpore自定义算子基于自身表达的灵活性，也在科学计算方面做出了探索：把面向HPC的编程前端以自定义算子的方式接入MindSpore。</p>
<section id="julia类型的自定义算子开发">
<h3>julia类型的自定义算子开发<a class="headerlink" href="#julia类型的自定义算子开发" title="永久链接至标题"></a></h3>
<p>Julia是一种速度快且使用简单的高级通用编程语言，最初设计用于科学计算领域，而由于其高效而实用的特性，近些年来越来越受到用户的青睐，逐步迈向主流编程语言。 julia类型的自定义算子使用Julia语法定义算子实现函数，描述算子内部计算逻辑的实现。网络运行时框架会自动调用执行相应的Julia函数。</p>
<p>算子输出shape和数据类型推导可以通过定义Python函数实现，描述算子输出shape和数据类型的推导逻辑。</p>
<p>若自定义算子只支持特定的输入输出数据类型，则需要定义算子信息，算子信息生成方式请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/operation/op_custom_adv.html#算子信息注册">算子信息注册</a>。</p>
<p>下面以两个输入张量相加为例，介绍julia类型的自定义算子开发流程:</p>
<p>首先，用户需要通过单独文件实现Julia函数，如(add.jl)：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># add.jl
module Add
# inputs: x, y, output: z, output should use .= to inplace assign
function add(x, y, z)
    z .= x + y
end
end
</pre></div>
</div>
<p>其次，在网络脚本中通过自定义算子方式引用上面所写的Julia函数，以test_custom_julia.py为例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import numpy as np
import mindspore as ms
import mindspore.ops as ops

ms.set_context(device_target=&quot;CPU&quot;)

if __name__ == &quot;__main__&quot;:
    # 定义julia类型的自定义算子
    op = ops.Custom(&quot;./add.jl:Add:add&quot;, out_shape=lambda x, _: x, out_dtype=lambda x, _: x, func_type=&quot;julia&quot;)
    x0 = np.array([[0.0, 0.0], [1.0, 1.0]]).astype(np.float32)
    x1 = np.array([[2.0, 2.0], [3.0, 3.0]]).astype(np.float32)
    output = op(ms.Tensor(x0), ms.Tensor(x1))
    print(output)
</pre></div>
</div>
<p>本例中，有如下几点需要说明：</p>
<ul class="simple">
<li><p>用Python lambda函数定义输出shape和数据类型推理函数，并分别传给<code class="docutils literal notranslate"><span class="pre">Custom</span></code>原语的<code class="docutils literal notranslate"><span class="pre">out_shape</span></code>和<code class="docutils literal notranslate"><span class="pre">out_dtype</span></code>参数。本例中lambda函数表明输出shape和数据类型和第一个输入张量的信息相同。</p></li>
<li><p>未注册算子信息，所以自定义算子的算子信息将会从算子输入中推理。</p></li>
</ul>
<p>执行用例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python test_custom_julia.py
</pre></div>
</div>
<p>执行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
<p>注意事项：</p>
<ol class="arabic">
<li><p>用户需确保下载正确版本的Julia，即version&gt;=1.6.0。</p></li>
<li><p>由于运行时调用的Julia C api是从<code class="docutils literal notranslate"><span class="pre">libjulia.so</span></code>中获取的，因此需要用户设置<code class="docutils literal notranslate"><span class="pre">julia/lib</span></code>到<code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>，以julia-1.6.5为例:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># download julia-1.6.5</span>
wget<span class="w"> </span>https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.5-linux-x86_64.tar.gz
<span class="c1"># extract file</span>
tar<span class="w"> </span>xvf<span class="w"> </span>julia-1.6.5-linux-x86_64.tar.gz
<span class="c1"># if $JULIA_DIR not exist</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$PWD</span>/julia-1.6.5/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="c1"># else</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$JULIA_DIR</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Custom</span></code> 第一个入参指定用户书写的Julia函数需按照<code class="docutils literal notranslate"><span class="pre">file_name:module_name:func_name</span></code>格式指定，<code class="docutils literal notranslate"><span class="pre">file_name</span></code>需包含文件路径，建议使用绝对路径。</p></li>
<li><p>Julia代码文件需包含<code class="docutils literal notranslate"><span class="pre">module</span></code>, <code class="docutils literal notranslate"><span class="pre">module</span></code>内包含<code class="docutils literal notranslate"><span class="pre">function</span></code>，且<code class="docutils literal notranslate"><span class="pre">module</span></code>/<code class="docutils literal notranslate"><span class="pre">function</span></code>都以<code class="docutils literal notranslate"><span class="pre">end</span></code>结束。</p></li>
<li><p>Julia函数的输入输出顺序需与算子的输入输出顺序一致。</p></li>
<li><p>Julia函数的最终输出，即kernel output的赋值需要使用<code class="docutils literal notranslate"><span class="pre">.=</span></code>，否则结果无法写入内存。</p></li>
<li><p>Julia代码支持<a class="reference external" href="https://docs.julialang.org/en/v1/">Julia</a>的常用语法，用户需自行保证语法正确，函数可正确执行。</p></li>
<li><p>用户想在Julia文件内使用Julia的第三方软件包，需自行下载对应软件以确保能正确调用，可以通过 <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pkg;</span> <span class="pre">pkg.add(&quot;somepkg&quot;)</span></code>进行安装。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">julia</span> <span class="pre">array</span></code>在内存上是<code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">major</span></code>排列的，而<code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">array</span></code>是<code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">major</span></code>排列的，如果Julia和numpy做比较，非elemwise计算需考虑内存排布。在Julia函数中，可以通过如下代码示例进行<code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">array</span></code>和<code class="docutils literal notranslate"><span class="pre">julia</span> <span class="pre">array</span></code>的相互转换:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">permutedims</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">reverse</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span><span class="w"> </span><span class="n">length</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">1</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">function</span><span class="w"> </span><span class="n">change_output_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">reshape</span><span class="p">(</span><span class="n">permutedims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">end</span>
</pre></div>
</div>
<p>以矩阵乘为例：</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># julia array is column-major, numpy array is row-major</span>
<span class="c"># user should change julia or numpy&#39;s layout to keep same behavior</span>
<span class="cm">#= EXAMPLE</span>
<span class="cm">A[2,3]               B[3,4]               C[2,4]</span>
<span class="cm">NUMPY:</span>
<span class="cm">[[1, 2, 3]       [[1, 2, 3, 4]         [[38, 44, 50,  56]</span>
<span class="cm"> [4, 5, 6]]       [5, 6, 7, 8]          [83, 98, 113,128]]</span>
<span class="cm">                  [9,10,11,12]]</span>
<span class="cm">JULIA:</span>
<span class="cm">change_input_to_row_major:</span>
<span class="cm">1.inputs read numpy data from memory:</span>
<span class="cm">[[1, 3, 5]       [[1, 4, 7,10]</span>
<span class="cm"> [2, 4, 6]]       [2, 5, 8,11]</span>
<span class="cm">                  [3, 6, 9,12]]</span>
<span class="cm">2.inputs after reshape(reverse(shape)):</span>
<span class="cm">[[1, 4]          [[1, 5, 9]</span>
<span class="cm"> [2, 5]           [2, 6,10]</span>
<span class="cm"> [3, 6]]          [3, 7,11]</span>
<span class="cm">                  [4, 8,12]]</span>
<span class="cm">3.inputs after transpose/permutedims:</span>
<span class="cm">[[1, 2, 3]       [[1, 2, 3, 4]         [[38, 44, 50,  56]</span>
<span class="cm"> [4, 5, 6]]       [5, 6, 7, 8]          [83, 98, 113,128]]</span>
<span class="cm">                  [9,10,11,12]]</span>
<span class="cm">change_output_to_row_major:</span>
<span class="cm">1.output after transpose/permutedims:</span>
<span class="cm">                                       [[38, 83]</span>
<span class="cm">                                        [44, 98]</span>
<span class="cm">                                        [50,113]</span>
<span class="cm">                                        [56,128]</span>
<span class="cm">2.output after reshape:</span>
<span class="cm">                                       [[38, 50, 83, 113]</span>
<span class="cm">                                        [44, 56, 98, 128]]</span>
<span class="cm">3.output read numpy data from memory:</span>
<span class="cm">                                       [[38, 44, 50,  56]</span>
<span class="cm">                                        [83, 98,113, 128]]</span>
<span class="cm">=#</span>
<span class="k">function</span><span class="w"> </span><span class="n">foo!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">change_input_to_row_major</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">gemm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span>
<span class="w">    </span><span class="n">z</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">change_output_to_row_major</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>
</div>
</li>
</ol>
<p>对于更多完整的jullia类型自定义算子的例子，参见MindSpore源码中的<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/tests/st/ops/graph_kernel/custom/test_custom_julia.py">用例</a>。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../parallel/ms_operator.html" class="btn btn-neutral float-left" title="在K8S集群上进行分布式训练" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="ms_kernel.html" class="btn btn-neutral float-right" title="MindSpore Hybrid 语法规范" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>