<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cell与参数 &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/training.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="参数初始化" href="initializer.html" />
    <link rel="prev" title="模型模块自定义" href="../modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">初学教程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introduction.html">基本介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/quick_start.html">快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/tensor.html">张量 Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dataset.html">数据集 Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transforms.html">数据变换 Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/model.html">网络构建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/autograd.html">函数式自动微分</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/train.html">模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/save_load.html">保存与加载</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">进阶教程</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../model.html">高阶封装：Model</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../modules.html">模型模块自定义</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Cell与参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="initializer.html">参数初始化</a></li>
<li class="toctree-l2"><a class="reference internal" href="loss.html">损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer.html">优化器</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">高级数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../derivation.html">高级自动微分</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compute_graph.html">计算图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mixed_precision.html">自动混合精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="../error_analysis.html">报错分析</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../modules.html">模型模块自定义</a> &raquo;</li>
      <li>Cell与参数</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/advanced/modules/layer.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.1/tutorials/zh_cn/advanced/modules/mindspore_layer.ipynb"><img alt="下载Notebook" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_notebook.png" /></a> <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.1/tutorials/zh_cn/advanced/modules/mindspore_layer.py"><img alt="下载样例代码" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_download_code.png" /></a> <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.1/tutorials/source_zh_cn/advanced/modules/layer.ipynb"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_source.png" /></a></p>
<section id="cell与参数">
<h1>Cell与参数<a class="headerlink" href="#cell与参数" title="Permalink to this headline"></a></h1>
<p>Cell作为神经网络构造的基础单元，与神经网络层(Layer)的概念相对应，对Tensor计算操作的抽象封装，能够更准确清晰地对神经网络结构进行表示。除了基础的Tensor计算流程定义外，神经网络层还包含了参数管理、状态管理等功能。而参数(Parameter)是神经网络训练的核心，通常作为神经网络层的内部成员变量。本节我们将系统介绍参数、神经网络层以及其相关使用方法。</p>
<section id="parameter">
<h2>Parameter<a class="headerlink" href="#parameter" title="Permalink to this headline"></a></h2>
<p>参数(Parameter)是一类特殊的Tensor，是指在模型训练过程中可以对其值进行更新的变量。MindSpore提供<code class="docutils literal notranslate"><span class="pre">mindspore.Parameter</span></code>类进行Parameter的构造。为了对不同用途的Parameter进行区分，下面对两种不同类别的Parameter进行定义：</p>
<ul class="simple">
<li><p>可训练参数。在模型训练过程中根据反向传播算法求得梯度后进行更新的Tensor，此时需要将<code class="docutils literal notranslate"><span class="pre">required_grad</span></code>设置为<code class="docutils literal notranslate"><span class="pre">True</span></code>。</p></li>
<li><p>不可训练参数。不参与反向传播，但需要更新值的Tensor（如BatchNorm中的<code class="docutils literal notranslate"><span class="pre">mean</span></code>和<code class="docutils literal notranslate"><span class="pre">var</span></code>变量），此时需要将<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>设置为<code class="docutils literal notranslate"><span class="pre">False</span></code>。</p></li>
</ul>
<blockquote>
<div><p>Parameter默认设置<code class="docutils literal notranslate"><span class="pre">required_grad=True</span></code>。</p>
</div></blockquote>
<p>下面我们构造一个简单的全连接层：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="c1"># weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span> <span class="c1"># bias</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">z</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>在<code class="docutils literal notranslate"><span class="pre">Cell</span></code>的<code class="docutils literal notranslate"><span class="pre">__init__</span></code>方法中，我们定义了<code class="docutils literal notranslate"><span class="pre">w</span></code>和<code class="docutils literal notranslate"><span class="pre">b</span></code>两个Parameter，并配置<code class="docutils literal notranslate"><span class="pre">name</span></code>进行命名空间管理。在<code class="docutils literal notranslate"><span class="pre">construct</span></code>方法中使用<code class="docutils literal notranslate"><span class="pre">self.attr</span></code>直接调用参与Tensor运算。</p>
<section id="获取parameter">
<h3>获取Parameter<a class="headerlink" href="#获取parameter" title="Permalink to this headline"></a></h3>
<p>在使用Cell+Parameter构造神经网络层后，我们可以使用多种方法来获取Cell管理的Parameter。</p>
<section id="获取单个参数">
<h4>获取单个参数<a class="headerlink" href="#获取单个参数" title="Permalink to this headline"></a></h4>
<p>单独获取某个特定参数，直接调用Python类的成员变量即可。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-1.2192779  -0.36789745  0.0946381 ]
</pre></div></div>
</div>
</section>
<section id="获取可训练参数">
<h4>获取可训练参数<a class="headerlink" href="#获取可训练参数" title="Permalink to this headline"></a></h4>
<p>可使用<code class="docutils literal notranslate"><span class="pre">Cell.trainable_params</span></code>方法获取可训练参数，通常在配置优化器时需调用此接口。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[Parameter (name=w, shape=(5, 3), dtype=Float32, requires_grad=True), Parameter (name=b, shape=(3,), dtype=Float32, requires_grad=True)]
</pre></div></div>
</div>
</section>
<section id="获取所有参数">
<h4>获取所有参数<a class="headerlink" href="#获取所有参数" title="Permalink to this headline"></a></h4>
<p>使用<code class="docutils literal notranslate"><span class="pre">Cell.get_parameters()</span></code>方法可获取所有参数，此时会返回一个Python迭代器。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;class &#39;generator&#39;&gt;
</pre></div></div>
</div>
<p>或者可以调用<code class="docutils literal notranslate"><span class="pre">Cell.parameters_and_names</span></code>返回参数名称及参数。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
w:
[[ 4.15680408e-02 -1.20311625e-01  5.02573885e-02]
 [ 1.22175144e-04 -1.34980649e-01  1.17642188e+00]
 [ 7.57667869e-02 -1.74758151e-01 -5.19092619e-01]
 [-1.67846107e+00  3.27240258e-01 -2.06452996e-01]
 [ 5.72323874e-02 -8.27963874e-02  5.94243526e-01]]
b:
[-1.2192779  -0.36789745  0.0946381 ]
</pre></div></div>
</div>
</section>
</section>
<section id="修改parameter">
<h3>修改Parameter<a class="headerlink" href="#修改parameter" title="Permalink to this headline"></a></h3>
<section id="直接修改参数值">
<h4>直接修改参数值<a class="headerlink" href="#直接修改参数值" title="Permalink to this headline"></a></h4>
<p>Parameter是一种特殊的Tensor，因此可以使用Tensor索引修改的方式对其值进行修改。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[ 1.         -0.36789745  0.0946381 ]
</pre></div></div>
</div>
</section>
<section id="覆盖修改参数值">
<h4>覆盖修改参数值<a class="headerlink" href="#覆盖修改参数值" title="Permalink to this headline"></a></h4>
<p>可调用<code class="docutils literal notranslate"><span class="pre">Parameter.set_data</span></code>方法，使用相同Shape的Tensor对Parameter进行覆盖。该方法常用于使用Initializer进行<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r2.1/advanced/modules/initializer.html#cell%E9%81%8D%E5%8E%86%E5%88%9D%E5%A7%8B%E5%8C%96">Cell遍历初始化</a>。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[3. 4. 5.]
</pre></div></div>
</div>
</section>
<section id="运行时修改参数值">
<h4>运行时修改参数值<a class="headerlink" href="#运行时修改参数值" title="Permalink to this headline"></a></h4>
<p>参数的主要作用为模型训练时对其值进行更新，在反向传播获得梯度后，或不可训练参数需要进行更新，都涉及到运行时参数修改。由于MindSpore的<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r2.1/advanced/compute_graph.html">计算图</a>编译设计，此时需要使用<code class="docutils literal notranslate"><span class="pre">mindspore.ops.assign</span></code>接口对参数进行赋值。该方法常用于<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r2.1/advanced/modules/optimizer.html#%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%99%A8">自定义优化器</a>场景。下面是一个简单的运行时修改参数值样例：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">modify_parameter</span><span class="p">():</span>
    <span class="n">b_hat</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">b_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="n">modify_parameter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[7. 8. 9.]
</pre></div></div>
</div>
</section>
</section>
<section id="parameter-tuple">
<h3>Parameter Tuple<a class="headerlink" href="#parameter-tuple" title="Permalink to this headline"></a></h3>
<p>变量元组ParameterTuple，用于保存多个Parameter，继承于元组tuple，提供克隆功能。</p>
<p>如下示例提供ParameterTuple创建方法：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ParameterTuple</span>
<span class="c1"># 创建</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">default_input</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">default_input</span><span class="o">=</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">default_input</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>

<span class="c1"># 从params克隆并修改名称为&quot;params_copy&quot;</span>
<span class="n">params_copy</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="s2">&quot;params_copy&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">params_copy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(Parameter (name=x, shape=(2, 3), dtype=Int64, requires_grad=True), Parameter (name=y, shape=(1, 2, 3), dtype=Float32, requires_grad=True), Parameter (name=z, shape=(), dtype=Float32, requires_grad=True))
(Parameter (name=params_copy.x, shape=(2, 3), dtype=Int64, requires_grad=True), Parameter (name=params_copy.y, shape=(1, 2, 3), dtype=Float32, requires_grad=True), Parameter (name=params_copy.z, shape=(), dtype=Float32, requires_grad=True))
</pre></div></div>
</div>
</section>
</section>
<section id="cell训练状态转换">
<h2>Cell训练状态转换<a class="headerlink" href="#cell训练状态转换" title="Permalink to this headline"></a></h2>
<p>神经网络中的部分Tensor操作在训练和推理时的表现并不相同，如<code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code>在训练时进行随机丢弃，但在推理时则不丢弃，<code class="docutils literal notranslate"><span class="pre">nn.BatchNorm</span></code>在训练时需要更新<code class="docutils literal notranslate"><span class="pre">mean</span></code>和<code class="docutils literal notranslate"><span class="pre">var</span></code>两个变量，在推理时则固定其值不变。因此我们可以通过<code class="docutils literal notranslate"><span class="pre">Cell.set_train</span></code>接口来设置神经网络的状态。</p>
<p><code class="docutils literal notranslate"><span class="pre">set_train(True)</span></code>时，神经网络状态为<code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">set_train</span></code>接口默认值为<code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">phase</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
train
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">set_train(False)</span></code>时，神经网络状态为<code class="docutils literal notranslate"><span class="pre">predict</span></code>：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">phase</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predict
</pre></div></div>
</div>
</section>
<section id="自定义神经网络层">
<h2>自定义神经网络层<a class="headerlink" href="#自定义神经网络层" title="Permalink to this headline"></a></h2>
<p>通常情况下，MindSpore提供的神经网络层接口和function函数接口能够满足模型构造需求，但由于AI领域不断推陈出新，因此有可能遇到新网络结构没有内置模块的情况。此时我们可以根据需要，通过MindSpore提供的function接口、Primitive算子自定义神经网络层，并可以使用<code class="docutils literal notranslate"><span class="pre">Cell.bprop</span></code>方法自定义反向。下面分别详述三种自定义方法。</p>
<section id="使用function接口构造神经网络层">
<h3>使用function接口构造神经网络层<a class="headerlink" href="#使用function接口构造神经网络层" title="Permalink to this headline"></a></h3>
<p>MindSpore提供大量基础的function接口，可以使用其构造复杂的Tensor操作，封装为神经网络层。下面以<code class="docutils literal notranslate"><span class="pre">Threshold</span></code>为例，其公式如下：</p>
<div class="math notranslate nohighlight">
\[\begin{split}y =\begin{cases}
   x, &amp;\text{ if } x &gt; \text{threshold} \\
   \text{value}, &amp;\text{ otherwise }
   \end{cases}\end{split}\]</div>
<p>可以看到<code class="docutils literal notranslate"><span class="pre">Threshold</span></code>判断Tensor的值是否大于<code class="docutils literal notranslate"><span class="pre">threshold</span></code>值，保留判断结果为<code class="docutils literal notranslate"><span class="pre">True</span></code>的值，替换判断结果为<code class="docutils literal notranslate"><span class="pre">False</span></code>的值。因此，对应实现如下：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Threshold</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>这里分别使用了<code class="docutils literal notranslate"><span class="pre">ops.gt</span></code>、<code class="docutils literal notranslate"><span class="pre">ops.fill</span></code>、<code class="docutils literal notranslate"><span class="pre">ops.select</span></code>来实现判断和替换。下面执行自定义的<code class="docutils literal notranslate"><span class="pre">Threshold</span></code>层：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tensor(shape=[3], dtype=Float32, value= [ 2.00000000e+01,  2.00000003e-01,  3.00000012e-01])
</pre></div></div>
</div>
<p>可以看到<code class="docutils literal notranslate"><span class="pre">inputs[0]</span> <span class="pre">=</span> <span class="pre">threshold</span></code>, 因此被替换为<code class="docutils literal notranslate"><span class="pre">20</span></code>。</p>
</section>
<section id="自定义cell反向">
<h3>自定义Cell反向<a class="headerlink" href="#自定义cell反向" title="Permalink to this headline"></a></h3>
<p>在特殊场景下，我们不但需要自定义神经网络层的正向逻辑，也需要手动控制其反向的计算，此时我们可以通过<code class="docutils literal notranslate"><span class="pre">Cell.bprop</span></code>接口对其反向进行定义。在全新的神经网络结构设计、反向传播速度优化等场景下会用到该功能。下面我们以<code class="docutils literal notranslate"><span class="pre">Dropout2d</span></code>为例，介绍如何自定义Cell反向：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Dropout2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">keep_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">out</span>
        <span class="n">dy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dout</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">)</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">dy</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dy</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">)</span>

<span class="n">dropout_2d</span> <span class="o">=</span> <span class="n">Dropout2d</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">dropout_2d</span><span class="o">.</span><span class="n">bprop_debug</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bprop</span></code>方法分别有三个入参：</p>
<ul class="simple">
<li><p><em>x</em>: 正向输入，当正向输入为多个时，需同样数量的入参。</p></li>
<li><p><em>out</em>: 正向输出。</p></li>
<li><p><em>dout</em>: 反向传播时，当前Cell执行之前的反向结果。</p></li>
</ul>
<p>一般我们需要根据正向输出和前层反向结果配合，根据反向求导公式计算反向结果，并将其返回。<code class="docutils literal notranslate"><span class="pre">Dropout2d</span></code>的反向计算需要根据正向输出的<code class="docutils literal notranslate"><span class="pre">mask</span></code>矩阵对前层反向结果进行mask，然后根据<code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>进行缩放。最终可得到正确的计算结果。</p>
</section>
</section>
<section id="hook功能">
<h2>Hook功能<a class="headerlink" href="#hook功能" title="Permalink to this headline"></a></h2>
<p>调试深度学习网络是每一个深度学习领域的从业者需要面对且投入精力较大的工作。由于深度学习网络隐藏了中间层算子的输入、输出数据以及反向梯度，只提供网络输入数据（特征量、权重）的梯度，导致无法准确地感知中间层算子的数据变化，从而降低了调试效率。为了方便用户准确、快速地对深度学习网络进行调试，MindSpore在动态图模式下设计了Hook功能，<strong>使用Hook功能可以捕获中间层算子的输入、输出数据以及反向梯度</strong>。</p>
<p>目前，动态图模式下提供了四种形式的Hook功能，分别是：HookBackward算子和在Cell对象上进行注册的register_forward_pre_hook、register_forward_hook、register_backward_hook功能。</p>
<section id="hookbackward算子">
<h3>HookBackward算子<a class="headerlink" href="#hookbackward算子" title="Permalink to this headline"></a></h3>
<p>HookBackward将Hook功能以算子的形式实现。用户初始化一个HookBackward算子，将其安插到深度学习网络中需要捕获梯度的位置。在网络正向执行时，HookBackward算子将输入数据不做任何修改后原样输出；在网络反向传播梯度时，在HookBackward上注册的Hook函数将会捕获反向传播至此的梯度。用户可以在Hook函数中自定义对梯度的操作，比如打印梯度，或者返回新的梯度。</p>
<p>示例代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="n">grad_out</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;打印梯度&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hook_fn print grad_out:&quot;</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span>

<span class="n">hook</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HookBackward</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">hook_test</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">hook_test</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output:&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
hook_fn print grad_out: (Tensor(shape=[], dtype=Float32, value= 2),)
output: (Tensor(shape=[], dtype=Float32, value= 4), Tensor(shape=[], dtype=Float32, value= 4))
</pre></div></div>
</div>
<p>更多HookBackward算子的说明可以参考<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.1/api_python/ops/mindspore.ops.HookBackward.html">API文档</a>。</p>
</section>
<section id="cell对象的register-forward-pre-hook功能">
<h3>Cell对象的register_forward_pre_hook功能<a class="headerlink" href="#cell对象的register-forward-pre-hook功能" title="Permalink to this headline"></a></h3>
<p>用户可以在Cell对象上使用<code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code>函数来注册一个自定义的Hook函数，用来捕获正向传入该Cell对象的数据。该功能在静态图模式下和在使用<code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code>修饰的函数内不起作用。<code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code>函数接收Hook函数作为入参，并返回一个与Hook函数一一对应的<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象。用户可以通过调用<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象的<code class="docutils literal notranslate"><span class="pre">remove()</span></code>函数来删除与之对应的Hook函数。每一次调用<code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code>函数，都会返回一个不同的<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象。Hook函数应该按照以下的方式进行定义。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_pre_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>这里的cell_id是Cell对象的名称以及ID信息，inputs是正向传入到Cell对象的数据。因此，用户可以使用register_forward_pre_hook函数来捕获网络中某一个Cell对象的正向输入数据。用户可以在Hook函数中自定义对输入数据的操作，比如查看、打印数据，或者返回新的输入数据给当前的Cell对象。如果在Hook函数中对Cell对象的原始输入数据进行计算操作后，再作为新的输入数据返回，这些新增的计算操作将会同时作用于梯度的反向传播。</p>
<p>示例代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_pre_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">input_x</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">forward_pre_hook_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_net</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
[2.]
forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
(Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]))
(Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]))
</pre></div></div>
</div>
<p>用户如果在Hook函数中直接返回新创建的数据，而不是返回由原始输入数据经过计算后得到的数据，那么梯度的反向传播将会在该Cell对象上截止。</p>
<p>示例代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_pre_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">forward_pre_hook_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_net</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
(Tensor(shape=[1], dtype=Float32, value= [ 0.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 0.00000000e+00]))
</pre></div></div>
</div>
<p>为了避免脚本在切换到图模式时运行失败，不建议在Cell对象的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 函数中调用 <code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code> 函数和 <code class="docutils literal notranslate"><span class="pre">handle</span></code> 对象的 <code class="docutils literal notranslate"><span class="pre">remove()</span></code> 函数。在动态图模式下，如果在Cell对象的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 函数中调用 <code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code> 函数，那么Cell对象每次运行都将新注册一个Hook函数。</p>
<p>更多关于Cell对象的 <code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code> 功能的说明可以参考<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.1/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.register_forward_pre_hook">API文档</a>。</p>
</section>
<section id="cell对象的register-forward-hook功能">
<h3>Cell对象的register_forward_hook功能<a class="headerlink" href="#cell对象的register-forward-hook功能" title="Permalink to this headline"></a></h3>
<p>用户可以在Cell对象上使用<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>函数来注册一个自定义的Hook函数，用来捕获正向传入Cell对象的数据和Cell对象的输出数据。该功能在静态图模式下和在使用<code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code>修饰的函数内不起作用。<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>函数接收Hook函数作为入参，并返回一个与Hook函数一一对应的<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象。用户可以通过调用<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象的<code class="docutils literal notranslate"><span class="pre">remove()</span></code>函数来删除与之对应的Hook函数。每一次调用<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>函数，都会返回一个不同的<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象。Hook函数应该按照以下的方式进行定义。</p>
<p>示例代码：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward outputs: &quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>这里的<code class="docutils literal notranslate"><span class="pre">cell_id</span></code>是Cell对象的名称以及ID信息，<code class="docutils literal notranslate"><span class="pre">inputs</span></code>是正向传入到Cell对象的数据，<code class="docutils literal notranslate"><span class="pre">outputs</span></code>是Cell对象的正向输出数据。因此，用户可以使用<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>函数来捕获网络中某一个Cell对象的正向输入数据和输出数据。用户可以在Hook函数中自定义对输入、输出数据的操作，比如查看、打印数据，或者返回新的输出数据。如果在Hook函数中对Cell对象的原始输出数据进行计算操作后，再作为新的输出数据返回，这些新增的计算操作将会同时作用于梯度的反向传播。</p>
<p>示例代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward outputs: &quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="n">outputs</span>
    <span class="k">return</span> <span class="n">outputs</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">forward_hook_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_net</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
forward outputs:  [2.]
(Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]))
(Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 1.00000000e+00]))
</pre></div></div>
</div>
<p>用户如果在Hook函数中直接返回新创建的数据，而不是将原始的输出数据经过计算后，将得到的新输出数据返回，那么梯度的反向传播将会在该Cell对象上截止。该现象可以参考<code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code>函数的用例说明。
为了避免脚本在切换到图模式时运行失败，不建议在Cell对象的<code class="docutils literal notranslate"><span class="pre">construct</span></code>函数中调用<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>函数和<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象的<code class="docutils literal notranslate"><span class="pre">remove()</span></code>函数。在动态图模式下，如果在Cell对象的<code class="docutils literal notranslate"><span class="pre">construct</span></code>函数中调用<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>函数，那么Cell对象每次运行都将新注册一个Hook函数。</p>
<p>更多关于Cell对象的<code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code>功能的说明可以参考<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.1/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.register_forward_hook">API文档</a>。</p>
</section>
<section id="cell对象的register-backward-hook功能">
<h3>Cell对象的register_backward_hook功能<a class="headerlink" href="#cell对象的register-backward-hook功能" title="Permalink to this headline"></a></h3>
<p>用户可以在Cell对象上使用<code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code>函数来注册一个自定义的Hook函数，用来捕获网络反向传播时与Cell对象相关联的梯度。该功能在图模式下或者在使用<code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code>修饰的函数内不起作用。<code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code>函数接收Hook函数作为入参，并返回一个与Hook函数一一对应的<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象。用户可以通过调用<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象的<code class="docutils literal notranslate"><span class="pre">remove()</span></code>函数来删除与之对应的Hook函数。每一次调用<code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code>函数，都会返回一个不同的<code class="docutils literal notranslate"><span class="pre">handle</span></code>对象。</p>
<p>与HookBackward算子所使用的自定义Hook函数有所不同，<code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code>使用的Hook函数的入参中，包含了表示Cell对象名称与id信息的<code class="docutils literal notranslate"><span class="pre">cell_id</span></code>、反向传入到Cell对象的梯度、以及Cell对象的反向输出的梯度。</p>
<p>示例代码：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward_hook_function</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">grad_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>这里的<code class="docutils literal notranslate"><span class="pre">cell_id</span></code>是Cell对象的名称以及ID信息，<code class="docutils literal notranslate"><span class="pre">grad_input</span></code>是网络反向传播时，传入到Cell对象的梯度，它对应于正向过程中下一个算子的反向输出梯度；<code class="docutils literal notranslate"><span class="pre">grad_output</span></code>是Cell对象反向输出的梯度。因此，用户可以使用<code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code>函数来捕获网络中某一个Cell对象的反向传入和反向输出梯度。用户可以在Hook函数中自定义对梯度的操作，比如查看、打印梯度，或者返回新的输出梯度。如果需要在Hook函数中返回新的输出梯度时，返回值必须是<code class="docutils literal notranslate"><span class="pre">tuple</span></code>的形式。</p>
<p>示例代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">backward_hook_function</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">grad_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma_init</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">backward_hook_function</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_net</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(Tensor(shape=[1, 2, 1, 1], dtype=Float32, value=
[[[[ 1.00000000e+00]],
  [[ 1.00000000e+00]]]]),)
(Tensor(shape=[1, 2, 1, 1], dtype=Float32, value=
[[[[ 9.99994993e-01]],
  [[ 9.99994993e-01]]]]),)
[[[[1.99999 1.99999]
   [1.99999 1.99999]]]]
-------------
 [[[[1.99999 1.99999]
   [1.99999 1.99999]]]]
</pre></div></div>
</div>
<p>当 <code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code> 函数和 <code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code> 函数、 <code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code> 函数同时作用于同一Cell对象时，如果 <code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code> 和 <code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code> 函数中有添加其他算子进行数据处理，这些新增算子会在Cell对象执行前或者执行后参与数据的正向计算，但是这些新增算子的反向梯度不在 <code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code> 函数的捕获范围内。 <code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code> 中注册的Hook函数仅捕获原始Cell对象的输入、输出梯度。</p>
<p>示例代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_pre_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">input_x</span>

<span class="k">def</span> <span class="nf">forward_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward inputs: &quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward outputs: &quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="n">outputs</span>
    <span class="k">return</span> <span class="n">outputs</span>

<span class="k">def</span> <span class="nf">backward_hook_fn</span><span class="p">(</span><span class="n">cell_id</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad input: &quot;</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad output: &quot;</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">forward_pre_hook_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">forward_hook_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">backward_hook_fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">grad_net</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
forward inputs:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
forward outputs:  [2.]
grad input:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
grad output:  (Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]),)
(Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]), Tensor(shape=[1], dtype=Float32, value= [ 2.00000000e+00]))
</pre></div></div>
</div>
<p>这里的 <code class="docutils literal notranslate"><span class="pre">grad_input</span></code> 是梯度反向传播时传入<code class="docutils literal notranslate"><span class="pre">self.relu</span></code>的梯度，而不是传入 <code class="docutils literal notranslate"><span class="pre">forward_hook_fn</span></code> 函数中，新增的 <code class="docutils literal notranslate"><span class="pre">Add</span></code> 算子的梯度。这里的 <code class="docutils literal notranslate"><span class="pre">grad_output</span></code> 是梯度反向传播时 <code class="docutils literal notranslate"><span class="pre">self.relu</span></code> 反向输出的梯度，而不是 <code class="docutils literal notranslate"><span class="pre">forward_pre_hook_fn</span></code> 函数中新增 <code class="docutils literal notranslate"><span class="pre">Add</span></code> 算子的反向输出梯度。 <code class="docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code> 函数和 <code class="docutils literal notranslate"><span class="pre">register_forward_hook</span></code> 函数是在Cell对象执行前后起作用，不会影响Cell对象上反向Hook函数的梯度捕获范围。 为了避免脚本在切换到图模式时运行失败，不建议在Cell对象的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 函数中调用
<code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code> 函数和 <code class="docutils literal notranslate"><span class="pre">handle</span></code> 对象的 <code class="docutils literal notranslate"><span class="pre">remove()</span></code> 函数。在PyNative模式下，如果在Cell对象的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 函数中调用 <code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code> 函数，那么Cell对象每次运行都将新注册一个Hook函数。</p>
<p>更多关于Cell对象的 <code class="docutils literal notranslate"><span class="pre">register_backward_hook</span></code> 功能的说明可以参考<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.1/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.register_backward_hook">API文档</a>。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../modules.html" class="btn btn-neutral float-left" title="模型模块自定义" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="initializer.html" class="btn btn-neutral float-right" title="参数初始化" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>