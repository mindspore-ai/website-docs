<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>IR文件分析 &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="prev" title="CANN常见错误分析" href="cann_error_cases.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">初学教程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introduction.html">基本介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/quick_start.html">快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/tensor.html">张量 Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dataset.html">数据集 Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transforms.html">数据变换 Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/model.html">网络构建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/autograd.html">函数式自动微分</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/train.html">模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/save_load.html">保存与加载</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/accelerate_with_static_graph.html">使用静态图加速</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">进阶教程</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../model.html">高阶封装：Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">模型模块自定义</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">高级数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../derivation.html">高级自动微分</a></li>
<li class="toctree-l1"><a class="reference internal" href="../static_graph_expert_programming.html">静态图高级编程技巧</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mixed_precision.html">自动混合精度</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../error_analysis.html">报错分析</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="error_scenario_analysis.html">错误分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="minddata_debug.html">数据处理调试方法与常见问题分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="mindrt_debug.html">网络构建与训练常见错误分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="cann_error_cases.html">CANN常见错误分析</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">IR文件分析</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../error_analysis.html">报错分析</a> &raquo;</li>
      <li>IR文件分析</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/advanced/error_analysis/mindir.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="ir文件分析">
<h1>IR文件分析<a class="headerlink" href="#ir文件分析" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/tutorials/source_zh_cn/advanced/error_analysis/mindir.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source.svg" /></a></p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>在图模式<code class="docutils literal notranslate"><span class="pre">set_context(mode=GRAPH_MODE)</span></code>下运行用MindSpore编写的模型时，若配置中设置了<code class="docutils literal notranslate"><span class="pre">set_context(save_graphs=2)</span></code>，运行时会输出一些图编译过程中生成的中间文件，我们称为IR文件。当前主要有两种格式的IR文件：</p>
<ul class="simple">
<li><p>ir后缀结尾的IR文件：一种比较直观易懂的以文本格式描述模型结构的文件，可以直接用文本编辑软件查看。可以通过设置环境变量<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_DEV_SAVE_GRAPHS_SORT_MODE=1</span></code>打印异序排序方式的ir文件。异序ir文件将按照图的调用顺序打印ir图。可以通过将该环境变量设置为<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_DEV_SAVE_GRAPHS_SORT_MODE=0</span></code>来切换为打印原来的排序方式的ir文件。</p></li>
<li><p>dot后缀结尾的IR文件：若在配置中设置了<code class="docutils literal notranslate"><span class="pre">set_context(save_graphs=3)</span></code>, 运行时会输出后缀为dot的ir文件。该文件描述了不同节点间的拓扑关系，可以用<a class="reference external" href="http://graphviz.org">graphviz</a>将此文件作为输入生成图片，方便用户直观地查看模型结构。对于算子比较多的模型，推荐使用可视化组件<a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r2.2/dashboard.html#%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96">MindSpore Insight</a>对计算图进行可视化。</p></li>
</ul>
</section>
<section id="如何保存ir">
<h2>如何保存IR<a class="headerlink" href="#如何保存ir" title="永久链接至标题"></a></h2>
<p>通过<code class="docutils literal notranslate"><span class="pre">set_context(save_graphs=2)</span></code>来保存各个编译阶段的中间代码。被保存的中间代码有两种格式，默认保存后缀名为<code class="docutils literal notranslate"><span class="pre">.ir</span></code>的文本格式的ir文件。如果设置<code class="docutils literal notranslate"><span class="pre">set_context(save_graphs=3)</span></code>会打印后缀名为<code class="docutils literal notranslate"><span class="pre">.dot</span></code>的图形化格式的ir文件。当网络规模不大时，建议使用更直观的图形化格式来查看，当网络规模较大时建议使用更高效的文本格式来查看。</p>
<p><code class="docutils literal notranslate"><span class="pre">.dot</span></code>文件可以通过graphviz转换为图片格式来查看，例如将dot转换为png的命令是<code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">-Tpng</span> <span class="pre">*.dot</span> <span class="pre">-o</span> <span class="pre">*.png</span></code>。</p>
<p>在训练脚本<code class="docutils literal notranslate"><span class="pre">train.py</span></code>中，我们在<code class="docutils literal notranslate"><span class="pre">set_context</span></code>函数中添加如下代码，运行训练脚本时，MindSpore会自动将编译过程中产生的IR文件存放到指定路径。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s2">&quot;path/to/ir/files&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>执行训练命令后，在指定的路径下生成了若干个文件：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>.
├──00_parse_0000.ir
├──00_parse_0001.dot
├──01_symbol_resolve_0002.ir
├──01_symbol_resolve_0003.dot
├──02_combine_like_graphs_0004.ir
├──02_combine_like_graphs_0005.dot
├──03_inference_opt_prepare_0006.ir
├──03_inference_opt_prepare_0007.dot
├──04_abstract_specialize_0008.ir
├──04_abstract_specialize_0009.dot
...
</pre></div>
</div>
<p>其中以数字下划线开头的IR文件是在前端编译图过程中生成的，编译过程中各阶段分别会保存一次计算图。下面介绍图编译过程中比较重要的阶段:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">parse</span></code>阶段负责解析入口函数，该阶段会初步生成MindIR，如果查看IR文件，我们能观察到该阶段仅仅解析了顶层Cell的图信息；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">symbol_resolve</span></code>阶段负责进一步解析入口函数，主要是递归解析入口函数直接或间接引用到的其他函数和对象。如果使用了尚不支持的语法，一般会在此阶段出错；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">abstract_specialize</span></code>阶段，会根据输入信息推导出IR中所有节点的数据类型和形状信息。当需要查看IR中具体算子的形状或数据类型，可查看该IR文件；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimize</span></code>阶段负责硬件无关的优化，自动微分与自动并行功能也是在该阶段展开。该阶段又可细分为若干个子阶段，在IR文件列表中，其中以<code class="docutils literal notranslate"><span class="pre">opt_pass_[序号]</span></code>为前缀的文件分别是这些子阶段结束后保存的IR文件，非框架开发人员无需过多关注；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">validate</span></code>阶段负责校验编译出来的计算图，如果到此阶段IR中还有仅临时使用的内部算子，则会报错退出；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task_emit</span></code>阶段负责将计算图传给后端进一步处理；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">execute</span></code>阶段负责启动执行图流程，该阶段的IR图是前端编译阶段的最终图。</p></li>
</ul>
<p>此外，后端由于比较贴近底层，后端优化过程中保存的其他IR文件（如以<code class="docutils literal notranslate"><span class="pre">hwopt</span></code>开头的文件）非框架开发人员也无需过多关注。非框架开发人员仅需查看名为<code class="docutils literal notranslate"><span class="pre">graph_build_[图序号]_[IR文件序号].ir</span></code>的文件，即经过前后端全部优化后的IR。</p>
<blockquote>
<div><p>由于后端以子图为单位进行优化，故可能会保存多份文件，与前端多个子图都保存在同一文件中的机制不同。</p>
</div></blockquote>
</section>
<section id="ir文件解读">
<h2>IR文件解读<a class="headerlink" href="#ir文件解读" title="永久链接至标题"></a></h2>
<p>下面以一个简单的例子来说明IR文件的内容，运行该脚本：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s2">&quot;./ir&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">b</span> <span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">b</span>

<span class="n">input1</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<section id="ir文件介绍">
<h3>ir文件介绍<a class="headerlink" href="#ir文件介绍" title="永久链接至标题"></a></h3>
<p>使用文本编辑软件（例如<code class="docutils literal notranslate"><span class="pre">vi</span></code>）打开执行完后输出的IR文件<code class="docutils literal notranslate"><span class="pre">14_validate_0042.ir</span></code>，内容如下所示（此处版本为MindSpore 2.1，后续版本中内容可能会有一些细微变化）：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 # IR entry: @20_1___main___Net_construct.295
  2 # Total subgraphs: 3
  3
  4 # Attrs:
  5 check_set_strategy_valid_once_only : 1
  6 auto_parallel_finish_pre_action : 1
  7
  8 # Total params: 2
  9 # Params:
 10 %para1_x : &lt;Tensor[Float32], ()&gt;
 11 %para2_y : &lt;Tensor[Float32], ()&gt;
 12
 13 subgraph attr:
 14 check_set_strategy_valid_once_only : 1
 15 auto_parallel_finish_pre_action : 1
 16 subgraph instance: 20_1___main___Net_construct.295 : 0x55da18f612a0
 17 # In file t6.py:15/    def construct(self, x, y):/
 18 subgraph @20_1___main___Net_construct.295() {
 19   %0(a) = Sub(%para1_x, Tensor(shape=[], dtype=Float32, value=1)) primitive_attrs: {output_names: [output], input_names: [x, y]}
 20       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], (), value=...&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 21       # Scope: (Default)
 22       # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:839/    return tensor_sub(input, other)/
 23   %1(b) = Add(%0, %para2_y) primitive_attrs: {output_names: [output], input_names: [x, y]}
 24       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 25       # Scope: (Default)
 26       # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:316/    return _get_cache_prim(P.Add)()(input, other)/
 27   %2([CNode]273) = Cast(%1, Bool) primitive_attrs: {output_names: [output], input_names: [x, dst_type], SrcT: F32, DstT: Bool}
 28       : (&lt;Tensor[Float32], ()&gt;, &lt;TypeType, NoShape&gt;) -&gt; (&lt;Tensor[Bool], ()&gt;)
 29       # Scope: (Default)
 30       # In file /workspace/mindspore/build/package/mindspore/_extends/parse/standard_method.py:3359/    return F.cast(x, mstype.bool_)/
 31   %3([CNode]298) = Partial(@21_3_✓__main___Net_construct.296, %1, %0) primitive_attrs: {side_effect_propagate: I64(1)}
 32       : (&lt;Func, NoShape&gt;, &lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Func, NoShape&gt;)
 33       # Scope: (Default)
 34   %4([CNode]299) = Partial(@22_15_✗__main___Net_construct.297, %1) primitive_attrs: {side_effect_propagate: I64(1)}
 35       : (&lt;Func, NoShape&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Func, NoShape&gt;)
 36       # Scope: (Default)
 37   %5([CNode]9) = Switch(%2, %3, %4)
 38       : (&lt;Tensor[Bool], ()&gt;, &lt;Func, NoShape&gt;, &lt;Func, NoShape&gt;) -&gt; (&lt;Func, NoShape&gt;)
 39       # Scope: (Default)
 40       # In file t6.py:18/        if b :/
 41   %6([CNode]12) = %5[@FuncUnion(@21_3_✓__main___Net_construct.296, @22_15_✗__main___Net_construct.297)]()
 42       : () -&gt; (&lt;Tensor[Float32], ()&gt;)
 43       # Scope: (Default)
 44       # In file t6.py:18/        if b :/
 45   Return(%6)
 46       : (&lt;Tensor[Float32], ()&gt;)
 47       # Scope: (Default)
 48       # In file t6.py:18/        if b :/
 49 }
 50
 51
 52 switch_input: 1
 53 subgraph attr:
 54 defer_inline : 0
 55 undeterminate : 0
 56 subgraph instance: 21_3_✓__main___Net_construct.296 : 0x55da18f59e20
 57 # In file t6.py:18/        if b :/
 58 subgraph @21_3_✓__main___Net_construct.296(%para3_b, %para4_a) {
 59   %0([CNode]8) = Div(%para4_a, %para3_b) primitive_attrs: {output_names: [output], input_names: [x, y]}
 60       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 61       # Scope: (Default)
 62       # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:998/    output = _get_cache_prim(P.Div)()(input, other)/
 63   %1(b) = Mul(%para3_b, %0) primitive_attrs: {output_names: [output], input_names: [x, y]}
 64       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 65       # Scope: (Default)
 66       # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:929/    return tensor_mul(input, other)/
 67   Return(%1)
 68       : (&lt;Tensor[Float32], ()&gt;)
 69       # Scope: (Default)
 70       # In file t6.py:19/            b = ops.mul(b, self.func(a, b))/
 71 }
 72
 73
 74 switch_input: 1
 75 subgraph attr:
 76 defer_inline : 0
 77 undeterminate : 0
 78 subgraph instance: 22_15_✗__main___Net_construct.297 : 0x55da18f62280
 79 # In file t6.py:18/        if b :/
 80 subgraph @22_15_✗__main___Net_construct.297(%para5_b) {
 81   Return(%para5_b)
 82       : (&lt;Tensor[Float32], ()&gt;)
 83       # Scope: (Default)
 84       # In file t6.py:18/        if b :/
 85 }
</pre></div>
</div>
<p>以上内容可分为两个部分，第一部分为图的输入信息，第二部分为图的结构信息：</p>
<ul class="simple">
<li><p>第1行告诉了我们该网络的顶图名称 <code class="docutils literal notranslate"><span class="pre">20_1___main___Net_construct.295</span></code>，也就是入口图。</p></li>
<li><p>第2行告诉我们该网络解析出来的图的数量，该IR文件展示了三张图的信息。 分别为第13行的入口图<code class="docutils literal notranslate"><span class="pre">20_1___main___Net_construct.295</span></code>；第52行的图<code class="docutils literal notranslate"><span class="pre">21_3_✓__main___Net_construct.296</span></code>，对应着网络中if条件为true时所运行的图；第74行的图<code class="docutils literal notranslate"><span class="pre">22_15_✗__main___Net_construct.297</span></code>，即对应着网络中if条件为false时所运行的图。</p></li>
<li><p>第8行告诉了我们该网络有多少个输入。</p></li>
<li><p>第10-11行是输入列表，遵循<code class="docutils literal notranslate"><span class="pre">%para[序号]_[name]</span> <span class="pre">:</span> <span class="pre">&lt;[data_type],</span> <span class="pre">(shape)&gt;</span></code>的格式。</p></li>
</ul>
<p>对于具体的图来说（此处我们以图<code class="docutils literal notranslate"><span class="pre">20_1___main___Net_construct.295</span></code>为例）：</p>
<ul class="simple">
<li><p>第13-49行展示了图结构的信息，图中含有若干个节点，即<code class="docutils literal notranslate"><span class="pre">CNode</span></code>。该图包含<code class="docutils literal notranslate"><span class="pre">Sub</span></code>、<code class="docutils literal notranslate"><span class="pre">Add</span></code>、<code class="docutils literal notranslate"><span class="pre">Mul</span></code>这些在网路所调用的接口中所用到的算子。</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">CNode</span></code>（<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.2/design/all_scenarios.html#%E6%96%87%E6%B3%95%E5%AE%9A%E4%B9%89">ANF-IR的设计请查看</a>）的信息遵循如下格式，从左到右分别为序号、节点名称-debug_name、算子名称-op_name、输入节点-arg、节点的属性-primitive_attrs、输入和输出的规格、源码解析调用栈等信息。
由于ANF图为单向无环图，所以此处仅根据输入关系来体现节点与节点的连接关系。关联代码行则体现了<code class="docutils literal notranslate"><span class="pre">CNode</span></code>与脚本源码之间的关系，例如第44行表明该节点是由脚本中<code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">b</span></code>这一行解析而来。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>%[序号]([debug_name]) = [op_name]([arg], ...) primitive_attrs: {[key]: [value], ...}
    : (&lt;[输入data_type]x[输入shape]&gt;, ...) -&gt; (&lt;[输出data_type]x[输出shape]&gt;, ...)
    # 关联代码行
</pre></div>
</div>
<p>关于关联代码行的说明：</p>
<ul class="simple">
<li><p>代码行展示有两种模式，第一种是显示完整的调用栈，前端或后端最后生成的IR文件(如前端的<code class="docutils literal notranslate"><span class="pre">17_execute_0765.ir</span></code>和后端的<code class="docutils literal notranslate"><span class="pre">graph_build_0_136.ir</span></code>)
按此模式展示代码行；第二种为了减小文件的体积，只显示第一行，即省去了调用过程（如<code class="docutils literal notranslate"><span class="pre">04_abstract_specialize_0012.ir</span></code>）。</p></li>
<li><p>如果算子是反向传播算子，关联代码行除了会显示本身的代码，还会显示对应的正向代码，通过“Corresponding forward node candidate:”标识。</p></li>
<li><p>如果算子是融合算子，关联代码行会显示出融合的相关代码，通过“Corresponding code candidate:”标识，其中用分隔符“-”区分不同的代码。</p></li>
</ul>
<blockquote>
<div><ul class="simple">
<li><p>经过编译器的若干优化处理后，节点可能经过了若干转换（如算子拆分、算子融合等），节点的源码解析调用栈信息与脚本可能无法完全一一对应，这里仅作为辅助手段。</p></li>
<li><p>在后端经过算子选择阶段后，输入输出规格信息（即<code class="docutils literal notranslate"><span class="pre">:</span></code>后内容）会有两行。第一行表示为<code class="docutils literal notranslate"><span class="pre">HOST</span></code>侧的规格信息，第二行为<code class="docutils literal notranslate"><span class="pre">DEVICE</span></code>侧的规格信息。</p></li>
</ul>
</div></blockquote>
</section>
<section id="异序ir文件介绍">
<h3>异序ir文件介绍<a class="headerlink" href="#异序ir文件介绍" title="永久链接至标题"></a></h3>
<p>使用文本编辑软件（例如<code class="docutils literal notranslate"><span class="pre">vi</span></code>）打开在设置了环境变量<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MS_DEV_SAVE_GRAPHS_SORT_MODE=1</span></code>并执行样例：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 import mindspore as ms
  2 import mindspore.nn as nn
  3 from mindspore import ops
  4
  5 ms.set_context(mode=ms.GRAPH_MODE)
  6 ms.set_context(save_graphs=2, save_graphs_path=&quot;./ir&quot;)
  7
  8 class Net(nn.Cell):
  9     def __init__(self):
 10         super().__init__()
 11
 12     def func(x, y):
 13         return ops.mul(x, y)
 14
 15     def construct(self, x, y):
 16         b = self.func(x, y)
 17         return b
 18
 19 input1 = ms.Tensor(3, ms.float32)
 20 input2 = ms.Tensor(2, ms.float32)
 21 net = Net()
 22 out = net(input1, input2)
 23 print(out)
</pre></div>
</div>
<p>输出的IR文件<code class="docutils literal notranslate"><span class="pre">04_abstract_specialize_0004.ir</span></code>，内容如下所示（此处版本为MindSpore 2.1，后续版本中内容可能会有一些细微变化）：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 #IR entry      : @1___main___Net_construct.12
  2 #Total subgraph: 3
  3
  4 #attrs         :
  5 # Total params: 2
  6 # Params:
  7 %para1_x : &lt;Tensor[Float32], ()&gt;
  8 %para2_y : &lt;Tensor[Float32], ()&gt;
  9
 10 subgraph attr:
 11 subgraph instance: 1___main___Net_construct.12 : 0x55844586acc0
 12 # In file t6.py:15/    def construct(self, x, y):/
 13 subgraph @1___main___Net_construct.12() {
 14   %0(b) = call @2_func.13(%para1_x, %para2_y)
 15       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 16       # Scope: (Default)
 17       # In file t6.py:16/        b = self.func(x, y)/
 18   Return(%0)
 19       : (&lt;Tensor[Float32], ()&gt;)
 20       # Scope: (Default)
 21       # In file t6.py:17/        return b/
 22 }
 23 # Order:
 24 #   1: @1___main___Net_construct.12:b{[0]: ValueNode&lt;FuncGraph&gt; 2_func.13, [1]: x, [2]: y}
 25 #   2: @1___main___Net_construct.12:[CNode]5{[0]: ValueNode&lt;Primitive&gt; Return, [1]: b}
 26
 27
 28 subgraph attr:
 29 undeterminate : 0
 30 subgraph instance: 2_func.13 : 0x55844588f4d0
 31 # In file t6.py:12/    def func(x, y):/
 32 subgraph @2_func.13(%para3_x, %para4_y) {
 33   %0([CNode]8) = call @3_mul.14(%para3_x, %para4_y)
 34       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 35       # Scope: (Default)
 36       # In file t6.py:13/        return ops.mul(x, y)/
 37   Return(%0)
 38       : (&lt;Tensor[Float32], ()&gt;)
 39       # Scope: (Default)
 40       # In file t6.py:13/        return ops.mul(x, y)/
 41 }
 42 # Order:
 43 #   1: @2_func.13:[CNode]8{[0]: ValueNode&lt;FuncGraph&gt; 3_mul.14, [1]: x, [2]: y}
 44 #   2: @2_func.13:[CNode]9{[0]: ValueNode&lt;Primitive&gt; Return, [1]: [CNode]8}
 45
 46
 47 subgraph attr:
 48 undeterminate : 0
 49 subgraph instance: 3_mul.14 : 0x558445891190
 50 # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:936/def mul(input, other):/
 51 subgraph @3_mul.14(%para3_input, %para4_other) {
 52   %0([CNode]10) = Mul(%para3_input, %para4_other) primitive_attrs: {output_names: [output], input_names: [x, y]}
 53       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 54       # Scope: (Default)
 55       # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:982/    return tensor_mul(input, other)/
 56   Return(%0)
 57       : (&lt;Tensor[Float32], ()&gt;)
 58       # Scope: (Default)
 59       # In file /workspace/mindspore/build/package/mindspore/ops/function/math_func.py:982/    return tensor_mul(input, other)/
 60 }
 61 # Order:
 62 #   1: @3_mul.14:[CNode]10{[0]: ValueNode&lt;PrimitivePy&gt; Mul, [1]: input, [2]: other}
 63 #   2: @3_mul.14:[CNode]11{[0]: ValueNode&lt;Primitive&gt; Return, [1]: [CNode]10}
</pre></div>
</div>
<p>以上内容，从顶图开始，以拓扑排序的方式展示了所有图的信息。图将根据调用顺序打印出来。如有需要追踪图的调用，可以使用此种排序的ir图。</p>
</section>
<section id="dot文件介绍">
<h3>dot文件介绍<a class="headerlink" href="#dot文件介绍" title="永久链接至标题"></a></h3>
<p>可以用<a class="reference external" href="http://graphviz.org">graphviz</a>将<code class="docutils literal notranslate"><span class="pre">dot</span></code>格式的IR文件作为输入生成图片。例如，在Linux操作系统下，可以通过以下命令转换成一张PNG图片。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>dot<span class="w"> </span>-Tpng<span class="w"> </span>-o<span class="w"> </span>04_abstract_specialize_0014.png<span class="w"> </span>04_abstract_specialize_0014.dot
</pre></div>
</div>
<p>转换后的图片如下所示，我们可以直观地查看模型结构。不同的黑框区分了不同的子图，图与图之间的蓝色箭头表示相互之间的调用。蓝色区域表示参数，矩形表示图的参数列表，六边形和黑色箭头表示该参数作为CNode的输入参与计算过程。黄色矩形表示CNode节点，从图中可以看出，CNode输入从下标0开始，第0个输入（即紫色或绿色区域）表示该算子将要进行怎样的计算，通过虚箭头连接。类型一般为算子原语，也可以是另一张图。下标1之后的输入则为计算所需要的参数。</p>
<p><img alt="04_abstract_specialize_0014.png" src="../../_images/dot_to_png.png" /></p>
<p>对于算子比较多的模型，图片会过于庞大，推荐使用可视化组件<a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r2.2/dashboard.html#%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96">MindSpore Insight</a>对计算图进行可视化。</p>
</section>
</section>
<section id="如何根据analyze-failir文件分析图推导失败的原因">
<h2>如何根据analyze_fail.ir文件分析图推导失败的原因<a class="headerlink" href="#如何根据analyze-failir文件分析图推导失败的原因" title="永久链接至标题"></a></h2>
<p>MindSpore在编译图的过程中，经常会出现<code class="docutils literal notranslate"><span class="pre">abstract_specialize</span></code>阶段的图推导失败的报错，通常我们能根据报错信息以及analyze_fail.ir文件，来定位出脚本中存在的问题。</p>
<section id="例子1-参数数量不匹配">
<h3>例子1：参数数量不匹配<a class="headerlink" href="#例子1-参数数量不匹配" title="永久链接至标题"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="mi">1</span> <span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
  <span class="mi">2</span> <span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
  <span class="mi">3</span> <span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
  <span class="mi">4</span>
  <span class="mi">5</span> <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
  <span class="mi">6</span> <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s2">&quot;./ir&quot;</span><span class="p">)</span>
  <span class="mi">7</span>
  <span class="mi">8</span> <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
  <span class="mi">9</span>     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
 <span class="mi">10</span>         <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
 <span class="mi">11</span>
 <span class="mi">12</span>     <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
 <span class="mi">13</span>         <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
 <span class="mi">14</span>
 <span class="mi">15</span>     <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
 <span class="mi">16</span>         <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
 <span class="mi">17</span>         <span class="n">b</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
 <span class="mi">18</span>         <span class="n">c</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
 <span class="mi">19</span>
 <span class="mi">20</span> <span class="n">input1</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
 <span class="mi">21</span> <span class="n">input2</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
 <span class="mi">22</span> <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
 <span class="mi">23</span> <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
 <span class="mi">24</span> <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>会出现如下的报错：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 Traceback (most recent call last):
  2   File &quot;t2.py&quot;, line 23, in &lt;module&gt;
  3     out = net(input1, input2)
  4   File &quot;/workspace/mindspore/build/package/mindspore/nn/cell.py&quot;, line 640, in __call__
  5     out = self.compile_and_run(*args, **kwargs)
  6   File &quot;/workspace/mindspore/build/package/mindspore/nn/cell.py&quot;, line 964, in compile_and_run
  7     self.compile(*args, **kwargs)
  8   File &quot;/workspace/mindspore/build/package/mindspore/nn/cell.py&quot;, line 942, in compile
  9     jit_config_dict=self._jit_config_dict, *compile_args, **kwargs)
 10   File &quot;/workspace/mindspore/build/package/mindspore/common/api.py&quot;, line 1639, in compile
 11     result = self._graph_executor.compile(obj, args, kwargs, phase, self._use_vm_mode())
 12 TypeError: The parameters number of the function is 2, but the number of provided arguments is 3.
 13 FunctionGraph ID : func.21
 14 NodeInfo: In file t2.py:12
 15     def func(x, y):
 16
 17 ----------------------------------------------------
 18 - The Traceback of Net Construct Code:
 19 ----------------------------------------------------
 20 The function call stack (See file &#39;/workspace/mindspore/rank_0/om/analyze_fail.ir&#39; for more details. Get instructions about `analyze_fail.ir` at https://www.mindspore.cn/search?inputValue=analyze_fail.ir):
 21 # 0 In file t2.py:18
 22         c = ops.mul(b, self.func(a, a, b))
 23                        ^
 24
 25 ----------------------------------------------------
 26 - C++ Call Stack: (For framework developers)
 27 ----------------------------------------------------
 28 mindspore/ccsrc/pipeline/jit/static_analysis/stack_frame.cc:102 DoJump
</pre></div>
</div>
<p>以上的报错信息为：“TypeError: The parameters number of the function is 2, but the number of provided arguments is 3…”。
表明<code class="docutils literal notranslate"><span class="pre">FunctionGraph</span> <span class="pre">ID</span> <span class="pre">:</span> <span class="pre">func.18</span></code>只需要2个参数，但是却提供了3个参数。从“The function call stack …”中，可以知道出错的代码为：“In file t2.py:18 … self.func(a, a, b)”，易知是该处的函数调用传入参数的数目过多。</p>
<p>但如果报错信息不直观或者需要查看IR中已推导出的部分图信息，我们使用文本编辑软件（例如，vi）打开报错信息中的提示的文件（第20行括号中）：<code class="docutils literal notranslate"><span class="pre">/home/workspace/mindspore/rank_0/om/analyze_fail.ir</span></code>，内容如下（此处版本为MindSpore 2.1，后续版本中内容可能会有一些细微变化）：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 # 1.This file shows the parsed IR info when graph evaluating failed to help find the problem.
  2 # 2.You can search the last `------------------------&gt;` to the node which is inferred failed.
  3 # 3.Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
  4 # ===============================================================================
  5
  6 subgraph attr:
  7 subgraph instance: __main___Net_construct.1 : 0x5592157f3640
  8 # In file t2.py:15/    def construct(self, x, y):/
  9 subgraph @__main___Net_construct.1(%para1_x, %para2_y) {
 10   %1(a) = call @sub.19(%para1_x, I64(1))
 11       : (&lt;Tensor[Float32], ()&gt;, &lt;Int64, NoShape&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 12       #scope: (Default)
 13       # In file t2.py:16/        a = ops.sub(x, 1)/
 14   %2(b) = call @add.20(%1, %para2_y)
 15       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;Tensor[Float32], ()&gt;)
 16       #scope: (Default)
 17       # In file t2.py:17/        b = ops.add(a, y)/
 18
 19 #------------------------&gt; 0
 20   %3([CNode]7) = call @func.21(%1, %1, %2)
 21       : (&lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;, &lt;Tensor[Float32], ()&gt;) -&gt; (&lt;null&gt;)
 22       #scope: (Default)
 23       # In file t2.py:18/        c = ops.mul(b, self.func(a, a, b))/
 24   %4(c) = call @mul.22(%2, %3)
 25       : (&lt;Tensor[Float32], ()&gt;, &lt;null&gt;) -&gt; (&lt;null&gt;)
 26       #scope: (Default)
 27       # In file t2.py:18/        c = ops.mul(b, self.func(a, a, b))/
 28   %5([CNode]8) = StopGradient(%4)
 29       : (&lt;null&gt;) -&gt; (&lt;null&gt;)
 30       #scope: (Default)
 31   %6([CNode]9) = Depend[side_effect_propagate: I64(1)](None, %5)
 32       : (&lt;null&gt;, &lt;null&gt;) -&gt; (&lt;null&gt;)
 33       #scope: (Default)
 34       # In file t2.py:15/    def construct(self, x, y):/
 35   Return(%6)
 36       : (&lt;null&gt;)
 37       #scope: (Default)
 38       # In file t2.py:15/    def construct(self, x, y):/
 39 }
 40 # Order:
 41 #   1: @__main___Net_construct.1:a{[0]: ValueNode&lt;FuncGraph&gt; sub.19, [1]: x, [2]: ValueNode&lt;Int64Imm&gt; 1}
 42 #   2: @__main___Net_construct.1:b{[0]: ValueNode&lt;FuncGraph&gt; add.20, [1]: a, [2]: y}
 43 #   3: @__main___Net_construct.1:[CNode]7{[0]: ValueNode&lt;FuncGraph&gt; func.21, [1]: a, [2]: a, [3]: b}
 44 #   4: @__main___Net_construct.1:c{[0]: ValueNode&lt;FuncGraph&gt; mul.22, [1]: b, [2]: [CNode]7}
 45 #   5: @__main___Net_construct.1:[CNode]18{[0]: ValueNode&lt;Primitive&gt; Return, [1]: [CNode]9}
 46
 47
 48 #===============================================================================
 49 # num of function graphs in stack: 1
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">analyze_fail.ir</span></code>文件与前文介绍过的异序ir文件格式一致，唯一有区别的地方在于<code class="docutils literal notranslate"><span class="pre">analyze_fail.ir</span></code>文件中会指出推导出错的节点所在的位置。
即第19行的<code class="docutils literal notranslate"><span class="pre">------------------------&gt;</span> <span class="pre">0</span></code>。该箭头指向了推导出错的节点，为<code class="docutils literal notranslate"><span class="pre">%3([CNode]5)</span> <span class="pre">=</span> <span class="pre">call</span> <span class="pre">&#64;func.21(%1,</span> <span class="pre">%1,</span> <span class="pre">%2)</span> <span class="pre">...</span></code>，如何查看analyze_fail.ir文件前文<code class="docutils literal notranslate"><span class="pre">异序ir文件介绍</span></code>一节中已经介绍，此处不再赘述。
根据<code class="docutils literal notranslate"><span class="pre">(%1,</span> <span class="pre">%1,</span> <span class="pre">%2)</span></code>可知，该节点的输入参数有三个。从源码解析调用栈中可以知道实际该函数为<code class="docutils literal notranslate"><span class="pre">self.func</span></code>，在脚本中的定义为<code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">dunc(x,</span> <span class="pre">y):...</span></code>。
在函数定义中，只需要两个参数，故会在此处出现推导失败的报错，我们需要修改脚本中传入的参数个数以解决该问题。</p>
</section>
<section id="例子2-biasadd输入之间shape不匹配">
<h3>例子2：BiasAdd输入之间shape不匹配<a class="headerlink" href="#例子2-biasadd输入之间shape不匹配" title="永久链接至标题"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
  <span class="mi">2</span> <span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
  <span class="mi">3</span> <span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span><span class="p">,</span> <span class="n">set_context</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
  <span class="mi">4</span> <span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
  <span class="mi">5</span>
  <span class="mi">6</span> <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
  <span class="mi">7</span>
  <span class="mi">8</span> <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
  <span class="mi">9</span>     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
 <span class="mi">10</span>         <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
 <span class="mi">11</span>         <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>
 <span class="mi">12</span>         <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">)</span>
 <span class="mi">13</span>
 <span class="mi">14</span>     <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
 <span class="mi">15</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
 <span class="mi">16</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
 <span class="mi">17</span>         <span class="k">return</span> <span class="n">x</span>
 <span class="mi">18</span>
 <span class="mi">19</span> <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
 <span class="mi">20</span> <span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
 <span class="mi">21</span> <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">22</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;out&#39;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>会出现如下的报错：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 Traceback (most recent call last):
  2   File &quot;t2.py&quot;, line 21, in &lt;module&gt;
  3     out = net(x)
  4   File &quot;/workspace/mindspore/build/package/mindspore/nn/cell.py&quot;, line 640, in __call__
  5     out = self.compile_and_run(*args, **kwargs)
  6   File &quot;/workspace/mindspore/build/package/mindspore/nn/cell.py&quot;, line 964, in compile_and_run
  7     self.compile(*args, **kwargs)
  8   File &quot;/workspace/mindspore/build/package/mindspore/nn/cell.py&quot;, line 942, in compile
  9     jit_config_dict=self._jit_config_dict, *compile_args, **kwargs)
 10   File &quot;/workspace/mindspore/build/package/mindspore/common/api.py&quot;, line 1639, in compile
 11     result = self._graph_executor.compile(obj, args, kwargs, phase, self._use_vm_mode())
 12 ValueError: For &#39;BiasAdd&#39;, bias[0] shape should be equal to input_x[1] shape when data_format is NCHW.
 13
 14 ----------------------------------------------------
 15 - The Traceback of Net Construct Code:
 16 ----------------------------------------------------
 17 The function call stack (See file &#39;/workspace/mindspore/rank_0/om/analyze_fail.ir&#39; for more details. Get instructions about `analyze_fail.ir` at https://www.mindspore.cn/search?inputValue=analyze_fail.ir):
 18 # 0 In file t2.py:16
 19         x = ops.bias_add(x, self.bias)
 20             ^
 21 # 1 In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5498
 22     return bias_add_op(input_x, bias)
 23            ^
 24
 25 ----------------------------------------------------
 26 - C++ Call Stack: (For framework developers)
 27 ----------------------------------------------------
 28 mindspore/core/ops/bias_add.cc:88 BiasAddInferShape
</pre></div>
</div>
<p>根据以上报错可知，是算子<code class="docutils literal notranslate"><span class="pre">BiasAdd</span></code>的第一个输入和第二个输入的<code class="docutils literal notranslate"><span class="pre">shape</span></code>不匹配导致的错误。为了进一步了解算子的<code class="docutils literal notranslate"><span class="pre">shape</span></code>是经过了什么样的变化，我们使用文本编辑软件（例如，vi）打开报错信息中的提示的文件：<code class="docutils literal notranslate"><span class="pre">/home/workspace/mindspore/rank_0/om/analyze_fail.ir</span></code>，内容如下（此处版本为MindSpore 2.1，后续版本中内容可能会有一些细微变化）：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  1 # 1.This file shows the parsed IR info when graph evaluating failed to help find the problem.
  2 # 2.You can search the last `------------------------&gt;` to the node which is inferred failed.
  3 # 3.Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
  4 # ===============================================================================
  5
  6 subgraph attr:
  7 subgraph instance: __main___Net_construct.1 : 0x5629496604e0
  8 # In file t2.py:14/    def construct(self, x1):/
  9 subgraph @__main___Net_construct.1(%para1_x1, %para2_bias, %para3_weight) {
 10   %1(x) = call @matmul.7(%para1_x1, %para3_weight)
 11       : (&lt;Tensor[Float32], (3, 32)&gt;, &lt;Ref[Tensor[Float32]], (32, 8)&gt;) -&gt; (&lt;Tensor[Float32], (3, 8)&gt;)
 12       #scope: (Default)
 13       # In file t2.py:15/        x = ops.matmul(x1, self.weight)/
 14
 15 #------------------------&gt; 0
 16   %2(x) = call @bias_add.6(%1, %para2_bias)
 17       : (&lt;Tensor[Float32], (3, 8)&gt;, &lt;Ref[Tensor[Float32]], (4)&gt;) -&gt; (&lt;null&gt;)
 18       #scope: (Default)
 19       # In file t2.py:16/        x = ops.bias_add(x, self.bias)/
 20   Return(%2)
 21       : (&lt;null&gt;)
 22       #scope: (Default)
 23       # In file t2.py:17/        return x/
 24 }
 25 # Order:
 26 #   1: @__main___Net_construct.1:x{[0]: ValueNode&lt;FuncGraph&gt; matmul.7, [1]: x1, [2]: weight}
 27 #   2: @__main___Net_construct.1:x{[0]: ValueNode&lt;FuncGraph&gt; bias_add.6, [1]: x, [2]: bias}
 28 #   3: @__main___Net_construct.1:[CNode]8{[0]: ValueNode&lt;Primitive&gt; Return, [1]: x}
 29
 30
 31 subgraph attr:
 32 subgraph instance: bias_add.6 : 0x56294970ce70
 33 # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5470/def bias_add(input_x, bias):/
 34 subgraph @bias_add.6(%para4_input_x, %para5_bias) {
 35   %1([CNode]10) = call @_get_cache_prim.9(ClassType)
 36       : (&lt;Func, NoShape&gt;) -&gt; (&lt;Func, NoShape&gt;)
 37       #scope: (Default)
 38       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5497/    bias_add_op = _get_cache_prim(P.BiasAdd)(data_format=&quot;NCHW&quot;)/
 39   %2([CNode]11) = S-Prim-MakeTuple(&quot;data_format&quot;)
 40       : (&lt;String, NoShape&gt;) -&gt; (&lt;Tuple[String], TupleShape(NoShape)&gt;)
 41       #scope: (Default)
 42       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5497/    bias_add_op = _get_cache_prim(P.BiasAdd)(data_format=&quot;NCHW&quot;)/
 43   %3([CNode]12) = S-Prim-MakeTuple(&quot;NCHW&quot;)
 44       : (&lt;String, NoShape&gt;) -&gt; (&lt;Tuple[String], TupleShape(NoShape)&gt;)
 45       #scope: (Default)
 46       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5497/    bias_add_op = _get_cache_prim(P.BiasAdd)(data_format=&quot;NCHW&quot;)/
 47   %4([CNode]13) = S-Prim-make_dict(%2, %3)
 48       : (&lt;Tuple[String], TupleShape(NoShape)&gt;, &lt;Tuple[String], TupleShape(NoShape)&gt;) -&gt; (&lt;Dictionary[[data_format,],[String]], NoShape&gt;)
 49       #scope: (Default)
 50       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5497/    bias_add_op = _get_cache_prim(P.BiasAdd)(data_format=&quot;NCHW&quot;)/
 51   %5(bias_add_op) = UnpackCall-unpack_call(%1, %4)
 52       : (&lt;Func, NoShape&gt;, &lt;Dictionary[[data_format,],[String]], NoShape&gt;) -&gt; (&lt;Func, NoShape&gt;)
 53       #scope: (Default)
 54       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5497/    bias_add_op = _get_cache_prim(P.BiasAdd)(data_format=&quot;NCHW&quot;)/
 55
 56 #------------------------&gt; 1
 57   %6([CNode]14) = %5(%para4_input_x, %para5_bias)
 58       : (&lt;Tensor[Float32], (3, 8)&gt;, &lt;Ref[Tensor[Float32]], (4)&gt;) -&gt; (&lt;null&gt;)
 59       #scope: (Default)
 60       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5498/    return bias_add_op(input_x, bias)/
 61   Return(%6)
 62       : (&lt;null&gt;)
 63       #scope: (Default)
 64       # In file /workspace/mindspore/build/package/mindspore/ops/function/nn_func.py:5498/    return bias_add_op(input_x, bias)/
 65 }
 66 # Order:
 67 #   1: @bias_add.6:[CNode]10{[0]: ValueNode&lt;FuncGraph&gt; _get_cache_prim.9, [1]: ValueNode&lt;ClassType&gt; class &#39;mindspore.ops.operations.nn_ops.BiasAdd&#39;}
 68 #   2: @bias_add.6:[CNode]11{[0]: ValueNode&lt;DoSignaturePrimitive&gt; S-Prim-MakeTuple, [1]: ValueNode&lt;StringImm&gt; data_format}
 69 #   3: @bias_add.6:[CNode]12{[0]: ValueNode&lt;DoSignaturePrimitive&gt; S-Prim-MakeTuple, [1]: ValueNode&lt;StringImm&gt; NCHW}
 70 #   4: @bias_add.6:[CNode]13{[0]: ValueNode&lt;DoSignaturePrimitive&gt; S-Prim-make_dict, [1]: [CNode]11, [2]: [CNode]12}
 71 #   5: @bias_add.6:bias_add_op{[0]: ValueNode&lt;UnpackCall&gt; MetaFuncGraph-unpack_call.15, [1]: [CNode]10, [2]: [CNode]13}
 72 #   6: @bias_add.6:[CNode]14{[0]: bias_add_op, [1]: input_x, [2]: bias}
 73 #   7: @bias_add.6:[CNode]16{[0]: ValueNode&lt;Primitive&gt; Return, [1]: [CNode]14}
 74
 75
 76 #===============================================================================
 77 # num of function graphs in stack: 2/3 (Ignored 1 internal frames).
</pre></div>
</div>
<p>搜索<code class="docutils literal notranslate"><span class="pre">------------------------&gt;</span></code>来到第15行，即推导出错的位置。根据<code class="docutils literal notranslate"><span class="pre">...(%1,</span> <span class="pre">%para2_bias)</span>&#160;&#160;&#160; <span class="pre">:(&lt;Tensor[Float32],</span> <span class="pre">(3,</span> <span class="pre">8)&gt;,</span> <span class="pre">&lt;Ref[Tensor(F32)],</span> <span class="pre">(4)&gt;)</span> <span class="pre">-&gt;</span> <span class="pre">(</span></code><null><code class="docutils literal notranslate"><span class="pre">)</span></code>可知，算子<code class="docutils literal notranslate"><span class="pre">BiasAdd</span></code>的输入是<code class="docutils literal notranslate"><span class="pre">%1</span></code>和<code class="docutils literal notranslate"><span class="pre">%para2_bias</span></code>这两个节点。其中，<code class="docutils literal notranslate"><span class="pre">%1</span></code>的shape是<code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">8]</span></code>，<code class="docutils literal notranslate"><span class="pre">%para2_bias</span></code>的shape是<code class="docutils literal notranslate"><span class="pre">[4]</span></code>，不符合算子API中<code class="docutils literal notranslate"><span class="pre">BiasAdd</span></code>算子的描述<code class="docutils literal notranslate"><span class="pre">bias</span> <span class="pre">(Tensor)</span> <span class="pre">-</span> <span class="pre">偏置Tensor，shape为</span> <span class="pre">(C)。C必须与</span> <span class="pre">input_x</span> <span class="pre">的通道维度C相同...</span></code>的要求，故此处报错。</p>
<p>因此，为了解决该问题，我们要么修改<code class="docutils literal notranslate"><span class="pre">%1</span></code>的shape，要么修改<code class="docutils literal notranslate"><span class="pre">%para2</span></code>（即<code class="docutils literal notranslate"><span class="pre">self.bias</span></code>）的shape。</p>
<ul class="simple">
<li><p>如果修改<code class="docutils literal notranslate"><span class="pre">self.bias</span></code>的维度，只需要改成<code class="docutils literal notranslate"><span class="pre">self.bias</span> <span class="pre">=</span> <span class="pre">Parameter(initializer('zeros',</span> <span class="pre">[8]),</span> <span class="pre">name=&quot;bias&quot;)</span></code>。</p></li>
<li><p>如果修改<code class="docutils literal notranslate"><span class="pre">%1</span></code>的shape，我们先要明白<code class="docutils literal notranslate"><span class="pre">%1</span></code>是什么。根据第10行可知，这是一个<code class="docutils literal notranslate"><span class="pre">MatMul</span></code>算子，输出shape是<code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">8]</span></code>。该算子的输入是<code class="docutils literal notranslate"><span class="pre">(%para1_x1,</span> <span class="pre">%para3_weight)</span></code>，第一个输入的shape是<code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">32]</span></code>（即我们传入的参数<code class="docutils literal notranslate"><span class="pre">x</span></code>），第二个输入shape是<code class="docutils literal notranslate"><span class="pre">[32,</span> <span class="pre">8]</span></code>（即<code class="docutils literal notranslate"><span class="pre">self.weight</span></code>）。为了满足和shape为<code class="docutils literal notranslate"><span class="pre">[4]</span></code>的数据<code class="docutils literal notranslate"><span class="pre">BiasAdd</span></code>的要求，需要使得<code class="docutils literal notranslate"><span class="pre">%1</span></code>的输出shape为<code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4]</span></code>，因此我们修改<code class="docutils literal notranslate"><span class="pre">self.weight</span></code>为<code class="docutils literal notranslate"><span class="pre">self.weight</span> <span class="pre">=</span> <span class="pre">Parameter(initializer('normal',</span> <span class="pre">[32,</span> <span class="pre">4]),</span> <span class="pre">name=&quot;weight&quot;)</span></code>。</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cann_error_cases.html" class="btn btn-neutral float-left" title="CANN常见错误分析" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>