<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using MindInsight Visualization &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Using Benchmarks" href="using_benchmarks.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">MindSpore XAI Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="using_explainers.html">Using Explainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_benchmarks.html">Using Benchmarks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using MindInsight Visualization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#operation-process">Operation Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#downloading-data-package">Downloading Data Package</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-the-script">Preparing the Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#restrictions">Restrictions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enabling-mindinsight">Enabling MindInsight</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pages-and-functions">Pages and Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#saliency-map-visualization">Saliency Map Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#explanation-method-assessment">Explanation Method Assessment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#comprehensive-assessment">Comprehensive Assessment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#classification-assessment">Classification Assessment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#uncertainty">Uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="#counterfactual">Counterfactual</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hierarchical-occlusion-counterfactual-hoc">Hierarchical Occlusion Counterfactual (HOC)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hoc-restrictions">HOC Restrictions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hoc-pages-and-functions">HOC Pages and Functions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Using MindInsight Visualization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/using_mindinsight.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="using-mindinsight-visualization">
<h1>Using MindInsight Visualization<a class="headerlink" href="#using-mindinsight-visualization" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindinsight/docs/source_en/model_explanation.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<section id="operation-process">
<h2>Operation Process<a class="headerlink" href="#operation-process" title="Permalink to this headline"></a></h2>
<section id="downloading-data-package">
<h3>Downloading Data Package<a class="headerlink" href="#downloading-data-package" title="Permalink to this headline"></a></h3>
<p>Please follow the <a class="reference external" href="https://www.mindspore.cn/xai/docs/en/r1.5/using_explainers.html#id4">Downloading Data Package</a> instructions to download the necessary files for the tutorial.</p>
</section>
<section id="preparing-the-script">
<h3>Preparing the Script<a class="headerlink" href="#preparing-the-script" title="Permalink to this headline"></a></h3>
<p>The tutorial below is referencing <a class="reference external" href="https://gitee.com/mindspore/xai/blob/r1.5/examples/using_mindinsight.py">using_mindinsight.py</a>.</p>
<p>Currently, <a class="reference external" href="https://www.mindspore.cn/xai/en">MindSpore XAI</a> provides the explanation methods and explanation evaluation Python API. You can use the provided explanation methods by  <code class="docutils literal notranslate"><span class="pre">mindspore_xai.explanation</span></code> and the provided explanation evaluation by <code class="docutils literal notranslate"><span class="pre">mindspore_xai.benchmark</span></code>. You need to prepare the black-box model and data to be explained, instantiate explanation methods or explanation evaluation according to your need and call the explanation API in your script to collect the explanation result and explanation evaluation result.</p>
<p>MindSpore XAI also provides <code class="docutils literal notranslate"><span class="pre">mindspore_xai.runner.ImageClassificationRunner</span></code> to run all explanation methods and explanation evaluation methods automatically. You just need to register the instantiated object and then all explanation methods and explanation evaluation methods will be executed. Explanation logs containing explanation results and explanation evaluation results will be automatically generated and stored.</p>
<p>The following uses ResNet-50 and multi-label dataset with 20 classes as an example. Initializing the explanation methods in <code class="docutils literal notranslate"><span class="pre">explanation</span></code> and the evaluation methods in <code class="docutils literal notranslate"><span class="pre">benchmark</span></code>, the users can then use <code class="docutils literal notranslate"><span class="pre">ImageClassificationRunner</span></code> to execute and explanation and evaluation for the black-box model. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># have to change the current directory to xai/examples/ first</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">load_checkpoint</span><span class="p">,</span> <span class="n">load_param_into_net</span>

<span class="kn">from</span> <span class="nn">mindspore_xai.explanation</span> <span class="kn">import</span> <span class="n">GradCAM</span><span class="p">,</span> <span class="n">GuidedBackprop</span>
<span class="kn">from</span> <span class="nn">mindspore_xai.benchmark</span> <span class="kn">import</span> <span class="n">Faithfulness</span>
<span class="kn">from</span> <span class="nn">mindspore_xai.runner</span> <span class="kn">import</span> <span class="n">ImageClassificationRunner</span>

<span class="kn">from</span> <span class="nn">common.resnet</span> <span class="kn">import</span> <span class="n">resnet50</span>
<span class="kn">from</span> <span class="nn">common.dataset</span> <span class="kn">import</span> <span class="n">classes</span><span class="p">,</span> <span class="n">load_dataset</span>


<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;xai_examples_data/ckpt/resnet50.ckpt&quot;</span><span class="p">)</span>
<span class="n">load_param_into_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>

<span class="c1"># initialize explainers with the loaded black-box model</span>
<span class="n">gradcam</span> <span class="o">=</span> <span class="n">GradCAM</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="s1">&#39;layer4&#39;</span><span class="p">)</span>
<span class="n">guidedbackprop</span> <span class="o">=</span> <span class="n">GuidedBackprop</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

<span class="c1"># initialize benchmarkers to evaluate the chosen explainers</span>
<span class="c1"># for Faithfulness, the initialization needs an activation function that transforms the output of the network to a probability is also needed</span>
<span class="n">activation_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># for multi-label classification</span>
<span class="n">faithfulness</span> <span class="o">=</span> <span class="n">Faithfulness</span><span class="p">(</span><span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;InsertionAUC&#39;</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">)</span>

<span class="c1"># returns the dataset to be explained, when localization is chosen, the dataset is required to provide bounding box</span>
<span class="c1"># the columns of the dataset should be in [image], [image, labels], or [image, labels, bbox] (order matters)</span>
<span class="c1"># You may refer to &#39;mindspore.dataset.project&#39; for columns managements</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;xai_examples_data/test&#39;</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">explainers</span> <span class="o">=</span> <span class="p">[</span><span class="n">gradcam</span><span class="p">,</span> <span class="n">guidedbackprop</span><span class="p">]</span>
<span class="n">benchmarkers</span> <span class="o">=</span> <span class="p">[</span><span class="n">faithfulness</span><span class="p">]</span>

<span class="c1"># initialize runner with specified summary_dir</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">ImageClassificationRunner</span><span class="p">(</span><span class="n">summary_dir</span><span class="o">=</span><span class="s1">&#39;./summary_dir&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">register_saliency</span><span class="p">(</span><span class="n">explainers</span><span class="p">,</span> <span class="n">benchmarkers</span><span class="p">)</span>

<span class="c1"># execute runner.run to generate explanation and evaluation results to save it to summary_dir</span>
<span class="n">runner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="restrictions">
<h3>Restrictions<a class="headerlink" href="#restrictions" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Only support image classification models, such as Lenet, Resnet, Alexnet.</p></li>
<li><p>Input images must be in 1, 3, or 4 channels format.</p></li>
<li><p>Only support GPU and Ascend devices with PyNative mode.</p></li>
<li><p>All instances of explanation and evaluation methods cannot be reused across runners. Explanation and evaluation methods have to be instantiated exclusively for each runner. Otherwise, errors may occur. A correct example is shown below.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gradcam</span> <span class="o">=</span> <span class="n">GradCAM</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="s1">&#39;layer4&#39;</span><span class="p">)</span>
<span class="n">guidedbackprop</span> <span class="o">=</span> <span class="n">GuidedBackprop</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">ImageClassificationRunner</span><span class="p">(</span><span class="n">summary_dir</span><span class="o">=</span><span class="s1">&#39;./summary_dir_1&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">register_saliency</span><span class="p">(</span><span class="n">explainers</span><span class="o">=</span><span class="p">[</span><span class="n">gradcam</span><span class="p">,</span> <span class="n">guidedbackprop</span><span class="p">])</span>
<span class="n">runner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

<span class="c1"># generate another summary with GradCAM only</span>
<span class="n">runner2</span> <span class="o">=</span> <span class="n">ImageClassificationRunner</span><span class="p">(</span><span class="n">summary_dir</span><span class="o">=</span><span class="s1">&#39;./summary_dir_2&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># reusing explainer instance in other runner, errors may occur</span>
<span class="c1"># runner2.register_saliency(explainers=[gradcam])</span>

<span class="c1"># instantiating a new GradCAM is the correct way</span>
<span class="n">gradcam2</span> <span class="o">=</span> <span class="n">GradCAM</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="s1">&#39;layer4&#39;</span><span class="p">)</span>
<span class="n">runner2</span><span class="o">.</span><span class="n">register_saliency</span><span class="p">(</span><span class="n">explainers</span><span class="o">=</span><span class="p">[</span><span class="n">gradcam2</span><span class="p">])</span>

<span class="n">runner2</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="enabling-mindinsight">
<h3>Enabling MindInsight<a class="headerlink" href="#enabling-mindinsight" title="Permalink to this headline"></a></h3>
<p>Enable MindInsight and click <strong>Model Explanation</strong> on the top of the page. All explanation log paths are displayed. When a log path meets the conditions, the <strong>Saliency Map Visualization</strong> buttons are displayed in the <strong>Operation</strong> column.</p>
<p><img alt="mi_index" src="_images/mi_index.png" /></p>
</section>
</section>
<section id="pages-and-functions">
<h2>Pages and Functions<a class="headerlink" href="#pages-and-functions" title="Permalink to this headline"></a></h2>
<section id="saliency-map-visualization">
<h3>Saliency Map Visualization<a class="headerlink" href="#saliency-map-visualization" title="Permalink to this headline"></a></h3>
<p>Saliency map visualization is used to display the image area that has the most significant impact on the model decision-making result. Generally, the highlighted regions can be considered as key features of the objective classification.</p>
<p><img alt="mi_saliency_map" src="_images/mi_saliency_map.png" /></p>
<p>The following information is displayed on the <strong>Saliency Map Visualization</strong> page:</p>
<ul class="simple">
<li><p>Objective dataset set by a user through the Python API of the dataset.</p></li>
<li><p>Ground truth tags, prediction tags, and the prediction probabilities of the model for the corresponding tags. The system adds the TP, FN, and FP flags(meanings are provided in the page’s information) in the upper left corner of the corresponding tag based on the actual requirements.</p></li>
<li><p>A saliency map given by the selected explanation method.</p></li>
</ul>
<p>Operations:</p>
<ol class="arabic simple">
<li><p>Select the required explanation methods. Currently, we support four explanation methods. More explanation methods will be provided in the future.</p></li>
<li><p>Click <strong>Overlay on Original Image</strong> in the upper right corner of the page to overlay the saliency map on the original image.</p></li>
<li><p>Click different tags to display the saliency map analysis results of the model for different tags. For different classification results, the focus of the model is usually different.</p></li>
<li><p>Check prediction type checkboxes to display images with the checked tag types: TP - true positive, FN - false negative, FP - false positive.</p></li>
<li><p>Use the tag filtering function on the upper part of the page to filter out images with specified tags.</p></li>
<li><p>Select an image display sequence from <strong>Sort Images By</strong> in the upper right corner of the page, options: “Probabilities in descending order” and “Uncertainties in descending order”.</p></li>
<li><p>Click <strong>View Score</strong> on the right of an explanation method. The page for assessing all explanation methods is displayed.</p></li>
<li><p>Click image you will see the higher resolution image.</p></li>
</ol>
<p><img alt="mi_saliency_map_detail" src="_images/mi_saliency_map_detail.png" /></p>
</section>
<section id="explanation-method-assessment">
<h3>Explanation Method Assessment<a class="headerlink" href="#explanation-method-assessment" title="Permalink to this headline"></a></h3>
<section id="comprehensive-assessment">
<h4>Comprehensive Assessment<a class="headerlink" href="#comprehensive-assessment" title="Permalink to this headline"></a></h4>
<p>The provided explanation methods are scored from different dimensions. We provide various dimensions scores to help users compare the performance and select the most suitable one. You can configure weights for metrics in a specific scenario to obtain the comprehensive score.</p>
<p><img alt="mi_metrix_comprehensive" src="_images/mi_metrix_comprehensive.png" /></p>
</section>
<section id="classification-assessment">
<h4>Classification Assessment<a class="headerlink" href="#classification-assessment" title="Permalink to this headline"></a></h4>
<p>The classification assessment page provides two types of comparison. One is to compare scores of different evaluation dimensions of the same explanation method in each tag. The other is to compare scores of different explanation methods of the same evaluation dimension in each tag.</p>
<p><img alt="mi_metrix_class" src="_images/mi_metrix_class.png" /></p>
</section>
</section>
</section>
<section id="uncertainty">
<h2>Uncertainty<a class="headerlink" href="#uncertainty" title="Permalink to this headline"></a></h2>
<p>The model predictions come with uncertainty, which is called <a class="reference external" href="https://www.mindspore.cn/probability/api/en/r1.5/nn_probability/mindspore.nn.probability.toolbox.UncertaintyEvaluation.html#mindspore.nn.probability.toolbox.UncertaintyEvaluation">Epistemic Uncertainty</a>. It inserts a dropout layer to the network and inferences multiple times. The results are standard deviation and 95% confidence interval of the model output predictions:</p>
<p><img alt="mi_saliency_map" src="_images/mi_uncertainty.png" /></p>
<p>The restrictions, preparation of network and data is the same as the saliency explanation methods, users enable uncertainty calculations by invoking <code class="docutils literal notranslate"><span class="pre">register_uncertainty()</span></code> of <code class="docutils literal notranslate"><span class="pre">ImageClassificiationRunner</span></code>. The sample code is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">runner</span> <span class="o">=</span> <span class="n">ImageClassificationRunner</span><span class="p">(</span><span class="n">summary_dir</span><span class="o">=</span><span class="s1">&#39;./summary_dir_1&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">register_saliency</span><span class="p">(</span><span class="n">explainers</span><span class="o">=</span><span class="p">[</span><span class="n">gradcam</span><span class="p">,</span> <span class="n">guidedbackprop</span><span class="p">])</span>
<span class="n">runner</span><span class="o">.</span><span class="n">register_uncertainty</span><span class="p">()</span>
<span class="n">runner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Please note that <code class="docutils literal notranslate"><span class="pre">register_uncertainty()</span></code> must be used together with <code class="docutils literal notranslate"><span class="pre">register_saliency()</span></code>, their calling order doesn’t matter.</p>
</section>
<section id="counterfactual">
<h2>Counterfactual<a class="headerlink" href="#counterfactual" title="Permalink to this headline"></a></h2>
<p>Counterfactual is a relatively new way of explaining a model’s decision, which inverts the decision by modifying the traits of the sample. For example, there is an animal image that is classified as a cat by the model. How can we edit that image in order to make the classification not happening? By answering that question, we can explain the model decision of classifying to “cat”. Counterfactuals come in various forms, currently, <code class="docutils literal notranslate"><span class="pre">ImageClassificationRunner</span></code> provides an easy-to-use method called Hierarchical Occlusion Counterfactual (HOC), more counterfactual methods will be provided in the future.</p>
<section id="hierarchical-occlusion-counterfactual-hoc">
<h3>Hierarchical Occlusion Counterfactual (HOC)<a class="headerlink" href="#hierarchical-occlusion-counterfactual-hoc" title="Permalink to this headline"></a></h3>
<p>HOC is an occlusion-based method, it searches for the smallest possible display region that is subjected to the constraint of the target label’s prediction confidence greater than a threshold (currently fixed at 0.5). The search process is conducted in a hierarchical manner, at the beginning, the original image was covered by its blurred version, then HOC searches large occlusion areas and recursively deeps down into smaller areas for achieving a more accurate result. It ends up with an area tree, each node represents a square display area and the smaller child areas are fall inside the parent. The root node represents the entire area of the original image, its immediate children are the first layer display areas.</p>
<p>At the moment, <code class="docutils literal notranslate"><span class="pre">ImageClassificationRunner</span></code> automatically generates the number of layers (1 to 3), the sizes of occluded areas, the strides, and the blur mask base on the image dimensions. The side length of the first layer occlusion square is defined as the round down of half of the short side of the image, we cut the side length in half in every next layer. Meanwhile, the side length has to be equals to or greater than 28, otherwise, stop adding layers. The stride is the round down of 1/5 of the occluded area’s side length.</p>
<p>The preparation of network and data is the same as the saliency explanation methods, users can employ HOC by invoking <code class="docutils literal notranslate"><span class="pre">register_hierarchical_occlusion()</span></code> of <code class="docutils literal notranslate"><span class="pre">ImageClassificiationRunner</span></code>. The sample code is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">runner</span> <span class="o">=</span> <span class="n">ImageClassificationRunner</span><span class="p">(</span><span class="n">summary_dir</span><span class="o">=</span><span class="s1">&#39;./summary_dir_1&#39;</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">register_hierarchical_occlusion</span><span class="p">()</span>
<span class="n">runner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Users may combine the use of <code class="docutils literal notranslate"><span class="pre">register_saliency()</span></code> with the same runner.</p>
<section id="hoc-restrictions">
<h4>HOC Restrictions<a class="headerlink" href="#hoc-restrictions" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>Apart from all the restrictions from saliency explanation methods, models must take 3 channels input images.</p></li>
<li><p>Input images must be in RGB 3 channels format and the length of the short side must be equals to or greater than 56.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">register_hierarchical_occlusion()</span></code> is called but <code class="docutils literal notranslate"><span class="pre">register_saliency()</span></code> is not called, then both PyNative and Graph mode are supported.</p></li>
</ul>
</section>
<section id="hoc-pages-and-functions">
<h4>HOC Pages and Functions<a class="headerlink" href="#hoc-pages-and-functions" title="Permalink to this headline"></a></h4>
<p>You can see that the ‘Counterfactual Explanation’ operations are enabled for those explanation jobs employed HOC. Clicking it will lead you to the HOC explanation page.</p>
<p><img alt="mi_hoc_index" src="_images/mi_hoc_index.png" /></p>
<p>The HOC explanation page displays all HOC results, includes:</p>
<ul class="simple">
<li><p>Samples with prediction confidence of any tag that greater than 0.5 and their original images.</p></li>
<li><p>Prediction confidence of the target tags.</p></li>
<li><p>The outcome images and their prediction confidences of each layer.</p></li>
</ul>
<p><img alt="mi_hoc" src="_images/mi_hoc.png" /></p>
<p>Operations:</p>
<ol class="arabic simple">
<li><p>In the upper right corner of “Picture list” panel, there is a “Hide” switch. When the switch is turned on, the samples without HOC explanation result will not be displayed. By default, the switch is on and the users can turn it off to display all samples.</p></li>
<li><p>Change the tag filter and sampler sorting on the left “Picture list” panel. Samples can be sorted by prediction confidence.</p></li>
<li><p>Browse samples or switch to the next page in the sample list inside the left “Picture list” panel. Select a sample then its HOC results will be shown on the other panels.</p></li>
<li><p>Change the tag of HOC result showing on the center “Original Image” panel. Only tags with prediction confidence greater than 0.5 have HOC results.</p></li>
<li><p>Inspect the HOC search process on the bottom “Layer-by-layer Masking Process” panel, select a step image then it will be enlarged and shown on the right “View Explanation” panel. (Notes: The occluded regions were darkened and converted to greyscale for display, but it is not the case in the actual HOC search process, only Gaussian blur is employed while brightness and saturation are not altered.)</p></li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="using_benchmarks.html" class="btn btn-neutral float-left" title="Using Benchmarks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>