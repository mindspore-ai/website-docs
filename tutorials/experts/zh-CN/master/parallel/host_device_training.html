

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Host&amp;Device异构 &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/translations.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="重计算" href="recompute.html" />
    <link rel="prev" title="优化器并行" href="optimizer_parallel.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">数据处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/augment.html">自动数据增强</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/cache.html">单节点数据缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/optimize.html">数据处理性能优化</a></li>
</ul>
<p class="caption"><span class="caption-text">图编译</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../network/control_flow.html">流程控制语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/op_overload.html">静态图网络编译性能优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/jit_class.html">调用自定义类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/constexpr.html">网络内构造常量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../network/dependency_control.html">依赖控制</a></li>
</ul>
<p class="caption"><span class="caption-text">模型训练优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/execution_opt.html">下沉模式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/gradient_accumulation.html">梯度累积</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/adaptive_summation.html">自适应梯度求和算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/dimention_reduce_training.html">降维训练算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/thor.html">二阶优化</a></li>
</ul>
<p class="caption"><span class="caption-text">自定义算子</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom.html">自定义算子（基于Custom表达）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/ms_kernel.html">MindSpore Hybrid 语法规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation/op_custom_adv.html">自定义算子进阶用法</a></li>
</ul>
<p class="caption"><span class="caption-text">自动向量化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vmap/vmap.html">自动向量化Vmap</a></li>
</ul>
<p class="caption"><span class="caption-text">模型推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../infer/inference.html">模型推理总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/ascend_310_air.html">Ascend 310 AI处理器上使用AIR模型进行推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/model_compression.html">模型压缩</a></li>
</ul>
<p class="caption"><span class="caption-text">调试调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug/function_debug.html">功能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/performance_optimization.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference external" href="https://mindspore.cn/mindinsight/docs/zh-CN/master/accuracy_problem_preliminary_location.html">精度调优↗</a></li>
</ul>
<p class="caption"><span class="caption-text">分布式并行</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_training_quickstart.html">快速入门分布式并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="communicate_ops.html">分布式集合通信原语</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_case.html">分布式案例</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">分布式推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load.html">保存和加载模型（HyBrid Parallel模式）</a></li>
<li class="toctree-l1"><a class="reference internal" href="fault_recover.html">分布式故障恢复</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="multi_dimensional.html">多维度混合并行</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="operator_parallel.html">算子级并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeline_parallel.html">流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizer_parallel.html">优化器并行</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Host&amp;Device异构</a></li>
<li class="toctree-l2"><a class="reference internal" href="recompute.html">重计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_graph_partition.html">分布式图切分</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="resilience_train_and_predict.html">分布式弹性训练与推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="other_features.html">其他特性</a></li>
</ul>
<p class="caption"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../env/env_var_list.html">环境变量</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="multi_dimensional.html">多维度混合并行</a> &raquo;</li>
        
      <li>Host&amp;Device异构</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/parallel/host_device_training.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hostdevice异构">
<h1>Host&amp;Device异构<a class="headerlink" href="#hostdevice异构" title="永久链接至标题">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/tutorials/experts/source_zh_cn/parallel/host_device_training.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png"></a></p>
<div class="section" id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题">¶</a></h2>
<p>在深度学习中，工作人员时常会遇到超大模型的训练问题，即模型参数所占内存超过了设备内存上限。为高效地训练超大模型，一种方案便是分布式并行训练，也就是将工作交由同构的多个加速器（如Ascend 910 AI处理器，GPU等）共同完成。但是这种方式在面对几百GB甚至几TB级别的模型时，所需的加速器过多。而当从业者实际难以获取大规模集群时，这种方式难以应用。另一种可行的方案是使用主机端（Host）和加速器（Device）的混合训练模式。此方案同时发挥了主机端内存大和加速器端计算快的优势，是一种解决超大模型训练较有效的方式。</p>
<p>在MindSpore中，用户可以将待训练的参数放在主机，同时将必要算子的执行位置配置为主机，其余算子的执行位置配置为加速器，从而方便地实现混合训练。此教程以推荐模型<a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/recommend/Wide_and_Deep">Wide&amp;Deep</a>为例，讲解MindSpore在主机和Ascend 910 AI处理器的混合训练。</p>
</div>
<div class="section" id="基本原理">
<h2>基本原理<a class="headerlink" href="#基本原理" title="永久链接至标题">¶</a></h2>
<p>流水线并行和算子级并行适用于模型的算子数量较大，同时参数较均匀的分布在各个算子中。如果模型中的算子数量较少，同时参数只集中在几个算子中呢？Wide&amp;Deep就是这样的例子，如下图所示。Wide&amp;Deep中的Embedding table作为需训练的参数可达几百GB甚至几TB，若放在加速器(device)上执行，那么所需的加速器数量巨大，训练费用昂贵。另一方面，若使用加速器计算，其获得的训练加速有限，同时会引发跨服务器的通信量，端到端的训练效率不会很高。</p>
<p><img alt="image" src="../_images/host_device_image_0_zh.png" /></p>
<p><em>图：Wide&amp;Deep模型的部分结构</em></p>
<p>仔细分析Wide&amp;Deep模型的特殊结构后可得：Embedding table虽然参数量巨大，但其参与的计算量很少，可以将Embedding table和其对应的算子EmbeddingLookup算子放置在Host端，利用CPU进行计算，其余算子放置在加速器端。这样做能够同时发挥Host端内存量大、加速器端计算快的特性，同时利用了同一台服务器的Host到加速器高带宽的特性。下图展示了Wide&amp;Deep异构切分的方式：</p>
<p><img alt="image" src="../_images/host_device_image_1_zh.png" /></p>
<p><em>图：Wide&amp;Deep异构方式</em></p>
</div>
<div class="section" id="操作实践">
<h2>操作实践<a class="headerlink" href="#操作实践" title="永久链接至标题">¶</a></h2>
<div class="section" id="样例代码说明">
<h3>样例代码说明<a class="headerlink" href="#样例代码说明" title="永久链接至标题">¶</a></h3>
<ol class="simple">
<li><p>准备模型代码。Wide&amp;Deep的代码可参见：<a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/recommend/Wide_and_Deep">https://gitee.com/mindspore/models/tree/master/official/recommend/Wide_and_Deep</a>，其中，<code class="docutils literal notranslate"><span class="pre">train_and_eval_auto_parallel.py</span></code>脚本定义了模型训练的主流程，<code class="docutils literal notranslate"><span class="pre">src/</span></code>目录中包含Wide&amp;Deep模型的定义、数据处理和配置信息等，<code class="docutils literal notranslate"><span class="pre">script/</span></code>目录中包含不同配置下的训练脚本。</p></li>
<li><p>准备数据集。请参考[1]中的论文所提供的链接下载数据集，并利用脚本<code class="docutils literal notranslate"><span class="pre">src/preprocess_data.py</span></code>将数据集转换为MindRecord格式。</p></li>
<li><p>配置处理器信息。在裸机环境（即本地有Ascend 910 AI 处理器）进行分布式训练时，需要配置加速器信息文件。此样例只使用一个加速器，故只需配置包含0号卡的<code class="docutils literal notranslate"><span class="pre">rank_table_1p_0.json</span></code>文件。MindSpore提供了生成该配置文件的自动化生成脚本及相关说明，可参考<a class="reference external" href="https://gitee.com/mindspore/models/tree/master/utils/hccl_tools">HCCL_TOOL</a>。</p></li>
</ol>
</div>
<div class="section" id="配置混合执行">
<h3>配置混合执行<a class="headerlink" href="#配置混合执行" title="永久链接至标题">¶</a></h3>
<ol>
<li><p>配置混合训练标识。在<code class="docutils literal notranslate"><span class="pre">default_config.yaml</span></code>文件中，设置<code class="docutils literal notranslate"><span class="pre">host_device_mix</span></code>默认值为<code class="docutils literal notranslate"><span class="pre">1</span></code>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">host_device_mix</span><span class="p">:</span> <span class="mi">1</span>
</pre></div>
</div>
</li>
<li><p>检查必要算子和优化器的执行位置。在<code class="docutils literal notranslate"><span class="pre">src/wide_and_deep.py</span></code>的<code class="docutils literal notranslate"><span class="pre">WideDeepModel</span></code>类中，检查<code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code>为主机端执行：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">deep_embeddinglookup</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">wide_embeddinglookup</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span>
</pre></div>
</div>
<p>在<code class="docutils literal notranslate"><span class="pre">src/wide_and_deep.py</span></code>文件的<code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">TrainStepWrap(nn.Cell)</span></code>中，检查两个优化器主机端执行的属性。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_w</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
<span class="bp">self</span><span class="o">.</span><span class="n">optimizer_d</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="训练模型">
<h3>训练模型<a class="headerlink" href="#训练模型" title="永久链接至标题">¶</a></h3>
<p>为了保存足够的日志信息，需在执行脚本前使用命令<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOG_v=1</span></code>将日志级别设置为INFO，且在MindSpore编译时添加-p on选项。如需了解MindSpore编译流程，可参考<a class="reference external" href="https://www.mindspore.cn/install/detail?path=install/master/mindspore_ascend_install_source.md&amp;highlight=%E7%BC%96%E8%AF%91mindspore">编译MindSpore</a>。</p>
<p>使用训练脚本<code class="docutils literal notranslate"><span class="pre">script/run_auto_parallel_train.sh</span></code>。执行命令：<code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">run_auto_parallel_train.sh</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">&lt;DATASET_PATH&gt;</span> <span class="pre">&lt;RANK_TABLE_FILE&gt;</span></code>。
其中第一个<code class="docutils literal notranslate"><span class="pre">1</span></code>表示用例使用的卡数，第二<code class="docutils literal notranslate"><span class="pre">1</span></code>表示训练的epoch数，<code class="docutils literal notranslate"><span class="pre">DATASET_PATH</span></code>是数据集所在路径，<code class="docutils literal notranslate"><span class="pre">RANK_TABLE_FILE</span></code>为上述<code class="docutils literal notranslate"><span class="pre">rank_table_1p_0.json</span></code>文件所在路径。</p>
<p>运行日志保存在<code class="docutils literal notranslate"><span class="pre">device_0</span></code>目录下，其中<code class="docutils literal notranslate"><span class="pre">loss.log</span></code>保存一个epoch内多个loss值，其值类似如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, wide_loss is 0.6873926, deep_loss is 0.8878349
epoch: 1 step: 2, wide_loss is 0.6442529, deep_loss is 0.8342661
epoch: 1 step: 3, wide_loss is 0.6227323, deep_loss is 0.80273706
epoch: 1 step: 4, wide_loss is 0.6107221, deep_loss is 0.7813441
epoch: 1 step: 5, wide_loss is 0.5937832, deep_loss is 0.75526017
epoch: 1 step: 6, wide_loss is 0.5875453, deep_loss is 0.74038756
epoch: 1 step: 7, wide_loss is 0.5798845, deep_loss is 0.7245408
epoch: 1 step: 8, wide_loss is 0.57553077, deep_loss is 0.7123517
epoch: 1 step: 9, wide_loss is 0.5733629, deep_loss is 0.70278376
epoch: 1 step: 10, wide_loss is 0.566089, deep_loss is 0.6884129
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">test_deep0.log</span></code>保存pytest进程输出的详细的运行时日志，搜索关键字<code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code>，可找到如下信息：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:34.928.275 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/network-VirtualDatasetCellTriple/_backbone-NetWithLossClass/network-WideDeepModel/EmbeddingLookup-op297 costs 3066 us.
[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:34.943.896 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/network-VirtualDatasetCellTriple/_backbone-NetWithLossClass/network-WideDeepModel/EmbeddingLookup-op298 costs 15521 us.
</pre></div>
</div>
<p>表示<code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code>在主机端的执行时间。
继续在<code class="docutils literal notranslate"><span class="pre">test_deep0.log</span></code>搜索关键字<code class="docutils literal notranslate"><span class="pre">FusedSparseFtrl</span></code>和<code class="docutils literal notranslate"><span class="pre">FusedSparseLazyAdam</span></code>，可找到如下信息：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:35.422.963 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/optimizer_w-FTRL/FusedSparseFtrl-op299 costs 54492 us.
[INFO] DEVICE(109904,python3.7):2020-06-27-12:42:35.565.953 [mindspore/ccsrc/device/cpu/cpu_kernel_runtime.cc:324] Run] cpu kernel: Default/optimizer_d-LazyAdam/FusedSparseLazyAdam-op300 costs 142865 us.
</pre></div>
</div>
<p>表示两个优化器在主机端的执行时间。</p>
</div>
</div>
<div class="section" id="参考文献">
<h2>参考文献<a class="headerlink" href="#参考文献" title="永久链接至标题">¶</a></h2>
<p>[1] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He. <a class="reference external" href="https://doi.org/10.24963/ijcai.2017/239">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction.</a> IJCAI 2017.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="recompute.html" class="btn btn-neutral float-right" title="重计算" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="optimizer_parallel.html" class="btn btn-neutral float-left" title="优化器并行" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>